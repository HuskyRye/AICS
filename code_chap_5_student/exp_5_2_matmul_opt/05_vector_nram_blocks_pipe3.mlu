// file: 05_vector_nram_blocks_pipe3.mlu

#include <bang.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>

#define DEBUG 0

#define STAGES 2
#define LOOPS 32
#define BLOCKS 128
#define M_PER_LOOP 128
#define M_PER_BLOCK (M_PER_LOOP * LOOPS)

#define M (M_PER_BLOCK * BLOCKS)
#define N 256
#define K 128

float relativeError(float a, float b) {
  float abs_diff = fabs(a - b);
  float max_value = fmax(fabs(a), fabs(b));
  float result = abs_diff / max_value;
  return result;
}

float generateRandomFloat(float min, float max) {
#if DEBUG
  return 1.1;
#else
  float scale = rand() / (float)RAND_MAX;
  return min + scale * (max - min);
#endif
}

__attribute__((noinline)) void multiplyMatricesCPU(float *left, float *right,
                                                   float *result, int m, int n,
                                                   int k) {
  for (int i = 0; i < m; i++) {
    for (int j = 0; j < k; j++) {
      result[i * k + j] = 0.0f;
      for (int x = 0; x < n; x++) {
        // TODO: 请补充标量矩阵乘计算部分代码
        ________________________________________________________
      }
    }
  }
}

__mlu_entry__ void multiplyMatricesMLU(float *left, float *right, float *result,
                                       int m, int n, int k) {
  // TODO: 请补充左矩阵NRAM数组的声明
  __nram__ float left_nram[_______________________];
  __wram__ float right_wram[N * K];
  // TODO: 请补充结果矩阵NRAM数组的声明
  __nram__ float result_nram[_______________________];
  int m_per_block = m / taskDim;
  int m_per_loop = m_per_block / LOOPS;
  // TODO: 请补充将右矩阵从GDRAM异步拷贝至WRAM的代码
  _____________________________________________________________________
  for (int loop = 0; loop < (LOOPS + STAGES); loop++) {
    if (loop < LOOPS) {
      // TODO: 请补充将左矩阵从GDRAM异步拷贝至NRAM的代码
      __memcpy_async(____________________________________________,
                     _______________________________________________________,
                     m_per_loop * n * sizeof(float), GDRAM2NRAM);
    }
    if (loop >= 1 && loop <= LOOPS) {
      __bang_matmul(result_nram + m_per_loop * k * ((loop - 1) % STAGES),
                    left_nram + m_per_loop * n * ((loop - 1) % STAGES),
                    right_wram, m_per_loop, n, k);
    }
    if (loop >= STAGES) {
      // TODO: 请补充将结果矩阵从NRAM写回GDRAM的代码
      __memcpy_async(
          ____________________________________________________________________,
          _________________________________________________________,
          m_per_loop * k * sizeof(float), NRAM2GDRAM);
    }
    __sync();
  }
}

int main() {
  int m = M, n = N, k = K;
  printf("\nM = %d, N = %d, K = %d\n", m, n, k);

  float *left = (float *)malloc(m * n * sizeof(float));
  float *right = (float *)malloc(n * k * sizeof(float));
  float *result = (float *)malloc(m * k * sizeof(float));

  float *left_mlu = NULL, *right_mlu = NULL, *result_mlu = NULL;
  CNRT_CHECK(cnrtMalloc((void **)&left_mlu, m * n * sizeof(float)));
  CNRT_CHECK(cnrtMalloc((void **)&right_mlu, n * k * sizeof(float)));
  CNRT_CHECK(cnrtMalloc((void **)&result_mlu, m * k * sizeof(float)));

  for (int i = 0; i < m; i++) {
    for (int j = 0; j < n; j++) {
      left[i * n + j] = generateRandomFloat(1.0f, 1.1f);
    }
  }

  for (int i = 0; i < n; i++) {
    for (int j = 0; j < k; j++) {
      right[i * k + j] = generateRandomFloat(1.0f, 1.1f);
    }
  }

  CNRT_CHECK(
      cnrtMemcpy(left_mlu, left, m * n * sizeof(float), cnrtMemcpyHostToDev));
  CNRT_CHECK(
      cnrtMemcpy(right_mlu, right, n * k * sizeof(float), cnrtMemcpyHostToDev));

  struct timeval st_cpu, et_cpu;
  gettimeofday(&st_cpu, NULL);
  multiplyMatricesCPU(left, right, result, m, n, k);
  gettimeofday(&et_cpu, NULL);

  float cpu_time_used = (et_cpu.tv_sec - st_cpu.tv_sec) * 1e3 +
                        (et_cpu.tv_usec - st_cpu.tv_usec) / 1e3;
  printf("\nCPU Time taken: %.3f ms\n", cpu_time_used);
#if DEBUG
  printf("\nCPU Result Matrix:\n");
  for (int i = 0; i < m; i++) {
    for (int j = 0; j < k; j++) {
      printf("%f\t", result[i * k + j]);
    }
    printf("\n");
  }
#endif

  cnrtNotifier_t st_mlu, et_mlu;
  CNRT_CHECK(cnrtNotifierCreate(&st_mlu));
  CNRT_CHECK(cnrtNotifierCreate(&et_mlu));
  cnrtQueue_t queue;
  CNRT_CHECK(cnrtQueueCreate(&queue));
  // TODO: 请补充核函数并行规模
  cnrtDim3_t dim = {______, 1, 1};
  cnrtFunctionType_t func_type = CNRT_FUNC_TYPE_BLOCK;
  // TODO: 请补充核函数开始计时代码
  _____________________________________________
  multiplyMatricesMLU<<<dim, func_type, queue>>>(left_mlu, right_mlu,
                                                 result_mlu, m, n, k);
  // TODO: 请补充核函数结束计时代码
  _____________________________________________
  CNRT_CHECK(cnrtQueueSync(queue));

  float mlu_time_used = 0.0f;
  // TODO: 请补充核函数耗时统计代码
  ____________________________________________________________________
  printf("\nMLU Time taken: %.3f ms\n", mlu_time_used);

  float *result_actual = (float *)malloc(m * k * sizeof(float));
  CNRT_CHECK(cnrtMemcpy(result_actual, result_mlu, m * k * sizeof(float),
                        cnrtMemcpyDevToHost));
#if DEBUG
  printf("\nMLU Result Matrix:\n");
  for (int i = 0; i < m; i++) {
    for (int j = 0; j < k; j++) {
      printf("%f\t", result_actual[i * k + j]);
    }
    printf("\n");
  }
#endif

  bool is_passed = true;
  for (int i = 0; i < m; i++) {
    for (int j = 0; j < k; j++) {
      float diff_rel =
          relativeError(result[i * k + j], result_actual[i * k + j]);
      if (diff_rel > 0.1) {
        printf("diff_rel = %.3f\n", diff_rel);
        printf("[%d, %d]: cpu = %f, mlu = %f\n", i, j, result[i * k + j],
               result_actual[i * k + j]);
        is_passed = false;
      }
    }
  }
  printf(is_passed ? "\n05PASSED\n" : "\n05FAILED\n");

  free(left);
  CNRT_CHECK(cnrtFree(left_mlu));
  free(right);
  CNRT_CHECK(cnrtFree(right_mlu));
  free(result);
  free(result_actual);
  CNRT_CHECK(cnrtFree(result_mlu));
  CNRT_CHECK(cnrtNotifierDestroy(st_mlu));
  CNRT_CHECK(cnrtNotifierDestroy(et_mlu));
  CNRT_CHECK(cnrtQueueDestroy(queue));

  return 0;
}
