/*************************************************************************
 * Copyright (C) [2019-2022] by Cambricon, Inc.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
 * OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 *************************************************************************/
#ifndef CNNL_H_
#define CNNL_H_

/******************************************************************************
 * CNNL: Cambricon Network Library
 ******************************************************************************/

#define CNNL_MAJOR 1
#define CNNL_MINOR 12
#define CNNL_PATCHLEVEL 1

/*********************************************************************************
 * deprecate CNNL_VERSION is not recommended to use, it is recommended to directly
 * use CNNL_MAJOR, CNNL_MINOR and CNNL_PATCHLEVEL to get cnnl version.
********************************************************************************/
#define CNNL_VERSION (CNNL_MAJOR * 1000 + CNNL_MINOR * 100 + CNNL_PATCHLEVEL)

#define CNNL_DIM_MAX 8

#include <stdint.h>
#include "cn_api.h"
#include "cnrt.h"

#ifndef CNNL_WIN_API
#ifdef _WIN32
#define CNNL_WIN_API __stdcall
#else
#define CNNL_WIN_API
#endif
#endif

#if defined(__cplusplus)
extern "C" {
#endif

/******************************************************************************
 * Cambricon CNNL Return Status
 ******************************************************************************/
/*! @brief Enumeration variables describing function return status.
 */
typedef enum {
  CNNL_STATUS_SUCCESS         = 0, /*!< The operation was successfully completed. */
  CNNL_STATUS_NOT_INITIALIZED = 1,
  /*!< Cambricon CNNL library was not initialized properly, which is usually caused by the
       failure of calling ::cnnlCreate, ::cnnlCreateTensorDescriptor or ::cnnlSetTensorDescriptor.
       Such error is usually due to incompatible MLU device or invalid driver environment.
       Notice that ::cnnlCreate should be called prior to any other cnnl functions.*/
  CNNL_STATUS_ALLOC_FAILED = 2,
  /*!< This error occurs when the resource allocation failed, usually caused by the failure
       of cnMallocHost, probably because of the exceeded memory usage. Please make sure that
       the memory allocated previously is deallocated as much as possible.*/
  CNNL_STATUS_BAD_PARAM = 3,
  /*!< Invalid value or parameters passed to the function, including data type, layout, and
       dimensions, etc.*/
  CNNL_STATUS_INTERNAL_ERROR = 4,
  /*!< Error occurred inside of the function, which may indicate an internal error or bug in
       the library. This error is usually due to the failure of cnrtMemcpyAsync.
       Please check whether the memory passed to the function was deallocated before the completion
       of the routine.*/
  CNNL_STATUS_ARCH_MISMATCH = 5,
  /*!< Invalid MLU device which was not supported by current function.*/
  CNNL_STATUS_EXECUTION_FAILED = 6,
  /*!< Error occurred when the function failed to execute on MLU device due to multiple reasons.
       You can check whether the hardware environment, driver version and other prerequisite
       libraries are correctly installed. For more information about prerequisite libraries,
       see "Cambricon CNNL User Guide".*/
  CNNL_STATUS_NOT_SUPPORTED = 7,
  /*!< Error when the requested functionality was not supported in
       this version but would be supported in the future. */
  CNNL_STATUS_NUMERICAL_OVERFLOW = 8,
  /*!< Numerical overflow occurred when executing the function,
       which is usually due to large scale or inappropriate range of value of input tensor.*/
} cnnlStatus_t;

/******************************************************************************
 * Cambricon CNNL Tensor Layout
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the data layouts in CNNL.
 *
 * The data can be defined in three, four, or five dimensions.
 *
 * Take images for example, the format of the data layout can be NCHW:
 * - N: The number of images.
 * - C: The number of image channels.
 * - H: The height of images.
 * - W: The width of images.
 *
 * Take sequence for example, the format of the data layout can be TNC:
 * - T: The timing steps of sequence.
 * - N: The batch size of sequence.
 * - C: The alphabet size of sequence.
 */
typedef enum {
  CNNL_LAYOUT_NCHW = 0,
/*!< The data layout is in the following order: batch size, channel, height, and width. */
  CNNL_LAYOUT_NHWC = 1,
/*!< The data layout is in the following order: batch size, height, width, and channel. */
  CNNL_LAYOUT_HWCN = 2,
/*!< The data layout is in the following order: height, width, channel and batch size. */
  CNNL_LAYOUT_NDHWC = 3,
/*!< The data layout is in the following order: batch size, depth, height, width, and channel.*/
  CNNL_LAYOUT_ARRAY = 4,
/*!< The data is multi-dimensional tensor. */
  CNNL_LAYOUT_NCDHW = 5,
/*!< The data layout is in the following order: batch size, channel, depth, height, and width.*/
  CNNL_LAYOUT_TNC = 6,
/*!< The data layout is in the following order: timing steps, batch size, alphabet size.

 * @deprecated
 *   CNNL_LAYOUT_TNC is deprecated and will be removed in the future release. It is recommended
 *   to use CNNL_SEQDATA_TNC defined in ::cnnlSeqDataLayout_t instead.*/

  CNNL_LAYOUT_NTC = 7,
/*!< The data layout is in the following order: batch size, timing steps, alphabet size.
 *
 * @deprecated
 *   CNNL_LAYOUT_NTC is deprecated and will be removed in the future release. It is recommended
 *   to use CNNL_SEQDATA_NTC defined in ::cnnlSeqDataLayout_t instead.*/
  CNNL_LAYOUT_NC = 8,
/*!< The data layout is in the following order: batch size, channel.*/
  CNNL_LAYOUT_NLC = 9,
/*!< The data layout is in the following order: batch size, length, channel.*/
} cnnlTensorLayout_t;

/******************************************************************************
 * Cambricon CNNL sequence data Layout
 ******************************************************************************/
//! @brief
/*! Enumeration variables describe the sequence data (SeqData) layouts. */
/*! N is batch, B is beam, T is sequence length, C is embedding size. */
typedef enum {
  CNNL_SEQDATA_TNC = 0,  /*!< Sequence data layout order: TNC. */
  CNNL_SEQDATA_TNC_PACKED = 1,  /*!< Sequence data layout order: TNC_PACKED. */
  CNNL_SEQDATA_NTC = 2,  /*!< Sequence data layout order: NTC. */
  CNNL_SEQDATA_NC = 3,  /*!< Sequence data layout order:  NC. */
  CNNL_SEQDATA_TNBC = 4,  /*!< Sequence data layout order: TNBC. */
  CNNL_SEQDATA_TBNC = 5,  /*!< Sequence data layout order: TBNC. */
  CNNL_SEQDATA_NBTC = 6,  /*!< Sequence data layout order: NBTC. */
  CNNL_SEQDATA_NTBC = 7,  /*!< Sequence data layout order: NTBC. */
  CNNL_SEQDATA_BNTC = 8,  /*!< Sequence data layout order: BNTC. */
  CNNL_SEQDATA_BTNC = 9,  /*!< Sequence data layout order: BTNC. */
  CNNL_SEQDATA_TN = 10,  /*!< Sequence data layout order: TN. */
  CNNL_SEQDATA_NT = 11,  /*!< Sequence data layout order: NT. */
} cnnlSeqDataLayout_t;

/******************************************************************************
 * Cambricon CNNL Data Type
 ******************************************************************************/
/*! @brief Enumeration variables describing the data types in Cambricon CNNL. */
typedef enum {
  CNNL_DTYPE_INVALID       =  0,   /*!< The data is an invalid data type. */
  CNNL_DTYPE_HALF          =  1,   /*!< The data is a 16-bit floating-point data type. */
  CNNL_DTYPE_FLOAT         =  2,   /*!< The data is a 32-bit floating-point data type. */
  CNNL_DTYPE_DOUBLE        =  14,  /*!< The data is a 64-bit floating-point data type. */
  CNNL_DTYPE_INT8          =  3,   /*!< The data is a 8-bit signed integer data type. */
  CNNL_DTYPE_INT16         =  4,   /*!< The data is a 16-bit signed integer data type. */
  CNNL_DTYPE_INT31         =  5,   /*!< The data is a 31-bit signed integer data type. */
  CNNL_DTYPE_INT32         =  6,   /*!< The data is a 32-bit signed integer data type. */
  CNNL_DTYPE_INT64         =  9,   /*!< The data is a 64-bit signed integer data type. */
  CNNL_DTYPE_UINT8         =  7,   /*!< The data is a 8-bit unsigned integer data type. */
  CNNL_DTYPE_UINT16        = 13,   /*!< The data is a 16-bit unsigned integer data type. */
  CNNL_DTYPE_UINT32        = 11,   /*!< The data is a 32-bit unsigned integer data type. */
  CNNL_DTYPE_UINT64        = 12,   /*!< The data is a 64-bit unsigned integer data type. */
  CNNL_DTYPE_BOOL          =  8,   /*!< The data is a boolean data type. */
  CNNL_DTYPE_COMPLEX_HALF  =  15,  /*!< The data is a 32-bit complex number of two fp16. */
  CNNL_DTYPE_COMPLEX_FLOAT =  16,  /*!< The data is a 64-bit complex number of two fp32. */
} cnnlDataType_t;

/*!
 * @brief Enumeration variables describing whether to propagate NaN numbers.
 */
typedef enum {
  CNNL_NOT_PROPAGATE_NAN = 0,
  /*!< The NaN numbers are not propagated .*/
  CNNL_PROPAGATE_NAN = 1,
  /*!< The NaN numbers are propagated.*/
} cnnlNanPropagation_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Customized Operation
 ******************************************************************************/

/*!
 * @brief Enumeration variables describing the operations that can be used to
 *        implement the ::cnnlOpTensor function.
 *
 */
typedef enum {
  CNNL_OP_TENSOR_ADD = 0, /*!< The addition operation is implemented. */
  CNNL_OP_TENSOR_SUB = 1, /*!< The subtraction operation is implemented. */
  CNNL_OP_TENSOR_MUL = 2, /*!< The multiplication operation is implemented. */
} cnnlOpTensorDesc_t;

/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the softmax function.
 *
 */
typedef enum {
  CNNL_SOFTMAX_FAST = 0,
  /*!< The Softmax function is implemented in higher speed, but in lower precision. */
  /*!< Note that the range of input should be in [-0.5, 0.5] using this algorithm. */
  CNNL_SOFTMAX_ACCURATE = 1,
  /*!< The Softmax function is implemented in lower speed, but in higher precision. */
  CNNL_SOFTMAX_LOG = 2,
  /*!< The LogSoftmax function is implemented. */
} cnnlSoftmaxAlgorithm_t;

/*!
 * @brief Enumeration variables describing the dimension reduction methods that are used in the
 * implementation of the softmax function.
 *
 */
typedef enum {
  CNNL_SOFTMAX_MODE_HIGH_DIMENSION = 0,   /*!< The reduction is implemented in high dimension. */
  CNNL_SOFTMAX_MODE_MEDIUM_DIMENSION = 1, /*!< The reduction is implemented in medium dimension. */
  CNNL_SOFTMAX_MODE_LOW_DIMENSION = 2,    /*!< The reduction is implemented in low dimension. */
} cnnlSoftmaxMode_t;

/*!
 * @brief Enumeration variables describing the mode of implementing the reduction operations.
 */
typedef enum {
  CNNL_REDUCEMODE_SUM = 0,
  /*!< sum mode, which computes the sum value when implementing reduction operations. */
  CNNL_REDUCEMODE_MEAN = 1,
  /*!< mean mode, which computes the mean value when implementing reduction operations. */
  CNNL_REDUCEMODE_MAX = 2,
  /*!< max mode, which computes the maximum value when implementing reduction operations. */
} cnnlReduceMode_t;

/*!
 * @brief Enumeration variables describing the operation methods that are used to specify the mode
 *        of normalization in ::cnnlBatchNormForwardInferenceV2 and
 *        ::cnnlBatchNormForwardTrainingV2 functions.
 */
typedef enum {
  CNNL_BATCHNORM_PER_ACTIVATION = 0,
  /*!< A type of mode, which performs normalization per-activation. */
  CNNL_BATCHNORM_SPATIAL = 1,
  /*!< A type of mode, which performs normalization over N+spatial dimensions. */
} cnnlBatchNormMode_t;

/*!
 * @brief Enumeration variables describing the activation methods that are used in the
 *        implementation of ::cnnlBatchNormForwardInferenceV2 and
 *        ::cnnlBatchNormForwardTrainingV2 functions.
 */
typedef enum {
  CNNL_BATCHNORM_OPS_BN = 0,
  /*!< A type of activation method, which only performs batch normalization, per-activation.*/
  CNNL_BATCHNORM_OPS_BN_ACTIVATION = 1,
  /*!< A type of activation method, which performs the batch normalization firstly,
       and then the activation.*/
  CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION = 2,
  /*!< A type of activation method, which performs the batch normalization,
       then element-wise addition, followed by the activation operation.*/
} cnnlBatchNormOps_t;

/*!
 * @brief Enumeration variables describing the logic operations in
 *        the ::cnnlLogicOp function.
 */
typedef enum {
  CNNL_LOGIC_OP_EQ = 0,
  /*!< The element-wise Equal To comparison is performed.*/
  CNNL_LOGIC_OP_NE = 1,
  /*!< The element-wise Not Equal To comparison is performed.*/
  CNNL_LOGIC_OP_GT = 2,
  /*!< The element-wise Greater Than comparison is performed.*/
  CNNL_LOGIC_OP_GE = 3,
  /*!< The element-wise Greater Than or Equal To comparison is performed.*/
  CNNL_LOGIC_OP_LT = 4,
  /*!< The element-wise Less Than comparison is performed.*/
  CNNL_LOGIC_OP_LE = 5,
  /*!< The element-wise Less Than or Equal To comparison is performed.*/
  CNNL_LOGIC_OP_AND = 6,
  /*!< The element-wise Logical AND operation is performed.*/
  CNNL_LOGIC_OP_OR = 7,
  /*!< The element-wise Logical OR operation is performed.*/
  CNNL_LOGIC_OP_XOR = 8,
  /*!< The element-wise Logical XOR operation is performed.*/
  CNNL_LOGIC_OP_NOT = 9,
  /*!< The element-wise Logical NOT operation is performed.*/
} cnnlLogicOp_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of scatter_nd operation.
 */
typedef enum {
  CNNL_SCATTERND_ADD = 0,
  /*!< A type of mode, which implements the addition operation.*/
  CNNL_SCATTERND_SUB = 1,
  /*!< A type of mode, which implements the subtraction operation.
   * This mode is not supported currently.*/
  CNNL_SCATTERND_MUL = 2,
  /*!< A type of mode, which implements the multiplication operation.
   * This mode is not supported currently.*/
  CNNL_SCATTERND_UPDATE = 3,
  /*!< A type of mode, which implements the replacement operation.*/
} cnnlScatterNdMode_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the
 *        implementation of ::cnnlScatterRef function.
 */
typedef enum {
  CNNL_SCATTERREF_ADD = 0,
  /*!< The addition operation is implemented.*/
  CNNL_SCATTERREF_SUB = 1,
  /*!< The subtraction operation is implemented.*/
  CNNL_SCATTERREF_MUL = 2,
  /*!< The multiplication operation is implemented.*/
  CNNL_SCATTERREF_DIV = 3,
  /*!< The division operation is implemented.*/
  CNNL_SCATTERREF_MIN = 4,
  /*!< The minimum operation is implemented.*/
  CNNL_SCATTERREF_MAX = 5,
  /*!< The maximum operation is implemented.*/
  CNNL_SCATTERREF_UPDATE = 6,
  /*!< The replace operation is implemented.*/
} cnnlScatterRefMode_t;

/*!
 * @brief Enumeration variables describe the operations that are used in the
 *        implementation of the Bit Compute function.
 */
typedef enum {
  CNNL_CYCLE_BAND_OP = 0, /*!< The cycle bit and operation is implemented.*/
  CNNL_CYCLE_BOR_OP = 1,  /*!< The cycle bit or operation is implemented.*/
  CNNL_CYCLE_BXOR_OP = 2, /*!< The cycle bit xor operation is implemented.*/
  CNNL_BNOT_OP = 3, /*!< The bitwise not operation is implemented.*/
  CNNL_BLEFT_SHIFT_OP = 4, /*!< The bitwise left-shift operation is implemented.*/
  CNNL_BRIGHT_SHIFT_OP = 5, /*!< The bitwise right-shift operation is implemented.*/
} cnnlBitComputeOp_t;

/*!
 * @brief Enumeration variables describing the operations that are used in the
 *        ::cnnlCycleOp function.
 */
typedef enum {
  CNNL_CYCLE_ADD = 0,            /*!< The cycle "add" function is implemented.*/
  CNNL_CYCLE_SUB = 1,            /*!< The cycle "subtraction" function is implemented.*/
  CNNL_CYCLE_MUL = 2,            /*!< The cycle "multiplication" function is implemented.*/
  CNNL_CYCLE_MIN_EQUAL = 3,      /*!< The cycle "min equal" function is implemented.*/
  CNNL_CYCLE_MAX_EQUAL = 4,      /*!< The cycle "max equal" function is implemented.*/
  CNNL_CYCLE_LESS = 5,           /*!< The cycle "less" function is implemented.*/
  CNNL_CYCLE_LESS_EQUAL = 6,     /*!< The cycle "less or equal" function is implemented.*/
  CNNL_CYCLE_EQUAL = 7,          /*!< The cycle "equal" function is implemented.*/
  CNNL_CYCLE_NEQUAL = 8,         /*!< The cycle "not equal" function is implemented.*/
  CNNL_CYCLE_GREATER = 9,        /*!< The cycle "greater" function is implemented.*/
  CNNL_CYCLE_GREATER_EQUAL = 10, /*!< The cycle "greater or equal" function is implemented.*/
  CNNL_CYCLE_OR = 11,            /*!< The cycle "or" function is implemented.*/
  CNNL_CYCLE_AND = 12,           /*!< The cycle "and" function is implemented.*/
  CNNL_CYCLE_XOR = 13,           /*!< The cycle "xor" function is implemented.*/
} cnnlCycleOp_t;

/*!
 * @brief
 *
 * Enumeration variables describing the precision levels that are used in the
 * implementation of the activation function.
 *
 */
typedef enum {
  CNNL_ACTIVATION_FAST = 0,
  /*!< Implementation with the fastest algorithm and lower precision.*/
  CNNL_ACTIVATION_HIGH_PRECISION = 1,
  /*!< Implementation with the high-precision algorithm regardless the performance.*/
} cnnlActivationPreference_t;

/*!
 * @brief Enumeration variables describing the activation modes that are used in the
 *        implementation of the Activation operation.
 */
typedef enum {
  CNNL_ACTIVATION_SIGMOID = 0,      /*!< The Sigmoid function is implemented.*/
  CNNL_ACTIVATION_RELU = 1,         /*!< The ReLU function is implemented.*/
  CNNL_ACTIVATION_RELU6 = 2,        /*!< The ReLU6 function is implemented.*/
  CNNL_ACTIVATION_TANH = 3,         /*!< The Tanh function is implemented.*/
  CNNL_ACTIVATION_CLIPPED_RELU = 4, /*!< The Clipped Rectified Linear function is implemented.*/
  CNNL_ACTIVATION_ELU = 5,          /*!< The Exponential Linear function is implemented.*/
  CNNL_ACTIVATION_IDENTITY = 6,
  /*!< The Identity function is implemented, intended for bypassing the activation.*/
  CNNL_ACTIVATION_SELU = 7, /*!< The SELU function is implemented.*/
  CNNL_ACTIVATION_GELU = 8, /*!< The GELU function is implemented.*/
  CNNL_ACTIVATION_LEAKYRELU = 9, /*!< The LeakyReLU function is implemented.*/
  CNNL_ACTIVATION_TF_LEAKYRELU = 10, /*!< The TF_LeakyReLU function is implemented.*/
  CNNL_ACTIVATION_CAFFE_RELU6  = 11, /*!< The RELU6_CAFFE function is implemented.*/
  CNNL_ACTIVATION_GLU          = 12, /*!< The GLU function is implemented.*/
  CNNL_ACTIVATION_SWISH        = 13, /*!< The SWISH function is currently not implemented.*/
  CNNL_ACTIVATION_SILU         = 14, /*!< The SILU function is currently implemented.*/
  CNNL_ACTIVATION_HARDSIGMOID  = 15, /*!< The HARDSIGMOID function is currently implemented.*/
  CNNL_ACTIVATION_HARDSWISH    = 16, /*!< The HARDSWISH function is currently implemented.*/
  CNNL_ACTIVATION_ELU_V2       = 17, /*!< The new ELU function is currently implemented.*/
} cnnlActivationMode_t;

/*!
 * @brief Enumeration variables describing the options that can help choose
 *        the best suited algorithm used for implementation of the activation
 *        and accumulation operations.
 **/
typedef enum {
  CNNL_COMPUTATION_FAST = 0,
  /*!< Implementation with the fastest algorithm and lower precision.*/
  CNNL_COMPUTATION_HIGH_PRECISION = 1,
  /*!< Implementation with the high-precision algorithm regardless the performance.*/
  CNNL_COMPUTATION_ULTRAHIGH_PRECISION = 2,
  /*!< Implementation with the ultrahigh-precision algorithm regardless the performance.*/
} cnnlComputationPreference_t;

/*!
 * @brief
 *
 * Enumeration variables describe the modes that are used in the
 * implementation of the Reduce function.
 *
 */
typedef enum {
  CNNL_REDUCE_ADD = 0,   /*!< The reduce addition operation is implemented.*/
  CNNL_REDUCE_AVG = 1,   /*!< The reduce average operation is implemented.*/
  CNNL_REDUCE_MUL = 2,   /*!< The reduce multiplication operation is implemented.*/
  CNNL_REDUCE_MAX = 3,   /*!< The reduce maximum operation is implemented.*/
  CNNL_REDUCE_MIN = 4,   /*!< The reduce minimum operation is implemented.*/
  CNNL_REDUCE_AND = 5,   /*!< The reduce and operation is implemented.*/
  CNNL_REDUCE_OR = 6,    /*!< The reduce or operation is implemented.*/
  CNNL_REDUCE_NORM1 = 7, /*!< The sum of absolute values operation is implemented.*/
  CNNL_REDUCE_NORM2 = 8, /*!< The square root of sum of squares operation is implemented.*/
  CNNL_REDUCE_MAX_LAST_INDEX = 9,
  /*!< The operation of returning the index of the last maximum value is implemented.*/
  CNNL_REDUCE_MIN_LAST_INDEX = 10,
  /*!< The operation of returning the index of the last minimum value is implemented.*/
  CNNL_REDUCE_NORMP = 11, /*!< The 1/p power of sum of p power operation is implemented.*/
  CNNL_REDUCE_ASUM = 12,
  /*!< The sum of absolute values operation adapted to Caffe framework is implemented.*/
  CNNL_REDUCE_SUMSQ = 13,
  /*!< The sum of the squared values operation adapted to Caffe framework is implemented.*/
} cnnlReduceOp_t;

/*!
 * @brief Enumeration variables describing the masked modes that can be used to implement
 *        the Masked operation.
 */
typedef enum {
  CNNL_MASKED_FILL = 0,
  /*!< Fills the element of the input tensor with the specified value where the mask value
   * is 1.*/
  CNNL_MASKED_SCATTER = 1,
  /*!< Fills the elements of the input tensor with the specified values of an array where
   * the mask value is 1.*/
  CNNL_MASKED_SELECT = 2,
  /*!< Selects the elements from the input tensor where the corresponding mask values is not 0.*/
} cnnlMaskedOp_t;

/*!
 * @brief Enumeration variables describing the unique modes that can be used to implement
 *        the Unique operation.
 */
typedef enum {
  CNNL_UNSORT_FORWARD = 0,
  /*!< Returns the data in the same order as the input data after eliminating the
   * duplicated values.*/
  CNNL_SORT_ASCEND = 1,
  /*!< Returns the data sorted in ascending order by input value after eliminating
   * the duplicated values.*/
  CNNL_UNSORT_REVERSE = 2,
  /*!< Returns the data in the reversed order as the input data after eliminating
   * the duplicated values.*/
} cnnlUniqueSort_t;

/*!
 * @brief
 *
 * Enumeration variables describing the mode of scatter indices that are used in the
 * implementation of the Scatter function.
 *
 */
typedef enum {
  CNNL_SCATTER = 0,
  /*!< The replace operation is implemented.*/
  CNNL_SCATTER_ADD = 1,
  /*!< The add operation is implemented.*/
} cnnlScatterMode_t;

/*!
 * @brief Enumeration variables describe the mode of reduce indices that are used in the
 * implementation of the Reduce function.
 *
 */
typedef enum {
  CNNL_REDUCE_NO_INDICES = 0,        /*!< The indices is not calculated.*/
  CNNL_REDUCE_FLATTENED_INDICES = 1, /*!< The indices is calculated.*/
  CNNL_REDUCE_ONLY_INDICES = 2,      /*!< Only the indices is calculated.*/
} cnnlReduceIndices_t;

/*!
 * @brief Enumeration variables describing the data type of reduce indices.
 */
typedef enum {
  CNNL_32BIT_INDICES = 0, /*!< The unsigned int indices is computed.*/
  CNNL_16BIT_INDICES = 1, /*!< The unsigned short indices is computed.*/
} cnnlIndicesType_t;

/*!
 * @brief Enumeration variables describing the options that can help
 * choose the best suited convolution algorithm used for implementation
 * of the convolution forward operation.
 *
 * This enum is used in the ::cnnlGetConvolutionForwardAlgorithm function.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_FWD_FASTEST = 0,
  /*!< Implementation with the fastest convolution algorithm regardless of the workspace
   *   memory to be used. */
} cnnlConvolutionFwdPreference_t;

/*!
 * @brief Enumeration variables describing the convolution algorithms that
 *        can be used to implement the Convolution Forward operation.
 */
typedef enum {
  CNNL_CONVOLUTION_FWD_ALGO_DIRECT = 0,
  /*!< The direct convolution is implemented without performing matrix multiplication.*/
  CNNL_CONVOLUTION_FWD_ALGO_GEMM = 1,
  /*!< The convolution is implemented using the matrix multiplication algorithm.*/
} cnnlConvolutionForwardAlgo_t;

/*!
 * @brief Enumeration variables describing the quantization modes used for the
 *        convolution quantization.
 */
typedef enum {
    CNNL_OFFLINE_SYMMETRIC_QUANTIZE = 0,
    /*!< The offline symmetric quantization.*/
    CNNL_OFFLINE_ASYMMETRIC_QUANTIZE = 1,
    /*!< The offline asymmetric quantization.*/
    CNNL_PARTIAL_ONLINE_QUANTIZE = 2,
    /*!< The partial online quantization.*/
    CNNL_GLOBAL_ONLINE_QUANTIZE = 3,
    /*!< The global online quantization.*/
    CNNL_NO_QUANTIZE = 255,
    /*!< no quantization for input onchip type is float or half.*/
} cnnlConvolutionCastMode_t;

/*!
 * @brief Enumeration variables describing the cnnlFusedOps descriptor pointer.
 */
typedef enum {
  CNNL_PTR_NULL = 0,
  /*!< Indicates that the pointer to the tensor in the cparam_pack will be NULL.*/
  CNNL_PTR_VALID = 1,
  /*!< Indicates that the pointer to the tensor in the cparam_pack will be valid.*/
} cnnlFusedOpsPointerPlaceHolder_t;

/*!
 * @brief Enumeration variables describing the reorder type used for the
 *        reorder filter data or bias data on host.
 *
 * This enumeration is used in the ::cnnlSetConvolutionDescriptorReorderType
 * and ::cnnlSetDeconvolutionDescriptorReorderType functions.
 */
typedef enum {
  CNNL_REORDER = 0,
  /*!< The reorder data on host will be used for filter data or bias data.*/
  CNNL_NO_REORDER = 1,
  /*!< The reorder data on host will not be used for filter data or bias data.*/
} cnnlReorderType_t;

/*!
 *
 * @brief Enumeration variables describing the MLU device information used for the
 *        host reorder.
 *
 */
typedef enum {
  CNNL_UNKNOWN_DEV = 0, /*!< Unknown MLU device.*/
  CNNL_MLU_220 = 1, /*!< MLU 220.*/
  CNNL_MLU_270 = 2, /*!< MLU 270.*/
  CNNL_MLU_290 = 3, /*!< MLU 290.*/
  CNNL_MLU_370_S = 4, /*!< MLU 370 S4.*/
  CNNL_MLU_370_X = 5, /*!< MLU 370 X4.*/
  CNNL_CE_3226 = 6, /*!< CE3226.*/
  CNNL_MLU_365_D2 = 7, /*!< MLU 365 D2.*/
  CNNL_MLU_370_X8 = 8, /*!< MLU 370 X8.*/
  CNNL_MLU_370_M8 = 9, /*!< MLU 370 M8.*/
  CNNL_MLU_580_H5 = 10, /*!< MLU 590.*/
  CNNL_MLU_590_H8 = 11, /*!< MLU 590.*/
  CNNL_MLU_590_M9 = 12, /*!< MLU 590.*/
} cnnlDeviceType_t;

/*!
 * @brief Enumeration variables describing the options that can help choose
 *        the best suited algorithm used for implementation of the convolution
 *        backward data operation.
 *
 * This enumeration is used in the ::cnnlGetConvolutionBackwardDataAlgorithm function.
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_DATA_FASTEST = 0,
  /*!< Implementation with the fastest convolution backward data algorithm
   *   regardless of the workspace memory to be occupied.*/
  CNNL_CONVOLUTION_BWD_DATA_LOW_MEMORY_OCCUPY = 1,
  /*!< Implementation with the convolution backward data algorithm that
   *   occupies the least workspace memory.*/
} cnnlConvolutionBwdDataPreference_t;

/*!
 * @brief Enumeration variables describing the options that can help choose the best suited
 *        deconvolution algorithms used for implementation of the deconvolution operation.
 *        Same as ::cnnlConvolutionBwdDataPreference_t.
 *
 * This enum is used in the ::cnnlGetDeconvolutionAlgorithm and ::cnnlGetDeconvolutionAlgorithm_v2
 * function.
 */
typedef cnnlConvolutionBwdDataPreference_t cnnlDeconvolutionPreference_t;

/*!
 * @brief Enumeration variables describing the convolution backward data
 *        algorithms that can be used to implement the convolution backward
 *        data operation.
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_DATA_ALGO_DIRECT = 0,
  /*!< The basic implementation of convolution backward data operation.*/
} cnnlConvolutionBwdDataAlgo_t;

/*!
 * @brief Enumeration variables describing the deconvolution algorithms that can be used
 *        to implement the deconvolution operation.
 *        Same as ::cnnlConvolutionBwdDataAlgo_t.
 */
typedef cnnlConvolutionBwdDataAlgo_t cnnlDeconvolutionAlgo_t;

/*!
 * @brief Enumeration variables describing the quantization modes used for
 *        the deconvolution operation.
 *        Same as ::cnnlConvolutionCastMode_t.
 */
typedef cnnlConvolutionCastMode_t cnnlDeconvolutionCastMode_t;

/*!
 * @brief Enumeration variables describing the pooling modes that can be used to
 * implement the pooling operation.
 */
typedef enum {
  CNNL_POOLING_MAX = 0, /*!< The max pooling mode is implemented.*/
  CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING = 1,
  /*!< The average pooling with padding mode is implemented.*/
  CNNL_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING = 2,
  /*!< The average pooling without padding mode is implemented.*/
  CNNL_POOLING_FIXED = 3,
  /*!< The fixed mode is implemented. This mode is used in the unpool operation.
   * In this mode, each input pixel will be put to the center of the pooling kernel
   * regardless of the index.*/
} cnnlPoolingMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Nllloss
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the operations that are used in the
 * implementation of the Negative-Log-Likelihood Loss function.
 *
 */
typedef enum {
  CNNL_REDUCTION_NONE = 0,
  /*!< The NONE mode is implemented, no reduction will be applied.*/
  CNNL_REDUCTION_SUM = 1,
  /*!< The SUM mode is implemented, the output will be summed.*/
  CNNL_REDUCTION_MEAN = 2,
  /*!< The MEAN mode is implemented, the weighted mean of the output is taken.*/
} cnnlNlllossAlgorithm_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: LossReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the operations that are used in the
 * implementation of the loss function.
 *
 */
typedef enum {
  CNNL_LOSS_REDUCTION_NONE = 0,
  /*!< The NONE mode is implemented, no reduction is applied in the operation.*/
  CNNL_LOSS_REDUCTION_SUM = 1,
  /*!< The SUM mode is implemented, the elements of output are summed in the operation.*/
  CNNL_LOSS_REDUCTION_MEAN = 2,
  /*!< The MEAN mode is implemented, the weighted mean of the output is applied in the operation.*/
} cnnlLossReduction_t;

/*!
 * @brief Enumeration variables describing the modes that are used in the implementation
 * of the determinant computing function.
 */
typedef enum {
  CNNL_DET_MODE_DET = 0,
  /*!< The DET mode is implemented, the determinant will be computed.*/
  CNNL_DET_MODE_LOGDET = 1,
  /*!< The LOGDET mode is implemented, the logarithm of the determinant will be computed.*/
} cnnlDetMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: SmoothL1Loss
 ******************************************************************************/
/*!
 * @brief
 *
 * Enumeration variables describe the operations that are used in the
 * implementation of the SmoothL1 Loss function.
 *
 */
typedef enum {
  CNNL_SMOOTHL1LOSS_REDUCTION_NONE = 0,
  /*!< The NONE mode is implemented, no reduction is applied in the operation.*/
  CNNL_SMOOTHL1LOSS_REDUCTION_SUM  = 1,
  /*!< The SUM mode is implemented, the elements of output are summed in the operation.*/
  CNNL_SMOOTHL1LOSS_REDUCTION_MEAN = 2,
  /*!< The MEAN mode is implemented, the weighted mean of the output is applied in the operation.*/
} cnnlSmoothL1LossAlgorithm_t;

/*!
 * @brief Enumeration variables describing the options that can help to
 * choose the best suited algorithm used for implementation
 * of the convolution backward filter operation.
 * Currently only \b CNNL_CONVOLUTION_BWD_FILTER_FASTEST is supported.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_FILTER_FASTEST = 0,
  /*!< Implementation with the fastest convolution backward filter algorithm
   *   regardless of the workspace memory to be occupied.*/
  CNNL_CONVOLUTION_BWD_FILTER_LOW_MEMORY_OCCUPY = 1,
  /*!< Implementation with the convolution backward filter algorithm that
   *   occupies the least workspace memory.*/
} cnnlConvolutionBwdFilterPreference_t;

/*!
 * @brief Enumeration variables describing the different algorithms that can be used to
 * implement the convolution backward filter operation.
 *
 */
typedef enum {
  CNNL_CONVOLUTION_BWD_FILTER_ALGO_DIRECT = 0,
  /*!< The basic implementation of the convolution backward filter operation.*/
  CNNL_CONVOLUTION_BWD_FILTER_ALGO_GEMM = 1,
  /*!< The implementation of convolution as explicit matrix multiplication.*/
} cnnlConvolutionBwdFilterAlgo_t;

/*!
 * @brief
 * Enumeration variables describing the supported data conversion modes of the Cast function that
 * converting one data type to another.
 */
typedef enum {
  CNNL_CAST_FLOAT_TO_HALF = 210,
  /*!< Converts float to half. Round‑to‑nearest‑off‑zero mode is used,
   * when the device is MLU200 series, and in other devices, round-nearest-to-even mode is used. */
  CNNL_CAST_FLOAT_TO_INT32 = 260, /*!< Converts float to int32. The to_zero rounding mode is used.*/
  CNNL_CAST_FLOAT_TO_INT16 = 240,
  /*!< Converts float to int16. The to_zero rounding mode is used. */
  CNNL_CAST_FLOAT_TO_INT8 = 230,  /*!< Converts float to int8. The to_zero rounding mode is used.*/
  CNNL_CAST_FLOAT_TO_UINT8 = 270, /*!< Converts float to uint8. */
  CNNL_CAST_FLOAT_TO_BOOL = 280,  /*!< Converts float to bool.  */
  CNNL_CAST_HALF_TO_FLOAT = 120,  /*!< Converts half to float.  */
  CNNL_CAST_HALF_TO_INT32 = 160,  /*!< Converts half to int32. The to_zero rounding mode is used.*/
  CNNL_CAST_HALF_TO_INT16 = 140,  /*!< Converts half to int16. The to_zero rounding mode is used.*/
  CNNL_CAST_HALF_TO_INT8 = 130,   /*!< Converts half to int8.  The to_zero rounding mode is used.*/
  CNNL_CAST_HALF_TO_UINT8 = 170,  /*!< Converts half to uint8.  */
  CNNL_CAST_HALF_TO_BOOL = 180,   /*!< Converts half to bool.   */
  CNNL_CAST_INT32_TO_FLOAT = 620, /*!< Converts int32 to float. */
  CNNL_CAST_INT32_TO_HALF = 610,  /*!< Converts int32 to half. */
  CNNL_CAST_INT32_TO_INT8 = 630,  /*!< Converts int32 to int8.  */
  CNNL_CAST_INT32_TO_INT16 = 640, /*!< Converts int32 to int16. */
  CNNL_CAST_INT16_TO_FLOAT = 420, /*!< Converts int16 to float. */
  CNNL_CAST_INT16_TO_HALF = 410,  /*!< Converts int16 to half.  */
  CNNL_CAST_INT16_TO_INT32 = 460, /*!< Converts int16 to int32. */
  CNNL_CAST_INT8_TO_FLOAT = 320,  /*!< Converts int8 to float.  */
  CNNL_CAST_INT8_TO_HALF = 310,   /*!< Converts int8 to half.   */
  CNNL_CAST_INT8_TO_INT32 = 360,  /*!< Converts int8 to int32.  */
  CNNL_CAST_UINT8_TO_FLOAT = 720, /*!< Converts uint8 to float. */
  CNNL_CAST_UINT8_TO_HALF = 710,  /*!< Converts uint8 to half.  */
  CNNL_CAST_BOOL_TO_FLOAT = 820,  /*!< Converts bool to float.  */
  CNNL_CAST_BOOL_TO_HALF = 810,   /*!< Converts bool to half.   */
  CNNL_CAST_BOOL_TO_INT32 = 860,  /*!< Converts bool to int32.  */
  CNNL_CAST_UINT8_TO_INT32 = 760, /*!< Converts uint8 to int32.  */
  CNNL_CAST_INT32_TO_INT64 = 690, /*!< Converts int32 to int64. */
  CNNL_CAST_INT64_TO_INT32 = 960, /*!< Converts int64 to int32. */
  CNNL_CAST_INT32_TO_BOOL = 680, /*!< Converts int32 to bool. */
  CNNL_CAST_UINT8_TO_INT64 = 790, /*!< Converts uint8 to int64. */
  CNNL_CAST_UINT64_TO_UINT32 = 213, /*!< Converts uint64 to uint32. */
  CNNL_CAST_INT64_TO_UINT32 = 912, /*!< Converts int64 to uint32. */
  CNNL_CAST_UINT32_TO_INT64 = 191, /*!< Converts uint32 to int64. */
  CNNL_CAST_UINT32_TO_UINT64 = 123, /*!< Converts uint32 to uint64. */
  CNNL_CAST_INT8_TO_INT16 = 340, /*!< Converts int8 to int16. */
  CNNL_CAST_HALF_TO_FLOAT_INF = 129, /*!< Converts half to float for amp training. */
  CNNL_CAST_FLOAT_TO_HALF_IEEE754 = 219, /*!< Converts float to half for ieee754. */
  CNNL_CAST_FLOAT_TO_DOUBLE = 202,  /*!< Converts float to double. */
  CNNL_CAST_DOUBLE_TO_FLOAT = 21, /*!< Converts double to float. */
  CNNL_CAST_INT64_TO_FLOAT = 920, /*!< Converts int64 to float. */
  CNNL_CAST_INT64_TO_HALF = 910, /*!< Converts int64 to half. */
  CNNL_CAST_FLOAT_TO_INT64 = 290, /*!< Converts float to int64. */
  CNNL_CAST_HALF_TO_INT64 = 190, /*!< Converts half to int64. */
} cnnlCastDataType_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BceWithLogitsReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the dimension reduction methods that are used in the
 * implementation of the BceWithLogits and BceWithLogitsBackward functions.
 *
 */
typedef enum {
  CNNL_BCE_WITH_LOGITS_NONE = 0,
  /*!< Computes the binary cross entropy for BceWithLogits, and computes the gradient of input
   * for BceWithLogitsBackward. */
  CNNL_BCE_WITH_LOGITS_MEAN = 1,
  /*!< Averages the sum of result for BceWithLogits with
   * BceWithLogitsBackWard not used. */
  CNNL_BCE_WITH_LOGITS_SUM = 2
  /*!< Calculates the sum of the result for BceWithLogits with
   * BceWithLogitsBackward not used. */
} cnnlBceWithLogitsReduction_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BceLossReduction
 ******************************************************************************/
/*!
 * @brief An enum.
 *
 * Enumeration variables describing the reduction methods that can be used to
 * implement the BceLoss and BceLossBackward operation.
 *
 */
typedef enum {
  CNNL_BCE_LOSS_NONE = 0,
  /*!< No reduction will be applied in this mode. */
  CNNL_BCE_LOSS_MEAN = 1,
  /*!< Average the sum of result in this mode. */
  CNNL_BCE_LOSS_SUM = 2
  /*!< Get the sum of the result in this mode.*/
} cnnlBceLossReduction_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Lrn
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the computing modes that are used in the
 * implementation of the LRN function.
 *
 */
typedef enum {
  CNNL_LRN_LOCAL_SIZE = 0,
  /*!< Computes lrn in local_size which is the side length of the square region. */
  CNNL_LRN_LOCAL_SIZE_ORIGINAL = 1,
  /*!< Computes lrn in local_size_original which is the side length of the square region. */
  CNNL_LRN_CROSS_CHANNEL = 2,
  /*!< Computes lrn in cross channel of input. */
  CNNL_LRN_WITHIN_CHANNEL = 3
  /*!< Computes lrn in nearby spatial locations of input. */
} cnnlLrnMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Gru
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the Gated Recurrent Unit (GRU) operation. For detailed information
 * about algorithm, you can see "GRU Operator" section in "Cambricon CNNL User Guide".
 */
typedef enum {
  CNNL_GRU_ALGO_V1 = 0, /*!< Implements with formula: ``h_new = (1 - z) * h + z * n``. */
  CNNL_GRU_ALGO_V2 = 1, /*!< Implements with formula: ``h_new = (1 - z) * n + z * h``. */
  /*!< z means the update gate;
       n means the candidate hidden state;
       h means the hidden state in the current time step;
       h_new means the hidden state in the next time step. */
} cnnlGruAlgo_t;

/*!
 * @brief Enumeration variables describing the processing direction of input sequence data
 * in the implementation of the Gated Recurrent Unit (GRU) operation.
 */
typedef enum {
  CNNL_GRU_FORWARD = 0,
  /*!< The GRU network iteratively processes the input sequence data from front to back.*/
  CNNL_GRU_BACKWARD = 1,
  /*!< The GRU network iteratively processes the input sequence data from back to front.*/
  CNNL_GRU_BIDIRECTIONAL = 2,
  /*!< Two GRU network iteratively processes the input sequence data from two directions,
       first from front to back, and then from back to front. The results of the two GRU
       network are concatenated at each iteration as the output.
       Note that two GRU networks have different model parameters.*/
} cnnlGruDir_t;

/*!
 * @brief Enumeration variables describing the order of filter parameters in the implementation
 * of the Gated Recurrent Unit (GRU) operation.
 */
typedef enum {
  CNNL_GRU_RZN = 0,
  /*!< The filter parameters order of GRU gate and state:
       ``W_r_i, W_r_h, W_z_i, W_z_h, W_n_i，W_n_h``.
       The order of bias parameters are consistent with the filter parameters, which is
       ``B_r_i, B_r_h, B_z_i, B_z_h, B_n_i，B_n_h``.*/
  CNNL_GRU_ZRN = 1,
  /*!< The filter parameters order of GRU gate and state:
       ``W_z_i, W_z_h, W_r_i, W_r_h, W_n_i, W_n_h``.
       The order of bias parameters are consistent with the filter parameters, which is
       ``B_z_i, B_z_h, B_r_i, B_r_h, B_n_i，B_n_h``.*/
  CNNL_GRU_IH_RZN = 2,
  /*!< The filter parameters order of GRU gate and state:
       ``W_r_i, W_z_i, W_n_i, W_r_h, W_z_h, W_n_h``.
       The order of bias parameters are consistent with the filter parameters, which is
       ``B_r_i, B_z_i, B_n_i, B_r_h, B_z_h，B_n_h``.*/
  CNNL_GRU_IH_ZRN = 3,
  /*!< The filter parameters order of GRU gate and state:
       ``W_z_i, W_r_i, W_n_i, W_z_h, W_r_h, W_n_h``.
       The order of bias parameters are consistent with the filter parameters, which is
       ``B_z_i, B_r_i, B_n_i, B_z_h, B_r_h，B_n_h``.*/
  /*!< r means for the reset gate;
       z means for the update gate;
       n means for the candidate hidden state;
       i means for the input;
       h means for the hidden state. */
} cnnlGruWeightOrder_t;

/*!
 * @brief Enumeration variables describing the mode that is used in the
 * implementation of the multi-layer bidirectional Gated Recurrent Unit (GRU) operation. For detailed information
 * about mode, you can see "GRU Operator" section in "Cambricon CNNL User Guide".
 */
typedef enum {
  CNNL_GRU_MODE_V1 = 0,
  /*!< Implements multi-layer bidirectional gru with the same mode as TensorFlow. */
  CNNL_GRU_MODE_V2 = 1,
  /*!< Implements multi-layer bidirectional gru with the same mode as PyTorch. */
} cnnlGruMode_t;

/*!
 * @brief Enumeration variables describing the data layouts of state and state_output in cnnlGru_v2.
 *
 * The data can be defined in four dimensions:
 * - L: The number of layers of GRU.
 * - D: The number of directions of GRU.
 * - N: The batch size of sequence.
 * - C: The hidden size of sequence.
 */
typedef enum {
  CNNL_GRU_LDNC = 0,
  /*!< The data layout is in the following order:
  number of layers, number of directions, batch, hidden size. */
  CNNL_GRU_DLNC = 1,
  /*!< This enum is not supported currently. */
  CNNL_GRU_LNDC = 2,
  /*!< The data layout is in the following order:
  number of layers, batch, number of directions, hidden size. */
} cnnlGruLayout_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Attention
 ******************************************************************************/
/*!
 * @brief
 *
 * Enumeration variables describe the algorithms that are used in the
 * implementation of the ATTENTION function.
 *
 */
typedef enum {
  CNNL_LOCATION_SENSITIVE_ATTENTION = 0, /*!< Location-Sensitive-Attention.*/
} cnnlAttentionAlgo_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Nms
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the input box modes that can be used
 *        in the implementation of the Nms function.
 */
typedef enum {
  CNNL_NMS_BOX_DIAGONAL = 0,
  /*!< A type of box mode which defines the data structure of box
   * as [x1, y1, x2, y2]. */
  CNNL_NMS_BOX_CENTER = 1,
  /*!< A type of box mode which defines the data structure of box
   * as [x_center, y_center, width, height] where width > 0 and
   * height > 0*/
} cnnlNmsBoxPointMode_t;

/*!
 * @brief Enumeration variables describing the output modes that can be used
 *        in the implementation of the Nms function.
 */
typedef enum {
  CNNL_NMS_OUTPUT_TARGET_INDICES = 0,
  /*!< Returns target indices, sorted in decreasing order of confidences. */
  CNNL_NMS_OUTPUT_TARGET_CONFIDENCE_AND_POS_1 = 1,
  /*!< Returns target confidences and positions with the order of:
   * confidence_0, x_01, y_01, x_02, y_02, confidence_1, x_11, y_11, x_12, y_12,
   * ... ,confidence_n, x_n1, y_n1, x_n2, y_n2. The (x_01, y_01) and (x_02, y_02)
   * represent the top left corner and bottom right corner coordinates of first box,
   * respectively. */
  CNNL_NMS_OUTPUT_TARGET_CONFIDENCE_AND_POS_2 = 2,
  /*!< Returns target confidences and positions with the order of:
   * confidence_0, confidence_1, ... , confidence_n, x_01, x_11, ... , x_n1,
   * y_01, y_11, ... , y_n1, x_02, x_12, ... , x_n2, y_02, y_12, ... , y_n2.
   * The (x_01, y_01) and (x_02, y_02) represent the top left corner and bottom right
   * corner coordinates of first box, respectively. */
  CNNL_NMS_OUTPUT_TARGET_BATCH_AND_CLASS = 3,
  /*!< Returns batch indices, class indices and positions with the order of:
   * batch_0, class_0, box_0, ... ,batch_0, class_0, box_m, batch_0, class_1, box_0, ... ,
   * batch_0, class_1, box_m, ... , ... , batch_s, class_n, box_m. */
} cnnlNmsOutputMode_t;

/*!
 * @brief Enumeration variables describing the algorithms that can be used
 *        in the update of confidence in Nms function.
 */
typedef enum {
  CNNL_NMS_HARD_NMS = 0,
  /*!< A type of algorithm which updates confidence using hard nms, i.e.
   * confidence = IOU < IOU_threshold ? confidence : 0 */
  CNNL_NMS_SOFT_NMS_LINEAR = 1,
  /*!< A type of algorithm which updates confidence using linear method, i.e.
   * confidence = IOU < IOU_threshold ? confidence : confidence * (1 - IOU) */
  CNNL_NMS_SOFT_NMS_GAUSSIAN = 2,
  /*!< A type of algorithm which updates confidence using Gaussian method, i.e.
   * confidence = confidence * exp{- IOU ^ 2 / (2 * sigma)} */
} cnnlNmsMethodMode_t;

/*!
 * @brief Enumeration variables describing the algorithms that can be used in
 *        in the implementation of the Nms function.
 */
typedef enum {
  CNNL_NMS_ALGO_EXCLUDE_BOUNDARY = 0,
  /*!< Implements with the height or width of boxes is ``(x2 - x1)``. */
  CNNL_NMS_ALGO_INCLUDE_BOUNDARY = 1,
  /*!< Implements with the height or width of boxes is ``(x2 - x1 + offset)``. */
} cnnlNmsAlgo_t;

/*!
 * @brief Enumeration variables describing the algorithms that are used in the
 * implementation of the ::cnnlTrigonForward function.
 */
typedef enum {
  CNNL_TRIGON_SIN = 0, /*!< Implements: ``y = sin(x)``. */
  CNNL_TRIGON_COS = 1, /*!< Implements: ``y = cos(x)``. */
  CNNL_TRIGON_TAN = 2, /*!< Implements: ``y = tan(x)``. */
  CNNL_TRIGON_ASIN = 3, /*!< Implements: ``y = asin(x)``. */
  CNNL_TRIGON_ACOS = 4, /*!< Implements: ``y = acos(x)``. */
  CNNL_TRIGON_ATAN = 5, /*!< Implements: ``y = atan(x)``. */
  CNNL_TRIGON_SINH = 6, /*!< Implements: ``y = sinh(x)``. */
  CNNL_TRIGON_COSH = 7, /*!< Implements: ``y = cosh(x)``. */
  CNNL_TRIGON_TANH = 8, /*!< Implements: ``y = tanh(x)``. */
  CNNL_TRIGON_ASINH = 9, /*!< Implements: ``y = asinh(x)``. */
  CNNL_TRIGON_ACOSH = 10, /*!< Implements: ``y = acosh(x)``. */
  CNNL_TRIGON_ATANH = 11, /*!< Implements: ``y = atanh(x)``. */
} cnnlTrigonFunctionMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: MatMul
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the attribute names of the matrix multiplication computation.
 */
typedef enum {
  CNNL_MATMUL_DESC_COMPUTE_TYPE = 0,
  /*!< Defines data type used for multiplication and accumulation operations, and the
       accumulator for implementing matrix multiplication. It \p must be set before
       doing matrix multiplication. */
  CNNL_MATMUL_DESC_SCALE_TYPE = 1,
  /*!< Defines data type of the scaling factors \b alpha and \b beta. Default value
       is the same as ::CNNL_MATMUL_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_MATMUL_DESC_POINTER_MODE = 2,
  /*!< Specifies whether \b alpha and \b beta are stored on the host or on the device.
       It is not supported now. */
  CNNL_MATMUL_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A. Default
       value is 0 (false). */
  CNNL_MATMUL_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B. Default
       value is 0 (false). */
  CNNL_MATMUL_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C. Default
       value is 0 (false). It is not supported now. */
  CNNL_MATMUL_DESC_EPILOGUE = 6,
  /*!< Specifies epilogue function. It is not supported now. */
  CNNL_MATMUL_DESC_BIAS_POINTER = 7,
  /*!< Pointer to bias vector on MLU device memory. Currently, it is only supported to set
       the attribute \b matmul_desc in the interface :: cnnlMatMulInference. */
  CNNL_MATMUL_DESC_EPILOGUE_TYPE = 8,
  /*!< Specifies matmul multiplication epilogue fusion type. */
  CNNL_MATMUL_DESC_EPILOGUE_OPERAND = 9,
  /*!< Specifies matmul multiplication epilogue fusion operand. */
  CNNL_MATMUL_ALLOW_TF32 = 10,
  /*!< Determines enabling TensorFloat-32 mode.
       TensorFloat-32 is enabled by default. */
  CNNL_MATMUL_USE_BETA = 11,
  /*!< Specifies whether to use \b beta on matrix C. */
} cnnlMatMulDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: MatMulInferenceV2
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the transformation operation
 * performed on matrix multiplication.
 */
typedef enum {
  CNNL_MATMUL_OP_N = 0,
  /*!< The transformation operation is not performed when doing the matrix multiplication. */
  CNNL_MATMUL_OP_T = 1,
  /*!< The transformation operation is performed when doing the matrix multiplication. */
} cnnlMatrixOperation_t;

/*!
 * @brief Enumeration variables describing the fused operation
 * performed after matrix multiplication.
 */
typedef enum {
  CNNL_MATMUL_EPI_NONE = 0,
  /*!< No fused operation*/
  CNNL_MATMUL_EPI_BIAS = 1,
  /*!< Specifies the fused operation only with bias.*/
  CNNL_MATMUL_EPI_BIAS_SCALE_BN_ACTIVATION = 2,
  /*!< Specifies the fused operation with bias, scale, bn and activation.*/
} cnnlMatMulEpilogueType_t;

/*! The descriptor of the matrix multiplication inference algorithm. It is reserved for future use and
    should be set to NULL When called by cnnlMatMulInference_v2 */
typedef struct cnnlMatMulInferenceAlgo *cnnlMatMulInferenceAlgo_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BatchMatMul
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the attribute names of the batch matrix multiplication computation.
 */
typedef enum {
  CNNL_BMM_DESC_COMPUTE_TYPE = 0,
  /*!< Defines data type used for multiplication and accumulation operations, and the
       accumulator for implementing batch matrix multiplication. It \p must be set before
       doing matrix multiplication. */
  CNNL_BMM_DESC_SCALE_TYPE = 1,
  /*!< Defines data type of the scaling factors \b alpha and \b beta. Default value
       is the same as ::CNNL_BMM_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_BMM_DESC_POINTER_MODE = 2,
  /*!< Specifies whether \b alpha and \b beta are stored on the host or on the device.
       It is not supported now. */
  CNNL_BMM_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A. Default
       value is 0 (false). */
  CNNL_BMM_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B. Default
       value is 0 (false). */
  CNNL_BMM_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C. Default
       value is 0 (false). It is not supported now. */
} cnnlBatchMatMulDescAttribute_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: Normalize
 ******************************************************************************/
/**
 * @brief Enumeration variables describe the algorithms that are used in the
 * implementation of the Normalize function.
 *
 * The normalize mode only supports euclidean now, more normalize modes will be supported later.
 *
 */

typedef enum {
  CNNL_NORMALIZE_EUCLIDEAN = 0, /*!<The square root of sum of squares operation is implemented.*/
} cnnlNormalizeMode_t;

/******************************************************************************
 * Cambricon CNNL Data Structure: BatchMatMulBCast
 ******************************************************************************/
/**
 * @brief Enumeration variables describing the attribute names of the batch matrix multiplication computation with broadcasting.
 *
 */
typedef enum {
  CNNL_BMM_BCAST_DESC_COMPUTE_TYPE = 0,
  /*!< Defines data type used for multiplication and accumulation operations, and the accumulator
       for implementing batch matrix multiplication with broadcasting. It \p must be set before doing
       batch matrix multiplication with broadcasting. */
  CNNL_BMM_BCAST_DESC_SCALE_TYPE = 1,
  /*!< Defines data type of the scaling factors \b alpha and \b beta. Default value is the same
       as ::CNNL_BMM_BCAST_DESC_COMPUTE_TYPE. It is not supported now. */
  CNNL_BMM_BCAST_DESC_POINTER_MODE = 2,
  /*!< Specifies whether \b alpha and \b beta are stored on the host or on the device.
       It is not supported now. */
  CNNL_BMM_BCAST_DESC_TRANSA = 3,
  /*!< Specifies whether transpose should be performed on matrix A.
       Default value is 0 (false). */
  CNNL_BMM_BCAST_DESC_TRANSB = 4,
  /*!< Specifies whether transpose should be performed on matrix B.
       Default value is 0 (false). */
  CNNL_BMM_BCAST_DESC_TRANSC = 5,
  /*!< Specifies whether transpose should be performed on matrix C.
       Default value is 0 (false). It is not supported now. */
} cnnlBatchMatMulBCastDescAttribute_t;

/*!
 * @brief Enumeration variables describing the preference of matrix multiplication algorithm.
 */
typedef enum {
  CNNL_MATMUL_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_MATMUL_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. This is not supported now. */
} cnnlMatMulPreference_t;

/*!
 * @brief Enumeration variables describing the preference of batch matrix multiplication algorithm.
 */
typedef enum {
  CNNL_BMM_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_BMM_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. This is not supported now. */
} cnnlBatchMatMulPreference_t;

/*!
 * @brief Enumeration variables describing the preference of batch matrix multiplication with broadcasting algorithm.
 */
typedef enum {
  CNNL_BMM_BCAST_FASTEST = 0,
  /*!< The high-speed preference is used. */
  CNNL_BMM_BCAST_LOW_MEMORY_OCCUPY = 1,
  /*!< The low-memory preference is used. It is not supported now. */
} cnnlBatchMatMulBCastPreference_t;


/*!
 * @brief Enumeration variables describing that the scalar values are passed by reference on the host or device.
 */
typedef enum {
  CNNL_POINTER_MODE_HOST = 0,
  /*!< The scale pointer is a host pointer. */
  CNNL_POINTER_MODE_DEVICE = 1,
  /*!< The scale pointer is a device pointer. */
} cnnlPointerMode_t;

/*!
 * @brief Enumeration variables describing the atomics mode in CNNL.
 */
typedef enum {
  CNNL_ATOMICS_NOT_ALLOWED = 1,
  /*!< The atomics is not allowed to cumulate results. */
  CNNL_ATOMICS_ALLOWED = 2,
  /*!< The atomics is allowed to cumulate results. */
} cnnlAtomicsMode_t;

/*!
 * @brief Enumeration variables describing that the rounding mode of quantization conversion.
 */
typedef enum {
  CNNL_ROUND_HALF_TO_EVEN = 0,
  /*!< The rounding mode to round towards the nearest even neighbor
   *   is used for quantization conversion.*/
  CNNL_ROUND_HALF_UP = 1,
  /*!< The rounding mode to round up towards the nearest neighbor is
   *   used for quantization conversion.*/
  CNNL_ROUND_HALF_OFF_ZERO = 2,
  /*!< The rounding mode to round half away from zero is
   *   used for quantization conversion.*/
} cnnlQuantizeRoundMode_t;

/*!
 * @brief Enumeration variables describing the detection and output mode of the ::cnnlIsInf operation.
 */
typedef enum {
  CNNL_NEG_INF = 0,
  /*!< This mode only detects the negative infinity in the input and outputs true
       in corresponding position of input. */
  CNNL_POS_INF = 1,
  /*!< This mode only detects the positive infinity in the input and outputs true
       in corresponding position of input. */
  CNNL_INF = 2,
  /*!< This mode detects negative infinity or positive infinity in the input and returns true
       at the position of negative infinity or positive infinity in output. */
  CNNL_INF_INDICATOR = 3
  /*!< This mode detects the type of negtive infinite and positive infinite and outputs the corresponding indicators.
       If the value is a negtive infinity, outputs 1 in corresponding position of input;
       if the value is a positive infinity, outputs 2 in corresponding position of input;
       if the value is neither a positive infinity nor a negtive infinity,
       outputs 0 in corresponding position of input. */
} cnnlIsInfMode_t;

/******************************************************************************
 * CNNL Runtime Management
 ******************************************************************************/

/*!
 * @struct cnnlContext
 * @brief The cnnlContext is a structure describing the Cambricon CNNLcontext.
 *
 *
 */
struct cnnlContext;
/*!
 * A pointer to ::cnnlContext struct that holds the Cambricon CNNL context.
 *
 * MLU device resources cannot be accessed directly, so Cambricon CNNL uses
 * handle to manage Cambricon CNNL context including MLU device information
 * and queues.
 *
 * The Cambricon CNNLcontext is created with ::cnnlCreate and the returned
 * handle should be passed to all the subsequent function calls.
 * You need to destroy the Cambricon CNNL context at the end with ::cnnlDestroy.
 * For more information, see "Cambricon CNNL User Guide".
 *
 */
typedef struct cnnlContext *cnnlHandle_t;

// Group:Runtime Management
/*!
 *  @brief Initializes the Cambricon CNNL library and creates a \b handle to a structure
 *  that holds the Cambricon CNNL library context. It allocates hardware resources on the host
 *  and device. You need to call this function before any other Cambricon CNNL functions.
 *
 *  You need to call the ::cnnlDestroy function to release the resources later.
 *
 *  @param[out] handle
 *    Output. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCreate(cnnlHandle_t *handle);

// Group:Runtime Management
/*!
 *  @brief Updates the Cambricon CNNL context information that holds by the \b handle. This function
 *  should be called if you call Cambricon Driver API cnSetCtxConfigParam to set the context
 *  information. The related context information will be synchronized to Cambricon CNNL with this function.
 *  For detailed information, see Cambricon Driver API Developer Guide.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlUpdateContextInformation(cnnlHandle_t handle);

// Group:Runtime Management
/*!
 *  @brief Releases the resources of the specified Cambricon CNNL \b handle that was
 *  created by the ::cnnlCreate function.
 *  It is usually the last call to destroy the handle to the Cambricon CNNL handle.
 *
 *  @param[in] handle
 *    Input. Pointer to the MLU devices that holds information to be destroyed.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroy(cnnlHandle_t handle);

// Group:Runtime Management
/*!
 *  @brief Sets the runtime \b queue in the \b handle. The queue is used to
 *  launch kernels or to synchronize to this queue.
 *
 *  Before setting a \b queue, you need to call the ::cnnlCreate function to initialize
 *  Cambricon CNNL library.
 *
 *  If the \b queue is set to NULL, the default queue is used.
 *  Otherwise, you need to call `cnrtQueueCreate` to create a \b queue.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] queue
 *    Input. The runtime queue to be set to the Cambricon CNNL handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - When the \b queue is set to NULL and you are in multi-threaded programming,
 *    remember that the default queue in different threads differs from each other, and
 *    CNRT function calls involving the \b queue (e.g. `cnrtQueueSync`)
 *    only affect Cambricon CNNL function calls in the same thread.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetQueue(cnnlHandle_t handle, cnrtQueue_t queue);

// Group:Runtime Management
/*!
 *  @brief Retrieves the \b queue that was previously set to the \b handle.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[out] queue
 *    Output. Pointer to the queue that was previously set to the specified handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQueue(cnnlHandle_t handle, cnrtQueue_t *queue);

// Group:Runtime Management
/*!
 *  @brief Retrieves the device ID in the handle created by ::cnnlCreate.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[out] device
 *    Output. Pointer to the MLU device ID that was previously created by ::cnnlCreate.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDevice(cnnlHandle_t handle, CNdev *device);

// Group:QuantizeRoundMode
/*!
 *  @brief Updates the specific rounding mode of Cambricon CNNL context information that holds by the \b handle. This function
 *  should be called if you want to change the cnnl rounding mode that used to cumulate the results.
 *  For detailed information, see Cambricon Driver API Developer Guide.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] round_mode
 *    Input. The rounding mode of quantization conversion to be set to the CNNL handle.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - On MLU200 series:
 *    You can't set CNNL_ROUND_HALF_TO_EVEN for the rounding mode because the hardware does not support it.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetQuantizeRoundMode(cnnlHandle_t handle,
                                                   cnnlQuantizeRoundMode_t round_mode);
// Group:QuantizeRoundMode
/*!
 *  @brief Retrieves the rounding mode of a specific Cambricon CNNL context.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[out] round_mode
 *    Output. The rounding mode of quantization conversion that was previously set to the specified handle.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The default round mode of default initialized cnnlHandle_t is CNNL_ROUND_TO_EVEN.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeRoundMode(cnnlHandle_t handle,
                                                   cnnlQuantizeRoundMode_t *round_mode);

// Group:Runtime Management
/*!
 *  @brief Updates the specific atomics mode of Cambricon CNNL context information that holds by the \b handle. This function
 *  should be called if you want to change the atomics mode that is used to cumulate the results.
 *  For detailed information, see Cambricon Driver API Developer Guide.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[in] atomics_mode
 *    Input. The atomics mode.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetAtomicsMode(cnnlHandle_t handle, cnnlAtomicsMode_t atomics_mode);

// Group:Runtime Management
/*!
 *  @brief Retrieves the atomics mode of a specific Cambricon CNNL context.
 *
 *  @param[in] handle
 *    Input. Pointer to the Cambricon CNNL context that is used to manage MLU devices and
 *    queues. For detailed information, see ::cnnlHandle_t.
 *
 *  @param[out] atomics_mode
 *    Output. The atomics mode.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The default atomics mode of default initialized cnnlHandle_t is ::CNNL_ATOMICS_NOT_ALLOWED.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAtomicsMode(cnnlHandle_t handle, cnnlAtomicsMode_t *atomics_mode);

// Group:Runtime Management
/**
 *  @brief Retrieves the configuration parameters in the handle created by ::cnnlCreate.
 *
 *  @param[in] handle
 *     Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *     queues. For detailed information, see ::cnnlHandle_t.
 *  @param[in] type
 *     Input. The configuration parameter type. The parameter type includes
 *     CN_CTX_CONFIG_UNION_LIMIT and CN_CTX_CONFIG_VISIBLE_CLUSTER_NUM.
 *     For more information, see "Cambricon Driver API User Guide".
 *  @param[out] param
 *     Output. Pointer to the host memory that stores the value of configuration parameter.
 *     For more information, see "Cambricon Driver API User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Scale Limitation
 *  - The value of \b type can only be CN_CTX_CONFIG_UNION_LIMIT or
 *    CN_CTX_CONFIG_VISIBLE_CLUSTER_NUM.

 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetContextParam(cnnlHandle_t handle,
                                              CNctxConfigParamType type,
                                              CNctxConfigParam *param);

// Group:Runtime Management
/*!
 *  @brief Gets the size of device memory reserved for CNNL.
 *
 *  @param[out] mem_size
 *    Output. The memory size reserved for CNNL.
 *  @return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This parameter \b mem_size can be configured by the environment variable CNNL_MEM_POOL_SIZE.
 *    If CNNL_MEM_POOL_SIZE not configured, this function will return a built-in hardcode value.
 *    For more information about setting environment variable, see "Cambricon CNNL User Guide".
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReservedMemSize(uint64_t* mem_size);

// Group:Runtime Management
/*!
 *  @brief Converts the enumerated status code to ASCIIZ static string and returns
 *  a pointer to host memory that holds information about ASCIIZ static string with the status name.
 *  For example, when the input argument is
 *  ::CNNL_STATUS_SUCCESS, the returned string is CNNL_STATUS_SUCCESS. When an invalid status value is passed
 *  to the function, the returned string is CNNL_STATUS_BAD_PARAM.
 *
 *  @param[in] status
 *    Input. The enumerated status code.
 *  @return
 *  - CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
const char *cnnlGetErrorString(cnnlStatus_t status);

/******************************************************************************
 * Cambricon CNNL Data Structure: Descriptor
 * The struct represent input, filter and the AI network layer
 ******************************************************************************/
/*! The descriptor of a tensor that holds the information including tensor
 *  layout, data type, the number of dimensions, shape and strides.
 *
 *  You need to call the ::cnnlCreateTensorDescriptor function to create a descriptor,
 *  and call the ::cnnlSetTensorDescriptor function or the ::cnnlSetTensorDescriptorEx
 *  function to set the tensor information to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlDestroyTensorDescriptor function.
 */
typedef struct cnnlTensorStruct *cnnlTensorDescriptor_t;
/*! The descriptor of Sequence Data that holds the dimensions,
 * layout, data type, sequence length, padding fill, position, and scale.
 * The total size of the tensor descriptor only support less than 2 Giga-elements.
 * You need to call the ::cnnlCreateSeqDataDescriptor function to create a descriptor, and
 * call the ::cnnlSetSeqDataDescriptor to set the sequence data information to the descriptor.
 * If the sequence data is in fixed-point data type, call ::cnnlSetSeqDataDescriptorPositionAndScale
 * to set the position and scale of the sequence data.
 * At last, you need to destroy the descriptor at the end with the ::cnnlDestroySeqDataDescriptor
 * function. */
typedef struct cnnlSeqDataStruct *cnnlSeqDataDescriptor_t;
/*! The descriptor of the ::cnnlSpace2batchNd_v2 and ::cnnlBatch2spaceNd_v2
 * operations that holds operation information including the number of block
 * dimensions and padding dimensions.
 *
 *  You need to call the ::cnnlCreateSpaceBatchNdDescriptor function
 *  to create a descriptor, and call the ::cnnlSetSpaceBatchNdDescriptor
 *  function to set the information of the operation to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with
 *  the ::cnnlDestroySpaceBatchNdDescriptor function.
 */
typedef struct cnnlSpaceBatchNdStruct *cnnlSpaceBatchNdDescriptor_t;
/*! The descriptor of the convolution operation that holds convolution information
 *  including the number of input dimensions, padding, stride, dilation, and group_count.
 *
 *  You need to call the ::cnnlCreateConvolutionDescriptor function
 *  to create a descriptor, and call the ::cnnlSetConvolutionDescriptor function to set
 *  the information of the convolution operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyConvolutionDescriptor function.
 */
typedef struct cnnlConvolutionStruct *cnnlConvolutionDescriptor_t;
/*! The descriptor of the deconvolution operation that holds deconvolution information
 *  including the number of input dimensions, padding, stride, dilation, group_count
 *  and compute_type.
 *
 *  You need to call the ::cnnlCreateDeconvolutionDescriptor function
 *  to create a descriptor, and call the ::cnnlSetDeconvolutionDescriptor function to set
 *  the information of the deconvolution operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyDeconvolutionDescriptor function.
 */
typedef struct cnnlDeconvolutionStruct *cnnlDeconvolutionDescriptor_t;
/*! The descriptor of the matrix multiplication function that holds compute type, bias type, transpose flag, and
 *  other attributes defined in ::cnnlMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlMatMulDescCreate function to create a descriptor, and call the ::cnnlSetMatMulDescAttr
 *  function to set the information of the matrix multiplication to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlMatMulDescDestroy function.
 */
typedef struct cnnlMatMulStruct *cnnlMatMulDescriptor_t;
/*! The descriptor of the matrix multiplication that holds the configured matrix multiplication
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateMatMulHeuristicResult function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulHeuristicResult function.
 */
typedef struct cnnlMatMulHeuristicResult *cnnlMatMulHeuristicResult_t;
/*! The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *  configuration.
 */
typedef struct cnnlMatMulPrefer *cnnlMatMulPrefer_t;
/*! The descriptor of the batch matrix multiplication function that holds compute type, bias type, transpose flag,
 *  and other attributes defined in ::cnnlBatchMatMulDescAttribute_t.
 *
 *  You need to call the ::cnnlBatchMatMulDescCreate function to create a descriptor, and call the ::cnnlSetBatchMatMulDescAttr
 *  function to set the information of the batch matrix multiplication to the descriptor. Also, you need to destroy
 *  the Cambricon CNNL context at the end with the ::cnnlBatchMatMulDescDestroy function.
 */
typedef struct cnnlBatchMatMulStruct *cnnlBatchMatMulDescriptor_t;
/*! The descriptor of the matrix multiplication computation algorithm.
 *
 *  You need to call the ::cnnlMatMulAlgoCreate function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlMatMulAlgoDestroy function.
 */
typedef struct cnnlMatMulAlgoStruct *cnnlMatMulAlgo_t;
/*! The descriptor of the batch matrix multiplication computation algorithm.
 *
 *  You need to call the ::cnnlBatchMatMulAlgoCreate function to create a descriptor, and call the ::cnnlGetQuantizeBatchMatMulAlgorithm
 *  function to set the information to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlBatchMatMulAlgoDestroy function.
 */
typedef struct cnnlBatchMatMulAlgoStruct *cnnlBatchMatMulAlgo_t;
/*! The descriptor of the batch matrix multiplication with broadcasting function that holds compute type, bias type,
 *  transpose flag, and other attributes defined in ::cnnlBatchMatMulBCastDescAttribute_t.
 *
 *  You need to call the ::cnnlBatchMatMulBCastDescCreate function to create a descriptor, and call the
 *  ::cnnlSetBatchMatMulBCastDescAttr function to set the information of the matrix multiplication to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlBatchMatMulBCastDescDestroy function.
 */
typedef struct cnnlBatchMatMulBCastStruct *cnnlBatchMatMulBCastDescriptor_t;
/*! The descriptor of the batch matrix multiplication with broadcasting computation algorithm.
 *
 *  You need to call the ::cnnlBatchMatMulBCastAlgoCreate function to create a descriptor, and call the
 *  ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to set the information to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlBatchMatMulBCastAlgoDestroy function.
 */
typedef struct cnnlBatchMatMulBCastAlgoStruct *cnnlBatchMatMulBCastAlgo_t;
/*! The descriptor of the pooling operation that holds pooling information
 *  including the pooling mode, the NaN propagation mode, the number of input dimensions, padding, window and stride.
 *
 *  You need to call the ::cnnlCreatePoolingDescriptor function
 *  to create a descriptor, and call the ::cnnlSetPooling2dDescriptor, ::cnnlSetPooling2dDescriptor_v2,
 *  ::cnnlSetPoolingNdDescriptor or ::cnnlSetPoolingNdDescriptor_v2 function to set the information of the pooling operation to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyPoolingDescriptor function.
 */
typedef struct cnnlPoolingStruct *cnnlPoolingDescriptor_t;
/*! The descriptor of the activation operation that holds activation information including
 *  the \b activation_desc, \b mode, \b nan_prop, and \b coef.
 *
 *  You need to call the ::cnnlCreateActivationDescriptor function to create a descriptor,
 *  and call the ::cnnlSetActivationDescriptor_v5 function to set the information of the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyActivationDescriptor
 *  function.
 */
typedef struct cnnlActivationStruct *cnnlActivationDescriptor_t;
/*! The descriptor of customized Activation function that holds the activation modes.*/
/*! The descriptor of the customized activation operation that holds customized activation information
 *  including the number of input range, output range, and segment.
 *
 *  You need to call the ::cnnlCreateCustomizedActiveDescriptor function
 *  to create a descriptor, and call the ::cnnlSetCustomizedActiveDescriptor function to set
 *  the information of the customized activation operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyCustomizedActiveDescriptor function.
 */
typedef struct cnnlCustomizedActiveStruct *cnnlCustomizedActiveDescriptor_t;
/*! The descriptor of OpTensor operation that holds OpTensor information including
 *  the ::cnnlOpTensorDesc_t, ::cnnlDataType_t, and ::cnnlNanPropagation_t.
 *
 *  You need to call the ::cnnlCreateOpTensorDescriptor function to create a descriptor,
 *  and call the ::cnnlSetOpTensorDescriptor function to set the information of the
 *  OpTensor operation to the descriptor. Also, you need to destroy the descriptor at the
 *  end with the ::cnnlDestroyOpTensorDescriptor function.
 */
typedef struct cnnlOpTensorStruct *cnnlOpTensorDescriptor_t;
/*! The descriptor of Reduce function that holds ::cnnlReduceOp_t,
 * ::cnnlDataType_t, ::cnnlNanPropagation_t, ::cnnlReduceIndices_t, and ::cnnlIndicesType_t.*/
typedef struct cnnlReduceStruct *cnnlReduceDescriptor_t;
/*! The descriptor of Trigon function that holds trigon function mode and computation preference
 *  mode. You need to call the ::cnnlCreateTrigonDescriptor function to create a descriptor, and
 *  call the ::cnnlSetTrigonDescriptor or ::cnnlSetTrigonDescriptor_v2 to set the trigon function
 *  mode and computation preference to the descriptor. At last, you need to destroy the descriptor
 *  at the end with the ::cnnlDestroyTrigonDescriptor function. */
typedef struct cnnlTrigonStruct *cnnlTrigonDescriptor_t;
/*! The descriptor of the gru operation that holds gru information
 *  including the algorithm defined in ::cnnlGruAlgo_t, flag to determine whether the operation
 *  is bidirectional, and the number of layer.
 *
 *  You need to call the ::cnnlCreateGruDescriptor function
 *  to create a descriptor, and call the ::cnnlSetGruDescriptor function to set
 *  the information of the gru operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyGruDescriptor function.
 */
typedef struct cnnlGruStruct *cnnlGruDescriptor_t;
/*! The descriptor of Attention function that holds cnnlAttentionAlgo_t. */
typedef struct cnnlAttentionStruct *cnnlAttentionDescriptor_t;
/*! The descriptor of multi-head attention function that holds the information required in the
 * multi-head attention operation. You need to call the ::cnnlCreateMultiHeadAttnDescriptor to
 * create a descriptor, and call the ::cnnlSetMultiHeadAttnDescriptor to set the information
 * of the multi-head attention operation to the descriptor. If the filters are in fixed-point data
 * type, you also need to call ::cnnlSetMultiHeadAttnWeightsQuantifyInfo to set the position and
 * scale of all filters. At last, you need to destroy the descriptor at the end with the
 * ::cnnlDestroyMultiHeadAttnDescriptor function.*/
typedef struct cnnlMultiHeadAttnStruct *cnnlMultiHeadAttnDescriptor_t;
/*! The descriptor of the grep operation that holds grep information
 *  including \b begin, \b size, \b space_number, \b mlu_input_dim, \b mlu_begin,
 *  \b mlu_size, \b mlu_mlutiplier, and \b mlu_divisor.
 *
 *  You need to call the ::cnnlSetGrepDescriptor to set the information of the grep operation
 *  to the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyGrepDescriptor function. */
typedef struct cnnlGrepStruct *cnnlGrepDescriptor_t;
/*! The descriptor of the reorg operation that holds reorg information
 *  including the number of reorg height dimension, reorg width dimension and whether to
 *  forward or not.
 *
 *  You need to call the ::cnnlCreateReorgDescriptor function
 *  to create a descriptor, and call the ::cnnlSetReorgDescriptor function to set
 *  the information of the reorg operation to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyReorgDescriptor function.
 */
typedef struct cnnlReorgStruct *cnnlReorgDescriptor_t;
/*! The descriptor of Unique function that holds cnnlUniqueSort_t, dim, return_inverse,
 *  and return_counts.
 *
 *  You need to call the ::cnnlCreateUniqueDescriptor to creat a descriptor,
 *  and call the ::cnnlSetUniqueDescriptor to set the information of the unique operator to
 *  the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyUniqueDescriptor function.*/
typedef struct cnnlUniqueStruct *cnnlUniqueDescriptor_t;
/*! The descriptor of the Nms function that holds ::cnnlNmsOutputMode_t, ::cnnlNmsAlgo_t,
 * iou_threshold, max_output_size, confidence_threshold, offset and input_layout.
 *
 *  You need to call the ::cnnlCreateNmsDescriptor to create a descriptor, and call
 *  the ::cnnlSetNmsDescriptor, ::cnnlSetNmsDescriptor_v2 or ::cnnlSetNmsDescriptor_v3
 *  to set the information of the Nms operation to the descriptor. At last, you need to
 *  destroy the descriptor at the end with the ::cnnlDestroyNmsDescriptor function.*/
typedef struct cnnlNmsStruct *cnnlNmsDescriptor_t;
/*! The function pointer of CustomizedActive function that holds activation function.
 *  In function pointer, input data type and return data type must be double. For example:
 *  ``double relu(double input) {input < 0 ? 0 : input}`` */
typedef double (*active_func_type)(double);
/*! The descriptor of Normalize function that holds the information required in the
 *  normalize operation.
 *
 *  You need to call the ::cnnlCreateNormalizeDescriptor to create
 *  a descriptor, and call the ::cnnlSetNormalizeDescriptor to set the information of
 *  the normalize operation to the descriptor. At last, you need to destroy the descriptor
 *  at the end with the ::cnnlDestroyNormalizeDescriptor function.*/
typedef struct cnnlNormalizeStruct *cnnlNormalizeDescriptor_t;

/*! The descriptor of RNN function that holds the information required in the
 * RNN operation. You need to call the ::cnnlCreateRNNDescriptor to create a descriptor,
 * and call the ::cnnlSetRNNDescriptor set the basic information of the RNN operation to the descriptor.
 * In addition to basic information, you can optionally call the following function to set additional
 * information: ::cnnlSetRNNProjectionLayers, ::cnnlSetRNNPeepholeMode, ::cnnlSetRNNBiasMode,
 * ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip, ::cnnlSetRNNPaddingMode.
 * Also, you need to destroy the descriptor at the end with the
 * ::cnnlDestroyRNNDescriptor function.*/
typedef struct cnnlRNNParam *cnnlRNNDescriptor_t;

/*! The descriptor of the collection of tensor which is used in the RNN operation, such as filter, bias, etc.
 *  You need to call the ::cnnlCreateTensorSetDescriptor function to create a descriptor, and
 *  call the ::cnnlInitTensorSetMemberDescriptor to set the information about each tensor in
 *  the tensor set. If the data type of the tensor in the tensor set is in fixed-point data type,
 *  call ::cnnlInitTensorSetMemberDescriptorPositionAndScale function to set quantization parameters.
 *  At last, you need to destroy the descriptor at the end with the ::cnnlDestroyTensorSetDescriptor
 *  function. */
typedef struct cnnlTensorSetStruct *cnnlTensorSetDescriptor_t;

/*! The descriptor of CTC_Loss function that holds ::cnnlCTCLossNormalizationMode_t,
 *  ::cnnlCTCLossReduceMode_t, ::cnnlCTCLossZeroInfinityMode_t, blank, maximum input length,
 *  maximum label length.
 *
 *  You need to call the ::cnnlCreateCTCLossDescriptor to creat a descriptor,
 *  and call the ::cnnlSetCTCLossDescriptor to set the information of the CTC_Loss operator to
 *  the descriptor. At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyCTCLossDescriptor function.*/
typedef struct cnnlCTCLossStruct *cnnlCTCLossDescriptor_t;

/*! The descriptor of Interp function that holds ::cnnlInterpMode_t,
 *  ::cnnlInterpCoordinateTransformationMode_t, ::cnnlInterpRoundMode_t, scale_factors, pad,
 *  cubic_coeff_a and exclude_outside.
 *
 *  You need to call the ::cnnlCreateInterpDescriptor to creat a descriptor,
 *  and call the ::cnnlSetInterpDescriptor to set the information of the Interp operation to
 *  the descriptor. When the extra parameters are used, you need to call the ::cnnlSetInterpDescriptorEx.
 *  When the extra parameters are not used, you don't need to call the ::cnnlSetInterpDescriptorEx.
 *  At last, you need to destroy the descriptor at the end with the
 *  ::cnnlDestroyInterpDescriptor function.*/

typedef struct cnnlInterpStruct *cnnlInterpDescriptor_t;

/*! The descriptor of deformable convolution function that holds the deformable convolution
 * information including the number of input dimensions, padding, stride, dilation,
 * deformable group, convolution group, and img2col_step.
 *
 * You need to call the ::cnnlCreateDCNDescriptor function to create a descriptor, and call the
 * ::cnnlSetDCNDescriptor function to set the information of the deformable convolution operation
 * to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyDCNDescriptor function.
 */
typedef struct cnnlDCNStruct *cnnlDCNDescriptor_t;

/*! The descriptor of CARAFE (Content-Aware ReAssembly of FEatuers) operator that holds
 * CARAFE information including the number of input dimensions, kernel size, group size,
 * and scale factor.
 *
 * You need to call the ::cnnlCreateCarafeDescriptor function to create a descriptor,
 * and call the ::cnnlSetCarafeDescriptor function to set the information of the CARAFE operator
 * to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyCarafeDescriptor function.
 */
typedef struct cnnlCarafeStruct *cnnlCarafeDescriptor_t;

/*! The descriptor of FFT (Fast Fourier Transform) operation that holds FFT information including
 * the tensor descriptor of input tensor and output tensor, the rank of FFT, the FFT size on each
 * dimension, the size of reserved space and the size of workspace.
 *
 * You need to call the ::cnnlCreateFFTPlan function to create a descriptor for the FFT operation, and call
 * the ::cnnlMakeFFTPlanMany function to set the information of the FFT operation to the descriptor.
 * Then, you need to allocate the reserved space and set the space to the FFT descriptor by ::cnnlSetReserveArea.
 * Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyFFTPlan.
 */
typedef struct cnnlFFTStruct *cnnlFFTPlan_t;

/*! The descriptor of grid_sample operation that holds the information
 *  including the interpolation mode, padding mode and align_corners mode.
 *
 * You need to call the ::cnnlCreateGridSampleDescriptor function to create a descriptor,
 * and call the ::cnnlSetGridSampleDescriptor function to set the information of the grid_sample
 * operation to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyGridSampleDescriptor function.
 */
typedef struct cnnlGridSampleStruct *cnnlGridSampleDescriptor_t;

// Group:Tensor
/*!
 *  @brief Gets the size of a data type in ::cnnlDataType_t.
 *
 *  @param[in] data_type
 *    Input. The data type. For detailed information, see ::cnnlDataType_t.
 *  @param[out] size
 *    Output. Host pointer to the size of the data type.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSizeOfDataType(cnnlDataType_t data_type, size_t *size);

// Group:Tensor
/*!
 *  @brief Creates a tensor descriptor pointed by \b desc that holds the dimensions, data type,
 *  and layout of input tensor. If the input tensor is in fixed-point data type,
 *  the ::cnnlSetTensorDescriptorPositionAndScale function or the ::cnnlSetTensorDescriptorPosition
 *  function needs to be called to set quantization parameters.
 *
 *  The ::cnnlDestroyTensorDescriptor function needs to be called to destroy the
 *  tensor descriptor later.
 *
 *  @param[in] desc
 *    Input. Pointer to the struct that holds information about the tensor descriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTensorDescriptor(cnnlTensorDescriptor_t *desc);

// Group:Tensor
/*!
 *  @brief Creates a group of tensor descriptors stored by \b group_desc that holds the dimensions,
 *  data type, and layout of input tensors. If the input tensor is in fixed-point data type,
 *  the ::cnnlSetTensorDescriptorPositionAndScale function or the ::cnnlSetTensorDescriptorPosition
 *  function need to be called to set quantization parameters.
 *
 *  @param[in] group_desc
 *    Input. An array of pointers to the structs that hold information about the tensor descriptors.
 *  @param[in] desc_num
 *    Input. The length of the input array \b group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlDestroyTensorDescriptor function needs to be called for each descriptor to destroy all tensors
 *   in group_desc or the ::cnnlDestroyGroupTensorDescriptors needs to be called to destroy the all tensor
 *   descriptors in group_desc later.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                           const int desc_num);

// Group:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \b desc that is previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, data type, and layout of the input tensor.
 *
 *  If ::cnnlSetTensorDescriptor is called, you do not need to specify the strides of all
 *  dimensions. The strides are inferred by parameters passed to this function. Also, the data
 *  will be treated as contiguous in memory with no padding between dimensions. To specify the
 *  strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx. But the data might not
 *  be treated as contiguous in memory.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *    the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 *  - This function cannot be called continuously. You need to call ::cnnlResetTensorDescriptor
 *    before calling another ::cnnlSetTensorDescriptor to avoid memory leaks.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptor(cnnlTensorDescriptor_t desc,
                                                  cnnlTensorLayout_t layout,
                                                  cnnlDataType_t dtype,
                                                  int dimNb,
                                                  const int dimSize[]);
// Group:Tensor
/*!
 *  @brief Initializes the group of tensor descriptors stored by \b group_desc that is previously
 *  created with the ::cnnlCreateTensorDescriptor function or ::cnnlCreateGroupTensorDescriptors
 *  function, and sets the information about the dimensions, data type, and layout of all the
 *  input tensors.
 *
 *  If ::cnnlSetTensorDescriptor or ::cnnlSetGroupTensorDescriptors is called, you do not need
 *  to specify the strides of all dimensions. The strides are inferred by parameters passed to
 *  this function. Also, the data will be treated as contiguous in memory with no padding between
 *  dimensions. To specify the strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx.
 *  But the data might not be treated as contiguous in memory.
 *
 *  @param[in] group_desc
 *    Input. An array of pointers to the struct that hold information about the tensor descriptor.
 *  @param[in] group_layout
 *    Input. An array that stores the layouts of all input tensors. For detailed information, see
 *    ::cnnlTensorLayout_t.
 *  @param[in] group_dtype
 *    Input. An array that stores the data types of all input tensors. For detailed information, see
 *    ::cnnlDataType_t.
 *  @param[in] group_dimNb
 *    Input. An array that stores the dimensions of all input tensors.
 *  @param[in] group_dimSize
 *    Input. An array that stores the size of each dimension of all tensors.
 *  @param[in] desc_num
 *    Input. The length of the input array \b group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The group_dimSize includes dimensions of all tensors. You need to store the dimension of each
 *  tensor one by one in order. For example, If we have three tensors, the first tensor dimension is
 *  [3,4,5,6], the second tensor dimension is [9,7,8], and the third tensor dimension is [4,7], the
 *  group_dimSize should be [3,4,5,6,9,7,8,4,7].
 *  - For better performance, there is no overflow check in this function. Please make sure that the
 *  size of each tensor is in the range of [0, 2^31]. Otherwise, you will get wrong result.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                        const cnnlTensorLayout_t group_layout[],
                                                        const cnnlDataType_t group_dtype[],
                                                        const int group_dimNb[],
                                                        const int group_dimSize[],
                                                        const int desc_num);
// Group:Tensor
/*!
 *  @brief Resets the tensor descriptor pointed by \b desc that is previously created
 *  with the ::cnnlCreateTensorDescriptor function.
 *
 *  If ::cnnlResetTensorDescriptor is called, all the information about the tensor will be reset to
 *  initial value, which means layout is CNNL_LAYOUT_ARRAY, dtype is CNNL_DTYPE_FLOAT, dimsNb is 0,
 *  and dimSize points to an \b CNNL_DIM_MAX-dimension array.
 *
 *  @param[in] desc
 *    Input. The descriptor of the tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This function is used to avoid memory leaks when more than one ::cnnlSetTensorDescriptor
 *    function is called. You should call this function before calling another
 *    ::cnnlSetTensorDescriptor
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlResetTensorDescriptor(cnnlTensorDescriptor_t desc);

// Group:Tensor
/*!
 *  @brief Initializes the tensor descriptor pointed by \b desc that is previously created
 *  with the ::cnnlCreateTensorDescriptor function, and sets the information about
 *  the dimensions, strides, data type, and layout of the input tensor.
 *
 *  Compare with ::cnnlSetTensorDescriptor, you can specify the strides of all dimensions with
 *  this function. If ::cnnlSetTensorDescriptor is called, you do not need to specify the
 *  strides of all dimensions and the strides are inferred by parameters passed to this function.
 *
 *  This function does not support all the operations in this version. You can check
 *  if an operation supports this function in the "note" section of the operation description.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] layout
 *    Input. The layout of the input tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the input tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the initialized operation.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the tensor for each dimension.
 *  @param[in] dimStride
 *    Input. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents
 *    the lowest dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorEx(cnnlTensorDescriptor_t desc,
                                                    cnnlTensorLayout_t layout,
                                                    cnnlDataType_t dtype,
                                                    int dimNb,
                                                    const int dimSize[],
                                                    const int dimStride[]);
// Group:Tensor
/*!
 * @brief Sets the \b dimNb and \b dimSize factors to the input tensor descriptor.
 *
 * If ::cnnlSetTensorDescriptorDim is called, you do not need to specify the strides of all
 * dimensions. The strides are inferred by parameters passed to this function. Also, the data
 * will be treated as contiguous in memory with no padding between dimensions. To specify the
 * strides of all dimensions, you can call ::cnnlSetTensorDescriptorEx. But the data might not
 * be treated as contiguous in memory.
 *
 * @param[in] desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the initialized operation.
 * @param[in] dimSize
 *   Input. An array that contains the size of the tensor for each dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @note
 * - dimSize[0] represents the highest dimension, dimSize[DIM_MAX - 1] represents
 *   the lowest dimension, and DIM_MAX represents the number of dimensions in the input tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlSetTensorDescriptorDim(cnnlTensorDescriptor_t desc,
                                        int dimNb,
                                        const int *dimSize);
// Group:Tensor
/*!
 *  @brief Sets the on-chip data type to the descriptor of a tensor \b desc.
 *  The on-chip data type \b onchip_dtype can be different from the off-chip data type of the tensor.
 *  This function is optional. If the on-chip data type is not set with this function, the
 *  ::CNNL_STATUS_BAD_PARAM data type is used by default.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. The on-chip data type of the tensor used in the operations that support fixed-point computing.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The on-chip data type is only used on the operations that support fixed-point computing. It
 *    has no effect on other operations. If you call this function to get on-chip data type for an
 *    operation that does not support fixed-point computing, ::CNNL_STATUS_BAD_PARAM is returned. To check
 *    if an operation supports fixed-point computing, see the detailed description of the operation.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorOnchipDataType(cnnlTensorDescriptor_t desc,
                                                                cnnlDataType_t onchip_dtype);
// Group:Tensor
/*!
 * @brief Sets the \b position factor to the descriptor \b desc of fixed-point data in
 * fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION mode. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \b position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
*/
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorPosition(cnnlTensorDescriptor_t desc,
                                                          int position);
// Group:Tensor
/*!
 * @brief Sets the \b position and \b scale factors to the descriptor of fixed-point data in
 * fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION_SCALE mode. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \b position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
*/
cnnlStatus_t CNNL_WIN_API cnnlSetTensorDescriptorPositionAndScale(cnnlTensorDescriptor_t desc,
                                                                  int position,
                                                                  float scale);
// Group:Tensor
/*!
 * @brief Sets the \b position, \b scale and \b offset factors to the descriptor of fixed-point
 * data in fixed-point quantization. It is used in ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET mode.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] position
 *   Input. A scalar of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @param[in] offset
 *   Input. A scalar of offset factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \b position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
*/
cnnlStatus_t CNNL_WIN_API
cnnlSetTensorDescriptorPositionScaleAndOffset(cnnlTensorDescriptor_t desc,
                                              int position,
                                              float scale,
                                              int offset);
// Group:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \b desc that is previously created with the
 *  ::cnnlCreateTensorDescriptor function, and sets the information about the dimensions,
 *  data type, and layout of input tensor.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptor(const cnnlTensorDescriptor_t desc,
                                                  cnnlTensorLayout_t *layout,
                                                  cnnlDataType_t *dtype,
                                                  int *dimNb,
                                                  int dimSize[]);
// Group:Tensor
/*!
 *  @brief Retrieves a tensor descriptor \b desc that is previously created with the
 *  ::cnnlCreateTensorDescriptor and sets the information about the dimensions, data type,
 *  stride and layout of input tensor with ::cnnlSetTensorDescriptorEx.
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] layout
 *    Output. Pointer to the host memory that holds information about the layout of the input tensor.
 *  For detailed information, see ::cnnlTensorLayout_t.
 *  @param[out] dtype
 *    Output. Pointer to the host memory that holds information about the data type of the input tensor.
 *  For detailed information, see ::cnnlDataType_t.
 *  @param[out] dimNb
 *    Output. Pointer to the host memory that holds information about the dimension of input tensor.
 *  @param[out] dimSize
 *    Output. An array that contains the size of the tensor for each dimension.
 *  @param[out] dimStride
 *    Output. An array that contains the stride of the tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - dimSize[0] represents the highest dimension, and dimSize[DIM_MAX - 1] represents the lowest
 *    dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorEx(const cnnlTensorDescriptor_t desc,
                                                    cnnlTensorLayout_t *layout,
                                                    cnnlDataType_t *dtype,
                                                    int *dimNb,
                                                    int dimSize[],
                                                    int dimStride[]);

// Group:Tensor
/*!
 *  @brief Retrieves the number of elements according to the input descriptor \b desc. You
 *  need to call the ::cnnlSetTensorDescriptor function first to create a tensor descriptor
 *  before calling this function.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @return
 *  - ::CNNL_STATUS_SUCCESS
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
     @verbatim
      cnnlTensorDescriptor_t input_desc;
      cnnlCreateTensorDescriptor(&input_desc);
      cnnlSetTensorDescriptor(input_desc, CNNL_LAYOUT_ARRAY,CNNL_DTYPE_FLOAT, 2,{2, 3});
      size_t nums=cnnlGetTensorElementNum(input_desc);  // nums = 6

      input one array by 2 * 3
      input: [[1,2,3],[4,5,6]]
      output: 6
     @endverbatim
 */
size_t CNNL_WIN_API cnnlGetTensorElementNum(const cnnlTensorDescriptor_t desc);

// Group:Tensor
/*!
 *  @brief Retrieves the on-chip data type of a tensor descriptor \b desc set by
 *  ::cnnlSetTensorDescriptorOnchipDataType.
 *  If the on-chip data type is not set with the ::cnnlSetTensorDescriptorOnchipDataType function,
 *  the ::CNNL_STATUS_BAD_PARAM is returned.
 *
 *  @param[in] desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. Pointer to the MLU memory that holds information about the on-chip data type of the tensor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The on-chip data type is only used on the operations that support fixed-point computing. It
 *    has no effect on other operations. If you call this function to get on-chip data type for an
 *    operation that does support fixed-point computing, ::CNNL_STATUS_BAD_PARAM is returned. To check
 *    if an operation supports fixed-point computing, see the detailed description of the operation.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorOnchipDataType(const cnnlTensorDescriptor_t desc,
                                                                cnnlDataType_t *onchip_dtype);
// Group:Tensor
/*!
 * @brief Gets the \b position factor to the descriptor \b desc of fixed-point data in
 * fixed-point quantization. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] position
 *   Output. A host pointer of fixed position factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
*/
cnnlStatus_t CNNL_WIN_API cnnlGetTensorDescriptorPosition(const cnnlTensorDescriptor_t desc,
                                                          int *position);
// Group:Tensor
/*!
 *  @brief Gets the position and scale factors of a tensor descriptor \b desc used in
 *  fixed-point quantization. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 *  @param[in] desc
 *    Input. The descriptor of the input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] position
 *    Output. Pointer to the MLU memory that holds information about fixed position
 *    used for quantization.
 *  @param[out] scale
 *    Output. Pointer to the MLU memory that holds information about scale factor
 *    used for quantization.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorDescriptorPositionAndScale(const cnnlTensorDescriptor_t desc,
                                        int *position,
                                        float *scale);
// Group:Tensor
/*!
 * @brief Gets the \b position, \b scale and \b offset factors to the descriptor \b desc of
 * fixed-point data in fixed-point quantization. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. The descriptor of the tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] position
 *   Output. A host pointer of fixed position factor that is used for quantization.
 * @param[out] scale
 *   Output. A host pointer of scale factor that is used for quantization.
 * @param[in] offset
 *   Output. A host pointer of offset factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
*/
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorDescriptorPositionScaleAndOffset(const cnnlTensorDescriptor_t desc,
                                              int *position,
                                              float *scale,
                                              int *offset);
// Group:Tensor
/*!
 *  @brief Destroies a tensor descriptor that was created by
 *         ::cnnlCreateTensorDescriptor.
 *
 *  @param[in] desc
 *    Input. A tensor descriptor created by ::cnnlCreateTensorDescriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTensorDescriptor(cnnlTensorDescriptor_t desc);

// Group:Tensor
/*!
 *  @brief Destroys a group of tensor descriptors that was created by
 *         ::cnnlCreateTensorDescriptor or ::cnnlCreateGroupTensorDescriptors.
 *
 *  @param[in] group_desc
 *    Input. An array of pointers to the struct that hold information about the tensor descriptor.
 *  @param[in] desc_num
 *    Input. The length of the input array \b group_desc.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGroupTensorDescriptors(cnnlTensorDescriptor_t *group_desc[],
                                                            const int desc_num);

// Group:SeqData
/*!
 *  @brief Creates a sequence data instance \b seq_data_desc that holds the dimensions, data type,
 *  sequence lengths, padding fill and layout of sequence data on the host memory.
 *
 *  Use ::cnnlSetSeqDataDescriptor to configure the descriptor and ::cnnlDestroySeqDataDescriptor
 *  function to destroy the sequence data descriptor.
 *
 *  @param[in] seq_data_desc
 *    Input. Pointer to the host memory that holds information about
 *  the struct of the sequence data descriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateSeqDataDescriptor(cnnlSeqDataDescriptor_t *seq_data_desc);

// Group:SeqData
/*!
 * @brief Sets the sequence data descriptor \b seq_data_desc that holds the dimensions,
 * data type, sequence lengths, padding fill and layout of the sequence data.
 *
 * The number of dimensions in the \b dimSize[] is defined by \b dimNb. For example,
 * if the layout of the sequence data is set to ::CNNL_SEQDATA_NC, the \b dimNb is 2, with \b dimSize={batch, embedding}.
 *
 * The ::cnnlSeqDataDescriptor_t container is a collection of fixed-length sequential
 * vectors, similar to the words constructing sentences. The T dimension described in the
 * ::cnnlSeqDataLayout_t is the time dimension. Different sequences are bundled together to a
 * batch. The beam dimension described in the ::cnnlSeqDataLayout_t is
 * different candidates presenting a similar meaning in a typical translation task. The original
 * sentence can be translated to several versions before picking the optimal one, and the number
 * of candidates is beam.
 *
 * Note that different sentences have different sequence lengths, even inside a beam.
 * \b seqLengthArray is to record the real sequence lengths before padding to the maximum sequence
 * length. The value of \b seqLengthArray should follow a batch-beam order, in despite of
 * sequence data layout. Take a sequence of batch=3, beam=2 for example, the \b seqLengthArray
 * should be as following
@verbatim
 {batch_idx = 0, beam_idx = 0}
 {batch_idx = 0, beam_idx = 1}
 {batch_idx = 1, beam_idx = 0}
 {batch_idx = 1, beam_idx = 1}
 {batch_idx = 2, beam_idx = 0}
 {batch_idx = 2, beam_idx = 1}
@endverbatim
 * If the real sequence lengths are not requested, pass NULL to \b seqLengthArray in this function.
 *
 * The \b seqLengthArraySize should be batch * beam, which is 6 in the example above.
 *
 * The \b PaddingFill describes whether the sequence data needs to be padded using a
 * specified value. In the multi-head attention operation, the padding part should be zero before
 * entering the attention part to ensure the result validity. If the sequence data is padding
 * zero in advance, pass NULL to \b PaddingFill in this function. Otherwise, pass a pointer to padding
 * value (e.g. float a = 0, &a) to \b PaddingFill to indicate this function that extra padding are
 * needed.
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] layout
 *   Input. The layout of the sequence data. See ::cnnlSeqDataLayout_t for the description of the
 *   enumeration type.
 * @param[in] dtype
 *   Input. The data type of the sequence data. See ::cnnlDataType_t for the description of the
 *   enumeration type.
 * @param[in] dimNb
 *   Input. The number of dimensions of the sequence data.
 * @param[in] dimSize
 *   Input. An array that contains the size of the sequence data for each dimension.
 * @param[in] seqLengthArraySize
 *   Input. Number of elements in sequence length array, \b seqLengthArray[]. It should be
 *   batch * beam. The batch and beam are described in the ::cnnlSeqDataLayout_t.
 * @param[in] seqLengthArray
 *   Input. An integer array recording the length of all sequences. Note that the array should be
 *   set in the batch-beam order, in despite of sequence data layout. Set this parameter to NULL
 *   when sequence length array is not requested.
 * @param[in] paddingFill
 *   Input. A host pointer to the data type \b dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *   The \b seqLengthArray and \b PaddingFill of the descriptors of
 *   queries, keys, values, and outputs can only be NULL in the
 *   ::cnnlMultiHeadAttnForward function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetSeqDataDescriptor(cnnlSeqDataDescriptor_t seq_data_desc,
                                                   cnnlSeqDataLayout_t layout,
                                                   cnnlDataType_t dtype,
                                                   int dimNb,
                                                   const int dimSize[],
                                                   int seqLengthArraySize,
                                                   const int seqLengthArray[],
                                                   void *paddingFill);

// Group:SeqData
/*!
 * @brief Sets the position \b position and scale \b scale factors used in fixed-point quantization.
 * It is only used if you have quantized the input data with the symmetric fixed-point
 * quantization with scale factor quantization method. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] position
 *   Input. An integer of fixed position factor that is used for quantization.
 * @param[in] scale
 *   Input. A scalar of scale factor that is used for quantization.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor
 *   should be called.
 *
 * @note
 * - If the sequence data is in fixed-point data type, you need to call this function.
 *   This function is only used in the inference mode.
 * - The \b position should be limited in [-128, 127], otherwise the result is undefined.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSeqDataDescriptorPositionAndScale(cnnlSeqDataDescriptor_t seq_data_desc,
                                         int position,
                                         float scale);

// Group:SeqData
/*!
 * @brief Retrieves the sequence data descriptor \b seq_data_desc that holds the dimensions,
 * data type, layout, padding fill and the sequence lengths of the input sequence data.
 *
 * @param[in] seq_data_desc
 *   Input. The descriptor of the sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] layout
 *   Output. The layout of the sequence data. See ::cnnlSeqDataLayout_t.
 * @param[out] dtype
 *   Output. The data type of the sequence data. See ::cnnlDataType_t.
 * @param[out] dimNb
 *   Output. The number of dimensions of the sequence data.
 * @param[out] dimSize
 *   Output. An array that contains the size of the sequence data for each dimension.
 * @param[out] seqLengthArraySize
 *   Output. Number of elements in sequence length array, \b seqLengthArray[]. It is equal to
 *   batch * beam (N and B described in the ::cnnlSeqDataLayout_t).
 * @param[out] seqLengthArray
 *   Output. An integer array recording the length of all sequences. Note that the array is
 *   ordered in a batch-beam order, in despite of sequence data layout. Return NULL when sequence
 *   length array is not set.
 * @param[out] paddingFill
 *   Output. A host pointer to the data type \b dtype to fill up the padding vectors within
 *   the valid length of each sequence. Use NULL when extra padding is not requested.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor
 *   should be called.
 *
 * @note
 * - dimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *   the lowest dimension.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSeqDataDescriptor(const cnnlSeqDataDescriptor_t seq_data_desc,
                                                   cnnlSeqDataLayout_t* layout,
                                                   cnnlDataType_t* dtype,
                                                   int* dimNb,
                                                   int dimSize[],
                                                   int* seqLengthArraySize,
                                                   int seqLengthArray[],
                                                   void* paddingFill);

// Group:SeqData
/*!
 * @brief Destroys a sequence data descriptor \b seq_data_desc that was created by
 * ::cnnlCreateSeqDataDescriptor.
 *
 * @param[in] seq_data_desc
 *   Input. A sequence data descriptor created by ::cnnlCreateSeqDataDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroySeqDataDescriptor(cnnlSeqDataDescriptor_t seq_data_desc);

// Group:SeqData
/*!
 *  @brief Sets the on-chip data type to the descriptor of a data sequence \b desc.
 *  The on-chip data type \b onchip_dtype can be different from the off-chip data type of the data.
 *
 *  @param[in] desc
 *    Input. The descriptor of sequence data. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[in] onchip_dtype
 *    Input. The on-chip data type of the sequence data used in the operations that supports
 *    fixed-point computation.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - This function is optional, the on-chip data type is only used on the operations that supports
 *    fixed-point computation, and it has no effect on other operations.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetSeqDataDescriptorOnchipDataType(cnnlSeqDataDescriptor_t desc,
                                       cnnlDataType_t onchip_dtype);
// Group:SeqData
/*!
 *  @brief Retrieves the on-chip data type of sequence data set in the descriptor \b desc
 *         with ::cnnlSetSeqDataDescriptorOnchipDataType.
 *
 *  @param[in] desc
 *    Input. The descriptor of sequence data. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[out] onchip_dtype
 *    Output. Pointer to host memory where the on-chip data type saved.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - The on-chip data type is only used on the operations that support fixed-point computation,
 *    has no effect on other operations. If you call this function to get on-chip data type for an
 *    operation that does support fixed-point computation, ::CNNL_STATUS_BAD_PARAM is returned.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSeqDataDescriptorOnchipDataType(cnnlSeqDataDescriptor_t desc,
                                       cnnlDataType_t *onchip_dtype);

// Group:TensorSet
/*!
 *  @brief Creates a descriptor \b tensorSetDesc of tensor set that holds a series of tensors.
 *  The number of tensors of tensor set is jointly determined by \b setDimNb and \b setDimSize.
 *  Use ::cnnlInitTensorSetMemberDescriptor to set information for descriptor
 *  and ::cnnlDestroySeqDataDescriptor function to destroy the tensor set descriptor.
 *
 *  @param[out] tensorSetDesc
 *    Input. Pointer to the memory that holds information about the descriptor of tensor set.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] setDimSize
 *    Input. An array that contains the number of the tensors for each dimension of the tensor set.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 * - After calling this function, you can call the ::cnnlInitTensorSetMemberDescriptor function to initialize
 *   and set the information to the tensor set descriptor.
 * - You need to call the ::cnnlDestroyTensorSetDescriptor function to destroy the descriptor.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateTensorSetDescriptor(cnnlTensorSetDescriptor_t *tensorSet,
                              const int setDimNb,
                              const int setDimSize[]);

// Group:TensorSet
/*!
 *  @brief Retrieves a tensor set descriptor \b tensorSetDesc that is previously created with the
 *  ::cnnlCreateTensorSetDescriptor function.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of the tensor set. For detailed information,
 *    see ::cnnlSeqDataDescriptor_t.
 *  @param[out] setDimNb
 *    Output. The number of dimensions of the tensor set.
 *  @param[out] setDimSize
 *    Output. An array that contains the number of the tensor for each dimension of the tensor set.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par API Dependency
 *  - Before calling this function, ::cnnlCreateTensorSetDescriptor should be called.
 *
 *  @note
 *  - setDimSize[0] represents the highest dimension, and dimSize[dimNb - 1] represents
 *    the lowest dimension.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorSetDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc,
                           int *setdimNb,
                           int setDimSize[]);

// Group:TensorSet
/*!
 *  @brief Destroys a tensor set descriptor \b tensorSetDesc that is previously created by
 *  ::cnnlCreateTensorSetDescriptor.
 *
 *  @param[in] desc
 *    Input. A tensor descriptor created by ::cnnlCreateTensorSetDescriptor.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - This function should be called to destroy the tensor set descriptor.
 *    Otherwise, the memory leak may occur.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyTensorSetDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc);

// Group:TensorSet
/*!
 *  @brief Initializes a member tensor in the tensor set descriptors pointed by \b desc that is previously created
 *  with the ::cnnlCreateTensorSetDescriptor function, and sets the information about
 *  the dimensions, data type, and layout.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of the tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the index of each dimension of a member tensor to be initialized in the tensor set.
 *  @param[in] layout
 *    Input. The layout of the member tensor. For detailed information, see ::cnnlTensorLayout_t.
 *  @param[in] dtype
 *    Input. The data type of the member tensor. For detailed information, see ::cnnlDataType_t.
 *  @param[in] dimNb
 *    Input. The number of dimensions in the member tensor.
 *  @param[in] dimSize
 *    Input. An array that contains the size of the member tensor for each dimension.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - Before calling this function,
 *   You need to call the ::cnnlCreateTensorSetDescriptor functions to create the tensor descriptors
 *   \b tensorSetDesc.
 *  - All member tensors in the tensor set need to call this function to initialize related properties.
 *  - dimSize[0] and dimSize[DIM_MAX - 1] represent the highest and lowest dimension respectively, where
 *     DIM_MAX is the number of dimensions in the input tensor.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitTensorSetMemberDescriptor(cnnlTensorSetDescriptor_t tensorSetDesc,
                           const int setDimNb,
                           const int tensorIndex[],
                           cnnlTensorLayout_t layout,
                           cnnlDataType_t dtype,
                           const int dimNb,
                           const int dimSize[]);

// Group:TensorSet
/*!
 *  @brief Sets the position and scale factors used in fixed-point quantization.
 *  It is only used if you have quantized the input data with the symmetric fixed-point
 *  quantization with scale factor quantization method. For more information about quantization,
 *  see "Cambricon CNNL User Guide".
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] setDimNb
 *    Input. The number of dimensions of the tensor set.
 *  @param[in] tensorIndex
 *    Input. An array that contains the position index information of the member tensor in the tensor set.
 *  @param[in] position
 *    Input. A position of fixed position factor that is used for quantification.
 *  @param[in] scale
 *    Input. A scalar of scale factor that is used for quantification.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - If the member tensor is in floating-point data type, and  you need to call this function.
 *  - If the member tensor is in fixed-point data type, and  you need to call this function.
 *  - Before calling this function,
 *   You need to call the ::cnnlCreateTensorSetDescriptor functions to create the tensor descriptors
 *   \b tensorSetDesc.
 * - The \b position should be limited in [-128, 127], otherwise the result is undefined.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitTensorSetMemberDescriptorPositionAndScale(cnnlTensorSetDescriptor_t tensorSetDesc,
                                                  const int setDimNb,
                                                  const int tensorIndex[],
                                                  const int position,
                                                  const float scale);

// Group:TensorSet
/*!
 *  @brief Retrieves the size of tensor set according to the input descriptor \b tensorSetDesc. You
 *  need to call the ::cnnlInitTensorSetMemberDescriptor function first to create a tensor set descriptor
 *  before calling this function.
 *
 *  @param[in] desc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[Out] sizeInBytes
 *    Output. Size in bytes of tensor set.
 *    You can allocate MLU memory for the tensor set with this value.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorSetDescriptorSize(cnnlTensorSetDescriptor_t tensorSetDesc,
                               int *sizeInBytes);

// Group:TensorSet
/*!
 *  @brief Retrieves the tensor descriptor in the tensor set and the corresponding offset address
 *         based on the entire block of MLU memory through the index \b tensorIndex.
 *
 *  @param[in] tensorSetDesc
 *    Input. The descriptor of tensor set. For detailed information,
 *    see ::cnnlTensorSetDescriptor_t.
 *  @param[in] tensorIndex
 *    Input. An array that contains the position information of the member tensor in the tensor set.
 *  @param[in] data
 *    Input. Pointer to the MLU memory that is described by \b tensorSetDesc.
 *  @param[out] tensorDesc
 *    Output. Pointer to the host member. It is member tensor descriptor that indexed by \b tensorIndex in the tensor set.
 *    \b *tensorDesc contains tensor member information about dimensions, layout, data type, position and scale.
 *  @param[out] dataAddrInDevice
 *    Output. Pointer to the MLU memory that indexed by \b tensorIndex in the whole block of data
 *    \b dataAddrInDevice.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetTensorAndDataFromTensorSet(cnnlTensorSetDescriptor_t tensorSetDesc,
                                  const int setDimNb,
                                  const int tensorIndex[],
                                  void *data,
                                  cnnlTensorDescriptor_t *tensorDesc,
                                  void **dataAddrInDevice);

// Group:Version Management
/*!
 * @brief Retrieves the version of Cambricon CNNL library.
 *
 * @deprecated
 *   ::cnnlGetVersion is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGetLibVersion instead.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
size_t CNNL_WIN_API cnnlGetVersion(void);

// Group:Version Management
/*!
 * @brief Retrieves the version of Cambricon CNNL library. The version of Cambricon CNNL is composed
 * of \b major, \b minor and \b patch. For instance, major = 1, minor = 2, patch = 3,
 * the version of Cambricon CNNL library is 1.2.3.
 *
 * @param[in] major
 * Input. A pointer to scale factor that gets the major version of Cambricon CNNL library.
 * @param[in] minor
 * Input. A pointer to scale factor that gets the minor version of Cambricon CNNL library.
 * @param[in] patch
 * Input. A pointer to scale factor that gets the patch version of Cambricon CNNL library.
 *
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void cnnlGetLibVersion(int* major,
                       int* minor,
                       int* patch);

// Group:Debugging
/*!
 * @brief Sets the mode of a Cambricon CNNL debugging tool that can generate operator
 * information files for all the operators that are called. The generated file
 * contains the operator information including inputs shapes, outputs shapes,
 * parameters and inputs real data based on the setting of \b mode. For more
 * information, see "Cambricon CNNL User Guide".
 *
 * @param[in] mode
 * Input. The parameter determines what mode the Cambricon CNNL debugging tool will turn on.
 *
 * @note
 * - When \b mode is set to 0, the Cambricon CNNL debugging tool will turn off, and do not
 *   generate operator information files.
 *
 * - When \b mode is set to 1, the Cambricon CNNL debugging tool will generate operator
 *   information files for all the operators that are called. And the inputs real
 *   data of the operators is not included in the files.
 *
 * - When \b mode is set to 2, the Cambricon CNNL debugging tool will generate operator
 *   information files for all the operators that are called. And the inputs real
 *   data of the operators is included in the files.
 *
 * - When \b mode is set to 3, the Cambricon CNNL debugging tool will print operator
 *   information on the screen without the inputs real data for all the operators
 *   that are called instead generating information files.
 *
 * - When \b mode is out of range [0, 3], the Cambricon CNNL debugging tool will turn off,
 *   and do not generate operator information files.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
void cnnlSetGenCaseMode(int mode);

// Group:IndexSelect
/*!
 * @brief Retrieves a new tensor, which indexes the input tensor \b input along \b dim using the
 *  entries in index tensor \b index. The \b index tensor is with the data type of integer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the input tensor to be indexed.
 * @param[in] input_desc
 *   Input. The descriptors of the \b input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptors of the \b index tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index data.
 * @param[in] output_desc
 *   Input. The descriptors of the \b output tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Index Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Note that the combinations of input tensor and output tensor must be half-half
 *   or float-float or int32-int32 or int16-int16.
 * - \b input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - \b output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - \b dim: int32
 * - \b index: int32, int64
 *
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Reference
 * https://pytorch.org/docs/1.0.0/torch.html?highlight=index_select#torch.index_select
 *
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index select operation is as follows:
     @verbatim
     input array
       input = [[0, 1, 2, 3],
                [4, 5, 6, 7],
                [8, 9, 10, 11]]
     index array
       index = [0, 2]
     dim = 0
     output array
       output = [[0, 1, 2, 3],
                 [8, 9, 10, 11]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlIndexSelect(cnnlHandle_t handle,
                                          const int dim,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t index_desc,
                                          const void *index,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output);
// Group:Transform
/*!
 * @brief Transforms linearly for an input tensor with the following formula:
 *
 * output = alpha * input + beta
 *
 * where \b input and \b output are tensors, and \b alpha and \b beta are the scaling factors
 * used in the operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input.  A host pointer to scaling factor of tensor input.
 * @param[in] beta
 *   Input.  A host pointer to bias factor of tensor input.
 * @param[out] output
 *   Output. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Transform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for input tensor \b input and output tensor
 *   \b output must be half-half, float-float or int32-int32.
 * - \b alpha and \b beta: If the data type of tensors is float or half, the data
 *   type of \b alpha and \b beta should be float*. If the data type of tensors is
 *   int32, the data type of \b alpha and \b beta should be int*.
 *
 * @par Scale Limitation
 * - The tensors descriptor of input and output tensors must be the same.
 * - The value of input and output tensors, which data type is int32, should be in
 *   the range of [-2^23, 2^23] on MLU200 series.
 * - When the data type of \b input is int32, the intermediate result of \b input cannot
 *   exceed the value range of the corresponding data type.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor   :   [[1, 2, 3],
                           [4, 5, 6],
                           [7, 8, 9]]

       alpha          :   2

       beta           :   1

       Output tensor  :   [[3,  5,  7],
                           [9,  11, 13],
                           [15, 17, 19]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlTransform(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const void *beta,
                                        void *output);
// Group:Transform
/*!
 * @brief Transforms linearly for an input tensor with the following formula:
 *
 * output = alpha * input + beta
 *
 * The \b input and \b output are tensors, and \b alpha and \b beta are the scaling factors
 * used in the operation. \b cnnlTransform_v2 supports host and device scale pointers \b alpha
 * and \b beta while \b cnnlTransform only supports host scale pointers.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor \b input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] pointer_mode
 *   Input.  An enum value which indicates that the scalar values \b alpha and \b beta are
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] alpha
 *   Input.  A pointer to scaling factor of tensor input.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_DEVICE, the \b alpha should be a device pointer.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_HOST, the \b alpha should be a host pointer.
 * @param[in] beta
 *   Input.  A pointer to scaling factor of tensor input.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_DEVICE, the \b beta should be a device pointer.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_HOST, the \b beta should be a host pointer.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor \b output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Transform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for input tensor \b input and output tensor
 *   \b output must be half-half, float-float or int32-int32.
 * - \b alpha and \b beta: If the data type of tensors is float or half, the data
 *   type of \b alpha and \b beta should be float pointer. If the data type of tensors is
 *   int32, the data type of \b alpha and \b beta should be int pointer.
 *
 * @par Scale Limitation
 * - The tensor descriptors of input and output tensors must be the same.
 * - The value of input and output tensors, which data type is int32, should be in
 *   the range of [-2^23, 2^23] on MLU200 series.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor   :   [[1, 2, 3],
                           [4, 5, 6],
                           [7, 8, 9]]

       alpha          :   2

       beta           :   1

       Output tensor  :   [[3,  5,  7],
                           [9,  11, 13],
                           [15, 17, 19]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlTransform_v2(cnnlHandle_t handle,
                                           const cnnlPointerMode_t pointer_mode,
                                           const void *alpha,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const void *beta,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);
// Group:Tri
/*!
 * @brief Returns batches of the upper or lower triangular part of given matrices.
 *
 * When \b tri_up_mode is set to true, this function is in the triu mode and returns the upper
 * \b diagonal_k triangular part of the given matrix \b input with the other elements of the output
 * tensor set to zero.
 *
 * When \b tri_up_mode is set to false, this function is in the tril mode and returns the lower
 * \b diagonal_k triangular part of the given matrix \b input with the other elements of the output
 * tensor set to zero.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tri
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diagonal_k
 *   Input. The diagonal used in this operation.
 * @param[in] tri_up_mode
 *   Input. A boolean value which determines whether to use tril mode or triu mode.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Tri Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the tri operation is as follows:
     @verbatim
     input array by 4 * 4 --> input: [[1, 2, 3, 4],
                                      [5, 6, 7, 8],
                                      [9, 10,11,12],
                                      [13,14,15,16]]

     1.param:
       diagonal_k: 1, tri_up_mode: true

     output array by 4 * 4 --> output: [[0, 2, 3, 4],
                                        [0, 0, 7, 8],
                                        [0, 0, 0, 12],
                                        [0, 0, 0, 0]]

     2.param:
       diagonal_k: 1, tri_up_mode: false

     output array by 4 * 4 --> output: [[1, 2, 0, 0],
                                        [5, 6, 7, 0],
                                        [9, 10,11,12],
                                        [13,14,15,16]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.triu.html
 * - https://pytorch.org/docs/stable/generated/torch.tril.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTri(cnnlHandle_t handle,
                                  const int diagonal_k,
                                  const bool tri_up_mode,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Clip
/*!
 * @deprecated
 *   ::cnnlClip is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlClip_v2 instead, which supports the parameter \b pointer_mode that sets \b max
 *   and \b min to host pointer or device pointer.
 *
 * @brief Clips all elements from input into the range of [min, max].
 * If an element is less than \b min, the element will be set to \b min. If the element is bigger
 * than \b max, the element will be set to \b max. Otherwise, the element will not be reset.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the clamp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] min
 *   Input. A host pointer to the lower-bound of the range to be clamped to.
 * @param[in] max
 *   Input. A host pointer to the upper-bound of the range to be clamped to.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *   - input: half, float, int32.
 *   - output: half, float, int32.
 *
 * @note
 * - The data type of input should be same with output.
 * - If the data type of input is int32, the value of input should be less than 4194304.
 * - You can specify the stride of all dimensions for input_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Scale Limitation
 * - According to the definition of Clip function, the parameters should meet the following
 *   conditions:
 *   - The shape of input and output must be matched.
 *   - \b min and \b max should not be NULL at the same time.
 *
 * @par Example
 * - The example of the clip operation is as follows:
    @verbatim
    input one array by 2 * 3,
        input: [[1,2,3],[4,5,6]]

    param:
    min: 2, max: 5,
    output one array by 2 * 3 -->output: [[2,2,3],[4,5,5]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlClip(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const void *min,
                                   const void *max,
                                   void *y);

// Group:Clip
/*!
 * @brief Clips all elements from input into the range of [min, max].
 * If an element is less than \b min, the element will be set to \b min. If the element is bigger
 * than \b max, the element will be set to \b max. Otherwise, the element will not be reset.
 * Compared with ::cnnlClip, this function supports host or device pointer modes for \b max and \b min.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the clamp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pointer_mode
 *   Input.  An enum value which indicates that the scalar values \b min and \b max are
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] min
 *   Input. A pointer to the lower-bound of the range to be clamped to.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_DEVICE, the \b min should be a device pointer.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_HOST, the \b min should be a host pointer.
 * @param[in] max
 *   Input. A pointer to the upper-bound of the range to be clamped to.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_DEVICE, the \b max should be a device pointer.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_HOST, the \b max should be a host pointer.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *   - input: half, float, int32.
 *   - output: half, float, int32.
 *
 * @note
 * - The data type of input should be same with output.
 * - If the data type of input is int32, the value of input should be less than 4194304.
 * - You can specify the stride of all dimensions for input_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Scale Limitation
 * - According to the definition of Clip function, the parameters should meet the following
 *   conditions:
 *   - The shape of input and output must be matched.
 *   - \b min and \b max should not be NULL at the same time.
 *
 * @par Example
 * - The example of the clip operation is as follows:
    @verbatim
    input one array by 2 * 3,
        input: [[1,2,3],[4,5,6]]

    param:
    min: 2, max: 5,
    output one array by 2 * 3 -->output: [[2,2,3],[4,5,5]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlClip_v2(cnnlHandle_t handle,
                                   const cnnlPointerMode_t pointer_mode,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const void *min,
                                   const void *max,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:Ax
/*!
 * @deprecated
 *   ::cnnlAx is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlAx_v2 instead.
 *
 *  @brief Multiplies the input tensor \b a to the input tensor \b x.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ax operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the ax operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x
 *  - See "Ax Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a and \b x.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1.
 *  - If tensor \b a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the ax operation is as follows:
 *    @verbatim
 *    input two arrays by 2 * 1 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    output array by 2 * 2 --> x: [[8, 8], [4, 4]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAx(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t a_desc,
                                 const void *a,
                                 const cnnlTensorDescriptor_t x_desc,
                                 void *x,
                                 void *workspace);

// Group:Ax
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 * optimize the ax operation.
 *
 * The size of the extra workspace is based on the given information of the ax operation,
 * including the input tensor descriptor \b a_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ax operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the ax operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAxWorkspaceSize(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 size_t *size);

// Group:Ax
/*!
 *  @brief Multiplies the input tensor \b a to the input tensor \b x. Compared with ::cnnlAx,
 *  this function allows you to check the validation of \b workspace_size.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ax operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the ax operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the ax operation.
 *    You can get the size of the workspace with the ::cnnlGetAxWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x
 *  - See "Ax Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a and \b x.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1.
 *  - If tensor \b a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the ax operation is as follows:
 *    @verbatim
 *    input two arrays by 2 * 1 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    output array by 2 * 2 --> x: [[8, 8], [4, 4]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAx_v2(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const void *a,
                                    const cnnlTensorDescriptor_t x_desc,
                                    void *x,
                                    void *workspace,
                                    size_t workspace_size);


// Group:Axpy
/*!
 * @deprecated
 *   ::cnnlAxpy is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlAxpy_v2 instead.
 *
 *  @brief Multiplies the input tensor \b a to the input tensor \b x, then adds input tensor \b y.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpy operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \b y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpy operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x + \b y
 *  - See "Axpy Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a, \b x and \b y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1.
 *  - The tensor \b x and \b y must have the same shape.
 *  - If tensor \b a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpy operation is as follows:
 *    @verbatim
 *    input three arrays by 2 * 1, 2 * 2 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    --> y: [[6, 6], [6, 6]]
 *
 *    output array by 2 * 2 --> x: [[14, 14], [10, 10]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAxpy(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const void *a,
                                   const cnnlTensorDescriptor_t x_desc,
                                   void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   void *workspace);

// Group:Axpy
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 * optimize the axpy operation.
 *
 * The size of the extra workspace is based on the given information of the axpy operation,
 * including the input tensor descriptor \b a_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   axpy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] y_desc
 *   Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the axpy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAxpyWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t a_desc,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const cnnlTensorDescriptor_t y_desc,
                                                   size_t *size);

// Group:Axpy
/*!
 *  @brief Multiplies the input tensor \b a to the input tensor \b x, then adds input tensor \b y.
 *  Compared with ::cnnlAxpy, this function allows you to check the validation of \b workspace_size.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpy operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \b y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpy operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the axpy operation.
 *    You can get the size of the workspace with the ::cnnlGetAxpyWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x + \b y
 *  - See "Axpy Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a, \b x and \b y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1.
 *  - The tensor \b x and \b y must have the same shape.
 *  - If tensor \b a does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpy operation is as follows:
 *    @verbatim
 *    input three arrays by 2 * 1, 2 * 2 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    --> y: [[6, 6], [6, 6]]
 *
 *    output array by 2 * 2 --> x: [[14, 14], [10, 10]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAxpy_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t x_desc,
                                      void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Axpby
/*!
 * @deprecated
 *   ::cnnlAxpby is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlAxpby_v2 instead.
 *
 *  @brief Multiplies the input tensors \b a and \b b to tensors \b x and \b y separately,
 *  then adds the results.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] b_desc
 *    Input. Descriptor of input data \b b, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] b
 *    Input. Pointer to the MLU memory that stores the input tensor \b b.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \b y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpby operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x + \b b * \b y
 *  - See "Axpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a, \b x, \b b and \b y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1. The same rule applies to input
 *    tensors \b b and \b y.
 *  - The tensor \b x and \b y must have the same shape.
 *  - If tensors \b a and \b b does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpby operation is as follows:
 *    @verbatim
 *    input three arrays by 2 * 1, 2 * 2, 1 * 2 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    --> b: [[2, 1]]
 *
 *    --> y: [[6, 6], [6, 6]]
 *
 *    output array by 2 * 2 --> x: [[20, 14], [16, 10]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAxpby(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const void *a,
                                    const cnnlTensorDescriptor_t x_desc,
                                    void *x,
                                    const cnnlTensorDescriptor_t b_desc,
                                    const void *b,
                                    const cnnlTensorDescriptor_t y_desc,
                                    const void *y,
                                    void *workspace);

// Group:Axpby
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 * optimize the axpby operation.
 *
 * The size of the extra workspace is based on the given information of the axpby operation,
 * including the input tensor descriptors \b a_desc and \b b_desc. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   axpby operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] x_desc
 *   Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] b_desc
 *   Input. Descriptor of input data \b b, including dimension, data type (half and float),
 *   and data layout.
 * @param[in] y_desc
 *   Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *   and data layout.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the axpby operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAxpbyWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t a_desc,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t b_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    size_t *size);

// Group:Axpby
/*!
 *  @brief Multiplies the input tensors \b a and \b b to tensors \b x and \b y separately,
 *  then adds the results. Compared with ::cnnlAxpby, this function allows you to check the
 *  validation of \b workspace_size.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    axpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] a_desc
 *    Input. Descriptor of input data \b a, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] a
 *    Input. Pointer to the MLU memory that stores the input tensor \b a.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] b_desc
 *    Input. Descriptor of input data \b b, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] b
 *    Input. Pointer to the MLU memory that stores the input tensor \b b.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \b y.
 *  @param[in] workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the axpby operation.
 *    For more information about workspace, see "Cambricon CNNL User Guide".
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the axpby operation.
 *    You can get the size of the workspace with the ::cnnlGetAxpbyWorkspaceSize function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b a * \b x + \b b * \b y
 *  - See "Axpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b a, \b x, \b b and \b y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *
 *  @note
 *  - This operation supports broadcasting. Each dimension of the input tensor \b a must match the
 *    corresponding dimension of the tensor \b x, or equal to 1. The same rule applies to input
 *    tensors \b b and \b y.
 *  - The tensor \b x and \b y must have the same shape.
 *  - If tensors \b a and \b b does not need broadcasting, the workspace can be NULL.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the axpby operation is as follows:
 *    @verbatim
 *    input three arrays by 2 * 1, 2 * 2, 1 * 2 and 2 * 2 --> a: [[2], [1]]
 *
 *    --> x: [[4, 4], [4, 4]]
 *
 *    --> b: [[2, 1]]
 *
 *    --> y: [[6, 6], [6, 6]]
 *
 *    output array by 2 * 2 --> x: [[20, 14], [16, 10]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlAxpby_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t a_desc,
                                       const void *a,
                                       const cnnlTensorDescriptor_t x_desc,
                                       void *x,
                                       const cnnlTensorDescriptor_t b_desc,
                                       const void *b,
                                       const cnnlTensorDescriptor_t y_desc,
                                       const void *y,
                                       void *workspace,
                                       size_t workspace_size);

// Group:Caxpby
/*!
 *  @brief Multiplies coefficient scalar \b alpha and \b beta to tensors \b x and \b y separately,
 *  then adds the results.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    caxpby operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] alpha
 *    Input. A host pointer to scaling factor of tensor \b x, the default value is 1.0.
 *  @param[in] x_desc
 *    Input. Descriptor of input data \b x, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in,out] x
 *    Input and Output. Pointer to the MLU memory that stores the input and output tensor \b x.
 *  @param[in] beta
 *    Input. A host pointer to scaling factor of tensor \b y, the default value is 1.0.
 *  @param[in] y_desc
 *    Input. Descriptor of input data \b y, including dimension, data type (half and float),
 *    and data layout.
 *  @param[in] y
 *    Input. Pointer to the MLU memory that stores the input tensor \b y.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - \b x = \b alpha * \b x + \b beta * \b y
 *  - See "Caxpby Operation" section in "Cambricon CNNL User Guide" for details.
 *
 *  @par Data Type
 *  - This function supports the following data types for input tensor \b x and \b y.
 *    Data type of all above tensors should be the same.
 *    - all above tensors: half, float.
 *  - Data type of \b alpha and \b beta must be float.
 *
 *  @note
 *  - The tensor \b x and \b y must have the same shape.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the caxpby operation is as follows:
 *    @verbatim
 *    input two arrays by 2 * 2 and 2 * 2 --> x: [[4, 4], [4, 4]]
 *
 *    --> y: [[6, 6], [6, 6]]
 *
 *    alpha: 2.0
 *
 *    beta: 3.0
 *
 *    output array by 2 * 2 --> x: [[26, 26], [26, 26]]
 *    @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlCaxpby(cnnlHandle_t handle,
                                     const float *alpha,
                                     const cnnlTensorDescriptor_t x_desc,
                                     void *x,
                                     const float *beta,
                                     const cnnlTensorDescriptor_t y_desc,
                                     const void *y);

// Group:Addition and Division
/*!
 * @brief Performs the addition and division operations with the following formula:
 *
 * \b a + \b alpha * \b b / \b c
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcdiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input. An float value that scales the result of division.
 * @param[in] desc_b
 *   Input. The descriptor of the dividend tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] desc_c
 *   Input. The descriptor of the divisor tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the addcdiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the addcdiv
 *   operation. You can get the size of the workspace with the ::cnnlGetAddcdivWorkspaceSize
 *   function.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Addcdiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b a, dividend tensor \b b,
 *   divisor tensor \b c, and output tensor \b output. Data type of all above tensors should be the
 *   same.
 *   - all above tensors: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, dividend tensor, divisor tensor, and output tensor must meet the following
 *   requirements:
 *   - input tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - dividend tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - divisor tensor: Every dimension should be divisible by the same dimension in output tensor.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_a, desc_b, desc_c and
 *   desc_output with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the addcdiv operation is as follows:
     @verbatim
     input three arrays by 2 * 2, 1 * 2 and 2 * 1 --> a: [[1, 1], [1, 1]]

     --> b: [4, 4]

     --> c: [[2], [2]]

     param:
       alpha: 0.5

     output array by 2 * 2 --> output: [[2, 2], [2, 2]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.addcdiv
 */
cnnlStatus_t CNNL_WIN_API cnnlAddcdiv(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t desc_a,
                                      const void *a,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t desc_b,
                                      const void *b,
                                      const cnnlTensorDescriptor_t desc_c,
                                      const void *c,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t desc_output,
                                      void *output);

// Group:Addition and Division
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcdiv operation.
 *
 * The size of extra workspace is based on the given information of the addcdiv operation,
 * including the input tensor descriptor \b desc_a, dividend tensor descriptor \b desc_b, and
 * divisor tensor descriptor \b desc_c. For more information about the workspace, see
 * "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcdiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the dividend tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the divisor tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcdiv operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to
 *   create and set the tensor descriptors \b desc_a, \b desc_b, and \b desc_c before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlAddcdiv function to perform the
 *   addcdiv operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddcdivWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t desc_a,
                                                      const cnnlTensorDescriptor_t desc_b,
                                                      const cnnlTensorDescriptor_t desc_c,
                                                      size_t *size);

// Group:Embedding
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the embedding backward operation.
 *
 * The size of extra workspace is determined based on the given information of the embedding
 * backward operation, including the input tensor descriptor \b diff, output tensor descriptor
 * \b output, and the parameter \b scale_grad_by_freq. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale_grad_by_freq
 *   Input. A boolean value which determines whether to scale output tensor \b output by the
 *   inverse of frequency of the index.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   embedding backward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetEmbeddingBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t diff_desc,
                                      const cnnlTensorDescriptor_t output_desc,
                                      bool scale_grad_by_freq,
                                      size_t *workspace_size);

// Group:Embedding
/*!
 * @brief Computes gradients of embedding.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] padding_idx
 *   Input. Determines which index of the embedding vector \b output should be initialized to zero.
 * @param[in] scale_grad_by_freq
 *   Input. A boolean value which determines whether to scale output tensor \b output by the
 *   inverse of frequency of the index.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store index of each row of \b output in
 *   \b diff. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each row of
 *   \b output in \b diff.
 * @param[in] diff_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the embedding
 *   backward operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the embedding
 *   backward operation. You can get the size of the workspace with the
 *   ::cnnlGetEmbeddingBackwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "EmbeddingBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for index tensor
 *   \b indices, input tensor \b diff, and output tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor must be same.</b>
 *   - index tensor: int32.
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 2 and 2 * 2 * 3 --> indices: [[5, 1], [6, 5]]

     --> diff: [[[ 0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                [[-0.2285,  0.3081,  1.1171], [ 0.1585, -0.8696,  1.8683]]]

     param:
       padding_idx: 0, scale_grad_by_freq: false

     output array by 10 * 3 -->
         output: [[ 0.0000,  0.0000,  0.0000],
                  [-0.6622, -0.4790,  0.8539],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.6941,  0.7042,  1.3819],
                  [-0.2285,  0.3081,  1.1171],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingBackward(cnnlHandle_t handle,
                                                int padding_idx,
                                                bool scale_grad_by_freq,
                                                const cnnlTensorDescriptor_t indices_desc,
                                                const void *indices,
                                                const cnnlTensorDescriptor_t diff_desc,
                                                const void *diff,
                                                void *workspace,
                                                size_t workspace_size,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output);

// Group:Expand
/*!
 * @brief Copies and expands the input tensor \b input to the shape of output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the expand
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Expand Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     complex_half, complex_float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Every dimension of the input tensor should be divisible by the same dimension of the output
 *     tensor.
 *
 * @note
 * - The input tensor \b input and output tensor \b output are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the expand operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[1, 2], [3, 4]]

     output array by 3 * 2 * 2 --> output: [[[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand
 */
cnnlStatus_t CNNL_WIN_API cnnlExpand(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);
// Group:Masked
/*!
 * @brief Fills the input tensor \b input with the specified value of the tensor \b value based on
 * the masked tensor \b masked.
 *
 * @deprecated
 *   ::cnnlMasked is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlMasked_v3 instead, which supports input tensor or masked tensor broadcasting
 *   and supports setting \p CNNL_MASKED_SELECT mode for parameter \b masked_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   the ::cnnlMaskedOp_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the input tensor \b value to fill into \b input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the input tensor \b value to fill into \b input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor \b input, value tensor \b value and output tensor \b output must be the
 *   same. The supported data types of input tensor \b input, masked tensor \b masked, value tensor
 *   \b value and output tensor \b output are as follows:
 *   - When the \b masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, float.
 *     - output tensor: bool, int8, int16, int32, half, float.
 *   - When the \b masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, float.
 *     - output tensor: half, float.
 *
 * @par Scale Limitation
 * - The masked tensor \b masked must meet the following data range: [0,1].
 *
 * @note
 * - The masked tensor \b masked does not support broadcast.
 * - The shape of \b masked must be same with the shape of the \b input.
 * - You can specify the stride of all dimensions for \b input_desc, \b masked_desc, \b value_desc and \b output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 * - The input tensor \b input, masked tensor \b masked and output tensor \b output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
         masked_mode: \p CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: \p CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlMasked(cnnlHandle_t handle,
                                     cnnlMaskedOp_t masked_mode,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t masked_desc,
                                     const void *masked,
                                     const cnnlTensorDescriptor_t value_desc,
                                     const void *value,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Masked
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the masked operation.
 *
 * The size of extra workspace is based on the given information of the masked operation,
 * including the tensor descriptors \b input_desc, \b masked_desc, \b value_desc and \b output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode are defined in
 *   the ::cnnlMaskedOp_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] value_desc
 *   Input. The descriptor of the value tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the masked
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b x
 *   and \b y, c3_dim represents the dimension of \b z:
 *
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   - max(c1_dim, c2_dim) == c3_dim
 *
 * @note
 * - When the \b masked_mode is set to \p CNNL_MASKED_SELECT, the tensor \b value_desc should be null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMaskedWorkspaceSize(cnnlHandle_t handle,
                                                     cnnlMaskedOp_t masked_mode,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t masked_desc,
                                                     const cnnlTensorDescriptor_t value_desc,
                                                     const cnnlTensorDescriptor_t output_desc,
                                                     size_t *workspace_size);
// Group:Masked
/*!
 * @brief Fills the input tensor \b input with the specified value of the tensor \b value based on
 * the masked tensor \b masked.
 *
 * This function may need extra MLU memory as the workspace to improve the masked performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @deprecated
 *   ::cnnlMasked_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlMasked_v3 instead, which supports setting \p CNNL_MASKED_SELECT mode for parameter
 *   \b masked_mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   the ::cnnlMaskedOp_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the input tensor \b value to fill into \b input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the input tensor \b value to fill into \b input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetMaskedWorkspaceSize function to allocate extra
 *   workspace for \b workspace.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor \b input, value tensor \b value and output tensor \b output must be the
 *   same. The supported data types of input tensor \b input, masked tensor \b masked, value tensor
 *   \b value and output tensor \b output are as follows:
 *   - When the \b masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, float.
 *     - output tensor: bool, int8, int16, int32, half, float.
 *   - When the \b masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, float.
 *     - output tensor: half, float.
 *
 * @par Scale Limitation
 * - The masked tensor \b masked must meet the following data range: [0,1].
 * - When the \b masked_mode is set to \p CNNL_MASKED_FILL, the input tensor \b input and masked tensor
 *   \b masked support broadcast. According to the rule of tensor broadcast, the parameters should
 *   satisfy the following conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest
 *   dimension of \b input, \b masked, \b output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \b masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \b input must be same with the shape
 *   of the \b output.
 * - You can specify the stride of all dimensions for \b input_desc, \b masked_desc, \b value_desc and \b output_desc
 *   with ::cnnlSetTensorDescriptorEx.
 * - The input tensor \b input, masked tensor \b masked and output tensor \b output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
         masked_mode: \p CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: \p CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlMasked_v2(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_desc,
                                        const void *value,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);
// Group:Masked
/*!
 * @brief Fills the input tensor \b input with the specified value of the tensor \b value based on
 * the masked tensor \b masked, or selects the input value of tensor \b input based on the masked
 * tensor \b masked.
 *
 * Compared with ::cnnlMasked, this function supports input tensor and masked tensor to broadcast.
 * Compared with ::cnnlMasked_v2, this function supports \p CNNL_MASKED_SELECT mode.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetMaskedWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the masked
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] masked_mode
 *   Input. The masked mode used to compute the masked operation. The mode is defined in
 *   the ::cnnlMaskedOp_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input.
 * @param[in] masked_desc
 *   Input. The descriptor of the masked tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] masked
 *   Input. Pointer to the MLU memory that stores the masked tensor.
 * @param[in] value_desc
 *   Input. The descriptor of the tensor \b value to fill into \b input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the tensor \b value to fill into \b input.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the masked
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the masked
 *   operation. You can get the size of the workspace with the ::cnnlGetMaskedWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] number
 *   Output. Pointer to the MLU memory that stores the length of output selected data.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetMaskedWorkspaceSize function to
 *   allocate extra workspace for \b workspace.
 *
 * @par Formula
 * - See "Masked Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor \b input, value tensor \b value and output tensor \b output must be the
 *   same. The supported data types of input tensor \b input, masked tensor \b masked, value tensor
 *   \b value and output tensor \b output are as follows:
 *   - When the \b masked_mode is \p CNNL_MASKED_FILL:
 *     - input tensor: bool, int8, int16, int32, half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: bool, int8, int16, int32, half, float.
 *     - output tensor: bool, int8, int16, int32, half, float.
 *   - When the \b masked_mode is \p CNNL_MASKED_SCATTER:
 *     - input tensor: half, float.
 *     - masked tensor: uint8, int8, bool.
 *     - value tensor: half, float.
 *     - output tensor: half, float.
 *   - When the \b masked_mode is \p CNNL_MASKED_SELECT:
 *     - input tensor: bool, int8, uint8, int16, int32, half, float.
 *     - masked tensor: bool, half, float.
 *     - output tensor: bool, int8, uint8, int16, int32, half, float.
 *     - When the data type of masked tensor is not bool, the data type of input tensor must be
 *       same with the data type of the masked tensor.
 *
 * @par Scale Limitation
 * - When the \b masked_mode is \p CNNL_MASKED_FILL or \p CNNL_MASKED_SCATTER, the masked tensor \b masked
     must meet the following data range: [0,1].
 * - When the \b masked_mode is set to \p CNNL_MASKED_FILL or \p CNNL_MASKED_SELECT, the input tensor
 *   \b input and masked tensor \b masked support broadcasting. According to the rule of tensor broadcasting,
 *   the parameters should satisfy the following conditions. For example, c1_dim, c2_dim, and c3_dim
 *   represent the lowest dimension of \b input, \b masked, \b output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - When the \b masked_mode is set to \p CNNL_MASKED_SCATTER, the shape of \b input must be same with the shape
 *   of the \b output.
 * - When the \b masked_mode is set to \p CNNL_MASKED_SCATTER or \p CNNL_MASKED_FILL, you can specify the stride of
 *   all dimensions for \b input_desc, \b masked_desc, \b value_desc and \b output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - The input tensor \b input, masked tensor \b masked and output tensor \b output are multi-dimensional
 *   array, supporting up to \p CNNL_DIM_MAX dimensions.
 * - When the \b masked_mode is set to \p CNNL_MASKED_SELECT, the tensor \b value_desc and its related device memory
 *   pointer \b value should be null pointer.
 * - When the \b masked_mode is set to \p CNNL_MASKED_SCATTER or \p CNNL_MASKED_FILL, the tensor \b number should be null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the masked operation is as follows:
     @verbatim
       Example 1:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 1 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [-1,]

       param:
         masked_mode: \p CNNL_MASKED_FILL

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,-1],[-1,5,6]]]]

       Example 2:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

         value: [5,7,8]

       param:
         masked_mode: \p CNNL_MASKED_SCATTER

       output array by 1 * 1 * 2 * 3 --> [[[[1,2,5],[7,5,6]]]]

       Example 3:
       input three arrays by 1 * 1 * 2 * 3, 1 * 1 * 2 * 3 and 3 -->
         input: [[[[1,2,3],[4,5,6]]]]

         masked: [[[[0,0,1],[1,0,0]]]]

       param:
         masked_mode: \p CNNL_MASKED_SELECT

       output array --> [3,4]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/THC/THCTensorMasked.cuh
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlMasked_v3(cnnlHandle_t handle,
                                        cnnlMaskedOp_t masked_mode,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t masked_desc,
                                        const void *masked,
                                        const cnnlTensorDescriptor_t value_desc,
                                        const void *value,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        uint32_t *number);
// Group:Pad
/*!
 *  @brief Pads the input tensor using \b padding_value based on \b paddings that specifies how the tensor is
 *  padded or cropped. This operation is usually used in ResNet and Transformer networks.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the pad operation. For
 *    detailed information, see ::cnnlHandle_t.
 *  @param[in]  input_desc
 *    Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  paddings
 *    Input. A host pointer to the \b paddings data that holds the padding size to be added for each dimension
 *    of \b input. Positive and negative padding values represent the padding size and the cropping size, respectively.
 *  @param[in]  padding_value
 *    Input. A host pointer to the \b padding_value that holds the constant value to be added.
 *  @param[in]  output_desc
 *    Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[out]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Data Type
 *  - Data types of input tensor \b input, \b padding_value and output tensor \b output must be the same. The
 *   supported data types are as follows:
 *    - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *    - paddings: int32.
 *    - padding_value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *    - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 *  @par Scale Limitation
 *  - Only supports padding with a constant value in \b padding_value.
 *  - The input tensor and output tensor must meet the following requirements:
 *    - The number of dimensions of \b input is no more than 8.
 *    - The shape of the integer array of \b paddings is \p [n, 2]. Length of the first dimension of \b paddings is
 *    \p n, which equals to the number of dimensions of \b input. Length of the second dimension of \b paddings is 2,
 *    which means the two padding directions of before and after directions for each dimension of \b input. For each
 *    dimension \p k, \p paddings[k, 0] and \p paddings[k, 1] specify the padding size before and after the input data
 *    respectively.
 *    - \p output[k] = \p paddings[k, 0] + \p input[k] + \p paddings[k, 1]. \p k represents each dimension of \b input.
 *    \p output[k] represents the length of the dimension \p k in the output tensor. \p input[k] represents the length
 *    of the dimension \p k in the input tensor. \p paddings[k, 0] and \p paddings[k, 1] represent the length to be
 *    padded before and after the dimension \p k in the input tensor respectively.
 *    - \p paddings[k, 0] and \p paddings[k, 1] >= \p -input[k]. \p k represents each dimension of \b input.
 *    \p input[k] represents the length of the dimension \p k in the input tensor.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of pad is as follows:
    @verbatim
    For example below, if dimensions of input == 2, then the shape of paddings should be [n, 2] == [2, 2], where n holds the
    length of the first dimension of paddings. If paddings == [[len1, len2], [len3, len4]], then for the first dimension k,
    paddings[k, 0] == len1 == 1, paddings[k, 1] == len2 == 1. For the second dimension k + 1, paddings[k + 1, 0] == len3 == -1.
    paddings[k + 1, 1] == len4 == 2. So the paddings is [[1, 1], [-1, 2]].
      - len1 is the padding size to be padded before the first dimension k of the input data.
      - len2 is the padding size to be padded after the first dimension k of the input data.
      - len3 is the padding size to be padded before the second dimension k + 1 of the input data.
      - len4 is the padding size to be padded after the second dimension k + 1 of the input data.

    input: [[2, 4], [1, 3]]
    paddings: [[1, 1], [-1, 2]]
    padding_value: 0
    output: [[0, 0, 0], [4, 0, 0], [3, 0, 0], [0, 0, 0]]
    @endverbatim
 *
 *  @par Reference
 *  - https://www.tensorflow.org/api_docs/python/tf/pad
 *  - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py
 *  - https://pytorch.org/docs/stable/nn.functional.html?highlight=pad#torch.nn.functional.pad
 */

cnnlStatus_t CNNL_WIN_API cnnlPad(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const void *paddings,
                                  const void *padding_value,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);
// Group:ReplicationPad
/*!
 * @brief Pads the input tensor using the replication of the input boundary. This operation is usually used in YoloV3 network.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the replication_pad2d operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  paddings
 *   Input.  A host pointer to the padding parameter that holds the padding size in the order of left, right, top and bottom.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 *
 * @par Data Layout
 * - The supported data layout of the input tensor is as follows:
 *   - \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NCHW
 *
 * @par Scale Limitation
 * - Only supports 2-D padding for replication padding mode now.
 * - The input tensor and output tensor must meet the following requirements:
 *   - dimensions of \b input equals to 4, and length of \b paddings equals to 4
 *   - \p output_h >= 1 && \p output_w >= 1. \p output_h represents the length of the H dimension in the output tensor. \p output_w represents the length of the W *     dimension in the output tensor.
 *    - \p output_h = \p input_h + \p pad_top + \p pad_bottom && \p output_w = \p input_w + \p pad_left + \p pad_right. \p input_h represents the length of the H *      dimension in the input tensor. \p input_w represents the length of the W dimension in the input tensor. \p pad_top, \p pad_bottom, \p pad_left and \p
 *      pad_right respectively represent the length of corresponding dimension in \b paddings.
 *
 *  @par Performance Optimization
 *  - For best practices, to have better performance, set the layout of the input tensor and output tensor to NHWC.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of replication_pad2d is as follows:
    @verbatim
    input: tensor([[[[0., 1., 2.],
                     [3., 4., 5.],
                     [6., 7., 8.]]]])
    paddings: array([2., 2., 2., 2.])
    output: tensor([[[[0., 0., 0., 1., 2., 2., 2.]
                      [0., 0., 0., 1., 2., 2., 2.]
                      [0., 0., 0., 1., 2., 2., 2.]
                      [3., 3., 3., 4., 5., 5., 5.]
                      [6., 6., 6., 7., 8., 8., 8.]
                      [6., 6., 6., 7., 8., 8., 8.]
                      [6., 6., 6., 7., 8., 8., 8.]]]])
    @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/master/generated/torch.nn.ReplicationPad2d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlReplicationPad2d(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               const int padding[],
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);
// Group:ReflectionPad
/*!
 * @brief Pads the input tensor using the reflection of the input boundary based on \b paddings that specifies
 * how the tensor is padded or cropped. This operation is usually used in YoloV3 network for Pytorch.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the reflection_pad2d
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  paddings
 *   Input. A host pointer to the host padding parameter that holds the padding size to be added for certain
 *   dimensions of \b input in the order of left, right, top and bottom. Positive, negative and zero padding
 *   values represent the padding size, the cropping size and no padding size, respectively.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - paddings: int32.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \b input and output tensor \b output must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - Only supports 2-D padding for reflection padding mode now.
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \b input equals to 4, and length of \b paddings equals to 4.
 *   - \p pad_top < \p input_h && \p pad_bottom < input_h. \p input_h represents the length of the H dimension in
 *   the input tensor. \p pad_top and \p pad_bottom represent the length of corresponding dimension in \b paddings
 *   respectively.
 *   - \p pad_left < \p input_w && \p pad_right < input_w. \p input_w represents the length of the W dimension in
 *   the input tensor. \p pad_left and \p pad_right represent the length of corresponding dimension in \b paddings
 *   respectively.
 *   - \p output_h >= 1 || \p output_w >= 1. \p output_h represents the length of the H dimension in the output tensor.
 *   \p output_w represents the length of the W dimension in the output tensor.
 *   - \p output_h = \p input_h + \p pad_top + \p pad_bottom && \p output_w = \p input_w + \p pad_left + \p pad_right.
 *   \p input_h represents the length of the H dimension in the input tensor. \p input_w represents the length of the
 *   W dimension in the input tensor. \p pad_top, \p pad_bottom, \p pad_left and \p pad_right represent the length of
 *   corresponding dimension in \b paddings respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and output tensor to NHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of reflection_pad2d is as follows:
   @verbatim
   input: tensor([[[[0., 1., 2.],
                    [3., 4., 5.],
                    [6., 7., 8.]]]])
   paddings: array([2., 2., 2., 2.])
   output: tensor([[[[8., 7., 6., 7., 8., 7., 6.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [2., 1., 0., 1., 2., 1., 0.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [8., 7., 6., 7., 8., 7., 6.]
                     [5., 4., 3., 4., 5., 4., 3.]
                     [2., 1., 0., 1., 2., 1., 0.]]]])
   @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.ReflectionPad2d.html
 */

cnnlStatus_t CNNL_WIN_API cnnlReflectionPad2d(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const int paddings[],
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);
// Group:TopKTensor
/*!
 * @brief Computes the top \b k largest or smallest elements of the input tensor in the specified dimension.
 *
 * @deprecated
 *   ::cnnlTopKTensor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlTopKTensor_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An int value which determines the number of top elements in \b input to be returned.
 * @param[in] dim
 *   Input. An int value which determines the dimension to sort along.
 * @param[in] largest
 *   Input. A boolean value which determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A boolean value which determines whether to sort the returned data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b input - \b output - \b index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - float-float-int32.
 *   - float-float-uint32.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \b output and \b index should be the same.
 * - When \b input contains NaN:
 *   - On MLU200 series:
 *     - NaN will be considered as saturation value.
 *   - On MLU300 series and CE3226:
 *     - The \b output is unpredictable since a NaN value is noncomparable.
 * - When the data type of \b input is CNNL_DTYPE_INT32, the value of \b input should
 *   be in the range of [-2^24, 2^24].
 * - When the data type of \b input is CNNL_DTYPE_FLOAT, the value of \b input should
*    be in the range of (-2^127, 2^127).
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const int k,
                                         const int dim,
                                         const bool largest,
                                         const bool sorted,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output,
                                         const cnnlTensorDescriptor_t index_desc,
                                         void *index);

// Group:TopKTensor
/*!
 * @brief Computes the top \b k largest or smallest elements of the input tensor in the specified dimension.
 *   Compared with ::cnnlTopKTensor, this function adds another parameter \b lower_index_first, which regulates
 *   behavior of selecting the elements with smaller indices when their values are the same.
 *
 * @deprecated
 *   ::cnnlTopKTensor_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlTopKTensor_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An int value which determines the number of top elements in \b input to be returned.
 * @param[in] dim
 *   Input. An int value which determines the dimension to sort along.
 * @param[in] largest
 *   Input. A boolean value which determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A boolean value which determines whether to sort the returned data.
 * @param[in] lower_index_first
 *   Input. A boolean value which determines whether the behavior of lower-index element coming first
 *   is guaranteed when multiple elements have the same value.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b input - \b output - \b index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - float-float-int32.
 *   - float-float-uint32.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \b output and \b index should be the same.
 * - When \b input contains NaN:
 *   - On MLU200 series:
 *     - NaN will be considered as saturation value.
 *   - On MLU300 series and CE3226:
 *     - The \b output is unpredictable since a NaN value is noncomparable.
 * - When the data type of \b input is CNNL_DTYPE_INT32, the value of \b input should
 *   be in the range of [-2^24, 2^24].
 * - When the data type of \b input is CNNL_DTYPE_FLOAT, the value of \b input should
*    be in the range of (-2^127, 2^127).
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor_v2(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const int k,
                                            const int dim,
                                            const bool largest,
                                            const bool sorted,
                                            const bool lower_index_first,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            const cnnlTensorDescriptor_t index_desc,
                                            void *index);

// Group: TopKTensor
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used to get
 *        extra space size in topk operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the topk operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The  descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The  descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] largest
 *   Input. A boolean value which determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the topk operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - This API is only used along with ::cnnlTopKTensor_v3. ::cnnlTopKTensor_v2 and ::cnnlTopKTensor do not require this API.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTopKTensorWorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input_desc,
                                                         const int k,
                                                         const int dim,
                                                         const bool largest,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         const cnnlTensorDescriptor_t index_desc,
                                                         size_t *workspace_size);

// Group:TopKTensor
/*!
 * @brief Computes the top \b k largest or smallest elements of the input tensor in the specified dimension.
 *   Compared with ::cnnlTopKTensor_v2, this function adds another two parameters \b workspace and \b workspace_size_inbytes,
 *   which provide the extra space needed in computation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the topk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. An int value which determines the number of top elements in \b input to be returned.
 * @param[in] dim
 *   Input. An int value which determines the dimension to sort along.
 * @param[in] largest
 *   Input. A boolean value which determines whether to return the largest data. When this parameter is true,
 *   return the top-k largest input values and their corresponding indices; when this parameter is false,
 *   return the top-k smallest input values and their corresponding indices.
 * @param[in] sorted
 *   Input. A boolean value which determines whether to sort the returned data.
 * @param[in] lower_index_first
 *   Input. A boolean value which determines whether the behavior of lower-index element coming first
 *   is guaranteed when multiple elements have the same value.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   topk operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   topk operation. You can get the size of the workspace with the
 *   ::cnnlGetTopKTensorWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index tensor, which is the indices of the returned values.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "TopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b input - \b output - \b index, the supported combinations of data types are shown below:
 *   - uint8-uint8-int16.
 *   - uint8-uint8-uint16.
 *   - uint8-uint8-int32.
 *   - uint8-uint8-uint32.
 *   - int8-int8-int16.
 *   - int8-int8-uint16.
 *   - int8-int8-int32.
 *   - int8-int8-uint32.
 *   - int16-int16-int32.
 *   - int16-int16-uint32.
 *   - int32-int32-int32.
 *   - int32-int32-uint32.
 *   - half-half-int16.
 *   - half-half-uint16.
 *   - half-half-int32.
 *   - half-half-uint32.
 *   - float-float-int32.
 *   - float-float-uint32.
 *
 * @par Scale Limitation
 * - The size of \b dim dimension multiplied by \b k should be no larger than 64*10^12.
 *
 * @par Note
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \b output and \b index should be the same.
 * - When \b input contains NaN:
 *   - On MLU200 series:
 *     - NaN will be considered as saturation value.
 *   - On MLU300 series and CE3226:
 *     - The \b output is unpredictable since a NaN value is noncomparable.
 * - When the data type of \b input is CNNL_DTYPE_INT32, the value of \b input should
 *   be in the range of [-2^24, 2^24].
 * - When the data type of \b input is CNNL_DTYPE_FLOAT, the value of \b input should
 *    be in the range of (-2^127, 2^127).
 * - When the data type of \b input is CNNL_DTYPE_HALF, if \b largest is true, the
 *   value of \b input should be in the range of [-65472, 65504], otherwise [-65504, 65472].
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.topk.html
 */
cnnlStatus_t CNNL_WIN_API cnnlTopKTensor_v3(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const int k,
                                            const int dim,
                                            const bool largest,
                                            const bool sorted,
                                            const bool lower_index_first,
                                            void *workspace,
                                            const size_t workspace_size,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            const cnnlTensorDescriptor_t index_desc,
                                            void *index);

// Group:NegTensor
/*!
 * @brief Retrieves the negation of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. Descriptor of the input tensor \b x.
 * @param[in] x
 *   Input.  Pointer to the MLU memory that stores the input data.
 * @param[in] y_desc
 *   Input.  Descriptor of the output tensor \b y.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input: float, half.
 *   - output: float, half.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_input and desc_output with
 *  ::cnnlSetTensorDescriptorEx.
 *
 * @par Performance Optimization
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Neg
 */
cnnlStatus_t CNNL_WIN_API cnnlNegTensor(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:Select
/*!
 * @brief Selects elements from then tensor \b p_then or else tensor \b p_else based on
 *        \b p_condition, and returns the results in the output tensor \b p_output.
 *
 * Compared with ::cnnlSelectV2, this function does not support broadcasting,
 * and the alignment is left while that of ::cnnlSelectV2 is right.
 *
 * If \b p_condition is true, \b p_output is equal to \b p_then; if \b p_condition is
 * false, \b p_output is equal to \b p_else.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_then
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p_then
 *   Input. Pointer to the MLU memory that stores the then tensor.
 * @param[in] desc_else
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p_else
 *   Input. Pointer to the MLU memory that stores the else tensor.
 * @param[in] p_condition
 *   Input. Pointer to the MLU memory that stores the condition tensor.
 * @param[in] condition_size
 *   Input. The size of the condition tensor used in the select operation.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] p_output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of then tensor, else tensor and output tensor must be the same.
 * - The supported data types of then tensor, else tensor, condition and output tensor are as follows:
 *   - then tensor: int8, int16, int32, half, float.
 *   - else tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 *   - condition tensor: bool.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - Shape of then tensor, else tensor and output tensor must be the same.
 * - If the shape of then tensor is [a, ..., b, ..., c], the supported sizes of condition tensor
 *   are as follows:
 *   - 1.
 *   - a.
 *   - a * ... * b * ... * c.
 *
 * @par Example
 * - The example of the select operation is as follows:
     @verbatim
      1. The size of condition is equal to 1
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true]
        condition size: 1

      output array by 2 * 3
      --> output: [[1, 8, 9], [6, 4, 3]]

      2. The size of condition is equal to the size of the first dimension of then tensor
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true, false]
        condition size: 2

      output array by 2 * 2
      --> output: [[1, 8, 9], [7, 5, 2]]

      3. The size of condition is equal to the size of the then tensor
      input two arrays by 2 * 3 and 2 * 3
      --> then: [[1, 8, 9], [6, 4, 3]]

      --> else: [[2, 3, 6], [7, 5, 2]]

      param:
        condition: [true, false, false, false, true, true]
        condition size: 6

      output array by 2 * 3
      --> output: [[1, 3, 6], [7, 4, 3]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Select
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/SelectV2
 */
cnnlStatus_t CNNL_WIN_API cnnlSelect(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t desc_then,
                                     const void *p_then,
                                     const cnnlTensorDescriptor_t desc_else,
                                     const void *p_else,
                                     const cnnlTensorDescriptor_t desc_output,
                                     void *p_output,
                                     const bool *p_condition,
                                     const int condition_size);
// Group:Select
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the select operation.
 *
 * The size of extra workspace is based on the given information of the selectV2 operation,
 * including the input tensor descriptors \b condition_desc, \b then_desc and \b else_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   selectV2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] condition_desc
 *   Input. The descriptor of the condition tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] then_desc
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] else_desc
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   selectV2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlSelectV2 function to perform the
 *   select operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSelectV2WorkspaceSize(cnnlHandle_t handle,
                             const cnnlTensorDescriptor_t condition_desc,
                             const cnnlTensorDescriptor_t then_desc,
                             const cnnlTensorDescriptor_t else_desc,
                             size_t *workspace_size);

// Group:Select
/*!
 * @brief Selects elements from then tensor \b then_ptr or else tensor \b else_ptr based on
 *        \b condition_ptr, and returns the results in the output tensor \b output_ptr.
 *
 * Compared with ::cnnlSelect, this function supports multidirectional(i.e., Numpy-style) broadcasting,
 * and the alignment is right while that of ::cnnlSelect is left.
 *
 * If \b condition_ptr is true, \b output_ptr is equal to \b then_ptr; if \b condition_ptr is
 * false, \b output_ptr is equal to \b else_ptr.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the select operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] condition_desc
 *   Input. The descriptor of then condition tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] condition_ptr
 *   Input. Pointer to the MLU memory that stores the condition tensor.
 * @param[in] then_desc
 *   Input. The descriptor of the then tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] then_ptr
 *   Input. Pointer to the MLU memory that stores the then tensor.
 * @param[in] else_desc
 *   Input. The descriptor of the else tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] else_ptr
 *   Input. Pointer to the MLU memory that stores then else tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the selectV2
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the selectV2
 *   operation. You can get the size of the workspace with the ::cnnlGetSelectV2WorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Select Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of then tensor, else tensor and output tensor must be the same.
 * - The supported data types of condition tensor, then tensor, else tensor and output tensor are as follows:
 *   - condition tensor: bool, uint8.
 *   - then tensor: int8, int16, int32, half, float.
 *   - else tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 * @par Scale Limitation
 * - According to the rule of tensor broadcasting, the parameters should satisfy the following
 *   conditions. The dimension number of the output tensor should be the largest dimension number in
 *   condition, then or else tensor. The tensors with smaller dimension number will be right aligned with
 *   the tensor with the largest dimension number and fill in 1 to the left to have the same dimension number
 *   with the largest dimension number of tensor.
 *   Take the lowest dimension for example, c1_dim, c2_dim and c3_dim represent
 *   the dimensions of condition tensor, then tensor and else tensor,
 *   c4_dim represents the dimension of output tensor:
 *
 *   c4_dim == max(c1_dim, c2_dim, c3_dim)
 *   c1_dim == c4_dim or c1_dim == 1
 *   c2_dim == c4_dim or c2_dim == 1
 *   c3_dim == c4_dim or c3_dim == 1
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the select operation is as follows:
     @verbatim
      1. The shape of condition array is equal to 1
      --> condition_ptr: [False]
      --> then_ptr: [1,2,3,4]
      --> else_ptr: [100]
      --> output_ptr: [100,100,100,100]

      2. no broadcasting (the same with ::cnnlSelect)
      --> condition_ptr: [True, False, False, True]
      --> then_ptr:  [1,2,3,4]
      --> else_ptr: [100,200,300,400]
      --> output_ptr: [1,200,300,4]

      3. broadcasting (right alignment)
      broadcast else array by 1 to 4
      --> condition_ptr: [True, False, False, True]
      --> then_ptr: [1,2,3,4]
      --> else_ptr: [100]
      --> output_ptr: [1,100,100,4]

      broadcast then array and else array by 1 to 4
      --> condition_ptr: [True, False, False, True]
      --> then_ptr: [1]
      --> else_ptr: [100]
      --> output_ptr: [1,100,100,1]

      broadcast condition array by 3 to 2*3
      --> condition_ptr: [True, False, True]
      --> then_ptr: [[1,2,3],[4,5,6]]
      --> else_ptr: [[100,200,300],[400,500,600]]
      --> output_ptr: [[1,200,3],[4,500,6]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/where
 * - https://github.com/onnx/onnx/blob/main/docs/Changelog.md#Where-9
 * - https://github.com/onnx/onnx/blob/main/docs/Broadcasting.md
 * - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/select.cc#L215
 */
cnnlStatus_t CNNL_WIN_API cnnlSelectV2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t condition_desc,
                                       const void *condition_ptr,
                                       const cnnlTensorDescriptor_t then_desc,
                                       const void *then_ptr,
                                       const cnnlTensorDescriptor_t else_desc,
                                       const void *else_ptr,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output_ptr);

// Group:RMSProp
/*!
 * @brief Creates an operation to update filters by Root Mean Square Prop (RMSProp) algorithm.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter, which is the learning rate.
 * @param[in] rho
 *   Input. Pointer to the MLU memory that stores the \b rho parameter, which is the discounting
 *   factor of gradient.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \b epsilon parameter, which is a small
 *   value to avoid zero denominator.
 * @param[in] momentum
 *   Input. Pointer to the MLU memory that stores the \b momentum parameter, which is a scale
 *   parameter.
 * @param[in] grad_desc
 *   Input. The descriptor of \b grad tensor, which is the gradient.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor.
 * @param[in] var_desc
 *   Input. The descriptor of \b var tensor, which is the filter to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \b var tensor.
 * @param[in] ms_desc
 *   Input. The descriptor of \b ms tensor, which is the mean square gradient to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] ms
 *   Input and output. Pointer to the MLU memory that stores the \b ms tensor.
 * @param[in] mom_desc
 *   Input. The descriptor of \b mom tensor, which is the delta of \b var.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] mom
 *   Input and output. Pointer to the MLU memory that stores the \b mom tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Apply RMSprop Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b grad tensor, \b var tensor, \b ms tensor and \b mom tensor must meet the following
 * requirements:
 *   - The shape of \b grad equals to the shape of \b var.
 *   - The shape of \b ms equals to the shape of \b var.
 *   - The shape of \b mom equals to the shape of \b var.
 *
 * @par Data Type
 * - Data type of \b grad tensor, \b var tensor, \b ms tensor, \b mom tensor must be the same.
 * - And also, the data type of \b lr parameter, \b rho parameter, \b epsilon parameter and
 *   \b momentum parameter must be the same as \b var tensor.
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - var tensors: half, float.
 *   - ms tensors: half, float.
 *   - mom tensors: half, float.
 *   - lr parameter: half, float
 *   - rho parameter: half, float
 *   - epsilon parameter: half, float
 *   - momentum parameter: half, float
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
    @verbatim
    --> grad: an array [64, 78, 1024];
    --> var: an array [64, 78, 1024];
    --> ms: an array [64, 78, 1024];
    --> mom: an array [64, 78, 1024];
    --> lr: an array [1];
    --> rho: an array [1];
    --> epsilon: an array [1];
    --> momentum: an array [1];
    Then we will get the output:
    --> var: an array [64, 78, 1024] same with grad;
    --> ms: an array [64, 78, 1024] same with grad;
    --> mom: an array [64, 78, 1024] same with grad;
    @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlRMSProp(cnnlHandle_t handle,
                                      const void *lr,
                                      const void *rho,
                                      const void *epsilon,
                                      const void *momentum,
                                      const cnnlTensorDescriptor_t grad_desc,
                                      const void *grad,
                                      const cnnlTensorDescriptor_t var_desc,
                                      void *var,
                                      const cnnlTensorDescriptor_t ms_desc,
                                      void *ms,
                                      const cnnlTensorDescriptor_t mom_desc,
                                      void *mom);

// Group:ApplyCenterRMSProp
/*!
 * @brief The CenterRMSProp algorithm uses an estimate of the centered second
 * moment (i.e., the variance) for normalization, as opposed to regular RMSProp,
 * which uses the (uncentered) second moment. This often helps with training,
 * but is slightly more expensive in terms of computation and memory.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \b var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *   Input and output. Pointer to the MLU memory that stores the \b var tensor to be updated
 *   according to CenterRMSProp algorithm.
 *   This input refers to filter in the artificial intelligence network generally.
 * @param[in] mg_desc
 *   Input. A descriptor of input tensor \b mg. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  mg
 *   Input and output. Pointer to the MLU memory that stores the moving average gradient.
 * @param[in] ms_desc
 *   Input. A descriptor of input tensor \b ms. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  ms
 *   Input and output. Pointer to the MLU memory that stores the moving average square gradient.
 * @param[in] mom_desc
 *   Input. A descriptor of input tensor \b mom. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in, out]  mom
 *   Input and output. Pointer to the MLU memory that stores the momentum.
 * @param[in] grad_desc
 *   Input. A descriptor of input tensor \b grad. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the gradient.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  rho
 *   Input. Pointer to the MLU memory that stores the moving average parameter.
 * @param[in]  momentum
 *   Input. Pointer to the MLU memory that stores the attenuation of impulse.
 * @param[in]  epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as 10^-8, to
 *   avoid division by 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyCenterRMSProp Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 * - The values in tensor \b mg should be in the range of [-1,1].
 * - The values in tensor \b ms should be in the range of [2,63486].
 * - The values in tensor \b grad should be in the range of [-1,1].
 * - The value \b lr points to should be in the range of [0,1].
 * - The value \b rho points to should be in the range of [0,1].
 * - The value which \b momentum points to should be in the range of [0,1].
 * - The value \b epsilon points to should be in the range of (0,1].
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyCenterRMSProp operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> mg: an array [64, 78, 1024];
     --> ms: an array [64, 78, 1024];
     --> mom: an array [64, 78, 1024];
     --> grad: an array [64, 78, 1024];
     --> lr: an array [1];
     --> rho: an array [1];
     --> momentum: an array [1];
     --> epsilon: an array [1];
     Then we will get the output:
     --> var: an array [64, 78, 1024] same with input;
     --> mg: an array [64, 78, 1024] same with input;
     --> ms: an array [64, 78, 1024] same with input;
     --> mom: an array [64, 78, 1024] same with input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlApplyCenterRMSProp(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t var_desc,
                                                 void *var,
                                                 const cnnlTensorDescriptor_t mg_desc,
                                                 void *mg,
                                                 const cnnlTensorDescriptor_t ms_desc,
                                                 void *ms,
                                                 const cnnlTensorDescriptor_t mom_desc,
                                                 void *mom,
                                                 const cnnlTensorDescriptor_t grad_desc,
                                                 const void *grad,
                                                 const void *lr,
                                                 const void *rho,
                                                 const void *momentum,
                                                 const void *epsilon);

// Group:QR
/*!
 * @brief This function is used to compute the QR decomposition of a matrix or
 * a batch of matrices \b a with the transformation algorithm used in this operation, and returns \b q,
 * \b r of tensors such that \b a = \b q * \b r with \b q being an orthogonal matrix or batch
 * of orthogonal matrices and \b r being an upper triangular matrxi or batch of
 * upper triangular matrices. The \b a tensor of size (*,m,n) where * is zero or
 * more batch dimension consisting of matrices of dimension (m,n).
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  a_desc
 *   Input. A descriptor of input tensor \b a. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  a
 *   Input. Pointer to the MLU memory that stores the \b a tensor to be decomposed
 *   according to the transformation algorithm used in this operation.
 * @param[in] q_desc
 *   Input. A descriptor of output tensor \b q. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  q
 *   Output. Pointer to the MLU memory that is used to store the decomposition result q.
 * @param[in] r_desc
 *   Input. A descriptor of input tensor \b r. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  r
 *   Output. Pointer to the MLU memory that is used to store the decomposition result r.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used to store the intermediate result.
 * @param[in] workspace_size
 *   Input. The size of \b workspace and can be calculated by ::cnnlGetQRWorkspaceSize.
 * @param[in]  some
 *   Input. Set to true for reduced QR decomposition and false for complete QR decomposition.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - None.
 *
 * @par Scale Limitation
 * - The input \b a tensor of size (*, m, n) where m <=512, and n <=512, and if *
 *   is too large, the process will be terminated by the watch dog. * is supposed
 *   to be less than or equal to 256. If some is false, the q must be (*, m, m),
 *   r must be (*, m, n); if \b some is true, the q must be (*, m, min(m,n)), r must
 *   be (*, min(m,n), n).
 * - The number of dimensions of each input tensor should be equal to or greater than 2,
 *   and no greater than 8.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetQRWorkspaceSize function.
 *
 * @note
 * - Precision may be lost if the magnitudes of the elements of \a are large.
 * - This function returns a valid decomposition, but the result may not be the same with
 *   other third party libraries such as lapack, eigen, cusolver.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the QR operation is as follows:
    @verbatim
     --> some: false;
     --> a: an array [64, 256, 128];
     Then we will get the output:
     --> q: an array [64, 256, 256];
     --> r: an array [64, 256, 128];
    @endverbatim

    @verbatim
     --> some: true;
     --> a: an array [64, 256, 128];
     Then we will get the output:
     --> q: an array [64, 256, 128];
     --> r: an array [64, 128, 128];
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlQR(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t a_desc,
                                 const void *a,
                                 const cnnlTensorDescriptor_t q_desc,
                                 void *q,
                                 const cnnlTensorDescriptor_t r_desc,
                                 void *r,
                                 void *workspace,
                                 size_t workspace_size,
                                 const bool some);

// Group:QR
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to save the
 * intermediate result of ::cnnlQR operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlGetQRWorkspaceSize operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  a_desc
 *   Input. The descriptor of a tensor. The value must be the same with the \b a_desc in ::cnnlQR.
 *   This parameter is used to calculate the workspace size.
 * @param[in]  some
 *   Input. A boolean value that must be the same with the \b some in ::cnnlQR.
 *   This parameter is used to calculate the workspace size.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the QR operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Scale Limitation
 * - The \b a_desc must be the same with \b a_desc in ::cnnlQR function.
 * - The \b some must be the same with \b some in ::cnnlQR function.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlQR function.
 *
 * @note
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetQRWorkspaceSize(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const bool some,
                                                 size_t *size);
// Group:IndexPut
/*!
 * @brief Inserts \b values into the input tensor according to the locations
 * described in the \b indices list.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index put operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The list of descriptors of the indices indicating the locations of input
 *   to be inserted. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. A host pointer to a list of MLU pointers, which point to the MLU memory storing
 *   the indices tensors.
 * @param[in] indices_num
 *   Input. The length of the list of indices tensors.
 * @param[in] values_desc
 *   Input. The descriptor of the values to be inserted. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the index put operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the index put operation.
 *   You can get the size of the workspace with the ::cnnlGetIndexPutWorkspaceSize function.
 * @param[in] accumulate
 *   Input. A boolean that controls the behavior of inserting values to the input. If accumulate is false,
 *   the values will replace the original input. Otherwise, the values will accumulate to the
 *   original input.
 * @param[in] unsafe
 *   Input. A boolean that controls whether to check all indices are valid. If unsafe is false, the operation
 *   will checkout whether any index values is over bound. Otherwise,
 *   the operation will not check the validity of index values.
 *   Note that this option is only valid when accumulate is true.
 *   Note that \b unsafe = false is not suported currently.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Index Put Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b input, \b values and \b output must be the same.
 *   Data types of indices should be int32, bool or uint8.
 *   The data type of indices must be the same.
 *   If \b accumulate is false, the supported data types of \b input, \b values and
 *   \b output are as follows:
 *   - input, values: half, float, bool, uint8, int8, int16, int32.
 *   - output: half, float, bool, uint8, int8, int16, int32.
 *   If \b accumulate is true, the supported data types of \b input, \b values and
 *   \b output are as follows:
 *   - input, values: half, float, int32, int16, int8, uint8.
 *   - output: half, float, int32, int16, int8, uint8.
 *
 * @par Scale Limitation
 * - If the data type of indices is bool or uint8, the number of non-null pointer in indices_desc should be 1.
 * - If the data type of indices is int32, the length of the indices
 *   list \b indices_num should be less than or equal to the dimension size of input.
 * - If the data type of indices is int32, inferred_indices_desc->dim + input_desc->dim - indices_not_nullptr_num <= 8,
 *   where inferred_indices_desc->dim represents the dimension of target indice shape inferred from indices_desc,
 *   input_desc->dim represents the dimension of the input tensor.
 *   indices_not_nullptr_num represents the number of non-null pointers in indices_desc list.
 * - The element in indices_desc can be NULL, but all of them cannot be NULL at the same time.
 * - When the element in indices_desc is NULL, the corresponding element in indices must be NULL.
 * - When the element in indices_desc is non-null, the corresponding element in indices must be non-null.
 * - The alignment of all indices in the \b indices list should be contiguous in the MLU memory.
 * - The alignment of \b values should be contiguous in the MLU memory.
 * - The alignment of \b input should be contiguous in the MLU memory.
 * - The alignment of \b output should be contiguous in the MLU memory.
 * - The data types of all indices in the indices_desc list should be the same.
 * - Mixed data type of indices is not supported.
 * - \b unsafe = false is not supported.
 * - When the data type of indices is int32, negative indexing is supported.
 *   However, unexpected results may occur if the values of indices are not in the range of [-a, a),
 *   where a is the corresponding dimension size of input.
 * - When the data type of indices is int32, the count of elements in input must be smaller than 2^31-1.
 * - When the data type of indices is bool or uint8, the count of elements in input must be smaller than 2^31-1.
 * - When \b accumulate = true and the date type of input is int32, the accumulate result must be in range of (-2^23,2^23).
 * - When \b accumulate = true and the data type of input is int16, int8 or uint8, the accumulate result cannot overflow.
 * @par API Dependency
 * - Before calling this function to implement index put, you need to call
 * ::cnnlGetIndexPutWorkspaceSize to get the extra space size needed in index put operation.
 *
 * @note
 * - If accumulate is false and duplicate indices exist, which means there are
 *   several values to be inserted into the same location, the result will be
 *   non-deterministic because the order of values to be inserted is not guaranteed.
 * - When accumulate is true, the type of indices is int32, and the type of values is half or float,
 *   the precision may be poor if the same location is added up too many times on mlu-200 series.
 * - Supports inplace operation, that output equals to input.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of index put operation are as follows:
     @verbatim
     input: [[0,0,0], [0,0,0]]
     indices[2]: [0,0,1], [1,2,1]
     values: [1.0]
     accumulate: false
     output: [[0,1,1],[0,1,0]]
     @endverbatim

     @verbatim
     input: [[0,0,0],[0,0,0]]
     indices[1]: [True, False]
     values: [1, 2, 3]
     accumulate: false
     output: [[1,2,3],[0,0,0]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/cppdocs/notes/tensor_indexing.html
 * - https://numpy.org/docs/stable/reference/arrays.indexing.html
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexPut(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t indices_desc[],
                                       const void *const *indices,
                                       const int indices_num,
                                       const cnnlTensorDescriptor_t values_desc,
                                       const void *values,
                                       void *workspace,
                                       const size_t workspace_size,
                                       const bool accumulate,
                                       const bool unsafe,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:IndexPut
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 *        optimize the index put operation.
 *
 * @param[in] handle
 *  Input: Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *         queues operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc
 *   Input. The list of descriptors of the indices indicating the locations of input
 *   to be inserted. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices_num
 *   Input. The length of the list of indices tensors.
 * @param[in] values_desc
 *   Input. The descriptor of the values to be inserted. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] accumulate
 *   Input. A boolean that controls the behavior of inserting values to the input. If accumulate is false,
 *   the values will replace the original input. Otherwise, the values will accumulate to the
 *   original input.
 * @param[out] size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the index put operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   The values of \b input_desc, \b indices_desc, \b indices_num, \b values_desc and \b accumulate
 *   should be set to the same with the ones in the ::cnnlIndexPut operation.
 *
 * @par API Dependency
 * - The allocated extra worksapce should be passed to the ::cnnlIndexPut function to perform the
 *   index put operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIndexPutWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t indices_desc[],
                                       const int indices_num,
                                       const cnnlTensorDescriptor_t values_desc,
                                       const bool accumulate,
                                       size_t *size);
// Group:Roll
/*!
 * @brief Rolls the input tensor according to the given dimensions and shifts. If no dimension is given,
 * the tensor will be regarded as a flattened tensor before rolling and restored to the original
 * shape.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the roll operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] shifts
 *   Input. A host pointer pointing to the host memory that stores the rolling length of each dimension.
 * @param[in] shifts_num
 *   Input. The length of input pointer \b shifts.
 * @param[in] dims
 *   Input. A host pointer pointing to the host memory that stores the rolling dimension.
 * @param[in] dims_num
 *   Input. The length of input pointer \b dims.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the rolling operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the rolling operation.
 *   You can get the size of the workspace with the ::cnnlGetRollWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Roll Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b input must be the same with \b output.
 *   Data types of input supports any data type supported by Cambricon CNNL except \p CNNL_DTYPE_INVALID.
 *
 * @par Scale Limitation
 * - The count of elements in input must be smaller than 2^31-1.
 *
 * @par API Dependency
 * - Before calling this function, you need to call
 * ::cnnlGetRollWorkspaceSize to get the extra space size needed in roll operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of roll operation are as follows:
     @verbatim
     input: [[0,1,2], [3,4,5]]
     shifts: [2]
     dims: []
     output: [[4,5,0],[1,2,3]]
     @endverbatim

     @verbatim
     input: [[0,1,2],[3,4,5]]
     shifts: [1,2]
     dims: [0,1]
     output: [[4,5,3],[1,2,0]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.roll.html?highlight=roll#torch.roll
 */
cnnlStatus_t CNNL_WIN_API cnnlRoll(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const int shifts[],
                                   const int shifts_num,
                                   const int dims[],
                                   const int dims_num,
                                   void *workspace,
                                   const size_t workspace_size,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:Roll
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 *        optimize the roll operation.
 *
 * @param[in] handle
 *  Input: Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *         queues operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *  Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *          the roll operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 * - The values of \b input_desc should be set to the same with the ones in the ::cnnlRoll operation.
 *
 * @par API Dependency
 * - The allocated extra worksapce should be passed to the ::cnnlRoll function to perform the
 *   roll operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRollWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   size_t *size);

// Group:ApplyAdaMax
/*!
 * @brief Updates filter using AdaMax algorithm.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the applyadamax operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. The descriptor of the variable tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *   Input. Pointer to the MLU memory that stores the variable tensor to be updated.
 * @param[in]  momentum_desc
 *   Input. The descriptor of the momentum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  momentum
 *   Input. Pointer to the MLU memory that stores the first momentum vector.
 * @param[in]  velocity_desc
 *   Input. The descriptor of the velocity tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  velocity
 *   Input. Pointer to the MLU memory that stores the velocity tensor.
 * @param[in]  diff_desc
 *   Input. The descriptor of the gradient tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 *   The diff is the gradients for updating variable. The values in this tensor
 *   should be in the range of [-500,500] to acheive high accuracy.
 * @param[in]  lr
 *   Input. The parameter of lr data. The device pointer to learning rate.
 *   The value of this parameter should be in the range of [0,1].
 * @param[in]  beta1
 *   Input. The parameter of beta1 data. The device pointer to exponential decay rate for
 *   momentum. The value of this parameter should be in the range of [0,1].
 * @param[in]  beta2
 *   Input. The parameter of beta2 data. The device pointer to exponential decay rate for
 *   velocity. The value of this parameter should be in the range of [0,1].
 * @param[in]  beta1_power
 *   Input. The parameter of beta1_power data.
 *   The value of this parameter should be in the range of [0,1).
 * @param[in]  epsilon
 *   Input. The parameter of epsilon data. The device pointer to a small positive number just as 10^-8, to
 *   avoid division by 0. The value of this parameter should be in the range of (0,1].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - All inputs and outputs should be of the same size.
 * - The number of dimensions is no more than 8.
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 *  @par Formula
 *  - See "ApplyAdaMax Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyAdaMax(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t var_desc,
                                          void *var,
                                          const cnnlTensorDescriptor_t momentum_desc,
                                          void *momentum,
                                          const cnnlTensorDescriptor_t velocity_desc,
                                          void *velocity,
                                          const cnnlTensorDescriptor_t diff_desc,
                                          const void *diff,
                                          const void *lr,
                                          const void *beta1,
                                          const void *beta2,
                                          const void *beta1_power,
                                          const void *epsilon);

// Group:ApplyAdam
/*!
 * @brief Implements the Adam optimization algorithm. Adam is a stochastic gradient
 * descent method that computes individual adaptive learning rates for different parameters
 * from estimates of first- and second-order moments of the gradients.
 * Set \b use_nesterov = true if you want to use Nesterov Adam, otherwise the default
 * Adam is used.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \b var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *  Input and output. Pointer to the MLU memory that stores the \b var tensor to be updated
 *  according to Adam algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in]  momentum_desc
 *   Input. A descriptor of input tensor \b momentum. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  momentum
 *   Input and output. Pointer to the MLU memory that stores the first-order momentum.
 * @param[in]  velocity_desc
 *   Input. A descriptor of input tensor \b velocity. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  velocity
 *   Input and output. Pointer to the MLU memory that stores the second-order momentum.
 * @param[in]  diff_desc
 *   Input. The descriptor of \b diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *  Input. Pointer to the MLU memory that stores the \b diff tensor. The \b diff is the gradient
 *  for updating \b var.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  beta1
 *   Input. Pointer to the MLU memory that stores the first estimated exponential decay rate.
 * @param[in]  beta2
 *   Input. Pointer to the MLU memory that stores the second estimated exponential decay rate.
 * @param[in]  beta1_power
 *   Input. Pointer to the MLU memory that stores the accumulator of beta1.
 * @param[in]  beta2_power
 *   Input. Pointer to the MLU memory that stores the accumulator of beta2.
 * @param[in]  epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as 10^-8, to
 *   avoid division by 0.
 * @param[in]  use_nesterov
 *   Input. Determines whether to use Nesterov Adam to update \b var or not.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyAdam Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 * - The values in tensor \b velocity should be in the range of [0,65504].
 * - The values in tensor \b diff should be in the range of [-255,255].
 * - The value \b lr points to should be in the range of [0,1].
 * - The value \b beta1 points to should be in the range of [0,1].
 * - The value \b beta2 points to should be in the range of [0,1].
 * - The value \b beta1_power points to should be in the range of [0,1).
 * - The value \b beta2_power points to should be in the range of [0,1].
 * - The value \b epsilon points to should be in the range of (0,1].
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyAdam operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> momentum: an array [64, 78, 1024];
     --> velocity: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> beta1: an array [1];
     --> beta2: an array [1];
     --> beta1_power: an array [1];
     --> beta2_power: an array [1];
     --> epsilon: an array [1];
     --> use_nesterov: false;
     Then we will get the output:
     --> var: an array [64, 78, 1024] same with input;
     --> momentum: an array [64, 78, 1024] same with input;
     --> velocity: an array [64, 78, 1024] same with input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlApplyAdam(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t var_desc,
                                        void *var,
                                        const cnnlTensorDescriptor_t momentum_desc,
                                        void *momentum,
                                        const cnnlTensorDescriptor_t velocity_desc,
                                        void *velocity,
                                        const cnnlTensorDescriptor_t diff_desc,
                                        const void *diff,
                                        const void *lr,
                                        const void *beta1,
                                        const void *beta2,
                                        const void *beta1_power,
                                        const void *beta2_power,
                                        const void *epsilon,
                                        const bool use_nesterov);

// Group:ApplyAdadelta
/*!
 * @brief Implements the adadelta optimization algorithm. adadelta is a per-dimension
 * learning rate method for stochastic gradient descent method.
 * The main advantage of adadelta is that we do not need to set a default learning rate when training the model.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. A descriptor of input tensor \b var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *  Input and output. Pointer to the MLU memory that stores the \b var tensor to be updated
 *  according to Adadelta algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in] accum_desc
 *   Input. A descriptor of input tensor \b accum. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  accum
 *   Input and output. Pointer to the MLU memory that stores the
 *   moving average of the second momentum of the gradient.
 * @param[in] accum_update_desc
 *   Input. A descriptor of input tensor \b accum_update. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum_update
 *   Input and output. Pointer to the MLU memory that stores the
 *    moving average of the second momentum of the change of parameters in the trainable model itself.
 * @param[in] diff_desc
 *   Input. The descriptor of \b diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *  Input. Pointer to the MLU memory that stores the \b diff tensor. The \b diff is the gradient
 *  for updating \b var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in] rho
 *   Input. Pointer to the MLU memory. The \b rho is the decay rate used to update accum and accum_update.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores a small positive number just as 10^-8, to
 *   avoid division by 0.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ApplyAdadelta Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than \p CNNL_DIM_MAX.
 * - The values in tensor \b accum should be in the range of [0, 1e6].
 * - The values in tensor \b accum_update should be in the range of [0, 1e6].
 * - The values in tensor \b diff should be in the range of [-1e3,1e3].
 * - The value \b lr points to should be in the range of [0,1].
 * - The value \b rho points to should be in the range of [0,1].
 * - The value \b epsilon points to should be in the range of (0,1].
 *
 * @par Data Type
 * - Data types of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ApplyAdadelta operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> accum: an array [64, 78, 1024];
     --> accum_update: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> rho: an array [1];
     --> epsilon: an array [1];
     Then we will get the output:
     --> var: an array [64, 78, 1024] same shape with input;
     --> accum: an array [64, 78, 1024] same shape with input;
     --> a: an array [64, 78, 1024] same shape with input;
    @endverbatim
 *
 * @par Reference
 * - Zeiler,M.D.(2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701
 * - http://github.com/tensorflow/tensorflow/blob/r1.14/tensorflow/python/training/adadelta.py
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyAdadelta(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t var_desc,
                                            void *var,
                                            const cnnlTensorDescriptor_t accum_desc,
                                            void *accum,
                                            const cnnlTensorDescriptor_t accum_update_desc,
                                            void *accum_update,
                                            const cnnlTensorDescriptor_t diff_desc,
                                            const void *diff,
                                            const void *lr,
                                            const void *rho,
                                            const void *epsilon);
// Group:ScatterNd
/*!
 * @brief Distributes the elements in tensor \b updates to tensor \b output according to the coordinates
 * in tensor \b indices. If \b indices contains duplicates, then their \b updates are accumulated. This
 * operator is the inverse of the ::cnnlGatherNd operator which extracts values or slices from a given tensor.
 *
 * @deprecated
 *   ::cnnlScatterNd is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlScatterNd_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the scatter_nd operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptor of the \b indices tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index data. The value of the lowest dimension in \b indices
 *   represents the coordinate of the element of \b updates in \b output tensor. If the coordinate contains negative
 *   numbers or numbers that exceeds the range, then it will be deprecated.
 * @param[in] updates_desc
 *   Input. The descriptor of the \b updates tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] updates
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "ScatterNd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The ScatterNd operation supports the following data types for input tensor \b indices, \b updates,
 * and output tensor \b output. The data type of \b updates and \b output must be the same.
 * - indices: int32, int64.
 * - updates: int32, half, float.
 * - output: int32, half, float.
 *
 * @par Scale Limitation
 * - If the rank of \b indices is n and indices[n-1] is ix, the shape of tensor \b indices, \b updates
 *   and \b output must meet the following restrictions:
 *
 *   indices.shape[0, n-2] = updates.shape[0, n-2]
 *   updates.rank - (n-1) = output.rank - ix
 *   updates.shape[n-1, updates.rank] = output.shape[ix, output.rank]
 *
 * @note
 * - This operator only supports TensorFlow framework.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of scatter_nd operation is as follows:
   @verbatim
    The shape of \b output: [4,4,4].
    indices: [[0], [2]]
    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]

   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlScatterNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t indices_desc,
                                       const void *indices,
                                       const cnnlTensorDescriptor_t updates_desc,
                                       const void *updates,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:ScatterNd
/*!
 * @brief Distributes the elements in tensor \b updates to tensor \b output according to the coordinates
 * in tensor \b indices. Compared with ::cnnlScatterNd, this function supports the \b mode parameter that contains
 * more calculation methods when \b indices contains duplicates. This operation is the inverse of the ::cnnlGatherNd
 * operator which extracts values or slices from a given tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the scatter_nd operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *  Input. The scatter_nd mode used when \b indices contains duplicates. For detailed information,
 *  see ::cnnlScatterNdMode_t.
 * @param[in] indices_desc
 *   Input. The descriptor of the \b indices tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index data. The value of the lowest dimension in \b indices
 *   represents the coordinate of the element of \b updates in \b output tensor. If the coordinate contains negative
 *   numbers or numbers that exceeds the range, then it will be ignored.
 * @param[in] updates_desc
 *   Input. The descriptor of the \b updates tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] updates
 *   Input. Pointer to the MLU memory that stores the data for updating.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "ScatterNd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The ScatterNd operation supports the following data types for input tensor \b indices, \b updates,
 * and output tensor \b output. The data type of \b updates and \b output must be the same.
 * - indices: int32, int64, uint32, uint64.
 * - updates: int32, half, float.
 * - output: int32, half, float.
 *
 * @par Scale Limitation
 * - On MLU200 series, when the \b updates and \b output data type is int32 and \b mode = ::CNNL_SCATTERND_ADD,
 *   the values of \b updates should be in the range of [-2^23, 2^23].
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - If the rank of \b indices is n and indices[n-1] is ix, the shape of tensor \b indices, \b updates
 *   and \b output must meet the following restrictions:
 *   - indices.shape[0, n-2] = updates.shape[0, n-2]
 *   - updates.rank - (n-1) = output.rank - ix
 *   - updates.shape[n-1, updates.rank] = output.shape[ix, output.rank]
 *
 * @note
 * - When the \b input is NULL, it will be treaded as zero vector. When the \b input is not NULL, the address
 *   can be equal to the \b output address, and in this case the performance is better.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of scatter_nd operation is as follows:
   @verbatim
    Example 1:
    The shape of \b output: [4,4,4].
    mode:    ::CNNL_SCATTERND_UPDATE
    indices: [[0], [0]]
    input:   [[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
              [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]

    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[5, 5, 5, 5], [6, 6, 6, 6], [7, 7, 7, 7], [8, 8, 8, 8]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
                [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]
    Example 2:
    The shape of \b output: [4,4,4].
    mode:    ::CNNL_SCATTERND_ADD
    indices: [[0], [0]]
    input:   NULL
    updates: [[[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]],
              [[5, 5, 5, 5], [6, 6, 6, 6],
               [7, 7, 7, 7], [8, 8, 8, 8]]]

    -->output: [[[10, 10, 10, 10], [12, 12, 12, 12], [14, 14, 14, 14], [16, 16, 16, 16]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
                [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]
   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlScatterNd_v2(cnnlHandle_t handle,
                                           cnnlScatterNdMode_t mode,
                                           const cnnlTensorDescriptor_t indices_desc,
                                           const void *indices,
                                           const cnnlTensorDescriptor_t updates_desc,
                                           const void *updates,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:GatherNd
/*!
 * @brief Gathers slices from \b params with shape specified by \b indices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather_nd
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_params
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_indices
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of each element of \b output in the
 *   corresponding dimension of input tensor \b params.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherNd Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b params, index tensor \b indices, and output tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor must be the same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - index tensor: int32, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - The item in \b indices must be in the range of [-rank, rank), where rank is the element size
 *   of each dimension of \b params. E.g.,params.shape is [3,2], indices' first data item be in
 *   [-3, 3) and second item must be in [-2, 2).
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather_nd operation is as follows:
     @verbatim
     input two arrays both by 3 * 2 --> params: [[1., 2.], [3., 4.], [5., 6.]]

     --> indices: [[-1, 0], [1, 1]]

     output array by 2 --> output: [5., 4.]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherNd
 */
cnnlStatus_t CNNL_WIN_API cnnlGatherNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t desc_params,
                                       const void *params,
                                       const cnnlTensorDescriptor_t desc_indices,
                                       const void *indices,
                                       const cnnlTensorDescriptor_t desc_output,
                                       void *output);

// Group:Addition and Multiplication
/*!
 * @brief Performs addition and multiplication operations with the following formula:
 *
 * \b a + \b alpha * \b b * \b c
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcmul
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] alpha
 *   Input. An int value that scales the result of multiplication.
 * @param[in] desc_b
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the multiplier tensor.
 * @param[in] desc_c
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the multiplier tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the addcmul operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the addcmul
 *   operation. You can get the size of the workspace with the ::cnnlGetAddcmulWorkspaceSize
 *   function.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Addcmul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b a, multiplier tensor \b b,
 *   multiplier tensor \b c, and output tensor \b output. Data type of all above tensors should be
 *   the same.
 *   - All above tensors: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, multiplier tensors, and output tensor must meet the following requirements:
 *   - input tensor: Every dimension should be divisible by the same dimension in output tensor.
 *   - multiplier tensors: Every dimension should be divisible by the same dimension in output
 *                         tensor.
 *
 * @note
 * - You can specify the stride of all dimensions for desc_a, desc_b, desc_c and
 *   desc_output with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the addcmul operation is as follows:
     @verbatim
     input three arrays by 2 * 2, 1 * 2 and 2 * 1 --> a: [[1, 1], [1, 1]]

     --> b: [4, 4]

     --> c: [[2], [2]]

     param:
       alpha: 0.5

     output array by 2 * 2 --> output: [[5, 5], [5, 5]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.addcmul
 */
cnnlStatus_t CNNL_WIN_API cnnlAddcmul(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t desc_a,
                                      const void *a,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t desc_b,
                                      const void *b,
                                      const cnnlTensorDescriptor_t desc_c,
                                      const void *c,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t desc_output,
                                      void *output);

// Group:Addition and Multiplication
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 * optimize the addcmul operation.
 *
 * The size of extra workspace is based on the given information of the addcmul operation,
 * including the input tensor descriptor \b desc_a, multiplier tensor descriptor \b desc_b, and
 * multiplier tensor descriptor \b desc_c. For more information about the workspace, see
 * "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the addcmul
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_a
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_b
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] desc_c
 *   Input. The descriptor of the multiplier tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   addcmul operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to
 *   create and set the tensor descriptors \b desc_a, \b desc_b, and \b desc_c before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlAddcmul function to perform the
 *   addcmul operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddcmulWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t desc_a,
                                                      const cnnlTensorDescriptor_t desc_b,
                                                      const cnnlTensorDescriptor_t desc_c,
                                                      size_t *size);
// Group:Split
/*!
 * @brief Splits an input tensor into \b split_num tensors on the specified dimension.
 * This operator is the inverse of the ::cnnlConcat operator.
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the split operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] split_num
 *   Input. Number of split output tensors.
 * @param[in] axis
 *   Input. Dimension along which to be split. The value must be in the range of [-rank, rank),
 *         where rank is the dimension of the input. Negative \b axis refers to 'axis + rank'.
 * @param[in]  input_desc
 *   Input. The descriptor of input. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the split operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   split operation. You can get the size of the workspace with the
 *   ::cnnlGetSplitWorkspaceSize function.
 * @param[in]  output_desc
 *   Output. The list of descriptors of output tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  outputs
 *   Output. A host pointer to a list of pointers, which point to the MLU memory
 *   that store the output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor \b input
 * and output tensors \b outputs.
 * <b> Note that all the tensors must have the same data type. If the input tensor \b input is
 * in fixed-point data type, the quantization parameters of all the tensors should be the same.
 * </b>
 * - input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The parameter \b split_num should be greater than 0.
 *   - The number of dimensions of all tensors must match, including input and outputs.
 *   - The dimensions of all outputs except \b axis must be equal, and the size of input on the splitting dimension
 *     should be equal to the sum of the outputs on \b axis.
 * @par API Dependency
 * - Before calling this function to implement split, you need to call
 * ::cnnlGetSplitWorkspaceSize to get the extra space size needed in split operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of split operation is as follows:
   @verbatim
   input: a tensor of 5 * 3 --> [[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15]]

   split_num: 3

   axis: 0

   num_splits: an array of [2, 2, 1]

   Then we will get the output:

   outputs: 3 tensors with the shapes of 2 * 3, 2 * 3 and 1 * 3, respectively
        --> [[1,2,3],[4,5,6]]
        --> [[7,8,9],[10,11,12]]
        --> [[13,14,15]]
   @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/split
 */
cnnlStatus_t CNNL_WIN_API cnnlSplit(cnnlHandle_t handle,
                                    const int split_num,
                                    const int axis,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc[],
                                    void *outputs[]);
// Group:Split
/*!
 *  @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 *  to optimize the split operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the split operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  split_num
 *    Input. Number of split output tensors.
 *  @param[out]  size
 *    Output. A host pointer to the returned size of the extra workspace in bytes
 *    that is used in the split operation.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Scale Limitation
 *  - The parameters must meet the following requirements:
 *    - The parameter \b split_num should be greater than 0.
 *
 *  @par API Dependency
 *  - This function must be called before the ::cnnlSplit function.
 *
 *  @note
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSplitWorkspaceSize(cnnlHandle_t handle,
                                                    const int split_num,
                                                    size_t *size);

// Group:Concat
/*!
 * @brief Concatenates the list of input tensors \b inputs along the given dimension \b axis.
 * This operator is the inverse of the ::cnnlSplit operator.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the concat operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] concat_num
 *   Input. Number of tensors needed to be concatenated.
 * @param[in] axis
 *   Input. Dimension along which to be concatenated. The value must be in the range of [-rank, rank),
 *          where rank is the number of dimensions in the input tensors,
 *          and negative \b axis refers to 'axis + rank'.
 * @param[in] inputs_desc
 *   Input. The list of descriptors of input tensors. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[in] inputs
 *   Input. A host pointer to a list of MLU pointers, which point to the MLU memory that store the
 *          input tensors.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the concat operation.
 *          For more information about workspace, see "Cambricon CNNL User Guide". Because ::cnnlConcat
 *          does not need extra workspace, the \b workspace can be set to NULL.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the concat operation.
 *          You can get the size of the workspace with the ::cnnlGetConcatWorkspaceSize function.
 *          Because ::cnnlConcat does not need extra workspace, the \b workspace_size can be set to 0.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Concat Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors \b inputs and output tensor
 *   \b output.
 *   <b>Note that all the tensors must have the same data type. If the tensors are in
 *      fixed-point data type, the quantization parameters of all the tensors should be the same.
 *   </b>
 *   - input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The parameter \b concat_num should be greater than 0.
 *   - The number of dimensions of all tensors must match, including inputs and output.
 *   - All dimensions except \b axis must be equal, and the dimension of output on \b axis must be
 *     equal to the sum of input dimensions on \b axis.
 *
 * @par API Dependency
 * - Before calling this function to implement concat, you need to call
 *   ::cnnlGetConcatWorkspaceSize to get the extra space size needed in concat operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of concat operation is as follows:
     @verbatim
     input: 3 tensors with the shapes of 2 * 3, 2 * 3 and 1 * 3, respectively
             --> [[1,2,3],[4,5,6]]
             --> [[7,8,9],[10,11,12]]
             --> [[13,14,15]]

     concat_num: 3

     axis: 0

     Then we will get the output:

     output: a tensor of 5 * 3 --> [[1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/concat
 */
cnnlStatus_t CNNL_WIN_API cnnlConcat(cnnlHandle_t handle,
                                     const int concat_num,
                                     const int axis,
                                     const cnnlTensorDescriptor_t inputs_desc[],
                                     const void *const inputs[],
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Concat
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to
 *        optimize the concat operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the concat operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] concat_num
 *   Input. Number of tensors needed to be concatenated.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the concat operation. At present, because ::cnnlConcat does not need extra workspace,
 *   the \b size will be returned with 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The parameter \b concat_num should be greater than 0.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlConcat function to perform the
 *   concat operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetConcatWorkspaceSize(cnnlHandle_t handle,
                                                     const int concat_num,
                                                     size_t *size);
// Group:ScatterRef
/*!
 * @brief Calculates sparse \b update to the \b dim dimension of the \b input referenced
 * by \b indices. Where operation modes include add, sub, mul, div, max, min and update.
 * And update represents replacing \b input with \b update directly.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *  in the scatterref operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *  Input. The descriptor of \b input tensor which represents the input of the operation.
 *  For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] input
 *  Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] indices_desc
 *  Input. The descriptor of \b indices tensor. \b indices contains some index values,
 *  which references to the dim dimension of \b input. So that \b input can do some operation
 *  with \b update. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *  Input. Pointer to the MLU memory that stores the \b indices tensor.
 * @param[in] update_desc
 *  Input. The descriptor of \b update tensor. \b update is used to do some operation with
 *  the dim dimension of \b input referenced by \b indices. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[in] update
 *  Input. Pointer to the MLU memory that stores the \b update tensor.
 * @param[in] dim
 *  Input. Specified which dimension to do sparse computing.
 * @param[in] mode
 *  Input. The descriptor of the scatterref mode. For detailed information,
 *  see ::cnnlScatterRefMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ScatterRef Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *
 *   - input - indices - update - output.
 *
 *   The supported data type combinations are:
 *
 *   - float - int32 - float - float
 *   - half - int32 - half - half
 *   - int32 - int32 - int32 - int32
 *   - float - int64 - float - float
 *   - half - int64 - half - half
 *   - int32 - int64 - int32 - int32
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The length of the \b dim dimension of the \b update is equal to the length of \b indices,
 *   and the length of other dimensions of \b update is equal to the length of the corresponding
 *   dimension of \b input.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ScatterRef operation is as follows:
     @verbatim
     input array:
     --> input: [1, 2, 3, 4, 5, 6]

     --> indices: [[2], [0], [5]]

     --> update: [9, 10, 11]

     param:
       mode: CNNL_SCATTERREF_ADD, dim: 0

     output one dimension array --> output: [11, 2, 12, 4, 5, 17]
     @endverbatim
*
* @par Reference
* - https://tensorflow.google.cn/api_docs/python/tf/compat/v1/
*
*/
cnnlStatus_t CNNL_WIN_API cnnlScatterRef(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t indices_desc,
                                         const void *indices,
                                         const cnnlTensorDescriptor_t update_desc,
                                         const void *update,
                                         const int dim,
                                         cnnlScatterRefMode_t mode);
// Group:PoolingIndex
/*!
 * @brief Computes the index of maximum pooling forward, and returns the results in
 * the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPoolingIndex operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \b y tensor is the result of this operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "PoolingIndex Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b x tensor, \b y tensor.
 *   <b>Note that the combinations of these tensors must be half-int16 or float-int32.</b>
 *   - \b x tensor: half, float.
 *   - \b y tensor: int16, int32.
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor and \b y tensor are as follows:
 *   <b>Note that the layout of these tensor must be same.</b>
 *   -\b x tensor: \p CNNL_LAYOUT_NHWC.
 *   -\b y tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters in the \b pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1.
 * - The parameters in the \b x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *   The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *   The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 * - The parameters in the \b y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *   The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *   The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - The parameters kernel in the \b pooling_desc should satisfy the following conditions:
 *   kw * kh < 3070. The kw and kh represent the width and the height of the pooling kernel size respectively.
 *
 * @note
 * - Currently, 2-D pooling have been supported.
 * - When the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The output index is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the output index is
 *    the index of the last value. Otherwise, the output index is the index of the maximum value after the last NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of this operation is as follows:
     @verbatim
      input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output:  [[[3],[3],[3],[3]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingIndex(cnnlHandle_t handle,
                                           const cnnlPoolingDescriptor_t pooling_desc,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y);

// Group:Pooling
/*!
 * @brief Creates a descriptor pointed by \b desc for pooling operation, and allocates
 * memory for holding the information about the pooling forward or backward operation.
 * The information is defined in ::cnnlPoolingDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in]  desc
 *   Input. Pointer to the pooling descriptor that holds information about the pooling operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetPooling2dDescriptor, ::cnnlSetPooling2dDescriptor_v2,
 * ::cnnlSetPoolingNdDescriptor or ::cnnlSetPoolingNdDescriptor_v2 function to initialize and set the information to the pooling descriptor.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Only supports 2-D and 3-D pooling. N-D pooling will be supported in the future.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreatePoolingDescriptor(cnnlPoolingDescriptor_t *desc);

// Group:Pooling
/*!
 * @brief Destroys a pooling descriptor \b desc that is previously created with the
 * ::cnnlCreatePoolingDescriptor function.
 *
 * The pooling descriptor is defined in ::cnnlPoolingDescriptor_t and holds
 * the information about the pooling forward or backward operation.
 *
 * @param[in] desc
 *   Input. The pooling descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the pooling operation.
 *   Otherwise, the memory leak may occur.
 * - Only supports 2-D and 3-D pooling. N-D pooling will be supported in the future.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlDestroyPoolingDescriptor(cnnlPoolingDescriptor_t desc);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \b pooling_desc that is previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \b pooling_desc. The information includes the pooling mode,
 * the NaN propagation mode \b maxpooling_nan_opt, the number of the pooling dimension \b dims,
 * the window size for each dimension \b window, the padding size for each dimension \b pad,
 * the stride of the sliding window for each dimension \b stride. To specify the stride of elements
 * in the window and whether to use ceil or floor to compute the output shape,
 * call the ::cnnlSetPoolingNdDescriptor_v2 function.
 *
 * @deprecated
 *   ::cnnlSetPoolingNdDescriptor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetPoolingNdDescriptor_v2 instead, which supports the parameters of \b dilation
 *   and \b ceil_mode that determines whether to use ceil or floor to compute the shape of output.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] dims
 *   Input. The number of dimensions in the input tensor of the pooling operation.
 *   The value of this parameter should be the same as the one you set in the input tensor descriptor.
 * @param[in] window
 *   Input. An array that stores the kernel for each dimension of the input tensor
 *   used in the pooling operation. If the dimension of input tensor is 5, the \b window is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the pooling operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If the dimension of input tensor is 5, the \b stride is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] padding
 *   Input. An array that stores the padding size for each dimension of the input tensor
 *   used in the pooling operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   starting and ending of that dimension. If \b dims is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 5-dimensional input tensor for pooling operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPoolingNdDescriptor(cnnlPoolingDescriptor_t pooling_desc,
                                                     cnnlPoolingMode_t mode,
                                                     cnnlNanPropagation_t maxpooling_nan_opt,
                                                     int dims,
                                                     const int window[],
                                                     const int padding[],
                                                     const int stride[]);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \b pooling_desc that is previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \b pooling_desc. Compared with ::cnnlSetPoolingNdDescriptor,
 * this function supports the stride of elements in the window \b dilation and whether to use ceil
 * or floor to compute the output shape \b ceil_mode. The information includes the pooling mode,
 * the NaN propagation mode \b maxpooling_nan_opt, the number of the pooling dimension \b dims,
 * the window size for each dimension \b window, the padding size for each dimension \b pad,
 * the stride of the sliding window for each dimension \b stride.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describing the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] dims
 *   Input. The number of dimensions in the input tensor of the pooling operation.
 *   The value of this parameter should be the same as the one you set in the input tensor descriptor.
 * @param[in] window
 *   Input. An array that stores the kernel for each dimension of the input tensor
 *   used in the pooling operation. If the dimension of input tensor is 5, the \b window is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the pooling operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If the dimension of input tensor is 5, the \b stride is
 *   on depth, height and width. The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] padding
 *   Input. An array that stores the padding size for each dimension of the input tensor
 *   used in the pooling operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   starting and ending of that dimension. If \b dims is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 * @param[in] dilation
 *   Input. A parameter that controls the stride of elements in the window.
 * @param[in] ceil_mode
 *   Input. When \b ceil_mode is true, the operator will use ceil mode to compute the output shape,
 *   otherwise, floor mode would be used.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 5-dimensional input tensor for pooling operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPoolingNdDescriptor_v2(cnnlPoolingDescriptor_t pooling_desc,
                                                        cnnlPoolingMode_t mode,
                                                        cnnlNanPropagation_t maxpooling_nan_opt,
                                                        int dims,
                                                        const int window[],
                                                        const int padding[],
                                                        const int stride[],
                                                        const int dilation[],
                                                        const bool ceil_mode);

// Group:Pooling
/*!
 * @brief Computes the pooling forward with the ::cnnlPoolingDescriptor_t \b pooling_desc,
 * and returns the results in the output tensor \b y.
 *
 * To set the factors for quantization:
 * - If offline symmetric quantization with position is used, you need call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors is used,
 *   you need call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization is used, you need call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \b y tensor is the result of pooling operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForward. You can get the size of the workspace with
 *   the ::cnnlGetPoolingWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - If the \b mode is not CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   - \b x - onchip_dtype  - \b y
 *   The supported combinations of data types are:
 *   - half-half-half.
 *   - float-float-float.
 * - If the \b mode is CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   - \b x - onchip_dtype  - \b y
 *   The supported combinations of data types are:
 *   - int16-float-float.
 *   - half-half-half.
 *   - float-float-float.
 *   - float-float-int16.
 *   - int16-float-int16.
 * - If the \b mode is CNNL_POOLING_MAX, this operation supports the int8 data type in 2-D pooling.
 *   The supported combinations of data types are:
 *   - int8-half-half.
 *   - int8-float-float.
 *   - int8-half-int8.
 *   - int8-float-int8.
 *   - half-half-int8.
 *   - float-float-int8.
 * - The \b x_desc parameter onchip_dtype must be either float or half, and if the \b x tensor
 *   data type is not int16 and int8, the onchip_dtype must be same to the data type of \b x tensor.
 *
 * @par Data Layout
 * - In the 2-D pooling forward operation, the supported data layout of the \b x tensor and \b y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 * - In the 3-D pooling forward operation, the supported data layout of the \b x tensor and \b y tensor are as follows:
 *   <b>Note that the layout of these tensor must be same.</b>
 *   - x tensor: \p CNNL_LAYOUT_NDHWC.
 *   - y tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - In the 2-D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \b x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \b y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - In the 3-D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \b x_desc should satisfy the following conditions: in_batch > 0, in_depth > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_depth is the depth of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \b y_desc should satisfy the following conditions: out_batch > 0, out_depth > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_depth is the depth of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 *    - In 3-D pooling forward operation, windows[0] * windows[1] * windows[2] must be less than 1530.
 *      The windows[0], windows[1] and windows[2] represent the depth, width and height of the pooling kernel size respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \b x
 *   tensor, \b y tensor, \b index tensor to NHWC in the 2-D pooling forward operation.
 *
 * @note
 * - Only supports 2-D and 3-D pooling.
 * - In the 2-D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN.
 *      Otherwise, the \b output value is the maximum value after the last NaN.
 * - In the 3-D maximum pooling, the \b output is unpredictable since a NaN value is noncomparable.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output: [[[6],[8],[14],[16]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingForward(cnnlHandle_t handle,
                                             const cnnlPoolingDescriptor_t pooling_desc,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y,
                                             void *workspace,
                                             size_t workspace_size);

// Group:Pooling
/*!
 * @brief Computes the pooling forward with the ::cnnlPoolingDescriptor_t \b pooling_desc,
 * returns the results in the output tensor \b y. Compared with ::cnnlPoolingForward,
 * ::cnnlPoolingForward_v2 provides better performance with extra input space.
 *
 * If you want to use the quantization function, set these factors as follows:
 *
 * - If offline symmetric quantization with position is used, you need call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors is used,
 *   you need call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization is used, you need call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitPoolingExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \b y tensor is the result of pooling operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForward_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForward_v2. You can get the size of the workspace with
 *   the ::cnnlGetPoolingWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - If the \b mode is not CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   - \b x - onchip_dtype  - \b y
 *   The supported combinations of data types are:
 *   - half-half-half.
 *   - float-float-float.
 * - If the \b mode is CNNL_POOLING_MAX, the supported combinations of data types are shown below with the following order:
 *   - \b x - onchip_dtype  - \b y
 *   The supported combinations of data types are:
 *   - int16-float-float.
 *   - half-half-half.
 *   - float-float-float.
 *   - float-float-int16.
 *   - int16-float-int16.
 * - If the \b mode is CNNL_POOLING_MAX, this operation supports the int8 data type in 2-D pooling.
 *   The supported combinations of data types are:
 *   - int8-half-half.
 *   - int8-float-float.
 *   - int8-half-int8.
 *   - int8-float-int8.
 *   - half-half-int8.
 *   - float-float-int8.
 * - The \b x_desc parameter onchip_dtype must be either float or half, and if the \b x tensor
 *   data type is not int16 and int8, the onchip_dtype must be same to the data type of \b x tensor.
 *
 * @par Data Layout
 * - In the 2-D pooling forward operation, the supported data layout of the \b x tensor and \b y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - In the 2-D pooling forward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - The parameters in the \b x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *      The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *      The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 *    - The parameters in the \b y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *      The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *      The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 *
 * @par API Dependency
 * - You need to initialize the \b extra_device_input with ::cnnlGetPoolingExtraInputSize and ::cnnlInitPoolingExtraInput
 *   before calling this function.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \b x
 *   tensor, \b y tensor, \b index tensor to NHWC in the 2-D pooling forward operation.
 *
 * @note
 * - Only supports 2-D and 3-D pooling.
 * - In the 2-D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN.
 *      Otherwise, the \b output value is the maximum value after the last NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input layout is NHWC, and input shape is (1,4,4,1).
       input: [[[1,2,3,4],
                [5,6,7,8],
                [9,10,11,12],
                [13,14,15,16]]]
       param:
         pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

     output: [[[6],[8],[14],[16]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingForward_v2(cnnlHandle_t handle,
                                                const cnnlPoolingDescriptor_t pooling_desc,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const void *extra_device_input,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y,
                                                void *workspace,
                                                size_t workspace_size);
// Group:Pooling
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the pooling operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_input_size.
 *
 * The size of extra workspace is based on the given information of the pooling
 * forward operation, including the pooling \b mode, the width of pooling output
 * \b out_w_size, the height of pooling output \b out_h_size.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  out_w_size
 *   Input. The width of pooling operation output.
 * @param[in]  out_h_size
 *   Input. The height of pooling operation output.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes that is used in
 *   the ::cnnlPoolingForward_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters should satisfy the following conditions:
 *   - The \b out_w_size and \b out_h_size should be greater than zero.
 *
 * @par API Dependency
 * - You need to call ::cnnlCreatePoolingDescriptor and ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2
 *   functions accordingly before calling this function.
 * - After calling this function, you need to call ::cnnlInitPoolingExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlPoolingForward_v2 function
 *   to perform the pooling operation.
 *
 * @note
 * - Only supports 2-D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetPoolingExtraInputSize(cnnlHandle_t handle,
                                                       cnnlPoolingMode_t mode,
                                                       const int out_w_size,
                                                       const int out_h_size,
                                                       size_t *extra_input_size);
// Group:Pooling
/*!
 * @brief Initializes the extra input data space \b extra_host_input on host. The data of
 * extra input is based on the given information of the pooling operation, including
 * the pooling descriptor \b pooling_desc, the input tensor descriptor \b x_desc
 * and the output tensor descriptor \b y_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[output] extra_host_input
 *   Output. Pointer to the host memory that is used as extra input space for
 *   ::cnnlPoolingForward_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call ::cnnlCreatePoolingDescriptor, ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2
 *   functions accordingly before calling this function.
 * - You need to get the size of the extra input data with ::cnnlGetPoolingExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - The allocated extra input should be passed to the function
 *   to perform the ::cnnlPoolingForward_v2 operation.
 *
 * @note
 * - Only supports 2-D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlInitPoolingExtraInput(cnnlHandle_t handle,
                                                    const cnnlPoolingDescriptor_t pooling_desc,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    void * extra_host_input);
// Group:Pooling
/*!
 * @brief Computes gradients of average or maximum pooling. This operation only supports 2-D and 3-D poolingbackward.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling_backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the pooling_backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the index of maximum in kernel.
 * @param[in] diff_y_desc
 *   Input. The descriptor of \b diff_y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \b diff_y tensor which is gradient used in average or maximum pooling operation.
 * @param[in] x_desc
 *   Input. The descriptor of the \b x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \b x tensor. This tensor is used to compute the index.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the \b diff_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the \b diff_x tensor which is the gradient of average or maximum pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "PoolingBackward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *
 *   - input - index - output.
 *
 *   The supported data type combinations are:
 *
 *   - half - int16 - half.
 *   - float - int32 - float.
 *
 * @par Data Layout
 * - <b>Note that the layout of input and output must be same.</b>
 * - In the 2-D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NHWC.
 * - In the 3-D pooling backward operation, the layout of input and output must be CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - For 2-D(height, width) pooling:
 *   - padding >= 0, kernel >= 1, stride >= 1.
 *   - batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *   - out_height > 0, out_width > 0.
 *   - In 2-D pooling backward operation, the product of the windows[0] and windows[1] must be
 *     less than or equal to 1500.
 *     The windows[0] and windows[1] represent the height and width of the pooling kernel size respectively.
 *
 * - In the 3-D pooling backward operation, the parameters should satisfy the following conditions:
 *    - padding >= 0, stride >= 1.
 *    - In 3-D pooling backward operation, windows[0] * windows[1] * windows[2] must be less than 3060.
 *      The windows[0], windows[1] and windows[2] represent the depth, height and width of the pooling kernel size respectively.
 *    - In 3-D maximum pooling backward operation, the input tensor \b x is not supported and the input
 *      \b y must not be NULL.
 * @note
 * - In the 2-D maximum pooling backward operation, you can use the input \b x to compute the index of maximum pooling with
 *   the input \b y not required. And in this situation, if the \b x has the NaN value, the output \b diff_x is unpredictable.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *   For 2-D poolingbackward:
     @verbatim
     4-D input[batch, in_height, in_width, in_channel].
     4-D index[batch, in_height, in_width, in_channel], maxpoolbackward need.
     4-D output[batch, out_height, out_width, in_channel].
     4-D padding[padding_top, padding_bottom, padding_left, padding_right].
     2-D kernel[kernel_height, kernel_width].
     2-D stride[stride_height, stride_width].
     Maxpoolbackward:
           input                index                              output
     ------------------ ------------------               -------------------------
     |       |        | |       |        |               |  1  |  0  |  0  |  0  |
     |   1   |  0.8   | |   0   |   2    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  | 0.8 |  0  |
     |-------+--------| |-------+--------| --backward--> |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  |  0  |  0  | 0.6 |
     |  0.4  |  0.6   | |   3   |   1    |               |-----+-----+-----+-----|
     |       |        | |       |        |               |  0  | 0.4 |  0  |  0  |
     ------------------ ------------------               -------------------------

     Avgpoolbackward:
           input                                                   output
     ------------------                                  -----------------------------
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |   1   |  0.8   |                                  |------+------+------+------|
     |       |        |                                  | 0.25 | 0.25 | 0.2  | 0.2  |
     |-------+--------|           --backward-->          |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     |  0.4  |  0.6   |                                  |------+------+------+------|
     |       |        |                                  | 0.1  | 0.1  | 0.15 | 0.15 |
     ------------------                                  -----------------------------
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingBackward(cnnlHandle_t handle,
                                              const cnnlPoolingDescriptor_t pooling_desc,
                                              const void *alpha,
                                              const cnnlTensorDescriptor_t y_desc,
                                              const void *y,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const void *beta,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \b pooling_desc that is previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * or backward operation to the pooling descriptor \b pooling_desc. The information includes the
 * pooling mode, the NaN propagation mode \b maxpooling_nan_op, the size of kernel for each dimension
 * \b window_height and \b window_width, the padding size for each dimension, the stride of the sliding
 * window for each dimension. To specify the stride of elements in the window and whether to use ceil or
 *  floor to compute the output shape, call the ::cnnlSetPooling2dDescriptor_v2 function.
 *
 * @deprecated
 *   ::cnnlSetPooling2dDescriptor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetPooling2dDescriptor_v2 instead, which supports the parameters of \b vertical_dilation,
 *   \b horizon_dilation and the \b ceil_mode that determines whether to use ceil or floor to compute the
 *   shape of output.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizon_stride
 *   Input. The horizon stride of the pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - You need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 4-dimensional input tensor for pooling operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPooling2dDescriptor(cnnlPoolingDescriptor_t pooling_desc,
                                                     const cnnlPoolingMode_t mode,
                                                     const cnnlNanPropagation_t maxpooling_nan_opt,
                                                     const int window_height,
                                                     const int window_width,
                                                     const int top_padding,
                                                     const int bottom_padding,
                                                     const int left_padding,
                                                     const int right_padding,
                                                     const int vertical_stride,
                                                     const int horizon_stride);

// Group:Pooling
/*!
 * @brief Initializes the pooling descriptor \b pooling_desc that is previously created with the
 * ::cnnlCreatePoolingDescriptor function, and sets the information about the pooling forward
 * operation to the pooling descriptor \b pooling_desc.
 * The information includes the pooling mode, the NaN propagation mode \b maxpooling_nan_opt,
 * the number of the pooling dimension \b dims, the window size for each dimension \b window_height & \b window_width,
 * the padding size for each dimension \b top_padding & \b bottom_padding & \b left_padding & \b right_padding,
 * the stride of the sliding window for each dimension \b vertical_stride & \b horizon_stride.
 * Compared with ::cnnlSetPooling2dDescriptor, this function supports the stride of elements in the window \b vertical_dilation & \b horizon_dilation
 * and whether to use ceil or floor to compute the output shape \b ceil_mode.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizon_stride
 *   Input. The horizon stride of the pooling.
 * @param[in] vertical_dilation
 *   Input. The vertical dilation of the pooling.
 * @param[in] horizon_dilation
 *   Input. The horizon dilation of the pooling.
 * @param[in] ceil_mode
 *   Input. Describes how the output shape is used. When \b ceil_mode is true, the operator will use ceil mode to compute the output shape,
 *   otherwise, floor mode would be used.
 *   For example, Maxpoolforward with ceil mode:
     @verbatim
                intput                                output
          -------------------                   ----------------
          |  1  |  0  |  0  |                   |   1   | 0.8  |
          |-----+-----+-----+                   |-------+------+
          |  0  |  0  | 0.8 |  --forward-->     |   1   |   1  |
          |-----+-----+-----+                   --------+------+
          |  1  |  0  |  1  |
          |-----+-----+-----+
     @endverbatim
 *   Maxpoolforward without ceil mode:
     @verbatim
                intput
          -------------------
          |  1  |  0  |  0  |                        output
          |-----+-----+-----+                       --------
          |  0  |  0  | 0.8 |  --forward-->         |   1  |
          |-----+-----+-----+                       -------+
          |  1  |  0  |  1  |
          |-----+-----+-----+
     @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - This function is prepared for ::cnnlPoolingForward or ::cnnlPoolingForward_v2 to compute pooling
 * - After finishing ::cnnlPoolingForward or ::cnnlPoolingForward_v2, you need to call the ::cnnlDestroyPoolingDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - Currently, only supports 4-dimensional input tensor for pooling operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetPooling2dDescriptor_v2(cnnlPoolingDescriptor_t pooling_desc,
                                           const cnnlPoolingMode_t mode,
                                           const cnnlNanPropagation_t maxpooling_nan_opt,
                                           const int window_height,
                                           const int window_width,
                                           const int top_padding,
                                           const int bottom_padding,
                                           const int left_padding,
                                           const int right_padding,
                                           const int vertical_stride,
                                           const int horizon_stride,
                                           const int vertical_dilation,
                                           const int horizon_dilation,
                                           const bool ceil_mode);

// Group:Pooling
/*!
 * @brief Returns the information of pooling descriptor \b pooling_desc. The information includes the
 * pooling mode, the NaN propagation mode \b maxpooling_nan_op, the size of kernel for each dimension
 * \b window_height and \b window_width, the padding size for each dimension, the stride of the sliding
 * window for each dimension.
 *
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in] maxpooling_nan_opt
 *   Input. The maxpooling_nan_opt describes whether to propagate NaN numbers.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @param[in] window_height
 *   Input. Height of pooling window.
 * @param[in] window_width
 *   Input. Width of pooling window.
 * @param[in] top_padding
 *   Input. Size of top padding.
 * @param[in] bottom_padding
 *   Input. Size of bottom padding.
 * @param[in] left_padding
 *   Input. Size of left padding.
 * @param[in] right_padding
 *   Input. Size of right padding.
 * @param[in] vertical_stride
 *   Input. The vertical stride of the pooling.
 * @param[in] horizon_stride
 *   Input. The horizon stride of the pooling.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function at first.
 * - ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2 needs to be called at second before this function.
 *
 * @note
 * - Only supports 2-D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPooling2dDescriptor(const cnnlPoolingDescriptor_t pooling_desc,
                                                     cnnlPoolingMode_t *mode,
                                                     cnnlNanPropagation_t *maxpooling_nan_opt,
                                                     int *window_height,
                                                     int *window_width,
                                                     int *top_padding,
                                                     int *bottom_padding,
                                                     int *left_padding,
                                                     int *right_padding,
                                                     int *vertical_stride,
                                                     int *horizontal_stride);
// Group:Pooling
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the pooling forward operation.
 *
 * The size of extra workspace is based on the given information of the pooling
 * forward operation, including the pooling \b mode, the width of pooling output
 * \b out_w_size, the height of pooling output \b out_h_size.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the pooling operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the pooling operation.
 *   For detailed information, see ::cnnlPoolingMode_t.
 * @param[in]  out_w_size
 *   Input. The width of pooling operation output.
 * @param[in]  out_h_size
 *   Input. The height of pooling operation output.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the pooling forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters should satisfy the following conditions:
 *   - The \b out_w_size and \b out_h_size should be greater than zero.
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follow:
 * - ::cnnlCreatePoolingDescriptor needs to be called before this function.
 * - ::cnnlSetPooling2dDescriptor or ::cnnlSetPooling2dDescriptor_v2 needs to be called before this function.

 * @note
 * - Only supports 2-D pooling.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPoolingWorkspaceSize(cnnlHandle_t handle,
                                                      cnnlPoolingMode_t mode,
                                                      const int out_w_size,
                                                      const int out_h_size,
                                                      size_t *workspace_size);
// Group:Sign
/*!
 * @brief Returns an output tensor \b y with signs of the elements of
 * input tensor \b x.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the sign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Sign Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for input tensor \b x and
 *   output tensor \b y must be half-half or float-float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - input tensor: each dimension must be more than zero
 *   - output tensor: each dimension must be more than zero
 *   - input tensor shape must be the same as output tensor shape.
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sign operation is as follows:
     @verbatim
      input an array by 1 * 2 * 2 * 1
      --> input: [[[[-1.0]], [[0.0]]], [[[2.0]], [[-6.5]]]]

      output an array by 1 * 2 * 2 * 1
      --> output: [[[[-1.0]], [[0.0]]], [[[1.0]], [[-1.0]]]]
      @endverbatim
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/cc/class/tensorflow/ops/sign
 * - https://pytorch.org/docs/stable/generated/torch.sign.html?highlight=sign
 */
cnnlStatus_t CNNL_WIN_API cnnlSign(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:Tile
/*!
 * @brief Copies and expands the input tensor to the shape of output tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the tile
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Tile Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     complex_half, complex_float.
 *
 * @par Data Layout
 * - The input tensor \b input and output tensor \b output are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - Every dimension of the input tensor should be divisible by the same dimension of the output
 *     tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the tile operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[1, 2], [3, 4]]

     output array by 3 * 2 * 2 --> output: [[[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]],
                                            [[1, 2], [3, 4]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/tile
 */
cnnlStatus_t CNNL_WIN_API cnnlTile(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:GradientDescent
/*!
 * @brief Creates an operation to update filter by gradient descent algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of \b grad tensor, which is the gradient.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter, which is the learning rate.
 * @param[in] var_desc
 *   Input. The descriptor of \b var tensor, which is the filter to be updated.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \b var tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Gradient Descent Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b grad tensor and \b var tensor must meet the following requirements:
 *   - The shape of \b grad tensor equals to the shape of \b var tensor.
 *
 * @par Data Type
 * - Data type of \b grad tensor, \b var tensor and \b lr parameter must be the same.
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - var tensors: half, float.
 *   - lr parameter: half, float
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
    @verbatim
    --> grad: an array [64, 78, 1024];
    --> var: an array [64, 78, 1024];
    --> lr: an array [1];
    Then we will get the output:
    --> var: an array [64, 78, 1024] same with grad;
    @endverbatim
 *
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGradientDescent(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t grad_desc,
                                              const void *grad,
                                              const void *lr,
                                              const cnnlTensorDescriptor_t var_desc,
                                              void *var);

// Group:Arange
/*!
 * @brief Creates a sequence of numbers that begins at the value pointed by \b start,
 * and extends by increments of the value pointed by \b step, up to but not including
 * the value pointed by \b end.
 *
 * @deprecated
 *   ::cnnlArange is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlArange_v2 instead, which offers tensor descriptor of output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the arange operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] start
 *   Input. Pointer to the host memory that stores the first value of output sequence.
 * @param[in] end
 *   Input. Pointer to the host memory that stores the upper limit of output sequence.
 * @param[in] step
 *   Input. Pointer to the host memory that stores the increment of output sequence.
 * @param[in] output_dtype
 *   Input. The data type of output sequence. Only supports int32, float, and half at present.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Arange Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *
 *   - start - end - step - output.
 *
 *   The supported data type combinations are:
 *
 *   - int32 - int32 - int32 - int32.
 *   - float - float - float - float.
 *   - float - float - float - half.
 *
 * @par Scale Limitation
 * - The input parameters should meet the following requirements:
 *   Where \p end_value is the value pointed to by \b end, \p start_value is the value pointed
 *   to by \b start, \p step_value is the value pointed to by \b step.
 *
 *   - The element number of \b output should be less than or equal to 16777216.
 *   - When the data type of output is set to int32, output value should be in the range of
 *   [-8388608, 8388607] on MLU200 series and [-16777216, 16777215] on MLU300 series respectively.
 *   - The sign of (\p end_value - \p start_value) should be consistent with the sign of
 *     \p step_value.
 *   - The \p step_value should be non-zero.
 *   - The \p end_value should not be equal to the \p start_value.
 *   - The \p start_value, \p step_value and \p end_value cannot be NaN or infinity.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the \b output_dtype to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the arange operation is as follows:
     @verbatim
     int start = 0;
     int end = 9;
     int step = 3;
     cnnlArange(handle, &start, $end, &step, CNNL_DTYPE_INT32, output);

     Then we will get the output:
     output: --> [0,3,6]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/range
 */

cnnlStatus_t CNNL_WIN_API cnnlArange(cnnlHandle_t handle,
                                     const void *start,
                                     const void *end,
                                     const void *step,
                                     const cnnlDataType_t output_dtype,
                                     void *output);

// Group:Arange
/*!
 * @brief Creates a sequence of numbers that begins at the value pointed by \b start,
 * and extends by increments of the value pointed by \b step. The length of sequence can
 * be inferred from output tensor descriptor \b output_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the arange operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t.
 * @param[in] start
 *   Input. Pointer to the host memory that stores the first value of output sequence.
 * @param[in] step
 *   Input. Pointer to the host memory that stores the increment of output sequence.
 * @param[in] output_desc
 *   Output. The descriptor of \b output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Arange Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *
 *   - start - step - output.
 *
 *   The supported data type combinations are:
 *
 *   - int32 - int32 - int32.
 *   - float - float - float.
 *   - float - float - half.
 *
 * @par Scale Limitation
 * - The input parameters should meet the following requirements:
 *   Where \p step_value is the value pointed to by \b step.
 *
 *   - The element number of \b output should be less than or equal to 16777216.
 *   - When the data type of output is set to int32, output value should be in the range of
 *   [-8388608, 8388607] on MLU200 series and [-16777216, 16777215] on MLU300 series respectively.
 *   - The \p step_value should be non-zero.
 *   - The \p start_value and \p step_value cannot be NaN or infinity.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the \b output_dtype to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the arange operation is as follows:
     @verbatim
     int start = 0;
     int step = 3;
     cnnlTensorDescriptor_t desc;
     cnnlCreateTensorDescriptor(&desc);
     cnnlSetTensorDescriptor(desc, CNNL_LAYOUT_ARRAY, CNNL_DTYPE_INT32, 1, [3]);
     cnnlArange_v2(handle, CNNL_COMPUTATION_FAST, &start, &step, desc, output);
     cnnlDestroyTensorDescriptor(desc);

     Then we will get the output:
     output: --> [0,3,6]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_docs/python/tf/range
 */

cnnlStatus_t CNNL_WIN_API cnnlArange_v2(cnnlHandle_t handle,
                                        const cnnlComputationPreference_t prefer,
                                        const void *start,
                                        const void *step,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);
// Group:Linspace
/*!
 * @brief Creates a one-dimensional evenly spaced number sequence over a specified interval.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLinspace operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] start
 *   Input. A float number that is the starting value of the sequence.
 * @param[in] end
 *   Input. A float number that is the ending value of the sequence.
 * @param[in] output_desc
 *   Input. The descriptor of \b output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Linspace Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of output tensor are:
 *   float, half, int32
 *
 * @par Performance Optimization
 * - To have better performance, set the data type of output to ::CNNL_DTYPE_FLOAT.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - The element number of \b output should be less than or equal to 33554432.
 * - On MLU200 series, when the data type of output is set to int32, output value should be in the
 *   range of [-8388608, 8388607].
 *
 * @par Example
 * - The example of the linspace operation is as follows:
     @verbatim
     float start = 0;
     float end = 9;
     shape of output = [3];
     data type = CNNL_DTYPE_FLOAT
     cnnlLinspace(handle, start, end, output_desc, output);

     Then we will get the output:
     output: --> [0,4.5,9]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/versions/r2.1/api_docs/python/tf/linspace
 * - https://pytorch.org/docs/1.9.0/generated/torch.linspace.html
 * - https://numpy.org/doc/stable/reference/generated/numpy.linspace.html
 */

cnnlStatus_t CNNL_WIN_API cnnlLinspace(cnnlHandle_t handle,
                                       const float start,
                                       const float end,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
// Group:MaskZero
/*!
 * @brief Maskzero is the fusion of masking and prelu, it mainly used in voice network.
 *
 * This operation performs two steps.
 * (1) If \b pad_label is equal to the lowest dimension of \b label, set the corresponding H * C
 * to zero in \b input. If the length of the lowest dimension of \b label(l_label) is greater than
 * the axis dimension of \b input(l_input), then define stride = l_label / l_input,  the step of
 * \b label is stride and the step of \b input is 1. So there may have useless data in \b label.
 * (2) If \b fuse_relu is true, then \b input multiplies \b slope when \b input is smaller
 * than zero.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *  maskzero operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *  Input. The descriptor of \b input tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[in] input
 *  Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] label_desc
 *  Input. The descriptor of \b label tensor. It is used to compare with \b pad_label. If it is
 *  equal to \b pad_label, the corresponding dimension in \b input is set to zero. Otherwise,
 *  this parameter is not used in the operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] label
 *  Input. Pointer to the MLU memory that stores the \b label tensor.
 * @param[in] output_desc
 *  Output. The descriptor of \b output tensor. For detailed information, see
 *  ::cnnlTensorDescriptor_t.
 * @param[out] output
 *  Output. Pointer to the MLU memory that stores the \b output tensor.
 * @param[in] fuse_relu
 *  Input. The parameter determines whether to multiply slope.
 * @param[in] axis
 *  Input. The dimension of \b input corresponding to the lowest dimension of \b label.
 * @param[in] pad_label
 *  Input. The parameter used to compare with \b label.
 * @param[in] slope
 *  Input. The parameter used to multiply \b input when \b input is smaller than zero.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "MaskZero Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor \b input,
 *   label tensor \b label and output tensor \b output.
 *  <b> Note that all the tensors must have the same data type. </b>
 *  - input: half, float.
 *  - label: half, float.
 *  - output: half, float.
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - input.dim[0] = label.dim[0]
 *   - input.dim[axis] <= label.dim[1]
 *   - \b axis should be greater than zero and less than the dimension of input.
 *   - The shape of input should be same as output.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the maskzero operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 * 3 and 2 * 2
     --> input: [[[[-1, 0, 1], [2, 3, 4]],
                  [[5, 6, 7], [8, 9, 10]]],
                 [[[11, 12, 13], [14, 15, 16]],
                  [[17, 18, 19], [20, 21, 22]]]]

     --> label: [[0, 1], [2, 3]]

     param:
       pad_label: 1.0, fuse_relu: 1, slope: 2.0, axis: 2,

     output array by 2 * 2 * 2 * 3 --> output: [[[[-2, 0, 1], [0, 0, 0]],
                                                 [[5, 6, 7], [0, 0, 0]]],
                                                [[[11, 12, 13], [14, 15, 16]],
                                                 [[17, 18, 19], [20, 21, 22]]]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlMaskZero(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t label_desc,
                                       const void *label,
                                       const cnnlTensorDescriptor_t output_desc,
                                       const void *output,
                                       const bool fuse_relu,
                                       const int axis,
                                       const float pad_label,
                                       const float slope);
// Group:Softmax
/*!
 * @brief Computes gradients of ::cnnlSoftmaxBackward with input tensors \b y and \b diff_y,
 * and returns the results in the output tensor \b diff_x.
 *
 * This function is used to create an operation of softmaxbackward and logsoftmaxbackward,
 * then is applied to a 3-dimensional input tensor rescaling them so that the elements of
 * the 3-dimensional output tensor is in the range of (0,1) and the summary along the reduction
 * dimension is 1.
 *
 * The softmax backward operation is only used in PyTorch.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The computing algorithm used to compute softmax backward, defined in
 *   ::cnnlSoftmaxAlgorithm_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computing procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] alpha
 *   Input. The parameter to scale the result of this function. This parameter is not supported
 *   currently. The value of alpha is 1 by default.
 * @param[in] beta
 *   Input. The parameter to scale the previous result in the output tensor \b diff_x. This
 *   parameter is not supported currently. The value of beta is 0 by default.
 * @param[in] y_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Softmax Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors \b y, \b diff_y and output tensor \b diff_x must be the same.
 * - The supported data types of the input tensor and output tensor are as follows:
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @par API Requirements
 * - You need to reshape the input tensor to 3 dimension before invoking this operation.
 *   and reshape the output tensor to original shape after invoking this operation.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set \b algorithm to \p CNNL_SOFTMAX_FAST,
 *   reshape the reduction dimension to the lowest dimension and set the \b mode to
 *   \p CNNL_SOFTMAX_MODE_LOW_DIMENSION. The result produced by \p CNNL_SOFTMAX_FAST algorithm is
 *   less precise than \p CNNL_SOFTMAX_ACCURATE. If the precision does not meet your requirement,
 *   you must replace the \b algorithm with \p CNNL_SOFTMAX_ACCURATE.
 *
 * @par Example
 * - The example of the softmax backward operation is as follows:
    @verbatim
      Example 1:
      input two arrays by 1 * 1 * 3 and 1 * 1 * 3
      --> y: [[[1,1,1]]]

      --> dy: [[[1,1,1]]]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION, algorithm: CNNL_SOFTMAX_FAST/CNNL_SOFTMAX_ACCURATE

      output an array by 1 * 1 * 3
      --> dx: [[[-2,-2,-2]]]

      Example 2:
      input two arrays by 1 * 1 * 3 and 1 * 1 * 3
      --> y: [[[1,1,1]]]

      --> dy: [[[1,1,1]]]

      params:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION, algorithm: CNNL_SOFTMAX_LOG

      output an array by 1 * 1 * 3
      --> dx: [[[-7.154,-7.154,-7.154]]]
    @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxBackward(cnnlHandle_t handle,
                                              cnnlSoftmaxAlgorithm_t algorithm,
                                              cnnlSoftmaxMode_t mode,
                                              const void *alpha,
                                              const cnnlTensorDescriptor_t y_desc,
                                              const void *y,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const void *beta,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:Copy
/*!
 * @brief Returns a copy of input tensor \b input in the output tensor \b output on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data types of input tensor \b input and output tensor \b output must be the same. The
 *  supported data types are as follows:
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double, complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double, complex_half, complex_float.
 *
 * @par Formula
 * - See "Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - Data type of input tensor and output tensor must be the same.
 * - Data layout of input tensor and output tensor must be the same.
 * - Shape of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - When the input or output tensor is non-contiguous, i.e. with non-contiguous strides set in the
 *   tensor descriptor, the total number of bytes spanned by either of the input or output tensor
 *   should be less than or equal to 2^31-1 (the maximum value for int32).
 *
 * @par Example
 * - The example of the copy operation is as follows:
     @verbatim
      input array by 2 * 2
      --> then: [[1, 8], [6, 4]]

      output array by 2 * 2
      --> output: [[1, 8], [6, 4]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/Snapshot
 */

cnnlStatus_t CNNL_WIN_API cnnlCopy(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:LRN
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the lrn operation.
 *
 * @deprecated
 * ::cnnlGetLrnWorkspaceSize is deprecated and will be removed in the future release. It is
 * recommended to use ::cnnlGetLrnWorkspaceSize_v2 instead.
 *
 * The size of extra workspace is based on the given information of the lrn operation,
 * including the input tensor descriptor \b input_desc, output tensor descriptor \b output_desc and
 * the lrn window width \b lrn_n.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cnnlLrn operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output_desc
 *    Output. The descriptor of output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] lrn_n
 *    Input. The width of lrn window.
 *  @param[out]  size
 *   Output. An extra space size needed in lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The shape of input should be the same with output's.
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra workspace should be passed to the ::cnnlLrn or ::cnnlLrn_v2 function
 *   to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  unsigned lrn_n,
                                                  size_t *size);

// Group:LRN
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the lrn operation. Compared with ::cnnlGetLrnWorkspaceSize,
 * ::cnnlGetLrnWorkspaceSize_v2 supports lrn within channel computing mode.
 *
 * The size of extra workspace is based on the given information of the lrn operation,
 * including the input tensor descriptor \b input_desc, output tensor descriptor \b output_desc and
 * the lrn window width \b lrn_n.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cnnlLrn operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output_desc
 *    Input. The descriptor of output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 *  @param[in] lrn_n
 *    Input. The width of lrn window.
 *  @param[out]  size
 *     Output. An extra space size needed in lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The shape of input should be the same with output's.
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra workspace should be passed to the ::cnnlLrn or ::cnnlLrn_v2 function
 *   to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnWorkspaceSize_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  cnnlLrnMode_t lrn_mode,
                                                  unsigned lrn_n,
                                                  size_t *size);

// Group:LRN
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host memory that is used
 * as an extra input data to optimize the lrn operation. You need to allocate memory both on host
 * and MLU based on the size returned in \b extra_input_size.
 *
 * @deprecated
 * ::cnnlGetLrnExtraInputSize is deprecated and will be removed in the future release. It is
 * recommended to use ::cnnlGetLrnExtraInputSize_v2 instead.
 *
 * The size of extra input data is based on the given information of the lrn operation, including
 * the output tensor descriptor \b output_desc and lrn window width \b lrn_n.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  extra_input_size
 *    Output. A host pointer to the returned size of the extra input data in bytes
 *    that is used in the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 * ::cnnlSetTensorDescriptor functions.
 * - After calling this function, you need to call ::cnnlInitLrnExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnExtraInputSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  unsigned lrn_n,
                                                  size_t *extra_input_size);
// Group:LRN
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host memory that is used
 * as an extra input data to optimize the lrn operation. You need to allocate memory both on host
 * and MLU based on the size returned in \b extra_input_size. Compared with ::cnnlGetLrnExtraInputSize,
 * ::cnnlGetLrnExtraInputSize_v2 supports lrn within channel computing mode.
 *
 * The size of extra input data is based on the given information of the lrn operation, including
 * the output tensor descriptor \b output_desc and lrn window width \b lrn_n.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[out]  extra_input_size
 *    Output. A host pointer to the returned size of the extra input data in bytes
 *    that is used in the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 * ::cnnlSetTensorDescriptor functions.
 * - After calling this function, you need to call ::cnnlInitLrnExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLrnExtraInputSize_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  cnnlLrnMode_t lrn_mode,
                                                  unsigned lrn_n,
                                                  size_t *extra_input_size);

// Group:LRN
/*!
 * @brief Initializes the extra input data space \b extra_host_input on host.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the lrn operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - The shape of input should be the same with output's.
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 * - \b lrn_k should be greater than 1e-5.
 * - \b lrn_beta should be greater than 0.01.
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetLrnExtraInputSize_v2.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions.
 * - The allocated extra input should be passed to the ::cnnlLrn_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitLrnExtraInput(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void* extra_host_input);

/*! The parameters of the ::cnnlSpace2batch or ::cnnlBatch2space operation that
 * hold space to batch or batch to space information including the height
 * dilation and the width dilation of the input tensor.
 *
 * The dilation is usually used in the convolution function. It represents the
 * size of which the convolution kernel increases in certain dimension. Based on
 * the dilation, the shape of the tensor in ::cnnlSpace2batch and
 * ::cnnlBatch2space functions are changed. If the dilation is set to 1,
 * the convolution is implemented normally. If the dilation is set to 2, the
 * interval of the input elements should be set to 1. The dilation may be in
 * different dimensions.
 */
typedef struct cnnlSpaceBatchParam_s {
  unsigned int dilation_height; /*!< The dilation in the height dimension. */
  unsigned int dilation_width;  /*!< The dilation in the width dimension. */
} cnnlSpaceBatchParam_t;

/*! The parameters of the ::cnnlSpace2batchNd or ::cnnlBatch2spaceNd operation
 * that hold space to batch or batch to space information including the \b block
 * array and the \b pad array.
 *
 * @deprecated
 *   ::cnnlSpaceBatchNdParam_t is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSpaceBatchNdDescriptor_t instead.
 *
 * The ::cnnlSpace2batchNd and ::cnnlBatch2spaceNd function pairs need a \b block
 * parameter, this can get the dilation value of every dimension. The function
 * pairs can add or cut the pads in some dimensions. So, they need a pad
 * parameter, this can get the pad value of every dimension.
 *
 * You can allocate the memory at static data area or at stack. If you allocate
 * it at static data area, it will occupy the memory at whole life cycle of process.
 * If you allocate memory as a local variable, the memory will be allocated and
 * freed automatically. You can also allocate the memory dynamically using
 * ``malloc()`` function and free the memory using ``free()`` function for
 * this structure.
 *
 * You can set value for this structure fields using "=" operator.
 */
typedef struct {
  uint32_t *block;
  /*!< The \b block parameter is an array. The element of the \b block array means
   * the dilation of the input tensor dimension. For example, if the input
   * layout is NHWC, the shape is [3, 4, 4, 1], then, the \b block_num parameter
   * may be 2, the \b block shape is [2, 2], this means the dilation of H and W is
   * 2.
   */
  uint32_t block_num;
  /*!< The number of elements in the \b block array. */
  uint32_t *pad;
  /*!< An array that stores the pre-pad and post-pad of
   * every dimension. For example, if you add pad on the H and W dimensions,
   * before H dimension and after H dimension add 1 pad, before W dimension and
   * after W dimension add 1 pad, then the \b pad_num parameter should be set
   * to 4, the pad shape should be set to [1, 1, 1, 1].
   */
  uint32_t pad_num;
  /*!< The number of elements in the \b pad array. */
} cnnlSpaceBatchNdParam_t;

// Group:Space2batch
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 * workspace to run the ::cnnlSpace2batch operation.
 *
 * The \b size of extra workspace is based on the given information of the
 * ::cnnlSpace2batch operation, according to algorithm and input dimension. The
 * parameter \b size is the output parameter, when the function returns
 * ::CNNL_STATUS_SUCCESS, also, the value of \b size is the memory size you
 * should allocate and pass to the ::cnnlSpace2batch operation function.
 *
 * If the operation does not need the extra memory, the \b size value will
 * return zero. If the \b size parameter returns zero, you do not need call
 * the memory allocation function for the operation workspace any more.
 *
 * This function supports 4-dimensional tensor only. Usually it uses in the
 * convolutional artificial intelligence network. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlSpace2batch operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor described in
 *   \b input_desc and output tensor described in \b output_desc. And the data
 *   type must be the same between input tensor and output tensor.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as
 *   follows:
 *   <b>Note that this function supports any combinations of the layout.</b>
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The total number of the input tensor dimensions should be equal to the output's.
 * - The parameter \b size is a pointer, should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlTensorDescriptor_t output_desc,
                                     size_t *size);
// Group:Space2batch
/*!
 * @brief Changes tensor shape from space to batch. It is usually used in
 * TensorFlow framework.
 *
 * Space to batch makes the convolution operation with dilation more faster
 * and easier. The idea is, reform the NHWC block to another shape. Usually,
 * it uses in the artificial intelligence, and this function supports
 * 4-dimensional tensor only. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches, the total number of pixels will not be changed. In this way,
 * in each small batch, the convolution operation can be operated without
 * dilation. Generally this will accelerate the convolution calculation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The dilation value, including the height dimension and the width
 *   dimension of the input tensor. For detailed information,
 *   see ::cnnlSpaceBatchParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetSpace2batchWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor described
 *   in \b input_desc and output tensor described in \b output_desc. And the
 *   data type must be the same between input tensor and output tensor.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as
 *   follows:
 *   <b>Note that this function supports any combinations of the layout.</b>
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor and the parameters must meet the following
 *   requirements:
 *   - \p ni * \p hi * \p wi * \p ci < 0xffffffff
 *   - The output data size must be equal to input data: \p Output == \p Input
 *   - \p ni * \p hi * \p wi * \p ci == \p no * \p ho * \p wo * \p co
 *   - \p no == \p ni * \p param.dilation_height * \p param.dilation_width
 *   - \p ho == \p hi / \p param.dilation_height
 *   - 0 == \p hi % \p param.dilation_height
 *   - \p wo == \p wi / \p param.dilation_height
 *   - 0 == \p wi % \p param.dilation_width
 *   - \p ci == \p co
 *   - \p param.dilation_width > 0
 *   - \p param.dilation_height > 0
 *   Where \p ni represents the input number of N-dimension, \p hi represents
 *   the input number of the height dimension, \p wi represents the input number
 *   of the width dimension, \p ci represents the input number of the channel
 *   dimension, \p no represents the output number of the N-dimension, \p ho
 *   represents the output number of the height dimension, \p wo represents the
 *   output number of the width dimension, \p co represents the output number of
 *   the channel dimension.About param.dilation_height and param.dilation_width,
 *   see ::cnnlSpaceBatchParam_t.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 * - You need to call the ::cnnlBatch2space function to reform the shape to the
 *   original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 * - Also, the input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch
 */
cnnlStatus_t CNNL_WIN_API cnnlSpace2batch(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchParam_t param,
                                      void *workspace,
                                      size_t workspace_size);
// Group:Batch2spaceNd
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlSpace2batchNd operation.
 *
 * @deprecated
 *   ::cnnlGetSpace2batchNdWorkspaceSize is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGetSpace2batchNdExtraInputSize instead.
 *
 * The size of extra workspace is based on the given information of the
 * ::cnnlSpace2batchNd operation, according to algorithm and input dimension.
 * The parameter \b workspace_size is the output parameter, when the function
 * return ::CNNL_STATUS_SUCCESS, also, the value of \b workspace_size is the
 * memory size you should allocate and pass to the ::cnnlSpace2batchNd
 * operation function.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlSpace2batchNd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b workspace_size is a pointer, should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchNdWorkspaceSize(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const cnnlSpaceBatchNdParam_t *param,
                                        size_t *workspace_size);
// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from space to batch. It is usually
 * used in TensorFlow framework, and in the artificial intelligence.
 *
 * @deprecated
 *   ::cnnlSpace2batchNd is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSpace2batchNd_v2 instead.
 *
 * Space to batch makes the convolution operation with dilation more faster
 * and easier. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches. In this way, in each small batch, the convolution operation
 * can be operated without dilation. Generally this will accelerate the
 * convolution calculation.
 *
 * Compared with the ::cnnlSpace2batch function, this function supports more
 * dimensional tensor than the ::cnnlSpace2batch function. The ::cnnlSpace2batch
 * function only supports 4-dimensional tensor, ::cnnlSpace2batchNd function supports
 * a larger dimensional tensor(the maximum dimension value is 8). But the tensor layout
 * parameter is not used in ::cnnlSpace2batchNd function. It will not change
 * the layout of the input tensor, so you must prepare appropriate data layout
 * for the function before you call it.
 *
 * In addition, this function supports adding pad in every dimension before
 * implementing the space to batch operation. This can fuse pad operation
 * and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetSpace2batchNdWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input
 *   and output tensor \b output. And the data type must be the same
 *   between input tensor and output tensor.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The spatial, block, and pad parameters must meet the following requirements:
 *   - (\p spatial[i] + \p pre-pad[i] + \p post-pad[i]) % \p parameter.block[i] == 0
 *     At the formula given above, The \b spatial is the dimension array from the
 *     second dimension of a tensor. For example:
 *     The dimension of a tensor is N * H * W * C,
 *     then the spatial is H * W, or H * W * C.
 *     The channel dimension may be included too.
 *   - The dimenstion number of spatial should be the same with the block.
 *   - \p dimension_number_of_tensor > \p dimension_number_of_block
 *   - All elements of the parameter \b block should be greater than 0.
 *   - The pixel number of the tensor:
 *     dimension0 * dimension1 * dimension2 * ... * dimensionN < 0xffffffff
 *   - The maximum dimension number of tensor: CNNL_DIM_MAX, the value is 8 now.
 *     That is to say:
 *     tensor[dimension1, dimension2, ... dimensionN], the maximum of N is 8.
 *   - The minimum dimension number of tensor: 2.
 *     That is to say:
 *     tensor[dimension1, dimension2] is valid tensor.
 *     tensor[dimension1] is invalid tensor.
 *   - In brief, the range of the parameter value is:
 *     - The value range of block_num is [1, 4].
 *     - The value range of pad_num is [2, 8], and must be even number.
 *     - The dimension number range of input tensor is [2, 8].
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchNdWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 * - You need to call the ::cnnlBatch2spaceNd function to reform the shape to the
 *   original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlSpace2batchNd(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       const cnnlSpaceBatchNdParam_t *param,
                                       void *workspace,
                                       size_t workspace_size);

// Group:Batch2space
/*!
 * @brief Returns in \b size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlBatch2space operation.
 *
 * The size of extra workspace is based on the given information of the
 * ::cnnlBatch2space operation, according to algorithm and input dimension.
 * The parameter \b size is the output parameter, when the function return
 * ::CNNL_STATUS_SUCCESS, also, the value of \b size is the memory size you
 * should allocate and pass to the ::cnnlBatch2space operation function.
 *
 * If the operation does not need the extra memory, the \b size value will
 * return zero. If the \b size parameter returns zero, you do not need call
 * the memory allocation function for the operation workspace any more.
 *
 * Usually this function uses in the artificial intelligence, and this
 * function supports 4-dimensional tensor only.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlBatch2space operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor described in
 *   \b input_desc and output tensor described in \b output_desc. And the data
 *   type must be the same between input tensor and output tensor.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as
 *   follows:
 *   <b>Note that this function supports any combinations of the layout.</b>
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The total number of the input tensor dimensions should be equal to the output's.
 * - The parameter \b size is a pointer, should not be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t output_desc,
                                       size_t *size);

// Group:Batch2space
/*!
 * @brief Changes tensor shape from batch to space. It is usually used in
 * TensorFlow framework.
 *
 * Batch to space is the reverse operation of the space to batch. The idea is,
 * reform the changed block back to the original shape. Usually it uses in the
 * artificial intelligence, and this function supports 4-dimensional tensor
 * only. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2space
 * should be called to reform the shape to the original shape.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The dilation value, including the height dimension and the width
 *   dimension of the input tensor. For detailed information,
 *   see ::cnnlSpaceBatchParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetBatch2spaceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor described in
 *   \b input_desc and output tensor described in \b output_desc. And the data
 *   type must be the same between input tensor and output tensor.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as
 *   follows:
 *   <b>Note that this function supports any combinations of the layout.</b>
 *   - input tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - \p ni * \p hi * \p wi * \p ci < 0xffffffff
 * - The data size of input must be equal to the output: \p Output = \p Input
 * - \p ni * \p hi * \p wi * \p ci == \p no * \p ho * \p wo * \p co
 * - \p no == \p ni / (\p param.dilation_height * \p param.dilation_width)
 * - \p ho == \p hi * \p param.dilation_height
 * - 0 == \p ho % \p param.dilation_height
 * - \p wo == \p wi * \p param.dilation_height
 * - 0 == \p wo % \p param.dilation_width
 * - \p ci == \p co
 * - \p param.dilation_width > 0
 * - \p param.dilation_height > 0
 *   Where \p ni represents the input number of N-dimension, \p hi represents
 *   the input number of the height dimension, \p wi represents the input number
 *   of the width dimension, \p ci represents the input number of the channel
 *   dimension, \p no represents the output number of the N-dimension, \p ho
 *   represents the output number of the height dimension, \p wo represents the
 *   output number of the width dimension, \p co represents the output number of
 *   the channel dimension.About param.dilation_height and param.dilation_width,
 *   see ::cnnlSpaceBatchParam_t.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 * - Also, the input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/batch-to-space-n-d
 */
cnnlStatus_t CNNL_WIN_API cnnlBatch2space(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchParam_t param,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Batch2spaceNd
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used
 * as an extra workspace to run the ::cnnlBatch2spaceNd operation.
 *
 * @deprecated
 *   ::cnnlGetBatch2spaceNdWorkspaceSize is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGetBatch2spaceNdExtraInputSize instead.
 *
 * The size of extra workspace is based on the given information of the
 * ::cnnlBatch2spaceNd operation, according to algorithm and input dimension.
 * The parameter \b workspace_size is the output parameter, when the function
 * return ::CNNL_STATUS_SUCCESS, also, the value of \b workspace_size is the
 * memory size you should allocate and pass to the ::cnnlBatch2spaceNd
 * operation function.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace
 *   in bytes that is used in the ::cnnlBatch2spaceNd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b workspace_size is a pointer to the MLU memory, should not
 *   be NULL.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceNdWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlSpaceBatchNdParam_t *param,
                                      size_t *workspace_size);
// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from batch to space.
 * It is usually used in TensorFlow framework, and in the artificial intelligence.
 *
 * @deprecated
 *   ::cnnlBatch2spaceNd is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlBatch2spaceNd_v2 instead.
 *
 * It is the reverse operation of the ::cnnlSpace2batchNd operation. The idea
 * is, reform the changed block back to the original shape. Take the following
 * diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2spaceNd
 * should be called to reform the shape to the original shape.
 *
 * Compared with the ::cnnlBatch2space function, this function supports more
 * dimensions than the ::cnnlBatch2space function.
 * The ::cnnlBatch2space function only supports 4-dimensional tensor,
 * ::cnnlBatch2spaceNd function supports a larger dimensional tensor(the maximum
 * dimension value is 8). But the tensor layout parameter is not used in
 * ::cnnlBatch2spaceNd function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports cutting pad in every dimension after
 * implementing the batch to space N-Dimensional operation.
 * This can fuse pad operation and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdParam_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for
 *   the operation. For more information about workspace, see "Cambricon CNNL
 *   User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the operation. You can get the size of the workspace with the
 *   ::cnnlGetBatch2spaceNdWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input
 *   and output tensor \b output. And the data type must be the same between
 *   input tensor and output tensor.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the parameters must meet the following
 *   requirements:
 *   - (\p spatial[i] * \p parameter.block[i] - \p pre-pad[i] - \p post-pad[i]) > 0
 *   - \p batch % (\p block[0] * \p block[1] * ... * \p block[i - 1]) == 0
 *     At the formula given above, the spatial is the dimension array from the
 *     second dimension of a tensor. For example:
 *     the dimension of a tensor is N * H * W * C,
 *     then the spatial is H * W, or H * W * C.
 *     The C dimension may be included too.
 *     - The dimension number of spatial should be the same with the block.
 *     - \p dimension_number_of_tensor > \p dimension_number_of_block
 *     - All elements of the parameter \b block should be greater than 0.
 *   - The pixel number of the tensor:
 *     dimension0 * dimension1 * dimension2 * ... * dimensionN < 0xffffffff
 *   - The maximum dimension number of tensor: CNNL_DIM_MAX, the value is 8 now.
 *     That is to say:
 *     tensor[dimension1, dimension2, ... dimensionN], the maximum of N is 8.
 *   - The minimum dimension number of tensor: 2.
 *     That is to say:
 *     tensor[dimension1, dimension2] is valid tensor.
 *     tensor[dimension1] is invalid tensor.
 *   - In brief, the range of the parameter value is:
 *     - The value range of block_num is [1, 4].
 *     - The value range of pad_num is [2, 8], and must be even number.
 *     - The dimension number range of input tensor is [2, 8].
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceNdWorkspaceSize function to get the size of the memory
 *   workspace in MLU device that the function uses.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlBatch2spaceNd(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlSpaceBatchNdParam_t *param,
                                      void *workspace,
                                      size_t workspace_size);

// Group:Batch2spaceNd
/*!
 * @brief Initializes the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 * operation descriptor \b param that is previously created with the
 * ::cnnlCreateSpaceBatchNdDescriptor function, and sets the information about
 * the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 operation to the descriptor
 * \b param. The information includes the block array \b block, the number of
 * the block dimensions \b block_num, the padding size for each dimension
 * \b pad, the number of pad dimensions \b pad_num.
 *
 * @param[in] param
 *   Input. The descriptor of the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 *   operation. For detailed information, see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] block
 *   Input. An array that stores the dilation of the input tensor dimension.
 * @param[in] block_num
 *   Input. The number of elements in the \b block array.
 *   Currently, the value of this parameter can only be in the range of [1, 8].
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of
 *   the input tensor used in the operation. For each dimension, the padding
 *   size represents the number of zeros to be concatenated at the start and end
 *   of that dimension. If the tensor dimension number is set to 4,
 *   the padding is on top, bottom, left, and right. If \b dimension number is
 *   set to 5, the padding is on front, back, top, bottom, left, and right.
 *   The value of this parameter should be greater than or equal to 0.
 * @param[in] pad_num
 *   Input. The number of elements in the \b pad array. Currently, the value of
 *   this parameter should be set to an even value, and the value is greater or
 *   equal to 2. Usually it is set two times larger than \b block_num.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlSetSpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t param,
                                     const uint32_t block[],
                                     uint32_t block_num,
                                     const uint32_t pad[],
                                     uint32_t pad_num);

// Group:Batch2spaceNd
/*!
 * @brief Creates a descriptor pointed by \b param for
 * ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 operation, and allocates memory
 * for holding the information about the operation. The information is defined
 * in ::cnnlSpaceBatchNdDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[out] param
 *   Output. A host pointer to the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 *   descriptor that holds information about the operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the
 *   ::cnnlSetSpaceBatchNdDescriptor function to initialize and set the
 *   information to the descriptor.
 * - You need to call the ::cnnlDestroySpaceBatchNdDescriptor function to
 *   destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateSpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t *param);

// Group:Batch2spaceNd
/*!
 * @brief Destroys a ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 descriptor
 * \b param that is previously created with the
 * ::cnnlCreateSpaceBatchNdDescriptor function.
 *
 * The descriptor is defined in ::cnnlSpaceBatchNdDescriptor_t and holds the
 * information about the ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2
 * operation.
 *
 * @param[in] param
 *   Input. The ::cnnlSpace2batchNd_v2 or ::cnnlBatch2spaceNd_v2 descriptor to be
 *   destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - This function should be called to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroySpaceBatchNdDescriptor(
                                     cnnlSpaceBatchNdDescriptor_t param);

// Group:Batch2spaceNd
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host
 * memory that is used as an extra input data for the ::cnnlSpace2batchNd_v2
 * operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_size.
 *
 * The size of extra input data is based on the given information of the
 * ::cnnlSpace2batchNd_v2 operation, according to algorithm and input dimension.
 * The parameter \b extra_input_size is the output parameter, if the function
 * returns ::CNNL_STATUS_SUCCESS. The value of \b extra_input_size is the
 * memory size you should allocate in host. You can allocate the memory
 * dynamically using ``malloc()`` function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_input_size
 *   Output. A host pointer to the returned size of the extra input data
 *   in bytes that is used in the ::cnnlSpace2batchNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_size is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - After calling this function, you can allocate the right size host memory
 *   and call the ::cnnlInitSpace2batchNdExtraInput function to initialize the
 *   memory.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSpace2batchNdExtraInputSize(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     size_t *extra_input_size);

// Group:Batch2spaceNd
/*!
 * @brief Initializes the extra input data space \b extra_host_input on host.
 *
 * When the ::cnnlSpace2batchNd_v2 operation runs, it needs a memory buffer on
 * MLU device, this buffer must be initialized before uses. So, you must give
 * the same size buffer in the host and initialize the memory first. This
 * function initializes the host memory buffer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the ::cnnlSpace2batchNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_host_input is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - Before you call this function, you should call the
 *   ::cnnlGetSpace2batchNdExtraInputSize function first to get the size of the
 *   host memory that the operation needs.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlInitSpace2batchNdExtraInput(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     void *extra_host_input);

// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from space to batch. It is usually
 * used in TensorFlow framework, and in the artificial intelligence.
 *
 * Space to batch makes the convolution operation with dilation more faster
 * and easier. Take the following diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  -->  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * As shown in the schematic diagram, each batch will be extended into several
 * small batches. In this way, in each small batch, the convolution operation
 * can be operated without dilation. Generally this will accelerate the
 * convolution calculation.
 *
 * Compared with the ::cnnlSpace2batch function, this function supports more
 * dimensions for input tensor than the ::cnnlSpace2batch function's. The
 * ::cnnlSpace2batch function only supports 4-dimensional tensor,
 * ::cnnlSpace2batchNd function supports a larger dimensional tensor (the
 * maximum dimension value is 8). But the tensor layout parameter is not used
 * in ::cnnlSpace2batchNd function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports adding pad in every dimension before
 * implementing the space to batch operation. This can fuse pad operation
 * and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitSpace2batchNdExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in
 *   the operation. You can get the size of the extra input with the
 *   ::cnnlGetSpace2batchNdExtraInputSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section in "Cambricon CNNL User
 *   Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input
 *   and output tensor \b output. And the data type must be the same
 *   between input tensor and output tensor.
 *   - input tensor: int8, int16, int32, half, float, uint8, uint16, uint32, uint64, int64, bool.
 *   - output tensor: int8, int16, int32, half, float, uint8, uint16, uint32, uint64, int64, bool.
 *
 * @par Scale Limitation
 * - The spatial, block, and pad parameters must meet the following requirements:
 *   - (\p spatial[i] + \p pre-pad[i] + \p post-pad[i]) % \p parameter.block[i] == 0
 *     At the formula given above, the \b spatial is the dimension array from the
 *     second dimension of the input tensor. For example:
 *     The dimension of an input tensor is N * H * W * C,
 *     then the spatial is H * W, or H * W * C.
 *     The channel dimension may be included too.
 *     There are two directions in every dimension: front and rear. At the
 *     formula given above, the \p pre-pad and \p post-pad mean adding or
 *     cutting pad in the two directions. About the \p block, see
 *     ::cnnlSpaceBatchNdDescriptor_t.
 *   - The dimenstion number of spatial should be the same with the block.
 *   - The number of dimensions of input tensor should be greater than the
 *     number of dimensions of \b block.
 *   - All elements of the parameter \b block should be greater than 0.
 *   - The pixel number of the input and output tensor:
 *     dimension0 * dimension1 * dimension2 * ... * dimensionN < 0xffffffff
 *   - The maximum dimension number of tensor: CNNL_DIM_MAX, the value is 8 now.
 *     That is to say:
 *     tensor[dimension1, dimension2, ... dimensionN], the maximum of N is 8.
 *   - The minimum dimension number of tensor: 2.
 *     That is to say:
 *     tensor[dimension1, dimension2] is valid tensor.
 *     tensor[dimension1] is invalid tensor.
 *   - In brief, the ranges of the parameter values are:
 *     - The \b block is an array of rank 1, the \b block_num is the element number of the array.
 *       The value range of \b block_num is >= 1 and <= 7, but the number of valid block element
 *       in \b block array must be >= 0 and <= 4. If the \b block_num is greater than 4,
 *       the operation may continue computing by dropping invalid elements, if the valid elements
 *       still greater than 4 after this, the operation will raise error and exit.
 *       When the invalid \b block elements meet the following requirements, it will be dropped:
 *
 *       1. Some of the \b block element value is 1, and at the same time,
 *          the \b pad element value of corresponding pad dimension is 0.
 *       2. The dimension of invalid \b block element must be at the front or rear of the valid
 *          value. The dimension of invalid \b pad element is the same with \b block.
 *
 *       The invalid elements meeting the above requirements have no effect to the operation
 *       and the operation will continue computing by dropping the invalid elements.
 *
 *       For example,
 *       A \b block shape is:  [1, 1, 2, 2, 1, 2, 1],
 *       and the \b pad shape is: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0].
 *       The \b block_num is 7, and the \b pad_num is 14.
 *       Although the \b pad is a rank 1 array as input, it is used as a rank 2 array in fact,
 *       because there are two directions (front and rear) in one dim. So, it is like this:
 *       [[0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0]], it matches with \b block
 *       one by one.
 *       The \b block_num is greater than 4. According to the description above,
 *       the valid block shape is: [2, 2, 1, 2], the front and rear of these dimensions
 *       is invalid: block value is 1 and pad value is 0.
 *       So, in conclusion, the \b block and \b block_num value is in the right range.
 *     - The value range of \b pad_num is >= 2 and <= 14, and must be even number.
 *       The number of valid pad element in \b pad array must be >= 0 and <= 8. Drops the invalid
 *       element number from \b pad_num, the valid number must be in this range, or the operation
 *       will raise error and exit. The invalid \b pad element value is described at \b block
 *       and \b block_num parameter above.
 *     - The dimension number range of input tensor is [2, 8].
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetSpace2batchNdExtraInputSize function to get the size of the memory
 *   in MLU device and host that the function uses.
 * - You need to call the ::cnnlBatch2spaceNd_v2 function to reform the shape to
 *   the original shape after the convolution operation.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - Gives a simple example for this function, note:
 *   - If gives "fake" comment at the line tail, means it is the fake code.
 *   - For more easier to understand, the following code does not check the
 *     return value of the function, please check the return value in the actual
 *     code.
 *   - The usage of the batch to space N-dimension operation is similar to the
 *     space to batch N-dimension operation, so only gives the usage of space to
 *     batch N-dimension operation.
   @verbatim
   // prepare the parameter, usually these parameters is given from caller:
   void *input = ...;  // fake
   void *output = ...;  // fake
   cnnlTensorDescriptor_t input_desc = ...;  // fake
   cnnlTensorDescriptor_t output_desc = ...;  // fake

   // create and set op descriptor:
   cnnlSpaceBatchNdDescriptor_t op_desc;
   uint32_t block[] = {2, 2};
   uint32_t block_num = 2;
   uint32_t pad[] = {1, 0, 1, 0};
   uint32_t pad_num = 4;
   cnnlCreateSpaceBatchNdDescriptor(&op_desc);
   cnnlSetSpaceBatchNdDescriptor(op_desc, block, block_num, pad, pad_num);

   // do the space to batch N-dimension process:
   size_t extra_input_size;
   cnnlGetSpace2batchNdExtraInputSize(handle, input_desc, op_desc, &extra_input_size);
   void *extra_host_input = allocate_host_memory(extra_input_size);  // fake
   void *extra_device_input = allocate_device_memory(extra_input_size);  // fake
   cnnlInitSpace2batchNdExtraInput(handle, input_desc, op_desc, extra_host_input);
   copy_from_host_to_device(extra_host_input, extra_device_input);  // fake
   free_the_host_memory(extra_host_input);  // fake
   cnnlSpace2batchNd_v2(handle,
       input_desc, input,
       output_desc, output,
       op_desc,
       extra_device_input,
       extra_input_size);

   // destroy the object which you have created:
   cnnlDestroySpaceBatchNdDescriptor(op_desc);

   // free the device memory when the queue is end:
   free_device_memory(extra_device_input);
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlSpace2batchNd_v2(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     const void *extra_device_input,
                                     size_t extra_input_size);

// Group:Batch2spaceNd
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host
 * memory that is used as an extra input data for the ::cnnlBatch2spaceNd_v2
 * operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_size.
 *
 * The size of extra input data is based on the given information of the
 * ::cnnlBatch2spaceNd_v2 operation, according to algorithm and input dimension.
 * The parameter \b extra_input_size is the output parameter, if the function
 * returns ::CNNL_STATUS_SUCCESS. The value of \b extra_input_size is the
 * memory size you should allocate in host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_input_size
 *   Output. A host pointer to the returned size of the extra host input
 *   in bytes that is used in the ::cnnlBatch2spaceNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_size is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - After calling this function, you can allocate the right size host memory
 *   and call the ::cnnlInitBatch2spaceNdExtraInput function to initialize the
 *   memory.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatch2spaceNdExtraInputSize(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     size_t *extra_input_size);

// Group:Batch2spaceNd
/*!
 * @brief Initializes the extra input data space \b extra_host_input on host.
 *
 * When the ::cnnlBatch2spaceNd_v2 operation runs, it needs a memory buffer on
 * MLU device, this buffer must be initialized before uses. So in the host must
 * have the same size buffer and be initialized first. This function initializes
 * the host memory buffer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  param
 *   Input. Descriptor of parameter. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[out]  extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the ::cnnlBatch2spaceNd_v2 operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_host_input is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - Before you call this function, you should call the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function first to get the size of the
 *   host memory that the operation needs.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See ::cnnlSpace2batchNd_v2
 */
cnnlStatus_t CNNL_WIN_API cnnlInitBatch2spaceNdExtraInput(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     void *extra_host_input);

// Group:Batch2spaceNd
/*!
 * @brief Changes N-Dimensional tensor shape from batch to space.
 * It is usually used in TensorFlow framework, and in the artificial intelligence.
 *
 * It is the reverse operation of the ::cnnlSpace2batchNd_v2 operation. The idea
 * is, reform the changed block back to the original shape. Take the following
 * diagram for example:
 *
   @verbatim
               # + # + # +       # # #
               x o x o x o       # # #    batch 0_0
    batch 0    # + # + # +  <--  # # #
               x o x o x o       -----
               # + # + # +       + + +
               x o x o x o       + + +    batch 0_1
                                 + + +
                                 -----
                                 x x x
                                 x x x    batch 0_2
                                 x x x
                                 -----
                                 o o o
                                 o o o    batch 0_3
                                 o o o
   @endverbatim
 *
 * After the convolution operation, the operation named ::cnnlBatch2spaceNd_v2
 * should be called to reform the shape to the original shape.
 *
 * Compared with the ::cnnlBatch2space function, this function supports more
 * dimensions for input tensor than the ::cnnlBatch2space function's.
 * The ::cnnlBatch2space function only supports 4-dimensional tensor,
 * ::cnnlBatch2spaceNd_v2 function supports a larger dimensional tensor (the
 * maximum dimension value is 8). But the tensor layout parameter is not used in
 * ::cnnlBatch2spaceNd_v2 function. It will not change the layout of the input
 * tensor, so you must prepare appropriate data layout for the function before
 * you call it.
 *
 * In addition, this function supports cutting pad in every dimension after
 * implementing the batch to space N-Dimensional operation.
 * This can fuse pad operation and increase the performance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] param
 *   Input. The block and pad value. For detailed information,
 *   see ::cnnlSpaceBatchNdDescriptor_t.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitBatch2spaceNdExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input in bytes that needs to be used in
 *   the operation. You can get the size of the extra input with the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Space2batch_batch2space Operator" section
 *   in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input
 *   and output tensor \b output. And the data type must be the same between
 *   input tensor and output tensor.
 *   - input tensor: int8, int16, int32, half, float, uint8, uint16, uint32, uint64, int64, bool.
 *   - output tensor: int8, int16, int32, half, float, uint8, uint16, uint32, uint64, int64, bool.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the parameters must meet the following
 *   requirements:
 *   - (\p spatial[i] * \p parameter.block[i] - \p pre-pad[i] - \p post-pad[i]) > 0
 *   - \p batch % (\p block[0] * \p block[1] * ... * \p block[i - 1]) == 0
 *     At the formula given above, the spatial is the dimension array from the
 *     second dimension of a tensor. For example:
 *     the dimension of an input tensor is N * H * W * C,
 *     then the spatial is H * W, or H * W * C.
 *     The C dimension may be included too.
 *     There are two directions in every dimension: front and rear. At the
 *     formula given above, the \p pre-pad and \p post-pad mean adding or
 *     cutting pad in the two directions. About the \p block, see
 *     ::cnnlSpaceBatchNdDescriptor_t.
 *     - The dimension number of spatial should be the same with the block.
 *     - The number of dimensions of input tensor should be greater than the
 *       number of dimensions of \b block.
 *     - All elements of the parameter \b block should be greater than 0.
 *   - The pixel number of the input and output tensor:
 *     dimension0 * dimension1 * dimension2 * ... * dimensionN < 0xffffffff
 *   - The maximum dimension number of tensor: CNNL_DIM_MAX, the value is 8 now.
 *     That is to say:
 *     tensor[dimension1, dimension2, ... dimensionN], the maximum of N is 8.
 *   - The minimum dimension number of tensor: 2.
 *     That is to say:
 *     tensor[dimension1, dimension2] is valid tensor.
 *     tensor[dimension1] is invalid tensor.
 *   - In brief, the ranges of the parameter values are:
 *     - The \b block is an array of rank 1, the \b block_num is the element number of the array.
 *       The value range of \b block_num is >= 1 and <= 7, but the number of valid block element
 *       in \b block array must be >= 0 and <= 4. If the \b block_num is greater than 4,
 *       the operation may continue computing by dropping invalid elements, if the valid elements
 *       still greater than 4 after this, the operation will raise error and exit.
 *       When the invalid \b block elements meet the following requirements, it will be dropped:
 *
 *       1. Some of the \b block element value is 1, and at the same time,
 *          the \b pad element value of corresponding pad dimension is 0.
 *       2. The dimension of invalid \b block element must be at the front or rear of the valid
 *          value. The dimension of invalid \b pad element is the same with \b block.
 *
 *       The invalid elements meeting the above requirements have no effect to the operation
 *       and the operation will continue computing by dropping the invalid elements.
 *
 *       For example,
 *       A \b block shape is:  [1, 1, 2, 2, 1, 2, 1],
 *       and the \b pad shape is: [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0].
 *       The \b block_num is 7, and the \b pad_num is 14.
 *       Although the \b pad is a rank 1 array as input, it is used as a rank 2 array in fact,
 *       because there are two directions (front and rear) in one dim. So, it is like this:
 *       [[0, 0], [0, 0], [1, 1], [1, 1], [0, 0], [1, 1], [0, 0]], it matches with \b block
 *       one by one.
 *       The \b block_num is greater than 4. According to the description above,
 *       the valid block shape is: [2, 2, 1, 2], the front and rear of these dimensions
 *       is invalid: block value is 1 and pad value is 0.
 *       So, in conclusion, the \b block and \b block_num value is in the right range.
 *     - The value range of \b pad_num is >= 2 and <= 14, and must be even number.
 *       The number of valid pad element in \b pad array must be >= 0 and <= 8. Drops the invalid
 *       element number from \b pad_num, the valid number must be in this range, or the operation
 *       will raise error and exit. The invalid \b pad element value is described at \b block
 *       and \b block_num parameter above.
 *     - The dimension number range of input tensor is [2, 8].
 *
 * @par API Dependency
 * - Before you call this function, you can first call the
 *   ::cnnlGetBatch2spaceNdExtraInputSize function to get the size of the memory
 *   in MLU device and host that the function uses.
 *
 * @par Performance Optimization
 * - The input channel and the output channel dimension should not be too
 *   small. For example, if the channel dimension is less than 256, the I/O
 *   performance may be very low. It is recommended that the size of channel
 *   dimension is the times of 64 and greater than 256.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The usage is similar to space to batch operation,
 *   see ::cnnlSpace2batchNd_v2
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/batch_to_space_nd
 */
cnnlStatus_t CNNL_WIN_API cnnlBatch2spaceNd_v2(
                                     cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output,
                                     const cnnlSpaceBatchNdDescriptor_t param,
                                     const void *extra_device_input,
                                     size_t extra_input_size);
// Group:Momentum
/*!
 * @brief Updates \b var according to the momentum scheme.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  var_desc
 *   Input. A descriptor of input tensor \b var. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  var
 *  Input and output. Pointer to the MLU memory that stores the \b var tensor to be updated
 *  according to momentum algorithm. This input refers to filter in the artificial intelligence generally.
 * @param[in] accum_desc
 *   Input. A descriptor of \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out]  accum
 *   Input and output. Pointer to the MLU memory that stores the \b accum tensor. The \b accum
 *   is the impulse, which indicates the accumulation of \b diff.
 * @param[in]  diff_desc
 *   Input. The descriptor of \b diff tensor. For detailed information,
 *  see ::cnnlTensorDescriptor_t.
 * @param[in]  diff
 *  Input. Pointer to the MLU memory that stores the \b diff tensor. The \b diff is the gradient
 *  for updating \b var.
 * @param[in]  lr
 *   Input. Pointer to the MLU memory that stores the learning rate.
 * @param[in]  moment
 *   Input. Pointer to the MLU memory that stores the attenuation of impulse.
 * @param[in]  use_nesterov
 *   Input. Determines whether to use Nesterov momentum to update \b var or not.
 *   Set \b use_nesterov = true if you want to use Nesterov momentum,
 *   otherwise the default momentum is used.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Momentum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the momentum operation is as follows:
    @verbatim
     --> var: an array [64, 78, 1024];
     --> accum: an array [64, 78, 1024];
     --> diff: an array [64, 78, 1024];
     --> lr: an array [1];
     --> moment: an array [1];
     --> use_nesterov: false;
     Then we will get the output:
     --> var: an array [64, 78, 1024] same with input;
     --> accum: an array [64, 78, 1024] same with input;
    @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlMomentum(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t var_desc,
                                       void *var,
                                       const cnnlTensorDescriptor_t accum_desc,
                                       void *accum,
                                       const cnnlTensorDescriptor_t diff_desc,
                                       const void *diff,
                                       const void *lr,
                                       const void *moment,
                                       const bool use_nesterov);

// Group:KerasMomentum
/*!
 * @brief Updates \b var using Keras Momentum or Nesterov Momentum algorithm.
 *
 * This function is used to update \b var according to the momentum scheme. Set \b
 * use_nesterov to true if you want to use Nesterov Momentum, otherwise the default
 * Keras Momentum is used.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of \b var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \b var tensor to be updated
 *   according to momentum scheme, which generally refers to filter in the convolutional network.
 * @param[in] accum_desc
 *   Input. The descriptor of \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input and output. Pointer to the MLU memory that stores the \b accum tensor. The \b accum
 *   is the impulse, which indicates the accumulation of \b diff.
 * @param[in] diff_desc
 *   Input. The descriptor of \b diff tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff
 *   Input. Pointer to the MLU memory that stores the \b diff tensor. The \b diff is the gradient
 *   for updating \b var.
 * @param[in] lr
 *   Input. The learning rate.
 * @param[in] momentum
 *   Input. The attenuation of impulse.
 * @param[in] use_nesttrov
 *   Input. Determines whether to use Nesterov momentum to update \b var or not.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "KerasMomentum Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The data shape of input tensors and output tensors must be the same.
 * - The number of dimensions of each input tensor should be no more than 8.
 *
 * @par Data Type
 * - The data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the keras momentum operation is as follows:
     @verbatim
      --> var: an array [64, 78, 1024];
      --> accum: an array [64, 78, 1024];
      --> diff: an array [64, 78, 1024];
      --> lr: an array [1];
      --> momentum: an array [1];
      --> use_nesterov: false;
      Then we will get the output:
      --> var: an array [64, 78, 1024] same with input;
      --> accum: an array [64, 78, 1024] same with input;
     @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlKerasMomentum(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t var_desc,
                                            void *var,
                                            const cnnlTensorDescriptor_t accum_desc,
                                            void *accum,
                                            const cnnlTensorDescriptor_t diff_desc,
                                            const void *diff,
                                            const void *lr,
                                            const void *momentum,
                                            bool use_nesterov);

// Group:LayerNorm
 /*!
 * @brief Computes layer normalization forward over last certain number of dimensions on input tensor
 *        \b x, \b filter and \b bias, and returns the results in the output tensor \b y.
 *
 * This function may need extra MLU memory as the workspace to improve the layer normalization performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetLayerNormOpWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *    Input. Normalize over the last dimensions from axis to end.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input filter and bias tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The value of this pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor. The value of this pointer can be NULL.
 *  @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the layer normalization forward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the layer normalization forward
 *   operation. You can get the size of the workspace with the ::cnnlGetLayerNormOpWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the output mean and rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output mean tensor.
 * @param[out] saved_rstd
 *   Output. Pointer to the MLU memory that stores the inverse of the variance tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLayerNormForward function to perform the
 *   layer normalization operation.
 *
 * @note
 * - \b filter and \b bias should be NULL or not NULL at the same time.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the layer normalization forward operation is as follows:
     @verbatim
      x: an array [64, 78, 1024]
      axis: 2 (layer normalization for the last dimension)
      filter and bias: [1024] or [1, 1, 1024]
      Then we will get the output:
      y: an array [64, 78, 1024] same with input
      saved_mean and saved_rstd: [64, 78] or [64, 78, 1]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLayerNormForward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               int axis,
                                               const cnnlTensorDescriptor_t filter_bias_desc,
                                               const void *filter,
                                               const void *bias,
                                               float eps,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t y_desc,
                                               void *y,
                                               const cnnlTensorDescriptor_t mean_rstd_desc,
                                               void *saved_mean,
                                               void *saved_rstd);

// Group:LayerNorm
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the layer normalization forward operation.
 *
 * The size of extra workspace is based on the given information of the layer normalization forward
 * operation, including the input tensor descriptors \b x_desc, and reduction axis \b axis. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer
 *   normalization forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. Normalize over the last dimensions from \b axis to the end.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *    Output. Pointer to the returned size of the extra workspace in bytes that is used in the layer
 *    normalization forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLayerNormOpWorkspaceSize(cnnlHandle_t handle,
                                                          int axis,
                                                          const cnnlTensorDescriptor_t x_desc,
                                                          size_t *size);

// Group:Activation
/*!
 * @brief Creates a descriptor pointed by \b activation_desc for an activation forward or
 *        backward operation, and allocates memory for holding the information about the
 *        activation operation. The information is defined in ::cnnlActivationDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] activation_desc
 *   Input. A host pointer to the activation descriptor that holds information about the
 *   activation operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetActivationDescriptor_v5 function to
 *   initialize and set the information to the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateActivationDescriptor(cnnlActivationDescriptor_t *activation_desc);

// Group:Activation
/*!
 * @brief Destroys an activation descriptor \b activation_desc that is previously created with the
 *        ::cnnlCreateActivationDescriptor function.
 *
 * The activation descriptor is defined in ::cnnlActivationDescriptor_t and holds the information
 * about the activation forward or backward operation.
 *
 * @param[in] activation_desc
 *   Input. The activation descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlActivationForward
 *   or ::cnnlActivationBackward function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the activation descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyActivationDescriptor(cnnlActivationDescriptor_t activation_desc);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, and TF_LeakyReLU.
 *
 * @deprecated
 *   ::cnnlSetActivationDescriptor is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlSetActivationDescriptor_v3 instead, which supports parameters of \b prefer
 *   that determines to compute with faster algorithm or higher precision, and \b sliced_dim that
 *   determines which dimension of the input to be sliced when \b mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @note
 * - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor(cnnlActivationDescriptor_t activation_desc,
                                                      cnnlActivationMode_t mode,
                                                      cnnlNanPropagation_t nan_prop,
                                                      float coef);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, and TF_LeakyReLU.
 *
 * @deprecated
 *   ::cnnlSetActivationDescriptor_v2 is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlSetActivationDescriptor_v3 instead, which supports parameter of \b sliced_dim
 *   that determines which dimension of the input to be sliced when \b mode is set to \p CNNL_ACTIVATION_GLU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \b prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, and ELU.
 *   - Activation backward:
 *     - GELU.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v2(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6, and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor and cnnlSetActivationDescriptor_v2
 * If set \b mode to \p CNNL_ACTIVATION_GLU, this function allows you to choose whether to
 * perform activation operations with \b sliced_dim.
 *
 * @deprecated
 *   ::cnnlSetActivationDescriptor_v3 is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlSetActivationDescriptor_v4 instead, which supports the configurable
 *   \b gamma and \b scale parameters when \b mode is set to \p CNNL_ACTIVATION_SELU
 *   or \p CNNL_ACTIVATION_ELU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \b prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, and ELU.
 *   - Activation backward:
 *     - GELU.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \b x is sliced in half along \b sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_GLU.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *   - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *   - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v3(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *   - Hardsigmoid, Hardswish and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2 and
 * cnnlSetActivationDescriptor_v3, this function allows you to choose whether to perform activation
 * operations with \b gamma and \b scale when \b mode is \p CNNL_ACTIVATION_ELU,
 * \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \b prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish and ELU.
 *   - Activation backward:
 *     - GELU and Hardswish.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \b x is sliced in half along \b sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   This parameter has a default value: 1.67326319217681884765625.
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   This parameter has a default value: 1.05070102214813232421875.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for different frameworks are as follows:
 *   - When \b mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \b gamma and \b scale. Details:
 *     - Pytorch Framework: \b gamma = 1/6 and \b scale = 1/2.
 *     - Tensorflow Framework: \b gamma = 1/5 and \b scale = 1/2.
 *     - Onnx Framework: \b gamma and \b scale are set according to requirements.
 *   - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *   - When \b mode is \p CNNL_ACTIVATION_SELU in ::cnnlActivationForward:
 *     - Pytorch: \b gamma and \b scale should be the default value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v4(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *     Hardsigmoid, Hardswish, ELU_V2 and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish, ELU,
 *     ELU_V2 and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2,
 * cnnlSetActivationDescriptor_v3 and cnnlSetActivationDescriptor_v4, this function supports the
 * \b is_result parameter that allows users to choose different implementation methods when \b mode
 * is \p CNNL_ACTIVATION_ELU.
 *
 * @deprecated
 *  \p CNNL_ACTIVATION_ELU mode will be deprecated gradually in the future release. It is
 *   recommended to use \p CNNL_ACTIVATION_ELU_V2 instead.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \b prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish, ELU and ELU_V2.
 *   - Activation backward:
 *     - GELU and Hardswish.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_LEAKYRELU,
 *   \p CNNL_ACTIVATION_TF_LEAKYRELU, or \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \b x is sliced in half along \b sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \b mode . For details, please refer to the "Note" section described below.)
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_ELU_V2, \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \b mode . For details, please refer to the "Note" section described below.)
 * @param[in] is_result
 *   Input. A boolean value describes the implementation logic of ::cnnlActivationBackward when
 *   the \b mode is \p CNNL_ACTIVATION_ELU or \p CNNL_ACTIVATION_ELU_V2. The default value of
 *   this parameter is true.
 *   When the \b mode is \p CNNL_ACTIVATION_ELU and this parameter is true, the implementation
 *   logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = gamma * (x + scale * coef) * diff_y, x <= 0;
     @endverbatim
 *   When the \b mode is \p CNNL_ACTIVATION_ELU and this parameter is false, the implementation
 *   logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(gamma * x) * diff_y, x <= 0;
     @endverbatim
 *   When the \b mode is \p CNNL_ACTIVATION_ELU_V2 and this parameter is true, the implementation
 *   logic of ELU_V2 is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = coef * (x + scale * gamma) * diff_y, x <= 0;
     @endverbatim
 *   When the \b mode is \p CNNL_ACTIVATION_ELU_V2 and this parameter is false, the implementation
 *   logic of ELU_V2 is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(coef * x) * diff_y, x <= 0;
     @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for different frameworks are as follows:
 *
 *   - When \b mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \b gamma and \b scale. Details:
 *     - Pytorch Framework: \b gamma = 1/6 and \b scale = 1/2.
 *     - Tensorflow Framework: \b gamma = 1/5 and \b scale = 1/2.
 *     - Onnx Framework: \b gamma and \b scale are set according to requirements.
 *     The implementation of HARDSIGMOID in ::cnnlActivationForward is given below:
       @verbatim
        HARDSIGMOID = max(0, min(1, gamma * x + scale));
       @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward,
 *     users can utilize the ELU_V2 to implement ELU in Pytorch Framework by setting:
 *     - Pytorch: \b coef = 1.0, \b scale = 1.0, \b gamma is a parameter passed in by the user.
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward,
 *     users can utilize the ELU_V2 to implement SELU in Pytorch Framework by setting:
 *     - Pytorch: \b coef = 1.0, \b scale = 1.05070102214813232421875, \b gamma = 1.67326319217681884765625.
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward, in addition to setting parameters
 *      as in ::cnnlActivationForward, you also need to set \b is_result = False in Pytorch Framework.
 *   - The difference between \p CNNL_ACTIVATION_ELU and \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward is that:
      @verbatim
       x>0: ELU = coef *(exp(x)-1); ELU_V2 = scale * gamma * (exp(coef * x)-1).
       x<0: ELU = x; ELU_V2 = scale * x.
      @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_SELU in ::cnnlActivationForward:
 *     - Pytorch: \b gamma and \b scale should be the default value,
 *       where gamma is 1.67326319217681884765625 and scale is 1.05070102214813232421875.
 *     The implementation of SELU in ::cnnlActivationForward is given below:
       @verbatim
        SELU = scale * gamma * (exp(x) - 1), x <= 0;
        SELU = scale * x, x > 0;
       @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v5(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale,
                                                         bool is_result);

// Group:Activation
/*!
 * @brief Initializes the activation descriptor \b activation_desc that is previously created
 * with the ::cnnlCreateActivationDescriptor function, and sets the information about the
 * activation forward or backward operation to the activation descriptor \b activation_desc.
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported for activation forward or backward operation separately:
 *
 * - Activation forward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, LeakyReLU, TF_LeakyReLU, Caffe_ReLU6,
 *   - Hardsigmoid, Hardswish and GLU.
 *
 * - Activation backward:
 *   - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, Hardsigmoid, Hardswish and TF_LeakyReLU.
 *
 * Compared with ::cnnlSetActivationDescriptor, cnnlSetActivationDescriptor_v2,
 * cnnlSetActivationDescriptor_v3, cnnlSetActivationDescriptor_v4 and cnnlSetActivationDescriptor_v5,
 * this function supports the \b approximate parameter that allows users to choose different
 * approximation algorithm when \b mode is \p CNNL_ACTIVATION_GELU.
 *
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The activation \b mode defined in ::cnnlActivationMode_t enum.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlActivationPreference_t enum.
 *   The following activation functions support \b prefer modes for activation forward or backward
 *   operation separately:
 *   - Activation forward:
 *     - Sigmoid, Tanh, SELU, GELU, Hardsigmoid, Hardswish and ELU.
 *   - Activation backward:
 *     - GELU and Hardswish.
 *   The default value of this parameter is \p CNNL_ACTIVATION_HIGH_PRECISION.
 * @param[in] nan_prop
 *   Input. The NaN propagation modes defined in ::cnnlNanPropagation_t enum.
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_RELU or
 *   \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward.
 * @param[in] coef
 *   Input. A scalar value which is only used when you set \b mode to \p CNNL_ACTIVATION_CLIPPED_RELU,
 *   \p CNNL_ACTIVATION_ELU, \p CNNL_ACTIVATION_LEAKYRELU, \p CNNL_ACTIVATION_TF_LEAKYRELU, or
 *   \p CNNL_ACTIVATION_CAFFE_RELU6.
 * @param[in] sliced_dim
 *   Input. An integer value deciding which dimension of the input to be sliced.
 *   The element number of sliced input dimension must be an even number.
 *   The activation input tensor \b x is sliced in half along \b sliced_dim
 *   to form a (in the lower dimension part) and b (in the higher dimension part).
 *   This parameter is only used when you set \b mode to \p CNNL_ACTIVATION_GLU.
 * @param[in] gamma
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \b mode . For details, please refer to the "Note" section described below.)
 * @param[in] scale
 *   Input. A hyper parameter which is only used when you set \b mode to \p CNNL_ACTIVATION_ELU,
 *   \p CNNL_ACTIVATION_HARDSIGMOID or \p CNNL_ACTIVATION_SELU.
 *   (The gamma and scale, as a pair of hyper parameters, have multiple meanings in different
 *   activation \b mode . For details, please refer to the "Note" section described below.)
 * @param[in] is_result
 *   Input. A boolean value describes the implementation logic of ::cnnlActivationBackward when
 *   the \b mode is \p CNNL_ACTIVATION_ELU. The default value of this parameter is true.
 *   When the value of this parameter is true, the implementation logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = gamma * (x + scale * coef) * diff_y, x <= 0;
     @endverbatim
 *   When the value of this parameter is false, the implementation logic of ELU is as follows:
     @verbatim
      diff_x = scale * diff_y, x > 0;
      diff_x = scale * coef * gamma * exp(gamma * x) * diff_y, x <= 0;
     @endverbatim
 * @param[in] approximate
 *   Input. A boolean value describes the implementation logic of different GELU approximation algorithm
 *   when the \b mode is \p CNNL_ACTIVATION_GELU. The default value of this parameter is true.
 *   When the value of this parameter is true, the implementation logic of GELU is as follows:
     @verbatim
      output = 0.5 * x * (1 + Tanh(sqrt(2/pi) * (x + 0.044715 * x^3)));
     @endverbatim
 *   When the value of this parameter is false, the implementation logic of GELU is as follows:
     @verbatim
      output = 0.5 * x * (1 + erf(x / sqrt(2)));
     @endverbatim
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateActivationDescriptor function to
 *   create the activation descriptor.
 * - You need to call the ::cnnlDestroyActivationDescriptor function to destroy the descriptor.
 *
 * @note
 * - The parameter settings for multiple activation \b mode in different frameworks are as follows:
 *
 *   - When \b mode is \p CNNL_ACTIVATION_HARDSIGMOID in Activation forward and backward, the semantics of
 *     hardsigmoid in different frameworks can be achieved by setting different values for \b gamma and \b scale. Details:
 *     - Pytorch Framework: \b gamma = 1/6 and \b scale = 1/2.
 *     - Tensorflow Framework: \b gamma = 1/5 and \b scale = 1/2.
 *     - Onnx Framework: \b gamma and \b scale are set according to requirements.
 *     The implementation of HARDSIGMOID in ::cnnlActivationForward is given below:
       @verbatim
        HARDSIGMOID = max(0, min(1, gamma * x + scale));
       @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward,
 *     users can utilize the ELU_V2 to implement ELU in Pytorch Framework by setting:
 *     - Pytorch: \b coef = 1.0, \b scale = 1.0, \b gamma is a parameter passed in by the user.
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward,
 *     users can utilize the ELU_V2 to implement SELU in Pytorch Framework by setting:
 *     - Pytorch: \b coef = 1.0, \b scale = 1.05070102214813232421875, \b gamma = 1.67326319217681884765625.
 *   - When \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward, in addition to setting parameters
 *      as in ::cnnlActivationForward, you also need to set \b is_result = False in Pytorch Framework.
 *   - The difference between \p CNNL_ACTIVATION_ELU and \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationForward is that:
      @verbatim
      x>0: ELU = coef *(exp(x)-1); ELU_V2 = scale * gamma * (exp(coef * x)-1).
      x<0: ELU = x; ELU_V2 = scale * x.
      @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_SELU:
 *     - Pytorch: \b gamma and \b scale should be the default value,
 *       where gamma is 1.67326319217681884765625 and scale is 1.05070102214813232421875.
 *     The implementation of SELU in ::cnnlActivationForward is given below:
       @verbatim
        SELU = scale * gamma * (exp(x) - 1), x <= 0;
        SELU = scale * x, x > 0;
       @endverbatim
 *
 *   - When \b mode is \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_RELU6 in ::cnnlActivationForward:
 *     - Pytorch: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.5: \b nan_prop = CNNL_PROPAGATE_NAN.
 *     - Tensorflow2.4 and earlier versions: \b nan_prop = CNNL_NOT_PROPAGATE_NAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetActivationDescriptor_v6(cnnlActivationDescriptor_t activation_desc,
                                                         cnnlActivationMode_t mode,
                                                         cnnlActivationPreference_t prefer,
                                                         cnnlNanPropagation_t nan_prop,
                                                         float coef,
                                                         int sliced_dim,
                                                         float gamma,
                                                         float scale,
                                                         bool is_result,
                                                         bool approximate);

// Group:Activation
/*!
 *  @brief Retrieves an activation descriptor \b activation_desc that is previously created with the
 *  ::cnnlSetActivationDescriptor_v6 function.
 *
 *  @param[in] activation_desc
 *    Input. The descriptor of the input activation parameters. For detailed information,
 *    see ::cnnlActivationDescriptor_t.
 *  @param[out] mode
 *    Output. Pointer to the host memory that holds information about the selected mode.
 *    For detailed information, see ::cnnlActivationMode_t.
 *  @param[out] prefer
 *    Output. Pointer to the host memory that holds information about the precision preference.
 *    For detailed information, see ::cnnlActivationPreference_t.
 *  @param[out] nan_prop
 *    Output. Pointer to the host memory that holds information about the NaN propagation.
 *    For detailed information, see ::cnnlNanPropagation_t.
 *  @param[out] coef
 *    Output. Pointer to the host memory that holds information about the scaling coefficient
 *    used in the ELU, LeakyReLU TF_LeakyReLU or Caffe_ReLU6.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] sliced_dim
 *    Output. Pointer to the host memory that holds information about the sliced dimension
 *    used in the GLU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] gamma
 *    Output. Pointer to the host memory that holds information about a hyper parameter \b gamma
 *    used in the ELU, Hardsigmoid or SELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] scale
 *    Output. Pointer to the host memory that holds information about a hyper parameter \b scale
 *    used in the ELU, Hardsigmoid or SELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] is_result
 *    Output. Pointer to the host memory that holds information about a boolean value that describes
 *    ELU back propagation implementation logic, which is only used in the ELU.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @param[out] approximate
 *    Output. Pointer to the host memory that holds information about a boolean value that describes
 *    different GELU approximation algorithm.
 *    For detailed information, see ::cnnlSetActivationDescriptor function.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetActivationDescriptor(cnnlActivationDescriptor_t activation_desc,
                                                      cnnlActivationMode_t *mode,
                                                      cnnlActivationPreference_t *prefer,
                                                      cnnlNanPropagation_t *nan_prop,
                                                      float *coef,
                                                      int *sliced_dim,
                                                      float *gamma,
                                                      float *scale,
                                                      bool *is_result,
                                                      bool *approximate);

// Group:Activation
/*!
 * @brief Applies a specified activation function element-wise over each input
 * value.
 *
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported:
 * - ReLU, ReLU6, Sigmoid, Tanh, SELU, GELU, ELU, ELU_V2, LeakyReLU, TF_LeakyReLU, CAFFE_RELU6, and GLU.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the activation operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Activation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \b x: float, half.
 *   - \b y: float, half.
 *
 * @par API Dependency
 * - Before calling this function to implement activation, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 * - You need to call the ::cnnlCreateActivationDescriptor and ::cnnlSetActivationDescriptor_v6
 *   functions to create and set the activation descriptor \b activation_desc before calling this function.
 *
 * @note
 * - The input tensor \b x should be in the following range to guarantee the accuracy of output:
 *   - On MLU 200 series:
 *        - When the \b mode is \p CNNL_ACTIVATION_TANH:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: (-inf, -0.005] and [0.005, +inf), where inf represents infinity.
 *            - float: (-inf, -0.002] and [0.002, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            (-inf, -0.02] and [0.02, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_SIGMOID:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: [-7.75, +inf).
 *            - float: [-60, 1e20].
 *            The result is 0, when the element value of input tensor \b x is less than -35.
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -40) and [-1.99, +inf).
 *            The result is 0, when the element value of input tensor \b x is less than -7.75; the
 *            result is 1, when the element value of input tensor \b x is greater than 7.75.
 *        - When the \b mode is \p CNNL_ACTIVATION_GELU and the bool parameter \b approximate is set to true:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION：
 *            - half: (-3.874, -0.003) and (0.003, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST：
 *            - half: (-2.4, -0.23) and [0.5, +inf).
 *            - float: (-3.874, -0.003) and (0.003, +inf).
 *            The result is -5.96e-8, when the element value of input tensor \b x is less than or equal
 *            to -3.875.
 *        - When the \b mode is \p CNNL_ACTIVATION_GELU and the bool parameter \b approximate is set to false:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-10, -0.003) and (0.5, +inf).
 *            - float: (-10, -0.003) and (0.003, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_ELU:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: (-inf, -0.0091] and [0.026, +inf).
 *            - float: (-inf, -0.09] and [0.03, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.44) and [0, +inf).
 *            - float: (-inf, -0.255) and [0.026, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_GLU:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -40) and [-1.99, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_SILU:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: [-1.99, +inf).
 *            - float: (-inf, -50) and [-1.99, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_SELU:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.05] and [0, +inf).
 *            - float: (-inf, -0.1] and [0.05, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: (-inf, -0.1] and [0.05, +inf).
 *            - float: (-inf, -0.01) and [0.05, +inf).
 *   - On MLU 300 and CE3226 series:
 *        - When the \b mode is \p CNNL_ACTIVATION_TANH:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.016] and [0.016, +inf).
 *            - float: (-inf, -0.017] and [0.017, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_GELU and the bool parameter \b approximate is set to true:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: (-3.874, -0.003) and (0.003, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-2.4, -0.23) and [0.5, +inf).
 *            - float: (-3.874, -0.003) and (0.003, +inf).
 *             The result is -5.96e-8, when the element value of input tensor \b x is less than or equal
 *              to -3.875.
 *        - When the \b mode is \p CNNL_ACTIVATION_GELU and the bool parameter \b approximate is set to false:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.003) and (0, +inf).
 *            - float: (-inf, -0.003) and (0, +inf).
 *        - When the \b mode is \p CNNL_ACTIVATION_ELU:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.5] and [0.1, +inf).
 *            - float: (-inf, -0.255] and [0.026, +inf).
 *         - When the \b mode is \p CNNL_ACTIVATION_SELU:
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_FAST:
 *            - half: (-inf, -0.05] and [0, +inf).
 *            - float: (-inf, -0.1] and [0.1, +inf).
 *          - If ::cnnlActivationPreference_t is set to \p CNNL_ACTIVATION_HIGH_PRECISION:
 *            - half: (-inf, -1e-5] and [-1e-7, +inf).
 *            - float: (-inf, -1e-5] and [0, +inf).
 *   For other activation modes, there are no restrictions on input tensor.
 * - Activation forward operation is an element-wise operation. The dimensions of \b x and
 *   \b y must be the same.
 * - You can specify the stride of all dimensions for x_desc and y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the activation forward operation is as follows:
     @verbatim
      - \b x: [4, 1, 3]
      - \b y: [4, 1, 3]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlActivationForward(cnnlHandle_t handle,
                                                const cnnlActivationDescriptor_t activation_desc,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y);

// Group:Activation
/*!
 * @brief Computes the gradient of an activation function.
 *
 * You can specify the activation function to be used in \b mode. The following activation
 * functions are supported:
 * - ReLU, ReLU6, Sigmoid, Tanh, GELU, LeakyReLU, TF_LeakyReLU, ELU and ELU_V2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the activation operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 *   The value of this parameter should be NULL when the \b mode is ReLU, ReLU6, GELU,
 *   LeakyReLU, or TF_LeakyReLU.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 *   The value of this parameter should be NULL when the \b mode is Tanh or Sigmoid.
 * @param[out] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Activation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @note
 * - The input tensors \b x and \b diff_y should be in the following range to
 *   guarantee the accuracy of output:
 *   - When the \b mode is \p CNNL_ACTIVATION_GELU:
 *     - Under \p CNNL_DTYPE_HALF, if ::cnnlActivationPreference_t is set to
 *       \p CNNL_ACTIVATION_HIGH_PRECISION, the data range of \b x should satisfy
 *       the conditions: [-3.5, 500].
 *     - Under \p CNNL_DTYPE_HALF, if ::cnnlActivationPreference_t is set to
 *       \p CNNL_ACTIVATION_FAST, the data range of \b x should satisfy
 *       the conditions: [-0.5, 500].
 *     - Under \p CNNL_DTYPE_FLOAT, the data range of \b x should satisfy the
 *       conditions: [-3.5, 500].
 *     - The \b diff_y with NaN or inf is not supported currently, where inf represents infinity.
 *   - When the \b mode is \p CNNL_ACTIVATION_RELU:
 *     - The \b diff_y with NaN or inf is not supported currently, and the data range of
 *       \b diff_y should satisfy the conditions: (-inf, inf).
 *   - When the \b mode is \p CNNL_ACTIVATION_RELU6:
 *     - The \b diff_y with NaN or inf is not supported currently, and the data range of
 *       \b diff_y should satisfy the conditions: (-inf, inf).
 *   - When the \b mode is \p CNNL_ACTIVATION_SILU:
 *     - On MLU 200 series:
 *       - Under \p CNNL_DTYPE_HALF, if ::cnnlActivationPreference_t is set to
 *         \p CNNL_ACTIVATION_FAST, the data range of \b x should satisfy
 *         the conditions: [-1.99, +inf).
 *       - Under \p CNNL_DTYPE_FLOAT, if ::cnnlActivationPreference_t is set to
 *         \p CNNL_ACTIVATION_FAST, the data range of \b x should satisfy
 *         the conditions: (-inf, -40) and [-1.99, +inf).
 *   - When the \b mode is \p CNNL_ACTIVATION_GLU:
 *     - On MLU 200 series:
 *       - The range of \b diff_y is recommended to be in [-50, 50] for higher precision.
 *       - Under \p CNNL_DTYPE_HALF, the data range of \b x should satisfy
 *         the conditions: [-1.99, 7.75) and [50, +inf).
 *       - Under \p CNNL_DTYPE_FLOAT, the data range of \b x should satisfy
 *         the conditions: (-inf, -40) and [-1.99, 7.75) and [50, +inf).
 *     - The \b diff_y with NaN or inf is not supported currently.
 *   For other activation modes, there are no restrictions on input tensor.
 * - Activation backward operation is an element-wise operation. The dimensions of input and
 *   output tensors must be the same.
 * - When input data contains NAN/infinity:
 *   - When the \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward:
 *     - On MLU 300 series and CE3226:
 *       - If \b x or \b diff_y is NaN, then \b diff_x is NaN.
 *       - If \b x is positive infinity, then \b diff_x is NaN.
 *       - If \b x is negative infinity and \b diff_y is infinity, then \b diff_x is NaN.
 *       - If \b x is negative infinity and \b diff_y is not set as infinity, then \b diff_x is 0.
 *       - If \b x is not set as negative infinity and \b diff_y is positive infinity, then \b diff_x is positive infinity.
 *       - If \b x is not set as positive infinity and \b diff_y is negative infinity, then \b diff_x is negative infinity.
 *
 * @par API Dependency
 * - Before calling this function to implement activation, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 * - You need to call the ::cnnlCreateActivationDescriptor and ::cnnlSetActivationDescriptor_v5
 *   functions to create and set the activation descriptor \b activation_desc before calling this function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the activation backward operation is as follows:
     @verbatim
      - input tensors: [4, 1, 3]
      - output tensors: [4, 1, 3]
     @endverbatim
 * - When the \b mode is \p CNNL_ACTIVATION_ELU_V2 in ::cnnlActivationBackward, the example is as follows:
     @verbatim
      - \b x:      [  1, NaN, +inf, +inf, +inf, -inf, -inf, -inf,   -3,    3 ]
      - \b diff_y: [NaN,   3,    3, +inf, -inf, +inf, -inf,    3,  +inf, -inf ]
      - \b diff_x: [NaN, NaN,  NaN,  NaN,  NaN,  NaN,  NaN     0,  +inf, -inf ]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlActivationBackward(cnnlHandle_t handle,
                                                 const cnnlActivationDescriptor_t activation_desc,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 const void *y,
                                                 const cnnlTensorDescriptor_t diff_y_desc,
                                                 const void *diff_y,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t diff_x_desc,
                                                 void *diff_x);

// Group:CustomizedActive
/*!
 * @brief Creates a descriptor pointed by \b customized_active_desc for a customized active
 *        operation, and allocates memory for holding the information about the customized
 *        active operation. The information is defined in ::cnnlCustomizedActiveDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Input. A host pointer to the customized active descriptor that holds information
 *  about the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetCustomizedActiveDescriptor function
 *   to initialize and set the information to the customized active descriptor.
 * - You need to call the ::cnnlDestroyCustomizedActiveDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t *customized_active_desc);

// Group:CustomizedActive
/*!
 * @brief Destroys a customized active descriptor \b customized_active_desc that
 *        is previously created with the ::cnnlCreateCustomizedActiveDescriptor function.
 *
 * The customized active descriptor is defined in ::cnnlCustomizedActiveDescriptor_t
 * and holds the information about the customized active operation.
 *
 *
 * @param[in] customized_active_desc
 *   Input. The customized active descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlCustomizedActiveForward or
 *   ::cnnlCustomizedActiveForward_v2 function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the customized active descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t customized_active_desc);

// Group:CustomizedActive
/*!
 * @brief Initializes the customized active descriptor \b customized_active_desc that
 * is previously created with the ::cnnlCreateCustomizedActiveDescriptor function, and sets
 * the information about the customized active operation to the customized active descriptor
 * \b customized_active_desc. The information includes the number of the range of
 * independent variables of activation function \b x_begin and \b x_end, the infimum of
 * activation function within the effective range \b y_min, and the number of segment of the
 * activation function \b segment_num.
 *
 * @param[in] customized_active_desc
 *    Input. Description of customized active operation. For detailed information,
 *    see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_begin
 *   Input. The starting value of activation function domain.
 * @param[in] x_end
 *   Input. The end value of activation function domain.
 * @param[in] y_min
 *   Input. The infimum of activation function within the effective range.
 * @param[in] segment_num
 *   Input. The number of segment of the activation function. When implemented,
 *   segment the activation function domain linearly according to this number.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetCustomizedActiveDescriptor(cnnlCustomizedActiveDescriptor_t customized_active_desc,
                                  float x_begin,
                                  float x_end,
                                  float y_min,
                                  int segment_num);

// Group:CustomizedActive
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an
 * extra workspace to optimize the customized active operation.
 *
 * The size of extra workspace is based on the given information of the customized
 * active operation, including the input tensor descriptor \b x,
 * customized active descriptor \b desc_t. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input.The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 *   This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor
 *   descriptors \b x before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlCustomizedActiveForward or
 *   ::cnnlCustomizedActiveForward_v2 function to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCustomizedActiveForwardWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            const cnnlCustomizedActiveDescriptor_t desc_t,
                                            size_t *size);

// Group:CustomizedActive
/*!
 * @brief Computes an active function on input tensor \b x with the function pointer
 *        \b active_func, and returns the results in the output tensor \b y.
 *        This function is used to define your own activation function.
 *        You can call ::cnnlActivationForward to implement pre-defined activation
 *        functions. To define your own activation functions, you can get more
 *        information in ::active_func_type. Different from ::cnnlCustomizedActiveForward,
 *        ::cnnlCustomizedActiveForward_v2 needs to allocate and initialize extra input space.
 *        You can get more information in ::cnnlGetCustomizedActiveForwardExtraInputSize and
 *        ::cnnlInitCustomizedActiveForwardExtraInput.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 *
 * @deprecated
 *   ::cnnlCustomizedActiveForward is deprecated and will be removed in the future release.
 *   It is recommended to use ::cnnlCustomizedActiveForward_v2 instead, which needs user to
 *   allocate extra space.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] customized_active_desc
 *   Input. The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   customized active operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the customized active operation. You can get the size of the workspace with
 *   the ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] active_func
 *   Input. The active function of the customized active operation. For detailed information,
 *   see ::active_func_type.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Customized Active Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Scale Limitation
 * - Customized activation operation is an element-wise operation.
 *   The dimensions of input tensor and output tensor must be the same.
 *
 * @par API Dependency
 * - Before calling this function to implement the customized active operation,
 *   you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the customized active operation is as follows:
     @verbatim
      input array by 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [-8, 1], [6, 4]],
               [[3, 8], [2,-6], [0, 6]],
               [[8, 5], [7,4], [9, -6]]]]

      param:
        active_func: y = x if (x > 0)
                     y = 0 if (x <= 0)

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 1], [0, 1], [6, 4]],
                [[3, 8], [2,0], [0, 6]],
                [[8, 5], [7,4], [9, 0]]]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlCustomizedActiveForward(cnnlHandle_t handle,
                            const cnnlCustomizedActiveDescriptor_t customized_active_desc,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t y_desc,
                            void *y,
                            void *workspace,
                            size_t workspace_size,
                            active_func_type active_func);

// Group:CustomizedActive
/*!
 * @brief Returns in \b extra_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the customized active operation. You need to allocate memory both on host and
 * MLU based on the size returned in \b extra_size.
 *
 * The size of extra input data is based on the given information of the customized active operation,
 * including the input tensor descriptor \b x, customized active descriptor \b desc_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input.The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor
 *   functions to create and set the tensor descriptors \b x.
 * - After calling this function, you need to call ::cnnlInitCustomizedActiveForwardExtraInput to
 *   initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlCustomizedActiveForward_v2 function
 *   to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetCustomizedActiveForwardExtraInputSize(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const cnnlCustomizedActiveDescriptor_t desc_t,
                                             size_t * size);

// Group:CustomizedActive
/*!
 * @brief Initializes the extra input data space \b extra_host_input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_t
 *   Input.The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] active_func
 *   Input. The active function of the customized active operation.
 *   For detailed information, see ::active_func_type.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the customized active operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetCustomizedActiveForwardExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \b x_desc.
 * - The allocated extra input should be passed to the ::cnnlCustomizedActiveForward_v2 function
 *   to perform the customized active operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlInitCustomizedActiveForwardExtraInput(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const cnnlCustomizedActiveDescriptor_t desc_t,
                                          active_func_type active_func,
                                          void * extra_host_input);

// Group:CustomizedActive
/*!
 * @brief Computes an active function on input tensor \b x with the function pointer
 *        \b active_func, and returns the results in the output tensor \b y.
 *        This function is used to define your own activation function.
 *        You can call ::cnnlActivationForward to implement Cambricon CNNL pre-defined activation
 *        functions. To define your own activation functions, you can get more
 *        information in ::active_func_type. Compared with ::cnnlCustomizedActiveForward,
 *        ::cnnlCustomizedActiveForward_v2 provides better performance with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the customized active operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] customized_active_desc
 *   Input. The descriptor of the customized active operation. For detailed information,
 *   see ::cnnlCustomizedActiveDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data. You need to copy
 *   the extra input data to MLU from the host that is initialized with
 *   ::cnnlInitCustomizedActiveForwardExtraInput. For more information
 *   about extra input data, see "Cambricon CNNL User Guide".
 * @param[in] extra_input_size
 *   Input. The size of the extra input data in bytes that needs to be used in
 *   the customized active operation. You can get the size of the extra input data with
 *   the ::cnnlGetCustomizedActiveForwardExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   customized active operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the customized active operation. You can get the size of the workspace with
 *   the ::cnnlGetCustomizedActiveForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] active_func
 *   Input. The active function of the customized active operation. For detailed information,
 *   see ::active_func_type.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Customized Active Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Scale Limitation
 * - Customized activation operation is an element-wise operation.
 *   The dimensions of input tensor and output tensor must be the same.
 *
 * @par API Dependency
 * - Before calling this function to implement the customized active operation,
 *   you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the customized active operation is as follows:
     @verbatim
      input array by 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [-8, 1], [6, 4]],
               [[3, 8], [2,-6], [0, 6]],
               [[8, 5], [7,4], [9, -6]]]]

      param:
        active_func: y = x if (x > 0)
                     y = 0 if (x <= 0)

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 1], [0, 1], [6, 4]],
                [[3, 8], [2,0], [0, 6]],
                [[8, 5], [7,4], [9, 0]]]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlCustomizedActiveForward_v2(cnnlHandle_t handle,
                               cnnlCustomizedActiveDescriptor_t customized_active_desc,
                               const cnnlTensorDescriptor_t x_desc,
                               const void * x,
                               const void * extra_device_input,
                               size_t extra_input_size,
                               const cnnlTensorDescriptor_t y_desc,
                               void * y,
                               void * workspace,
                               size_t workspace_size,
                               active_func_type active_func);

// Group:BboxOverlaps
/*!
 * @brief Computes the IOUs (intersection over union) or IOFs (intersection over foreground) between two sets of
 *        bounding-boxes. If \b aligned is False, then calculates the IOUs of each row between each bounding-box
 *        of \b bbox1 and \b bbox2, otherwise calculates the IOUs of the corresponding row between each aligned
 *        pair of \b bbox1 and \b bbox2. For input placed in the order of <x1, y1, x2, y2>, (x1, y1) and (x2, y2)
 *        respectively represents the top-left and bottom-right corner coordinates of bounding-box.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   bounding-box overlaps operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An integer value which decides to return a result IOUs or IOF.
 *   The integer 0 represents IOU and 1 represents IOF.
 * @param[in] aligned
 *   Input. A boolean value. If it is False, then calculate the IOUs[i][j] or IOFs[i][j] between
 *   the row i of \b bbox1 and the row j of \b bbox2, otherwise the IOUs[i] or IOFs[i] between
 *   the row i of \b bbox1 and the row i of \b bbox2 are calculated. The number of row of \b bbox1
 *   and \b bbox2 must be equal for aligned is True.
 * @param[in] offset
 *   Input. An integer value determines whether to increase the length and the width of the bounding-box by 0 or 1
 *   before calculating the area.
 * @param[in] bbox1_desc
 *   Input. The descriptor of the input tensor \b bbox1. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox1
 *   Input. Pointer to the MLU memory that stores the input tensor \b bbox1.
 * @param[in] bbox2_desc
 *   Input. The descriptor of the input tensor \b bbox2. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox2
 *   Input. Pointer to the MLU memory that stores the input tensor \b bbox2.
 * @param[in] ious_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] ious
 *   Output. IOUs or IOFs. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Bounding-Box Overlaps Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of data types with the following order:
 *    - \b bbox1 - \b bbox2 - \b ious
 *    The supported combinations of data types are:
 *      - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \b bbox1 and \b bbox2 tensors must be 2.
 * - The length of lowest dimension of input tensor must be 4.
 *   \b bbox1 (Tensor): shape [m, 4] in <x1, y1, x2, y2> format.
 *   \b bbox2 (Tensor): shape [n, 4] in <x1, y1, x2, y2> format.
 * - Input with NaN is not supported currently. Also you need to exclude the input with (inf - inf) or (inf - (-inf)),
 *   where inf represents infinity (because the result is NaN, the actual impact is that the input has NaN).
 * - For input in type <x1, y1, x2, y2>, the coordinates must satisfy x2 > x1, y2 > y1,
 *   otherwise incorrect results will be obtained.
 * - When aligned mode is True, for input \b bbox1 and \b bbox2 with n-rows, if n is zero, the output IOUs
 *   must be a two-dimensional matrix with shape n * 1, otherwise the output IOUs must be a one-dimensional
 *   array with n-elements. When aligned mode is False, for input \b bbox1 with n-rows and
 *   \b bbox2 with m-rows, the output IOUs must be a two-dimensional matrix with shape n * m.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The input tensor \b x should be in the following range to guarantee the accuracy of output:
 *  - If bbox_overlaps works on (m)tp_2xx :
 *    - half : [-300, 100].
 *    - float : [-300, 100].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bounding-box overlaps operation is as follows:
     @verbatim
      input array by 3 * 4, type is float -->
          input: bbox1 = [
            [0, 0, 10, 10],
            [10, 10, 20, 20],
            [32, 32, 38, 42],
          ]
      input array by 3 * 4, type is float -->
          input: bbox2 = [
            [0, 0, 10, 20],
            [0, 10, 10, 19],
            [10, 10, 20, 20],
          ]
      param:
        mode = 0
        aligned = False
        offset = 0

      output array by 3 * 3, type is float -->
          output: [[0.5000, 0.0000, 0.0000],
                   [0.0000, 0.0000, 1.0000],
                   [0.0000, 0.0000, 0.0000]]
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBboxOverlaps(cnnlHandle_t handle,
                                           const int mode,
                                           const bool aligned,
                                           const int offset,
                                           const cnnlTensorDescriptor_t bbox1_desc,
                                           const void *bbox1,
                                           const cnnlTensorDescriptor_t bbox2_desc,
                                           const void *bbox2,
                                           const cnnlTensorDescriptor_t ious_desc,
                                           void *ious);
// Group:Inverse
/*!
 * @brief Computes the intersection-over-union (Jaccard index, IOU) of rotated
 *        bounding-boxes. If \b aligned is false, then calculate the IOUs
 *        between each rotated bounding-box of \b bbox1 and \b bbox2, otherwise
 *        the IOUs between each aligned pair of rotated bounding-box of \b bbox1
 *        and \b bbox2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box iou rotated operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An integer value which decides to return a result of
 *   IOUs (intersection over union) or IOFs (intersection over foreground).
 *   The integer 0 represents IOU and 1 represents IOF.
 * @param[in] aligned
 *   Input. A boolean value. If it is false, then calculate the IOUs[i][j]
 *   or IOFs[i][j] between the row i of \b bbox1 and the row j of \b bbox2,
 *   otherwise calculate the IOUs[i] or IOFs[i] between the row i of \b bbox1
 *   and the row i of \b bbox2. Significantly, the number of row of \b bbox1
 *   and \b bbox2 must be equal when \b aligned is true.
 * @param[in] bbox1_desc
 *   Input. The descriptor of the input tensor \b bbox1 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox1
 *   Input. Pointer to the MLU memory that stores the input tensor \b bbox1.
 *   It has shape (n, 5), indicating (x, y, w, h, theta) for each row.
 *   Note that theta is in radian.
 * @param[in] bbox2_desc
 *   Input. The descriptor of the input tensor \b bbox2 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bbox2
 *   Input. Pointer to the MLU memory that stores the input tensor \b bbox2.
 *   It has shape (m, 5), indicating (x, y, w, h, theta) for each row.
 *   Note that theta is in radian.
 * @param[in] ious_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] ious
 *   Output. IOUs or IOFs of input rotated bounding-boxes. Pointer to the MLU
 *   memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Box Iou Rotated Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b bbox1 - \b bbox2 - \b ious, the supported data types of
 *    \b bbox1, \b bbox2 and \b ious are as follows:
 *   - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \b bbox1 and \b bbox2 tensors must be 2.
 * - The length of lowest dimension of \b bbox1 and \b bbox2 tensors must be 5.
 * - Both sets of boxes are expected to be in
 *   (x_center, y_center, width, height, angle) format.
 *   - \b bbox1 (Tensor): shape [n, 5] in (x, y, w, h, theta) format.
 *   - \b bbox2 (Tensor): shape [m, 5] in (x, y, w, h, theta) format.
 * - When aligned mode is true, for input \b bbox1 and \b bbox2 with n-rows,
 *   the output \b ious must be a one-dimensional array with n-elements. When
 *   \b aligned is false, for input \b bbox1 with n-rows and \b bbox2 with
 *   m-rows, the output \b ious must be a two-dimensional matrix with shape n*m.
 *
 * @note
 * - When finding the point with minimum y and minimum x in convex-hull-graham,
 *   BoxIouRotated performs min-pooling operation. If the input data of pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN.
 *      Otherwise, the \b output value is the minimum value after the last NaN.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - http://github.com/open-mmlab/mmcv/blob/master/mmcv/ops/box_iou_rotated.py
 */
cnnlStatus_t CNNL_WIN_API cnnlBoxIouRotated(cnnlHandle_t handle,
                                            const int mode,
                                            const bool aligned,
                                            const cnnlTensorDescriptor_t bbox1_desc,
                                            const void *bbox1,
                                            const cnnlTensorDescriptor_t bbox2_desc,
                                            const void *bbox2,
                                            const cnnlTensorDescriptor_t ious_desc,
                                            void *ious);
// Group:Inverse
/*!
 * @brief Computes the inverse of the square matrix \b input. \b input tensor must be 2D square
 *        matrix or their batches arranged in high dimension. This function also returns a \b infos
 *        vector of the same size as the number of input's batch that indicates the exception for
 *        each batch when the \b input is inverted.
 *
 * You can choose to return a row-major or column-major order result by specifying \b is_trans.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   inverse operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the inverse operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] is_trans
 *   Input. A boolean value to decide to return a  result by row-major or column-major order.
 *   when is_trans is true the column-major order result will be return and a row-major order result was
 *   returned if is_trans is false.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] infos_desc
 *   Input. The descriptor of the infos tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] infos
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *   In the Infos vector, the element values are all integers. If the vector's value i is 0 then the i-th batch
 *   square matrix is invertible. If i is greater than 0, it means that square matrix is not invertible,
 *   and that the principal diagonal element of the i-th row or column of its inverse is 0. If i is less than
 *   0, the square matrix has an illegal input, i.e. "NaN" or "infinity". The format of infos have to be 1(vector).
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - Date type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: float.
 *   - output tensor: float.
 *   - infos vector: int32.
 *
 * @par Scale Limitation
 * - Input tensor formats must be greater than one, less than or equal to eight.
 * - The two lowest dimensions of input tensor should be equal, i.e. a square matrix.
 * - If the data type of input tensor is float, then the dimension of square matrix in
 *   input tensor does not exceed 208.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - If a matrix is not invertible there is no guarantee correctness of result. It may simply return a garbage
 * result in that batch.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the inverse operation is as follows:
     @verbatim
      input tensor x[3, 100, 256, 256], is_trans:true, output tensor y[3, 100, 256, 256], output tensor infos[3*100].
      The input tensor x will be computed the inverse and return to output tensor y through column-major order and get
      a information vector indicates the exception for each batch when the input tensor x is inverted..
     @endverbatim
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlInverse(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const bool is_trans,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      const cnnlTensorDescriptor_t infos_desc,
                                      void *infos);


// Group:BoxOverlapBev
/*!
 * @brief Computes the overlaps between each rotated bounding-box of \b boxes1 and \b boxes2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box overlap bev operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] boxes1_desc
 *   Input. The descriptor of the input tensor \b boxes1 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes1
 *   Input. Pointer to the MLU memory that stores the input tensor \b boxes1.
 *   It has shape (n, 7), with the number 7 indicating (x, y, z, dx, dy, dz, heading) for each row.
 *   Note that heading is in radian.
 * @param[in] boxes2_desc
 *   Input. The descriptor of the input tensor \b boxes2 (rotated bounding-box).
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes2
 *   Input. Pointer to the MLU memory that stores the input tensor \b boxes2.
 *   It has shape (m, 7), with the number 7 indicating (x, y, z, dx, dy, dz, heading) for each row.
 *   Note that heading is in radian.
 * @param[in] overlaps_desc
 *   Input. The descriptor of the output tensor \b overlaps. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] overlaps
 *   Output. Pointer to the MLU memory that stores the output tensor \b overlaps.
 *   \b overlaps is the overlapped area of rotated bounding boxes.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Box Overlap Bev Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b boxes1 - \b boxes2 - \b overlaps, the supported data types of
 *    \b boxes1, \b boxes2 and \b overlaps are as follows:
 *   - float - float - float.
 *
 * @par Scale Limitation
 * - The number of dimensions of \b boxes1 and \b boxes2 tensors must be 2.
 * - The length of lowest dimension of \b boxes1 and \b boxes2 tensors must be 7.
 * - Both sets of boxes are expected to be in
 *   - \b boxes1 (Tensor): shape [n, 7] in (x, y, z, dx, dy, dz, heading) format.
 *   - \b boxes2 (Tensor): shape [m, 7] in (x, y, z, dx, dy, dz, heading) format.
 * - For input \b boxes1 with n-rows and \b boxes2 with
 *   m-rows, the output \b overlaps must be a two-dimensional matrix with shape n*m.
 * - For input \b boxes1 with n-rows and \b boxes2 with m-rows, the n-rows and m-rows must
 *   less than 1000.
 *
 * @note
 * - Same as BoxIouRotated operator,
 *   when finding the point with minimum y and minimum x in convex-hull-graham,
 *   BoxOverlapBev performs min-pooling operation. If the input data of pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN.
 *      Otherwise, the \b output value is the minimum value after the last NaN.
 * - The value  of each element of \b boxes1 and  \b boxes2 should be equal to or greater than 1.
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlBoxOverlapBev(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t boxes1_desc,
                                            const void *boxes1,
                                            const cnnlTensorDescriptor_t boxes2_desc,
                                            const void *boxes2,
                                            const cnnlTensorDescriptor_t overlaps_desc,
                                            void *overlaps);

// Group:Grep
/*!
 * @brief Creates a descriptor pointed by \b grep_desc for a grep operation. The imformation
 *        is defined in ::cnnlGrepDescriptor_t.
 *
 * @param[out] grep_desc
 *   Input. A host pointer to the grep descriptor that holds information about the grep operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGrepDescriptor function to initialize
 *   and set the information to the grep descriptor.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGrepDescriptor(cnnlGrepDescriptor_t *grep_desc);

// Group:Grep
/*!
 * @brief Destroys a grep descriptor \b grep_desc that is previously created with the
 *        ::cnnlCreateGrepDescriptor function.
 *
 * The grep descriptor is defined in ::cnnlGrepDescriptor_t and holds the information
 * about the grep operation.
 *
 * @param[in] grep_desc
 *   Input. The grep descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlGrep function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the grep descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGrepDescriptor(cnnlGrepDescriptor_t grep_desc);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \b grep_desc that is previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \b grep_desc. The information includes the beginning coordinates
 * use to crop the input tensor \b begin and \mlu_begin, the element number to crop the input tensor
 * \b size and \b mlu_size, element number of the input tensor \b mlu_input_dim, the value to fill the
 * area when crop exceeds the boundary of input tensor \b space_number, the stride of the input tensor
 * \b mlu_mlutiplier, and the stride of the output tensor \b mlu_divisor.
 *
 * @deprecated
 *   ::cnnlSetGrepDescriptor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetGrepDescriptor_v2 instead.
 *
 * @param[in] grep_desc
 *   Input. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @param[in] mlu_input_dim
 *   Input. Pointer to the MLU memory that saves the element number of the input tensor in each dimension.
 * @param[in] mlu_begin
 *   Input. Pointer to the MLU memory that saves the beginning coordinates uesd to crop the
 *   input tensor for each dimension.
 * @param[in] mlu_size
 *   Input. Pointer to the MLU memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] mlu_mlutiplier
 *   Input. Pointer to the MLU memory that saves the stride of the input tensor in each dimensions
 *   except the lowest dimension.
 * @param[in] mlu_divisor
 *   Input. Pointer to the MLU memory that saves the stride of the output tensor in each dimensions
 *   except the lowest dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - Currently, all tensor formats could be any dimension greater than one.
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \b grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 * - The ::cnnlDestroyGrepDescriptor() needs to be called after this function.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \b begin must be greater than one. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \b size must be greater than one. The value
 *   of each dimension must be greater than zero.
 * - The number of dimensions of the \b begin must be equal to the number of dimensions
 *   of the \b size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor(cnnlGrepDescriptor_t grep_desc,
                                                int32_t begin[],
                                                int32_t size[],
                                                float space_number,
                                                int32_t *mlu_input_dim,
                                                int32_t *mlu_begin,
                                                int32_t *mlu_size,
                                                int32_t *mlu_mlutiplier,
                                                int32_t *mlu_divisor);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \b grep_desc that is previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \b grep_desc. The information includes the beginning coordinates
 * use to crop the input tensor \b begin, the element number to crop the input tensor, the value to
 * fill the area when crop exceeds the boundary of input tensor \b space_number.
 *
 * Different from ::cnnlSetGrepDescriptor, ::cnnlSetGrepDescriptor_v2 does not need to input the pointers
 * to the MLU memory.
 *
 * @deprecated
 *   ::cnnlSetGrepDescriptor_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetGrepDescriptor_v3 instead, which can prevent memory leakage.

 * @param[in] grep_desc
 *   Input. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @note
 * - Currently, all tensor formats could be any dimension greater than one.
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \b grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 * - The ::cnnlDestroyGrepDescriptor() needs to be called after this function.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \b begin must be greater than zero. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \b size must be greater than zero. The value
 *   of each dimension must be greater than zero.
 * - The number of dimensions of the \b begin must be equal to the number of dimensions
 *   of the \b size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor_v2(cnnlGrepDescriptor_t grep_desc,
                                                   int32_t *begin,
                                                   int32_t *size,
                                                   float space_number);

// Group:Grep
/*!
 * @brief Initializes the grep descriptor \b grep_desc that is previously created with
 * the ::cnnlCreateGrepDescriptor function, and sets the information about the grep
 * operation to the grep descriptor \b grep_desc. The information includes the begining coordinates
 * \b begin to crop the input tensor, the element number \b size to crop the input tensor, the
 * value \b space_number to fill the area when crop exceeds the boundary of input tensor,
 * the dimension number \b input_dims_num of input tensor.
 *
 * Different from ::cnnlSetGrepDescriptor_v2, ::cnnlSetGrepDescriptor_v3 adds \b input_dims_num
 * that is the number of dimensions in the input tensor.
 *
 * @param[in] grep_desc
 *   Input. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] begin
 *   Input. Pointer to the host memory that saves the beginning coordinates used to crop
 *   the input tensor for each dimension.
 * @param[in] size
 *   Input. Pointer to the host memory that saves the element number used to crop the input tensor
 *   in each dimension.
 * @param[in] space_number
 *   Input. When the cropping area exceeds the boundary of the input tensor, this value will be filled in
 *   the empty space. The default value of this parameter is 0.
 * @param[in] input_dims_num
 *   Input. The number of dimensions in the input tensor of grep operation. Currently, the value of this parameter
 *   is between [1, MAX_DIM].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor
 *   function to create the \b grep_desc.
 * - You need to call the ::cnnlDestroyGrepDescriptor function to destroy the descriptor at the end of context.
 *
 * @par Scale Limitation
 * - The number of dimensions of the \b begin must be greater than zero. The value of
 *   each dimension cannot be negative and cannot exceed the size of corresponding
 *   dimension of the input tensor.
 * - The number of dimensions of the \b size must be greater than zero. The value
 *   of each dimension must be greater or equal to zero.
 * - The number of dimensions of the \b begin must be equal to the number of dimensions
 *   of the \b size.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGrepDescriptor_v3(cnnlGrepDescriptor_t grep_desc,
                                                   int32_t *begin,
                                                   int32_t *size,
                                                   float space_number,
                                                   int32_t input_dims_num);

// Group:Grep
/*!
 * @brief Crops the input tensor \b input according to the information in the \b grep_desc,
 *        and returns the results in the output tensor \b output.
 *
 * You can continuously crop the input tensor by specifying the beginning coordinates of input
 * tensor \b begin of and \b size. The part that exceeds the size of input tensor will be filled
 * with \b space_number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   grep operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grep_desc
 *   Input. The descriptor of the grep operation. For detailed information, see ::cnnlGrepDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - Date type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, half, float.
 *
 * @par Scale Limitation
 * - All tensor formats could be any dimension that is greater than one.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateGrepDescriptor function
 *   to create the \b grep_desc and to call the ::cnnlSetGrepDescriptor function to set the
 *   information.
 * - You need to cal the ::cnnlDestroyGrepDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the grep operation is as follows:
     @verbatim
      input tensor x[100, 256, 256, 3], begin[0, 28, 28, 0], size[100, 200, 200, 3], output tensor y[100, 200, 200, 3].
      The input tensor x will crop from [0, 28, 28, 0] to [0+100, 28+200, 28+200, 0+3] and get the output tensor y.
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGrep(cnnlHandle_t handle,
                                   const cnnlGrepDescriptor_t grep_desc,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:Reduce
/*!
 * @brief Creates a descriptor pointed by \b reduce_desc that holds the \b axis,\b reduce_op,
 *        \b tensor_type, \b nan_propagation, \b tensor_indices, \b indices_type and \b p.
 *        The information is defined in ::cnnlReduceDescriptor_t.
 *
 * @param[out] reduce_desc
 *   Output. A host pointer to the reduce descriptor that holds information about reduce.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetReorgDescriptor or ::cnnlSetReorgDescriptor_v2 function to
 *   initialize and set the information to the reduce descriptor.
 * - You need to call the ::cnnlDestroyReorgDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateReduceDescriptor(cnnlReduceDescriptor_t *reduce_desc);

// Group:Reduce
/*!
 * @brief Initializes the reduce descriptor \b reduce_desc that is previously created with
 *        the ::cnnlCreateReduceDescriptor function, and sets the information about the reduce
 *        operation. To use p-norm in this operation, call ::cnnlCreatReduceDescriptor_v2.
 *
 * @param[in] reduce_desc
 *   Input. The descriptor of the reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[in] axis[]
 *   Input. The axis dimension vector of the reduce operation.
 * @param[in] axis_num
 *   Input. The size of axis vector.
 * @param[in] reduce_op
 *   Input. Enumeration to specify the reduce mode.
 *   For detailed information, see ::cnnlReduceOp_t.
 * @param[in] tensor_type
 *   Input. The data type is used in computing the reduce operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] nan_propagation
 *   Input. Enumeration to specify the NaN propagation mode. Default
 *   value is NOT_PROPAGATE_NAN. Now reduce does not support this parameter.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] tensor_indices
 *   Input. Enumeration to specify the reduce indices mode.
 *   For detailed information, see ::cnnlReduceIndices_t.
 * @param[in]  indices_type
 *   Input. Enumeration to specify the bit width type of reduce indices.
 *   At present, this parameter can only be set as CNNL_32BIT_INDICES.
 *   For detailed information, see ::cnnlIndicesType_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetReduceDescriptor(cnnlReduceDescriptor_t reduce_desc,
                                                  int axis[],
                                                  int axis_num,
                                                  cnnlReduceOp_t reduce_op,
                                                  cnnlDataType_t tensor_type,
                                                  cnnlNanPropagation_t nan_propagation,
                                                  cnnlReduceIndices_t tensor_indices,
                                                  cnnlIndicesType_t indices_type);
// Group:Reduce
/*!
 * @brief Initializes the reduce descriptor \b reduce_desc that is previously created with
 *        the ::cnnlCreateReduceDescriptor_v2 function, and sets the information about the reduce
 *        operation. Compared with :: cnnlSetReduceDescriptor, this function supports \b
 *        reduce_op == \p CNNL_REDUCE_NORMP, and includes more information such as \b p.
 *
 * @param[in] reduce_desc
 *   Input. The descriptor of the reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[in] axis[]
 *   Input. The axis dimension vector of the reduce operation.
 * @param[in] axis_num
 *   Input. The size of axis vector.
 * @param[in] reduce_op
 *   Input. Enumeration to specify the reduce mode.
 *   For detailed information, see ::cnnlReduceOp_t.
 * @param[in] tensor_type
 *   Input. The data type is used in computing the reduce operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] nan_propagation
 *   Input. Enumeration to specify the NaN propagation mode. Default
 *   value is NOT_PROPAGATE_NAN. Now reduce does not support this parameter.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] tensor_indices
 *   Input. Enumeration to specify the reduce indices mode.
 *   For detailed information, see ::cnnlReduceIndices_t.
 * @param[in]  indices_type
 *   Input. Enumeration to specify the bit width type of reduce indices.
 *   At present, this parameter can only be set as CNNL_32BIT_INDICES.
 *   For detailed information, see ::cnnlIndicesType_t.
 * @param[in] p
 *   Input. The exponent value in the norm formulation. \b p cannot be 1.0, 2.0, INF, and -INF.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetReduceDescriptor_v2(cnnlReduceDescriptor_t reduce_desc,
                                                     int axis[],
                                                     int axis_num,
                                                     cnnlReduceOp_t reduce_op,
                                                     cnnlDataType_t tensor_type,
                                                     cnnlNanPropagation_t nan_propagation,
                                                     cnnlReduceIndices_t tensor_indices,
                                                     cnnlIndicesType_t indices_type,
                                                     float p);
// Group:Reduce
/*!
 * @brief Destroys a reduce descriptor \b reduce_desc that is previously created with
 *        the ::cnnlCreateReduceDescriptor.
 *
 * The reduce descriptor is defined in ::cnnlReduceDescriptor_t and holds the information
 * about the reduce.
 *
 * @param[in] reduce_desc
 *   Input. The reduce descriptor to be destroyed.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateReduceDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyReduceDescriptor(cnnlReduceDescriptor_t reduce_desc);

// Group:Reduce
/*!
 *  @brief Applies an operation of reduce to compute the sum value, mean value, maximum value,
 *         maximum index, minimum value and minimum index of tensor in the given dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] reduce_desc
 *   Input. A struct with information of reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   reduce operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   reduce operation. You can get the size of the workspace with the
 *   ::cnnlGetReduceOpWorkspaceSize function.
 * @param[in] alpha
 *   Input.  A host pointer to scaling factor of tensor output.
 * @param[in] beta
 *   Input.  A host pointer to bias factor of tensor output.
 * @param[in] input_desc
 *   Input. Descriptor of input data.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_size_inbytes
 *   Input. The size in bytes of indices.
 * @param[out] indices
 *   Output. Pointer to the MLU memory that stores the indices tensor.
 * @param[in] output_desc
 *   Input. Descriptor of output data.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - When \b reduce_op == \p CNNL_REDUCE_MAX || \b reduce_op == \p CNNL_REDUCE_MIN:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *     - indices: uint32.
 *
 *   - When \b reduce_op == \p CNNL_REDUCE_MUL:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *
 *   - When \b reduce_op == \p CNNL_REDUCE_AND || \b reduce_op == \p CNNL_REDUCE_OR:
 *     - input:   float, half, int8, uint8, bool.
 *     - output:  float, half, int8, uint8, bool.
 *
 *   - When \b reduce_op == \p CNNL_REDUCE_ADD || \b reduce_op == \p CNNL_REDUCE_AVG:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *
 *   - When \b reduce_op == \p CNNL_REDUCE_ASUM || \b reduce_op == \p CNNL_REDUCE_SUMSQ ||
 *          \b reduce_op == \p CNNL_REDUCE_NORM1 || \b reduce_op == \p CNNL_REDUCE_NORM2 ||
 *          \b reduce_op == \p CNNL_REDUCE_NORMP:
 *     - input:   float, half.
 *     - output:  float, half.
 *
 *   - When \b reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX ||
 *          \b reduce_op == \p CNNL_REDUCE_MIN_LAST_INDEX:
 *     - input:   float, half, int32.
 *     - output:  float, half, int32.
 *     - indices: uint32.
 * - \b alpha and \b beta: If the data type of input tensor is float or half, the data
 *   type of \b alpha and \b beta should be float pointer. If the data type of tensors is
 *   int32, the data type of \b alpha and \b beta should be int32 pointer.
 *
 * @par Data Layout
 * - The supported layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.

 * @par API Dependency
 * - Before calling this function to implement reduce, you need to prepare all the parameters
 *   passed to this function. Call ::cnnlCreateReduceDescriptor to create the parameter \b reduce_desc.
 *   Then call ::cnnlSetReduceDescriptor_v2 to set information about the parameter \b reduce_desc and
 *   call ::cnnlGetReduceOpWorkspaceSize to get extra MLU memory size in reduce operation.
 * - After calling this function, the ::cnnlDestroyReduceDescriptor needs to be called to destroyed the
 *   parameter \b reduce_desc.
 *
 * @note
 * - When the number of \b axis is greater than 1, the values of axis vector cannot be duplicated.
 *   For example, \b axis = [1,2,3] or \b axis = [1,2,4].
 * - The size of \b axis cannot be greater than the size of input.
 * - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND,
 *   \p CNNL_REDUCE_NORM1, \p CNNL_REDUCE_NORM2, and \p CNNL_REDUCE_NORMP support multi-axis numbers
 *   including continuous and discontinuous axis numbers except for p is 0.0 when ::cnnlReduceOp_t
 *   is \p CNNL_REDUCE_NORMP.
 * - \p CNNL_REDUCE_MAX and \p CNNL_REDUCE_MIN support multi-axis numbers including continuous and
 *   discontinuous axis numbers, when ::cnnlReduceIndices_t is \p CNNL_REDUCE_NO_INDICES.
 *   \p CNNL_REDUCE_MAX and \p CNNL_REDUCE_MIN support single-axis number and return the index of the
 *   first max or min value, when ::cnnlReduceIndices_t is \p CNNL_REDUCE_ONLY_INDICES or
 *   \p CNNL_REDUCE_FLATTENED_INDICES.
 * - \p CNNL_REDUCE_MAX_LAST_INDEX and \p CNNL_REDUCE_MIN_LAST_INDEX only support single-axis
 *   number and return the index of the last max or min value, when ::cnnlReduceIndices_t is
 *   \p CNNL_REDUCE_ONLY_INDICES or \p CNNL_REDUCE_FLATTENED_INDICES. \p CNNL_REDUCE_NO_INDICES
 *   is not supported now.
 * - When \b reduce_op == \p CNNL_REDUCE_MAX_LAST_INDEX || \b redece_op == \p CNNL_REDUCE_MIN_LAST_INDEX,
 *   the \b input with NaN or INFINITY is not supported currently, and the data range of \b input
 *   should satisfy the conditions: (-INFINITY, INFINITY).
 * - When \b reduce_op == \p CNNL_REDUCE_ASUM || \b redece_op == \p CNNL_REDUCE_SUMSQ, It should be
 *   noted that these two modes only support Caffe framework. \p CNNL_REDUCE_ASUM refers to the cumulative
 *   reduction after taking the absolute value of a specified dimension. Similarly, \p CNNL_REDUCE_SUMSQ
 *   calculates the square of the specified dimension and performs cumulative reduction. At the same time,
 *   these two modes do not support the use of compute_dtype parameter to improve accuracy.
 * - At present, only the following modes support input with stride:\p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG,
 *   \p CNNL_REDUCE_MAX, \p CNNL_REDUCE_MIN, \p CNNL_REDUCE_OR, \p CNNL_REDUCE_AND, \p CNNL_REDUCE_NORM1,
 *   \p CNNL_REDUCE_NORM2 and \p CNNL_REDUCE_MUL.
 * - This function reduces tensor input by implementing the equation output = alpha * reduce(input) + beta,
 *   given tensors \b input and \b output and scaling factors \b alpha and \b beta.
 *   - The following modes support \b alpha and \b beta:
 *     - \p CNNL_REDUCE_ADD, \p CNNL_REDUCE_AVG, \p CNNL_REDUCE_MUL, \p CNNL_REDUCE_NORM1,
 *       \p CNNL_REDUCE_NORM2, and \p CNNL_REDUCE_NORMP.
 *
 * @par Scale Limitations
 * - When ::cnnlReduceOp_t is \p CNNL_REDUCE_NORMP:
 *   - On MLU200 series:
 *     - The sum of p power of input absolute should be in range[7.2e-9, 507903] when data type is
 *       float and [6.1e-5,65504] when data type is half.
 *     - The p power of input absolute should be in range[-3.4e38, 16] when data type is float and
 *       [-65504,10.25] when data type is half.
 *     - The product of 1/p and sum of p power of input absolute should be in range[-3.4e38, 16]
 *       when data type is float and [-65504,10.25] when data type is half.
 *
 * @note
 * - When input data contains NaN:
 *   - On MLU300 series and CE3226:
 *     The CNNL_REDUCE_MIN and CNNL_REDUCE_MAX results are different with IEEE754.
 *     If the first operand is NaN and the second operand is finite value, then output is NaN.
 *     If the first operand is finite value and the second operand is finite value, then output is finite value.
 *     \p CNNL_REDUCE_NORMP results are different with IEEE754 when \b p is 0.0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the layer normalization forward operation is as follows:
     @verbatim
     input dimension = [n,c,h,w,d],
     When \b axis = 0:
      output dimension = [1,c,h,w,d].
      (indices dimension = [1,c,h,w,d], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     When \b axis = 1:
      output dimension = [n,1,h,w,d].
      (indices dimension = [n,1,h,w,d], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     When \b axis = 2:
      output dimension = [n,c,1,w,d].
      (indices dimension = [n,c,1,w,d], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     When \b axis = 3:
      output dimension = [n,c,h,1,d].
      (indices dimension = [n,c,h,1,d], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     When \b axis = 4:
      output dimension = [n,c,h,w,1].
      (indices dimension = [n,c,h,w,1], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     When \b axis = -1:
      output dimension = [1,1,1,1,1].
      (indices dimension = [1,1,1,1,1], \b reduce_op == \p CNNL_REDUCE_MAX or \p CNNL_REDUCE_MIN).
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_sum
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_mean
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_prod
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_max
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reduce_min
 */
cnnlStatus_t CNNL_WIN_API cnnlReduce(cnnlHandle_t handle,
                                     const cnnlReduceDescriptor_t reduce_desc,
                                     void *workspace,
                                     size_t workspace_size,
                                     const void *alpha,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const size_t indices_size_inbytes,
                                     void *indices,
                                     const void *beta,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);
// Group:Reduce
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to get
 *        extra space size in reduce operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reduce peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. A descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. A descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] reduce_op
 *   Input. An operation enum, indicating a specific reduce operation.
 *   For detailed information, see ::cnnlReduceDescriptor_t.
 * @param[out] workspace_size_inbytes
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the reduce operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReduceOpWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       const cnnlReduceDescriptor_t reduce_op,
                                                       size_t *workspace_size_inbytes);
// Group:InstanceNorm
/*!
 * @brief Returns in \b workspace_size, the size of the MLU memory that is used to get
 *        extra space size in instance normalization forward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the instance normalization forward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_x_desc
 *   Input. A descriptor of input_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the instance normalization forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetInstanceNormForwardWorkspaceSize(
             cnnlHandle_t handle,
             const cnnlTensorDescriptor_t input_x_desc,
             size_t *workspace_size);
/******************************************************************************
 * Cambricon CNNL OP: Dynamic_Stitch
 ******************************************************************************/

// Group:DynamicStitch
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * for the dynamic stitch operation.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_number
 *   Input. The number of input tensors.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the dynamic stitch operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \b input_size should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDynamicStitchWorkspaceSize(cnnlHandle_t handle,
                                                            const int inputs_number,
                                                            size_t *workspace_size);

// Group:DynamicStitch
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * for the dynamic stitch operation.
 *
 * @deprecated
 *   ::cnnlGetDynamicStitchtWorkspaceSize is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGetDynamicStitchWorkspaceSize instead.
 *
 * @param[in] handle
 *  Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *  queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inputs_number
 *   Input. The number of input tensors.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the dynamic stitch operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \b input_size should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDynamicStitchtWorkspaceSize(cnnlHandle_t handle,
                                                             const int inputs_number,
                                                             size_t *workspace_size);

// Group:DynamicStitch
/*!
 * @brief Interleaves the values from the \b data tensors into a single tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the dynamic stitch operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptors of the \b indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. A host pointer to an array that stores the \b indices pointers on MLU device.
 * @param[in] data_desc
 *   Input. The descriptors of the \b data tensors. The \b data tensors will be stitched into
 *   the \b output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] data
 *   Input. A host pointer to an array that stores the \b data pointers on MLU device.
 * @param[in] size
 *   Input. The size of input array.
 * @param[in] indices_dims
 *   Input. The dimensions of indices.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   dynamic stitch operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the dynamic stitch operation. You can get the size of the workspace with
 *   the ::cnnlGetDynamicStitchWorkspaceSize function.
 * @param[in] output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - \b indices: int32
 * - \b data: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - \b output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The values of indices must include 0. There can be duplicate values but no negative values.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetDynamicStitchWorkspaceSize.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the dynamic stitch operation is as follows:
     @verbatim
     input indices and data array
       indices[0] = 6
       indices[1] = [4, 1]
       indices[2] = [[5, 2], [0, 3]]

       data[0] = [61, 62]
       data[1] = [[41, 42], [11, 12]]
       data[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]
     output stitched array
       output = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],
                [51, 52], [61, 62]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDynamicStitch(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t *indices_desc,
                                            const int **indices,
                                            const cnnlTensorDescriptor_t *data_desc,
                                            const void **data,
                                            const int size,
                                            int *indices_dims,
                                            void *workspace,
                                            size_t workspace_size,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

/******************************************************************************
 * Cambricon CNNL OP: One_Hot
 ******************************************************************************/

// Group:OneHot
/*!
 * @brief Computes the onehot code in AI networks.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] indices_desc
 *   Input. The descriptor of \b indices tensor, which is the site to be
 *   set as on_value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the \b indices tensor.
 * @param[in] depth
 *   Input. The definition of code depth, which is number of classification.
 * @param[in] on_value
 *   Input. Pointer to the MLU memory that stores the \b on_value, the site
 *   will be set as on_value while site is in \b indices.
 * @param[in] off_value
 *   Input. Pointer to the MLU memory that stores the \b on_value, the site
 *   will be set as off_value while site is not in \b indices.
 * @param[in] axis
 *   Input. The definition of dimension which to be expanded.
 * @param[in] output_data_type
 *   Input. The data type of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "OneHot Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *  The combinations of the data types for \b indices, \b depth, \b on_value,
 *   \b off_value, \b axis and \b output tensor are as follows:
 *    - indices: int32
 *    - depth: int32
 *    - on_value: half, float, int32
 *    - off_value: half, float, int32
 *    - axis: int32
 *    - output: half, float, int32
 *
 * @par Scale Limitation
 * - The scale of \b indices tensor, \b depth, and \b axis must meet the following requirements:
 *   - The \b depth must be greater than 0.
 *   - The \b axis must be in [-1, n], where n is the indices's dimension.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None .
 *
 * @par Example
 * - The example of the onehot operation is as follows:
     @verbatim
     indices: a tensor of 1 --> [0, 2, -1, 1]
     depth: 3
     one_hot(indices, depth,
             on_value=5.0, off_value=0.0,
             axis=-1)
     output:
           [[5.0, 0.0, 0.0],  # one_hot(0)
            [0.0, 0.0, 5.0],  # one_hot(2)
            [0.0, 0.0, 0.0],  # one_hot(-1)
            [0.0, 5.0, 0.0]]  # one_hot(1)
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlOneHot(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t desc_indices,
                                     const void *indices,
                                     const int depth,
                                     const void *on_value,
                                     const void *off_value,
                                     const int axis,
                                     cnnlDataType_t output_data_type,
                                     void *output);

// Group:UnsortedSegmentSum
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the unsorted segment sum operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unsorted segment sum operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  data_desc
 *   Input. The descriptor of data tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the unsorted segment sum operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetUnsortedSegmentSumWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t data_desc,
                                       const cnnlTensorDescriptor_t output_desc,
                                       size_t *size);
// Group:UnsortedSegmentSum
/*!
 * @brief Computes the sum along segments of a tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unsorted segment sum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  data_desc
 *   Input. The descriptor of data tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  data
 *   Input. Pointer to the MLU memory that stores the data tensor.
 * @param[in]  ids_desc
 *   Input. The descriptor of segment indices tensor used as an index indicating
 *   the position of summation on output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  segment_ids
 *   Input. Pointer to the MLU memory that stores the segment indices.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   unsorted segment sum operation. For more information about workspace, see
 *   "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   unsorted segment sum operation. You can get the size of the workspace with the
 *   ::cnnlGetUnsortedSegmentSumWorkspaceSize function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "UnsortedSegmentSum Operation" section in "Cambricon CNNL User Guide" for details.

 * @par Data Type
 * - This function supports any combinations of the following data types data tensor \b data,
 * segment indices tensor \b segment_ids, and output tensor \b output.
 * <b>Note that the data type of data tensor and output tensor must be same.</b>
 * - data tensor: half, float.
 * - segment_ids tensor: int32.
 * - output tensor: half, float.
 * @par Scale Limitation
 * - The data tensor, segment_ids tensor, and the output tensor must meet the following
 * requirements:
 *   - data_dim - segment_ids_dim + 1 == output_dim,
 *     where data_dim, segment_ids_dim and output_dim represent the number of
 *     dimensions of \b data, \b segment_ids and \b output, respectively.
 *   - The dimension of \b segment_ids should be less than or equal to the dimension of \b data.
 *   - The \b data will not be added to the \b output if an invalid \b segment_ids values is set.
 *     The valid value of \b segment_ids is greater than or equal to 0, or less than dims[0] of the
 *     \b output.
 *
 * @par API Dependency
 * - Before calling this function to implement unsorted segment sum, you need to call
 * ::cnnlGetUnsortedSegmentSumWorkspaceSize to get the extra space size needed in
 * unsorted segment sum operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the unsorted segment sum operation is as follows:
 * @verbatim
   data: an array by 3 * 4 --> [[1, 2, 3, 4],
                                [5, 6, 7, 8],
                                [4, 3, 2, 1]]

   segment_ids: an array by 3 --> [0, 1, 0]

   output: an array by 2 * 4 --> [[5, 5, 5, 5],
                                  [5, 6, 7, 8]]
   @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/math#Segmentation
 */
cnnlStatus_t CNNL_WIN_API cnnlUnsortedSegmentSum(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t data_desc,
                                                 const void *data,
                                                 const cnnlTensorDescriptor_t ids_desc,
                                                 const int *segment_ids,
                                                 void *workspace,
                                                 size_t workspace_size,
                                                 const cnnlTensorDescriptor_t output_desc,
                                                 void *output);
// Group:MatrixBandPart
/*!
 * @brief Copies a tensor setting everything outside a central band in each innermost matrix to zero.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlMatrixBandPart. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] num_lower
 *   Input. A int32 scalar, the number of subdiagonals to keep.
 * @param[in] num_upper
 *   Input. A int32 scalar, the number of superdiagonals to keep.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "MatrixBandPart operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports data types as follow, and must be the same between input
 * and output.
 *   - input: float16, float32.
 *   - num_lower: int32.
 *   - num_upper: int32.
 *   - output: float16, float32.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlMatrixBandPart function is as follows:
     @verbatim
       input tensor by 1*4*4 --> input: [[ 0,  1,  2, 3]
                                         [-1,  0,  1, 2]
                                         [-2, -1,  0, 1]
                                         [-3, -2, -1, 0]

                                 num_lower: 1

                                 num_upper: -1

       datatype: FP32

       output tensor --> output:  [ 0,  1,  2, 3]
                                  [-1,  0,  1, 2]
                                  [ 0, -1,  0, 1]
                                  [ 0,  0, -1, 0]

     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/linalg/band_part
 */
cnnlStatus_t CNNL_WIN_API cnnlMatrixBandPart(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t data_desc,
                                             const void *input,
                                             const int num_lower,
                                             const int num_upper,
                                             void *output);

// Group:L2Loss
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the l2loss operation.
 *
 * The size of extra workspace is based on the given information of the l2loss operation,
 * including the input tensor descriptor \b x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *    Output. Pointer to the returned size of the extra workspace in bytes that is used in the l2loss
 *    operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlL2Loss_v2. ::cnnlL2Loss does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetL2LossWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     size_t *workspace_size);

// Group:L2Loss
/*!
 * @brief Computes half of the sum of squares of input tensor (also referred as to half the L2 normalization without sqrt),
 *        and returns the results in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "L2Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l2loss operation is as follows:
 *   @verbatim
      input tensor by 2*2 --> x: [[1, 2], [3, 4]]
      output scalar  --> y: [15]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/l2_loss
 */
cnnlStatus_t CNNL_WIN_API cnnlL2Loss(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     void *y);

// Group:L2Loss
/*!
 * @brief Computes half of the sum of squares of input tensor (also referred as to half the L2 normalization without sqrt),
 *        and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlL2Loss, this function requires you to allocate some extra workspace as an input
 * parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the l2loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlL2Loss_v2.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlL2Loss_v2. You can get the size of the workspace with
 *   the ::cnnlGetL2LossWorkspaceSize function.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Formula
 * - See "L2Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par API Dependency
 * - Before using this API, you need to get the size of the workspace with the ::cnnlGetL2LossWorkspaceSize
 *   function and pass the required extra workspace to the ::cnnlL2Loss_v2 function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l2loss operation is as follows:
 *   @verbatim
      input tensor by 2*2 --> x: [[1, 2], [3, 4]]
      output scalar  --> y: [15]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/l2_loss
 */
cnnlStatus_t CNNL_WIN_API cnnlL2Loss_v2(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        void *workspace,
                                        size_t workspace_size,
                                        void *y);

// Group:Scale
/*!
 * @brief Applies a linear transformation to the input tensor \b x along the specified \b axis
 *        with the filter tensor \b alpha and the bias tensor \b beta, and returns the results
 *        in the output tensor \b y.
 *
 * Support several consecutive dimensions applying multiplication and addition operations
 * along the specified \b axis.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the scale operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] axis
 *   Input.  Integer which marks the start operating dimension of input.
 *   May be negative to index from the end (e.g., -1 for the last axis).
 * @param[in]  desc_x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  desc_alpha
 *   Input. The descriptor of the \b alpha tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  alpha
 *   Input. Pointer to the MLU memory that stores the \b alpha tensor which is the scaling factor for input.
 *   Could be set to NULL which will be treated as scalar 1.
 * @param[in]  desc_beta
 *   Input. The descriptor of the \b beta tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  beta
 *    Input. Pointer to the MLU memory that stores the \b beta tensor which is the scaling factor for input.
 *    Could be set to NULL which will be treated as scalar 0.
 * @param[out]  desc_y
 *    Output. The descriptor of the output tensor. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *    Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Scale Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data Type of input tensor \b x, alpha tensor \b alpha, beta tensor \b beta
 *   and output tensor \b y must be the same.
 * - The supported data types of input, alpha, beta and output tensors are as follows:
 *   - input tensor: half, float.
 *   - alpha tensor: half, float.
 *   - beta tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - This function supports multi-dimensional input, alpha, and beta.
 * - The supported data layout of input, alpha, beta, output tensor must be \b CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 * - The alpha and beta could be scalar. If they are not scalar (which means tensor has more than one element), the dimensions they represent computing need to be the same.
 *
 * @note
 * - \b desc_alpha could be set to NULL if \b alpha is NULL which will be treated as scalar 1.
 * - \b desc_beta could be set to NULL if \b beta is NULL which will be treated as scalar 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scale operation is as follows:
     @verbatim

     i) About axis and supported shape.

     For input x with shape (2, 3, 5, 8, 13) which is 5-D tensor, axis should be inside [-5, 4).
     If axis is 1 or -4, which marks operating from 2nd dimension, alpha could be scalar or tensor in one of the following shapes:

         - 3:        (1, 3, 1, 1, 1)  or (3,)          or (3, 1, 1, 1)
         - 3x5:      (1, 3, 5, 1, 1)  or (3, 5)        or (3, 5, 1, 1)
         - 3x5x8:    (1, 3, 5, 8, 1)  or (3, 5, 8)     or (3, 5, 8, 1)
         - 3x5x8x13: (1, 3, 5, 8, 13) or (3, 5, 8, 13)

     And for this input x, the alpha and beta may have any of the following shapes (for the given value of axis):
        - (scalar)
        - (axis == 0 == -5) 2; 2x3; 2x3x5; 2x3x5x8; 2x3x5x8x13
        - (axis == 1 == -4)      3;   3x5;   3x5x8;   3x5x8x13
        - (axis == 2 == -3)             5;     5x8;     5x8x13
        - (axis == 3 == -2)                      8;       8x13
        - (axis == 4 == -1)                                 13


     ii) About computing

     If x is 4-D tensor, axis is 1, alpha/beta has two dimensions participating in computing.

     Computation could be denoted as follows:

        y[i, j, k, l] = alpha[0, j, k, 0] * x[i, j, k, l] + beta[0, j, k, 0]


     For input tensor x with shape (2,3,2,2), axis is 1, alpha/beta has shape (1,3,2,1):

           x = [[[[   0.,   1.], [   2.,    3.]],
                 [[   4.,   5.], [   6.,    7.]],
                 [[   8.,   9.], [  10.,   11.]]],

                [[[  12.,  13.], [  14.,   15.]],
                 [[  16.,  17.], [  18.,   19.]],
                 [[  20.,  21.], [  22.,   23.]]]]

       alpha = [[[[        1.0], [        -1.0]],
                 [[        0.5], [        -0.5]],
                 [[        2.0], [        -2.0]]]]

        beta = [[[[        0.0], [         0.0]],
                 [[        0.2], [        -0.2]],
                 [[        0.5], [        -0.5]]]]

     The output tensor y has shape (2,3,2,2):

           y = [[[[   0.,   1.], [  -2.,   -3.]],
                 [[  2.2,  2.7], [ -3.2,  -3.7]],
                 [[ 16.5, 18.5], [-20.5, -22.5]]],

                [[[  12.,  13.], [ -14.,  -15.]],
                 [[  8.2,  8.7], [ -9.2,  -9.7]],
                 [[ 40.5, 42.5], [-44.5, -46.5]]]]

     @endverbatim
 *
 * @par Reference
 *  - https://caffe.berkeleyvision.org/tutorial/layers/scale.html
 */

cnnlStatus_t CNNL_WIN_API cnnlScale(cnnlHandle_t handle,
                                    int axis,
                                    const cnnlTensorDescriptor_t desc_x,
                                    const void *x,
                                    const cnnlTensorDescriptor_t desc_alpha,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t desc_beta,
                                    const void *beta,
                                    const cnnlTensorDescriptor_t desc_y,
                                    void *y);

// Group:LRN
/*!
 * @brief Calculates a local response normalization on input tensor \b input, and
 * returns the results in the output tensor \b output. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * @deprecated
 * ::cnnlLrn is deprecated and will be removed in the future release. It is recommended
 * to use ::cnnlLrn_v2 instead.
 *
 * This function needs extra MLU memory as the workspace. You can get the size of
 * the workspace \b workspace_size with the ::cnnlGetLrnWorkspaceSize_v2 function.
 * This function has different calculation modes, for detailed information,
 * see ::cnnlLrnMode_t.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLrn. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlLrn. You can get the size of the workspace with
 *   the ::cnnlGetLrnWorkspaceSize_v2 function.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - Before you call this function, you can first call the ::cnnlGetLrnWorkspaceSize_v2 function to get
 *   the size of the memory workspace in MLU device that the function uses.
 *
 * @par Formula
 * - See "Lrn operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b input, and
 *   output tensor \b output on all hardware platforms.
 *   - \b input data type \b dtype: int8, int16, half, float.
 *   - \b output data type \b dtype: half, float.
 * - The \b input_desc data type should be set with the following rules:
 *   - If \b dtype in \b input_desc is fix-point type, its \b onchip_dtype will be automatically set to
 *     \b dtype. In this case, invoking ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype
 *     in \b input_desc is neither necessary nor effective. The corresponding quantization parameters
 *     will be used during computation.
 *   - If \b dtype in \b input_desc is float-point type:
 *      - In the \b CNNL_LRN_LOCAL_SIZE, \b CNNL_LRN_LOCAL_SIZE_ORIGINAL or \b CNNL_LRN_CROSS_CHANNEL mode:
 *        - On MLU200 series and CE3226, processing will be more efficient, if \b onchip_dtype is set to
 *          int8/int16. If not set or set to int31, online quantization would be enabled internally.
 *        - On MLU300 series, the \b onchip_dtype in \b input_desc is floating-point and cannot be set.
 *      - In the \b CNNL_LRN_WITHIN_CHANNEL mode, the \b onchip_dtype in \b input_desc is floating-point
 *        and cannot be set.
 * - You do not need to set \b onchip_dtype in \b output_desc.
 *
 * @par Data Layout
 *   -The layout of both the \b input_desc and \b output_desc should one of follows:
 *     - CNNL_LAYOUT_NHWC.
 *     - CNNL_LAYOUT_NCHW.
 *     - CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The shape of input should be the same with output's.
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 * - \b lrn_k should be greater than 1e-5.
 * - \b lrn_beta should be greater than 0.01.
 *
 * @par Reference
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and
 *   output tensor to NHWC.
 *
 * @note
 * - When \b input contains NaN/infinity:
 *   - On MLU200 series, \b output is saturation value.
 *   - On CE3226, \b output is random value.
 *   - On MLU300 series, \b output contains large amounts of unexpected NaN.
 */
cnnlStatus_t CNNL_WIN_API cnnlLrn(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t input_desc,
                                  void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);
// Group:LRN
/*!
 * @brief Calculates a local response normalization on input tensor \b input, and
 * returns the results in the output tensor \b output. Compared with ::cnnlLrn,
 * ::cnnlLrn_v2 provides better performance with extra input space. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace. You can get the size of
 * the workspace \b workspace_size with the ::cnnlGetLrnWorkspaceSize_v2 function.
 * This function has different calculation modes, for detailed information,
 * see ::cnnlLrnMode_t.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cnnlLrn operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input. The computing mode of lrn operation. For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrn window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrn formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrn formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the formula, as a hyper-parameter.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLrn. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlLrn. You can get the size of the workspace with
 *   the ::cnnlGetLrnWorkspaceSize_v2 function.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data for the extra.
 *   You need to copy the extra input data to MLU from the host that is initialized with
 *   ::cnnlInitLrnExtraInput. For more information about extra input data, see Cambricon CNNL user Guide.
 * @param[in] extra_device_input_size
 *   Input. The size of the extra input space in bytes that is used in the lrn operation.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetLrnWorkspaceSize_v2, ::cnnlGetLrnExtraInputSize_v2 and
 *   ::cnnlInitLrnExtraInput functions. ::cnnlGetLrnExtraInputSize_v2 gets the extra host and MLU memory
 *   size \b extra_input_size that is used as an extra input data to optimize the lrn operation. You need
 *   to allocate memory both on host and MLU based on the size returned in \b extra_input_size. Then call
 *   ::cnnlInitLrnExtraInput function to initialize the host memory \b extra_host_input and copy it to MLU
 *   memory \b extra_device_input. Finally, the extra workspace allocated by ::cnnlGetLrnWorkspaceSize_v2
 *   should be passed to the ::cnnlLrn_v2 function to perform the lrn operation.
 *
 * @par Formula
 * - See "Lrn operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b input, and
 *   output tensor \b output on all hardware platforms.
 *   - \b input data type \b dtype: int8, int16, half, float.
 *   - \b output data type \b dtype: half, float.
 * - The \b input_desc data type should be set with the following rules:
 *   - If \b dtype in \b input_desc is fix-point type, its \b onchip_dtype will be automatically set to
 *     \b dtype. In this case, invoking ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype
 *     in \b input_desc is neither necessary nor effective. The corresponding quantization parameters
 *     will be used during computation.
 *   - If \b dtype in \b input_desc is float-point type:
 *      - In the \b CNNL_LRN_LOCAL_SIZE, \b CNNL_LRN_LOCAL_SIZE_ORIGINAL or \b CNNL_LRN_CROSS_CHANNEL mode:
 *        - On MLU200 series and CE3226, processing will be more efficient, if \b onchip_dtype is set to
 *          int8/int16. If not set or set to int31, online quantization would be enabled internally.
 *        - On MLU300 series, the \b onchip_dtype in \b input_desc is floating-point and cannot be set.
 *      - In the \b CNNL_LRN_WITHIN_CHANNEL mode, the \b onchip_dtype in \b input_desc is floating-point
 *        and cannot be set.
 * - You do not need to set \b onchip_dtype in \b output_desc.
 *
 * @par Data Layout
 *   -The layout of both the \b input_desc and \b output_desc should one of follows:
 *     - CNNL_LAYOUT_NHWC.
 *     - CNNL_LAYOUT_NCHW.
 *     - CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The shape of input should be the same with output's.
 * - \b lrn_n should be in the range of (0, 15], and \b lrn_n % 2 must be equal to 1.
 * - \b lrn_k should be greater than 1e-5.
 * - \b lrn_beta should be greater than 0.01.
 *
 * @par Reference
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input tensor and
 *   output tensor to NHWC.
 *
 * @note
 * - When \b input contains NaN/infinity:
 *   - On MLU200 series, \b output is saturation value.
 *   - On CE3226, \b output is random value.
 *   - On MLU300 series, \b output contains large amounts of unexpected NaN.
 */
cnnlStatus_t CNNL_WIN_API cnnlLrn_v2(cnnlHandle_t handle,
                                  cnnlLrnMode_t lrn_mode,
                                  unsigned lrn_n,
                                  double lrn_alpha,
                                  double lrn_beta,
                                  double lrn_k,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t input_desc,
                                  void *input,
                                  void * extra_device_input,
                                  size_t extra_device_input_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:LRN
/*!
 * @brief Calculates the gradient of a local response normalization on input tensor \b x,
 * diff tensor \b dy, and returns the results in the gradient tensor \b dx.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the cnnlLrnGrad operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  lrn_mode
 *   Input.  The computing mode of lrnGrad operation.
 *   For detailed information, see ::cnnlLrnMode_t.
 * @param[in] lrn_n
 *   Input. The width of lrnGrad window.
 * @param[in]  lrn_alpha
 *   Input. Value of the alpha parameter in lrnGrad formula, as a scale factor for variance.
 * @param[in]  lrn_beta
 *   Input. Value of the beta parameter in the lrnGrad formula, as a power parameter.
 * @param[in]  lrn_k
 *   Input. Value of the k parameter in the lrnGrad formula, as a hyper-parameter.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] dy_desc
 *   Input. The descriptor of diff tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the diff tensor.
 * @param[in] dx_desc
 *   Input. The descriptor of gradient tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "LrnGrad operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The two input(x, dy) tensors should have the same data type.
 * - The data type of input and output(dx) tensors should be half or float, the LrnGrad operation
 *   only supports half and float data type.
 * - On MLU200 series, the data type of input tensor should be the same with the data type
 *   of output tensor, otherwise the precision will be uncertain.
 *
 * @par Data Layout
 * - The layout of input(x, dy) and output(dx) should be the same.
 * - The layout must be CNNL_LAYOUT_NHWC or CNNL_LAYOUT_NCHW.
 * - This operation does not support CNNL_LAYOUT_HWCN layout.
 *
 * @par Scale Limitation
 * - On MLU300 series or above:
 *   - The shape of input(\b x, \b dy) should be the same with output(\b dx).
 *   - The \b lrn_n should be in the range of [3, 15], and \b lrn_n % 2 must be equal to 1.
 *   - The \b lrn_beta should be equal to or greater than 0.01.
 *   - The \b lrn_k should be equal to or greater than 1e-5.
 * - On MLU200 series, for higher precision, the value ranges of the input and parameters
 *   should satisfy the requirements described below. Otherwise, the precision will be uncertain.
 *   - The value of input tensor should be in the range of [-10.0, 10.0].
 *   - The \b lrn_n should be in the range of [3, 9], and \b lrn_n % 2 must be equal to 1.
 *   - The \b lrn_alpha should be in the range of [1.0, 5.0].
 *   - The \b lrn_beta should be in the range of [1.0, 1.5].
 *   - The \b lrn_k should be in the range of [1.0, 5.0].
 *
 * @note
 * - The operation does not support NaN/infinity in input data.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLrnGrad(cnnlHandle_t handle,
                                      cnnlLrnMode_t lrn_mode,
                                      unsigned int lrn_n,
                                      float lrn_alpha,
                                      float lrn_beta,
                                      float lrn_k,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t dy_desc,
                                      const void *dy,
                                      const cnnlTensorDescriptor_t dx_desc,
                                      void *dx);
// Group:Rsqrt
/*!
 * @brief Computes gradient of rsqrt on input tensor \b y and \b diff_y, and returns the result
 *        in the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the gradient tensor from previous operation.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Rsqrt Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must have the same shape, and the input tensor \b y must meet
 *   the following input data range:
 *   - float: [1e-10,1e6].
 *   - half: [0.01,500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/rsqrt_grad
 */
cnnlStatus_t CNNL_WIN_API cnnlRsqrtBackward(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t data_desc,
                                            const void *y,
                                            const void *diff_y,
                                            void *output);

// Group:Sqrt
/*!
 * @brief Computes gradient of sqrt on input tensor \b y and \b diff_y, and returns the results
 *        in the output tensor \b diff_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] y_dy_dx_desc
 *   Input. The descriptor of the tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Sqrt Backward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must have the same shape, and the input tensor \b y must meet
 *   the following input data range:
 *   - float: [1e-10,1e6].
 *   - half: [0.01,500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/sqrt_grad
 */
cnnlStatus_t CNNL_WIN_API cnnlSqrtBackward(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t y_dy_dx_desc,
                                           const void *y,
                                           const void *diff_y,
                                           void *diff_x);

// Group:NumTrue
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace.
 *
 * The size of extra workspace is based on the given information of numtrue operation,
 * including input tensor descriptor \b data_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the numtrue operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlNumTrue function.
 * - The allocated extra workspace should be passed to the ::cnnlNumTrue function to perform
 *   the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNumTrueWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t data_desc,
                                                      size_t *size);

// Group:NumTrue
/*!
 * @brief Computes the index and the number of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on Tensorflow framework.
 *
 * @deprecated
 *   ::cnnlNumTrue is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlNumTrue_v2 instead, which does not need to compute the preliminary
 *   indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the coordinate of \b x which
 *   has been reshaped in one dimension.
 * @param[out] num_true
 *   Output. Pointer to the MLU memory that stores the number of non-zeros or true elements in
 *   input tensor \b x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x and output tensor
 *   \b index, and \b num_true.
 *   - input tensor: half, float, int32, bool.
 *   - index tensor: uint32.
 *   - num_true tensor: uint32.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetNumTrueWorkspaceSize function.
 * - Allocated extra workspace by ::cnnlGetNumTrueWorkspaceSize function should be passed to
 *   the ::cnnlNumTrue function to perform the numtrue operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7 -->
       input: [[0, 0, 0, 0, 5, 0, 0],
               [0, 0, 1, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 3],
               [6, 0, 0, 0, 0, 0, 0]]

    output array by 1 * 32 and 1 * 1 -->
       index: [4, 0, 0, 0, 0, 0, 0, 2,
               0, 0, 0, 0, 0, 0, 6, 0,
               0, 0, 0, 0, 0, 0, 0, 0,
               0, 0, 0, 0, 1, 1, 1, 1]

   --> num_true: [4]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlNumTrue(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      uint32_t *index,
                                      uint32_t *num_true);


// Group:NumTrue
/*!
 * @brief Computes the number of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on Tensorflow framework. Compared with ::cnnlNumTrue, it does not compute the indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   numtrue operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] num_true_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] num_true
 *   Output. Pointer to the MLU memory that stores the number of non-zeros or true elements in
 *   input tensor \b x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x and output tensor
 *   \b num_true.
 *   - input tensor: half, float, int32, bool.
 *   - num_true tensor: int32.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as following:
 *   - input tensor \b x: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor \b num_true: \p CNNL_LAYOUT_ARRAY.
 *
 * @par API Dependency
 * - None
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7 -->
       input: [[0, 0, 0, 0, 5, 0, 0],
               [0, 0, 1, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 3],
               [6, 0, 0, 0, 0, 0, 0]]

    output array with shape of 1 * 1 -->
       num_true: [4]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlNumTrue_v2(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t x_desc,
                                         const void *x,
                                         const cnnlTensorDescriptor_t num_true_desc,
                                         void *num_true);

// Group:Where
/*!
 * @brief Computes index of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on Tensorflow framework.
 *
 * @deprecated
 *   ::cnnlWhere is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlWhere_v2 instead, which is much faster in computing the indices of the non-zero elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] strides
 *   Output. Pointer to the MLU memory that stores stride of each dimension in \b x.
 * @param[in] index
 *   Output. Pointer to the MLU memory that stores the coordinate of \b x which has been reshaped
 *   in one dimension.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor which is the coordinate of
 *   non-zeros or true elements in \b x.
 * @param[in] as_tuple
 *   Input. A scalar. When \b as_tuple is false, \b y points to a tensor containing the indices of
 *   all non-zeros or true elements of input tensor \x. When \b as_tuple is true, \b y points to a
 *   tuple of 1-dimensional tensor, each containing the indices (in that dimension) of all
 *   non-zeros or true elements in the input tensor \x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x , \b strides,
 *   \b index and output tensor \y.
 *   - input tensor: half, float, int32, bool.
 *   - strides tensor: uint32.
 *   - index tensor: uint32.
 *   - output tensor: int32.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetNumTrueWorkspaceSize function and
 *   ::cnnlNumTrue function.
 * - The extra workspace allocated by ::cnnlGetNumTrueWorkspaceSize should be
 *   passed to the ::cnnlNumTrue function to perform the numTrue operation. Index tensor obtained
 *   by ::cnnlNumTrue function should be passed to the ::cnnlWhere function to perform the
 *   where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the numtrue operation is as follows:
   @verbatim
    input array by 4 * 7, 1 * 2 and 1 * 32 -->
        input: [[0, 0, 0, 0, 5, 0, 0],
                [0, 0, 1, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 3],
                [6, 0, 0, 0, 0, 0, 0]]

    --> strides: [7, 1]

    --> index: [4, 0, 0, 0, 0, 0, 0,
                2, 0, 0, 0, 0, 0, 0,
                6, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0,
               1, 1, 1, 1]

   output array by 4*2 --> output: [[0, 4],
                                    [1, 2],
                                    [2, 6],
                                    [3, 0]]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlWhere(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const uint32_t *strides,
                                    const uint32_t *index,
                                    const cnnlTensorDescriptor_t y_desc,
                                    int *y,
                                    const bool as_tuple);


// Group: Where
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace.
 *
 * The size of extra workspace is based on the given information of where operation,
 * including input tensor descriptor \b x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] num_true_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the where operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlWhere_v2 function.
 * - The allocated extra workspace should be passed to the ::cnnlWhere_v2 function to perform
 *   the where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetWhereWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t num_true_desc,
                                                    size_t *workspace_size);

// Group:Where
/*!
 * @brief Computes index of non-zeros or true elements in input tensor.
 *
 * It is used in transformer on Tensorflow framework. Compared with ::cnnlWhere, this function needs
 * extra workspace and does not need the preliminary indices (instead computes the ultimate indices in a faster way).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the where operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @parma[in] num_true_desc
 *   Input. The descriptor of the \b num_true tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] num_true
 *   Input. Pointer to the MLU memory that stores the number of none-zero elements in \b x.
 * @param[in] as_tuple
 *   Input. A scalar. When \b as_tuple is false, \b y points to a tensor containing the indices of
 *   all non-zeros or true elements of input tensor \x. When \b as_tuple is true, \b y points to a
 *   tuple of 1-dimensional tensor, each containing the indices of a specific dimension for all
 *   non-zeros or true elements in the input tensor \x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the where operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the where
 *   operation. You can get the size of the workspace with the ::cnnlGetWhereWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor which is the indices of
 *   non-zeros or true elements in \b x.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x , \b num_true,
 *   and output tensor \y.
 *   - input tensor: half, float, int32, bool.
 *   - num_true: int32
 *   - output tensor: int32.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetWhereWorkspaceSize function and
 *   ::cnnlNumTrue_v2 function.
 * - The extra workspace allocated by ::cnnlGetWhereWorkspaceSize should be
 *   passed to this function to perform the where operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the where operation is as follows:
   @verbatim
    input array with shape of 4 * 7 -->
        input: [[0, 0, 0, 0, 5, 0, 0],
                [0, 0, 1, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 3],
                [6, 0, 0, 0, 0, 0, 0]]


        --> num_true: 4

        --> as_tuple: false

    output array by 4*2 --> output: [[0, 4],
                                     [1, 2],
                                     [2, 6],
                                     [3, 0]]
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/where
 */
cnnlStatus_t CNNL_WIN_API cnnlWhere_v2(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t num_true_desc,
                                       const void *num_true,
                                       const bool as_tuple,
                                       void *workspace,
                                       const size_t workspace_size,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:Embedding
/*!
 * @brief Maps \b filter to \b output according to \b indices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store the index of \b filter which
 *   corresponds to each row of \b output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of \b filter that corresponds to each
 *   row of \b output. The value of indices should be less than the first dimension of filter.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "EmbeddingForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for index tensor
 *   \b indices, filter tensor \b filter, and output tensor \b output.
 *   <b>Note that the data type of filter tensor and output tensor must be same.</b>
 *   - filter tensor: uint8, int8, uint16, int16, uint32, int32, int64, uint64,
 *     bool, half, float, complexhalf, complexfloat, double.
 *   - index tensor: int32, int64 (int64 is not supported when layout is CNNL_LAYOUT_NHWC).
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, int64, uint64,
 *     bool, half, float, complexhalf, complexfloat, double.
 *
 * @par Data Layout
 * - The layout of the filter tensor, index tensor, and output tensor are as
 *   follows:
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY(default) and \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The filter tensor, index tensor, and output tensor must meet the following requirements:
 *   - filter tensor: The dimension should be 2.
 *   - output tensor: The element number should be less than \p UINT_MAX.
 *
 * @note
 * - From Cambricon CNNL 1.9.O and later versions, the layout of this operation is recommended to be set
 *   as \p CNNL_LAYOUT_ARRAY. However, to be compatible with the previous layout settings,
 *   you can still set the layout of output tensor to \p CNNL_LAYOUT_NHWC, which will
 *   transpose the last two dimensions of output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 3 and 2 * 2

     --> filter: [[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]]

     --> indices: [[0, 1], [1, 0]]

     output array by 2 * 2 * 3 (output array by 2 * 3 * 2 if layout is \p CNNL_LAYOUT_NHWC)

     --> output: [[[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                  [[-0.6622, -0.4790,  0.8539], [0.5356,  1.5739, -0.4864]]]
     if layout is \p CNNL_LAYOUT_NHWC:
     --> output: [[[0.5356, -0.6622], [1.5739, -0.4790], [-0.4864, 0.8539]],
                  [[-0.6622, 0.5356], [-0.4790, 1.5739], [0.8539, -0.4864]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingForward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t filter_desc,
                                               const void *filter,
                                               const cnnlTensorDescriptor_t indices_desc,
                                               const int *indices,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);

// Group:Embedding
/*!
 * @brief Maps \b filter to \b output according to \b indices.
 *
 * @deprecated
 *   ::cnnlEmbeddingForward is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlEmbeddingForward_v2 instead, which supports the parameter \b padding_idx that
 *   determines which index of the embedding vector \b output should be initialized to zero.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the embedding
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor used to store the index of \b filter which
 *   corresponds to each row of \b output. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the index of \b filter that corresponds to each
 *   row of \b output. The value of indices should be less than the first dimension of filter.
 * @param[in] padding_idx
 *   Input. Determines which index of the embedding vector \b output should be initialized to zero.
 *   The value of padding_idx should be included in the range of [-1,filter[0]).
 * @param[in] max_norm
 *   Input. Pointer to the MLU memory that stores the maximum value of the cut-off norm.
 *   Currently, \b max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the MLU memory that stores the type of used p-norm.
 *   Currently, \b norm_type is not supported and needs to be set as null.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "EmbeddingForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for index tensor
 *   \b indices, filter tensor \b filter, and output tensor \b output.
 *   <b>Note that the data type of filter tensor and output tensor must be same.</b>
 *   - filter tensor: uint8, int8, uint16, int16, uint32, int32, int64, uint64,
 *     bool, half, float, complexhalf, complexfloat, double.
 *   - index tensor: int32, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, int64, uint64,
 *     bool, half, float, complexhalf, complexfloat, double.
 *
 * @par Data Layout
 * - The layout of the filter tensor, index tensor, and output tensor are as
 *   follows:
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The filter tensor, index tensor, and output tensor must meet the following requirements:
 *   - filter tensor: The dimension should be 2.
 *   - output tensor: The element number should be less than \p UINT_MAX.
 *
 * @note
 *
 * @par Requirements
 * - The \b padding_idx should be included in the range of [-1, weight[0]).
 *
 * @par Example
 * - The example of the embedding backward operation is as follows:
     @verbatim
     input two arrays by 2 * 3 and 2 * 2

     --> filter: [[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]]

     --> indices: [[0, 1], [1, 0]]

     output array by 2 * 2 * 3 (output array by 2 * 3 * 2 if layout is \p CNNL_LAYOUT_NHWC)

     --> output: [[[0.5356,  1.5739, -0.4864], [-0.6622, -0.4790,  0.8539]],
                  [[-0.6622, -0.4790,  0.8539], [0.5356,  1.5739, -0.4864]]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html
 */
cnnlStatus_t CNNL_WIN_API cnnlEmbeddingForward_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t filter_desc,
                                                  const void *filter,
                                                  const cnnlTensorDescriptor_t indices_desc,
                                                  const int *indices,
                                                  const int padding_idx,
                                                  const float *max_norm,
                                                  const float *norm_type,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output);

// Group:EmbeddingBag
/*!
 * @brief Computes sums, means or the maximum value of bags of embedding forward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   embedding_bag operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the \b filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores embedding matrix. The \indices should be a
 *   two-dimensional tensor. Its first dimension represents the size of the dictionary of
 *   embeddings and its second dimension represents the size of each embedding vector.
 * @param[in] indices_desc
 *   Input. The descriptor of the \b indices tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores indices of \b filter which will be reduced to
 *   bags into \b output. The value of \b indices should be less than the first dimension of
 *   \b filter. The \indices should be a one-dimensional or two-dimensional tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the \b offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the starting index position of each bag in \b indices.
 *   Only used when \b indices is 1D. The value of \b offset must be less than the size of indices
 *   dimension. The element in \b offset is sorted in an ascending order and starts with 0.
 * @param[in] per_sample_filter_desc
 *   Input. The descriptor of the \b per_sample_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] per_sample_filter
 *   Input. The filter of \b indices, only used when mode is ::CNNL_REDUCEMODE_SUM.
 *   The shape of \b per_sample_filter must be same with \b indices.
 * @param[in] mode
 *   Input. The way to reduce the bag, see ::cnnlReduceMode_t for more details.
 * @param[in] max_norm
 *   Input. Pointer to the MLU memory that stores the maximum value of the cut-off norm.
 *   Currently, \b max_norm is not supported and needs to be set as null.
 * @param[in] norm_type
 *   Input. Pointer to the MLU memory that stores the type of used p-norm.
 *   Currently, \b norm_type is not supported and needs to be set as null.
 * @param[in] include_last_offset
 *   Input. A boolean value describing whether \b offset has one additional element. If true, the last
 *   value of \b offset is equivalent to the size of \b indices. If false, there is not an
 *   additional element in \b offset. Currently, \b include_last_offset is not supported and needs to be set as false.
 * @param[in] padding_idx
 *   Input. Pointer to the host memory that determines which index of the embedding vector should be
 *   initialized to zero and excluded from the reduction. Currently, \b padding_idx is not supported and needs to be set as null.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "EmbeddingBag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for filter tensor
 *   \b filter, indices tensor \b indices, offset tensor \b offset, per_sample_filter tensor
 *   \b per_sample_filter and output tensor \b output.
 *   <b>Note that the data types of \b filter tensor, \b per_sample_filter and \b output
 *   tensor must be same.</b>
 *   - filter tensor: half, float.
 *   - indices tensor: int32, int64.
 *   - offset tensor: int32, int64.
 *   - per_sample_filter tensor: half, float.
 *   - output tensor: half, float.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the embedding_bag operation is as follows:
     @verbatim
     filter array by 10 * 3 -->
         filter: [[0.5860, 0.6688, 0.2687],
                 [0.4091, 0.0169, 0.6869],
                 [0.1551, 0.2168, 0.2718],
                 [0.0460, 0.1324, 0.5958],
                 [0.3568, 0.3494, 0.4879],
                 [0.5781, 0.1850, 0.0506],
                 [0.7095, 0.9444, 0.3460],
                 [0.5262, 0.7910, 0.9559],
                 [0.5842, 0.8564, 0.8038],
                 [0.1000, 0.8825, 0.9091]]
     input two arrays by 1 * 8 and 1 * 3
     indices array by 1 * 8 -->
         indices: [1, 2, 3, 4, 5, 6, 7, 8]
     offset array by 1 * 3 -->
         offset: [0, 4, 6]
     param:
       mode: CNNL_REDUCEMODE_SUM

     output array by 3 * 3 -->
         output: [[0.9671, 0.7155, 2.0425],
                 [1.2877, 1.1294, 0.3965],
                 [1.1104, 1.6474, 1.7597]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html
 */
cnnlStatus_t cnnlEmbeddingBag(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t filter_desc,
                              const void *filter,
                              const cnnlTensorDescriptor_t indices_desc,
                              const int *indices,
                              const cnnlTensorDescriptor_t offset_desc,
                              const int *offset,
                              const cnnlTensorDescriptor_t per_sample_filter_desc,
                              const void *per_sample_filter,
                              const cnnlReduceMode_t mode,
                              const void *max_norm,
                              const void *norm_type,
                              const bool include_last_offset,
                              const int *padding_idx,
                              const cnnlTensorDescriptor_t output_desc,
                              void *output);

// Group:MatMul
/*!
 * @brief Computes the matrix multiplication operation, then returns the results in the output
 * tensor \b c. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_trans_a
 *   Input. Boolean value indicating whether \b a matrix is transposed.
 * @param[in] is_trans_b
 *   Input. Boolean value indicating whether \b b matrix is transposed.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \b a, the default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \b c, the default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - On all hardware platforms, this function supports any combinations of the following data types for
 *   input tensor \b a, \b b and output tensor \b c.
 *   - \b a data type: int8, int16, int31.
 *   - \b b data type: int8, int16, int31.
 *   - \b c offchip data type: half, float.
 *   - \b c onchip data type: half, float.
 * - On MLU300 series or above, this function supports the combinations of the following data types for
 *   input tensor \b a, \b b and output tensor \b c.
 *   - \b a, \b b, \b c offchip data type, \b c onchip data type: half, half, half, half.
 *   - \b a, \b b, \b c offchip data type, \b c onchip data type: half, half, half, float.
 *   - \b a, \b b, \b c offchip data type, \b c onchip data type: float, float, float, float.
 *
 * @note
 * - On all hardware platforms, the combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth of \b c onchip data type for operation computing is not shorter than \b c
 *     offchip data type.
 * - On CE3226 platform, the combinations of the data types should satisfy the following extra rules:
 *   - The \b a offchip and onchip data type must be fixed-point type and be the same as one another.
 *   - The \b b offchip and onchip data type must be fixed-point type and be the same as one another.
 *   - The \b b data type must be int8 if \b a data type is int8.
 *   - The data type bitwidth of \b c onchip data type for operation computing is not shorter than \b c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The \b a and \b b must be two dimensions.
 *   - The number of \b a matrix's columns must be equal to the number of \b b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_trans_a:                    false
      is_trans_b:                    false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMul(cnnlHandle_t handle,
                                     const bool is_trans_a,
                                     const bool is_trans_b,
                                     const void *alpha,
                                     const cnnlTensorDescriptor_t a_desc,
                                     const void *a,
                                     const cnnlTensorDescriptor_t b_desc,
                                     const void *b,
                                     const void *beta,
                                     const cnnlTensorDescriptor_t c_desc,
                                     void *c);

// Group:MatMul
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 *        to optimize the matrix multiplication operation.
 *
 * @deprecated
 *   ::cnnlGetMatMulWorkspaceSize is deprecated and will be removed in the future release. It is recommended
 *   to use::cnnlGetMatMulHeuristicResult instead, which gets the workspace size and algorithm for
 *   matrix multiplication.
 *
 * The size of extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \b matmul_desc, input tensor
 * descriptor of left matrix \b a_desc, input tensor descriptor of right matrix \b b_desc, output
 * tensor  descriptor \b c_desc, and the matrix multiplication algorithm \b algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. Needed to be set in out-of-place matrix multiplication.
 *   Currently not supported and should be set to NULL.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Currently not supported and should be set to NULL.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlSetMatMulDescAttr function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlMatMul_v2 function to
 *   perform the matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulWorkspaceSize(cnnlHandle_t handle,
                                                     cnnlMatMulDescriptor_t matmul_desc,
                                                     cnnlTensorDescriptor_t a_desc,
                                                     cnnlTensorDescriptor_t b_desc,
                                                     cnnlTensorDescriptor_t c_desc,
                                                     cnnlTensorDescriptor_t d_desc,
                                                     cnnlMatMulAlgo_t algo,
                                                     size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Computes the matrix multiplication operation, then returns the results in the output
 *        tensor \b d. For more information, see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlMatMul, it supports the use of extra workspace size, the use of \b algo
 * to pass the algorithm information and the use of \b matmul_desc to pass parameters
 * like ::CNNL_MATMUL_DESC_TRANSA.
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetMatMulAlgoHeuristic and ::cnnlGetMatMulHeuristicResult functions in turn.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \b a, the default value is 1.0.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \b c, the default value is 0.0.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] c
 *   Input. Pointer to the MLU memory that stores the input tensor \b c in out-of-place matrix multiplication
 *   where d = alpha * a * b + beta * c, or pointer to the MLU memory that stores the output tensor \b d in in-place
 *   matrix multiplication where c == d = alpha * a * b + beta * c.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulAlgoHeuristic and
 *   ::cnnlGetMatMulHeuristicResult functions in turn.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \b d.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - On all hardware platforms, this function supports any combinations of the following data types for
 *   input tensor \b a, \b b and output tensor \b d.
 *   - \b a data type: int8, int16, int31.
 *   - \b b data type: int8, int16, int31.
 *   - \b d offchip data type: half, float.
 *   - \b d onchip data type: half, float.
 * - On MLU300 series or above, this function supports the combinations of the following data types for
 *   input tensor \b a, \b b and output tensor \b d.
 *   - \b a, \b b, \b d offchip data type, \b d onchip data type: half, half, half, half.
 *   - \b a, \b b, \b d offchip data type, \b d onchip data type: half, half, half, float.
 *   - \b a, \b b, \b d offchip data type, \b d onchip data type: float, float, float, float.
 *
 * @note
 * - The value of \b c_desc is the same as that of \b d_desc.
 * - On all hardware platforms, the combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth of \b d onchip data type for operation computing is not shorter than \b d
 *     offchip data type.
 * - On CE3226 platform, the combinations of the data types should satisfy the following extra rules:
 *   - The \b a offchip and onchip data type must be fixed-point type and be the same as one another.
 *   - The \b b offchip and onchip data type must be fixed-point type and be the same as one another.
 *   - The \b b data type must be int8 if \b a data type is int8.
 *   - The data type bitwidth of \b d onchip data type for operation computing is not shorter than \b d
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The \b a and \b b must be two dimensions.
 *   - The number of \b a matrix's columns must be equal to the number of \b b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a should not be transposed and matrix \b b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      CNNL_MATMUL_USE_BETA:         false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor d: [99, 256]
      Dimension of bias tensor: [1, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMul_v2(cnnlHandle_t handle,
                                        cnnlMatMulDescriptor_t matmul_desc,
                                        cnnlMatMulAlgo_t algo,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        const cnnlTensorDescriptor_t b_desc,
                                        const void *b,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t d_desc,
                                        void *d);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \b result for a matrix multiplication heuristic result,
 *        and allocates memory for the result. The result is defined in ::cnnlMatMulHeuristicResult_t.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of matrix multiplication heuristic result.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlCreateMatMulHeuristicResult(cnnlMatMulHeuristicResult_t *result);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication heuristic result, that is previously created with
 *        the ::cnnlCreateMatMulHeuristicResult.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlDestroyMatMulHeuristicResult(cnnlMatMulHeuristicResult_t result);

// Group:MatMul
/*!
 * @brief Gets the matrix multiplication algorithm and workspace size from heuristic result,
 *        that is previously selected with ::cnnlGetMatMulAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result obtained by ::cnnlGetMatMulAlgoHeuristic.
 *
 * @param[out] algo
 *   Output. The matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulHeuristicResult(cnnlMatMulHeuristicResult_t result,
                                          cnnlMatMulAlgo_t algo,
                                          size_t *workspace_size);

// Group:MatMul
/*!
 * @brief Retrieves the possible algorithms can be used in the matrix multiplication.
 *        The output is placed in result_array[] in the order of increasing estimated compute time.
 *
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor used in out-of-place matrix multiplication. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 *   Not supported currently and should be set to NULL.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *   configuration. Currently not supported and should be set to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms. The maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms \b requested_algo_count only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulAlgoHeuristic(cnnlHandle_t handle,
                                        cnnlMatMulDescriptor_t matmul_desc,
                                        cnnlTensorDescriptor_t a_desc,
                                        cnnlTensorDescriptor_t b_desc,
                                        cnnlTensorDescriptor_t c_desc,
                                        cnnlTensorDescriptor_t d_desc,
                                        cnnlMatMulPrefer_t preference,
                                        int requested_algo_count,
                                        cnnlMatMulHeuristicResult_t result_array[],
                                        int *return_algo_count);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \b matmul_desc for a matrix multiplication operation,
 *        and allocates memory for holding the information about the matrix multiplication operation.
 *        The information is defined in ::cnnlMatMulDescriptor_t.
 *
 * @param[out] matmul_desc
 *   Output. A host pointer to the matrix multiplication descriptor that holds information about the matrix
 *   multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatMulDescAttr function to initialize
 *   and set the information to the matrix multiplication descriptor.
 * - You need to call the ::cnnlMatMulDescDestroy function to destroy the descriptor.
 *
 * @note
 * - The default compute data type of c is c_desc->dtype, use cnnlSetTensorDescriptorOnchipDataType() to
 *   set onchip data type if high accuracy of c is needed.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulDescCreate(cnnlMatMulDescriptor_t *matmul_desc);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication descriptor \b matmul_desc
 *        that is previously created with the ::cnnlMatMulDescCreate.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulDescriptor_t
 * and holds the information about the matrix multiplication operation.
 *
 * @param[in] matmul_desc
 *   Input. The matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulDescDestroy(cnnlMatMulDescriptor_t matmul_desc);

// Group:MatMul
/*!
 * @brief Initializes the matrix multiplication descriptor \b matmul_desc
 * that is previously created with the ::cnnlMatMulDescCreate function, and sets
 * the information about the matrix multiplication operation to the matrix multiplication
 * descriptor \b matmul_desc. The information includes the attribute defined in
 * ::cnnlMatMulDescAttribute_t \b attr, the host pointer to the attribute value \b buf, and
 * the size of buffer for verification.
 *
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operation. For detailed
 *   information, see ::cnnlMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatMulDescAttr(cnnlMatMulDescriptor_t matmul_desc,
                                                cnnlMatMulDescAttribute_t attr,
                                                const void *buf,
                                                size_t size_in_bytes);

// Group:MatMul
/*!
 * @brief Returns the pointer to the \b buf and size of the buffer \b size_written of the attribute
 * retrieved with the given matmul multiplication descriptor \b matmul_desc, attribute \b attr.
 * And \b size_in_bytes is used to check if the memory size is same with \b size_written.
 *
 * You can set the attribute in the matrix multiplication descriptor based on the return value
 * of this function.
 *
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operation. For detailed
 *   information, see ::cnnlMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulDescAttr(const cnnlMatMulDescriptor_t matmul_desc,
                                                cnnlMatMulDescAttribute_t attr,
                                                void *buf,
                                                size_t size_in_bytes,
                                                size_t *size_written);

// Group:MatMul
/*!
 * @brief Creates a descriptor pointed by \b algo for a matrix multiplication algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlMatMulAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the matrix multiplication algorithm that holds information about the matrix
 *   multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeMatMulAlgorithm function to initialize
 *   and set the information to the matrix multiplication algorithm.
 * - You need to call the ::cnnlMatMulAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulAlgoCreate(cnnlMatMulAlgo_t *algo);

// Group:MatMul
/*!
 * @brief Destroys a matrix multiplication algorithm descriptor \b algo
 *        that is previously created with the ::cnnlMatMulAlgoCreate.
 *
 * The matrix multiplication descriptor is defined in ::cnnlMatMulAlgo_t
 * and holds the information about the matrix multiplication algorithm.
 *
 * @param[in] algo
 *   Input. The matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulAlgoDestroy(cnnlMatMulAlgo_t algo);

// Group:MatMul
/*!
 * @brief Returns the most suited matrix multiplication algorithm that can be used
 * in the operation.
 *
 * The returned matrix multiplication is chosen from all supported matrix
 * algorithms by Cambricon CNNL defined in ::cnnlMatMulAlgo_t and is based on the given matrix
 * multiplication descriptor \b matmul_desc, tensor descriptor of left matrix \b a_desc, tensor
 * descriptor of right matrix \b b_desc, tensor descriptor of output matrix \b c_desc, and
 * matrix multiplication algorithm \b preference.
 *
 * The computing performance options \b preference defined in the ::cnnlMatMulPreference_t
 * enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *  Input. The descriptor of the matrix multiplication operation. For detailed
 *  information, see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the matrix multiplication operation to
 * get better performance. This parameter only supports CNNL_MATMUL_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the matrix
 *   multiplication. The algorithms are defined in the ::cnnlMatMulAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeMatMulAlgorithm(cnnlHandle_t handle,
                                                         const cnnlMatMulDescriptor_t matmul_desc,
                                                         const cnnlTensorDescriptor_t a_desc,
                                                         const cnnlTensorDescriptor_t b_desc,
                                                         const cnnlTensorDescriptor_t c_desc,
                                                         cnnlMatMulPreference_t preference,
                                                         cnnlMatMulAlgo_t *algo);

// Group:MatMul
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication operation.
 *
 * The size of extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \b matmul_desc, input tensor
 * descriptor of left matrix \b a_desc, input tensor descriptor of right matrix \b b_desc, output
 * tensor descriptor \b c_desc, and the matrix multiplication algorithm \b algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the matrix multiplication. The algorithms are defined
 *   in the ::cnnlMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeMatMulAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeMatMulAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeMatMul function to
 *   performs the matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeMatMulWorkspaceSize(cnnlHandle_t handle,
                                                          const cnnlMatMulDescriptor_t matmul_desc,
                                                          const cnnlTensorDescriptor_t a_desc,
                                                          const cnnlTensorDescriptor_t b_desc,
                                                          const cnnlTensorDescriptor_t c_desc,
                                                          cnnlMatMulAlgo_t algo,
                                                          size_t *workspace_size);
// Group:MatMul
/*!
 * @brief Quantizes data type of input tensor \b a and \b b, and computes the matrix
 * multiplication, then returns the results in the output tensor \b c. For more
 * information about quantization, see "Cambricon CNNL User Guide".
 *
 * The matrix multiplication is computed based on the matrix multiplication algorithm
 * set in \b algo. You can call the ::cnnlGetQuantizeMatMulAlgorithm function to get
 * the most suitable algorithm. This function needs extra MLU memory as the workspace to
 * improve the matrix multiplication performance. You can get the size of the workspace
 * \b workspace_size_in_bytes with the ::cnnlGetQuantizeMatMulWorkspaceSize function.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operation. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of alpha to 1.0 and the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores left matrix.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores right matrix.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output matrix.
 * @param[in] algo
 *   Input. The algorithm used to compute the matrix multiplication. The algorithm detail is stored
 *   in an opaque structure ::cnnlMatMulAlgo_t points to. You can get the best suitable algorithm with
 *   ::cnnlGetQuantizeMatMulAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size_in_bytes
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetQuantizeMatMulWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c.
 *   - \b a offchip data type: half, float, int8, int16, int31.
 *   - \b a onchip data type: int8, int16, int31.
 *   - \b b offchip data type: half, float, int8, int16, int31.
 *   - \b b onchip data type: int8, int16, int31.
 *   - \b c offchip data type: half, float.
 *   - \b c onchip data type: half, float.
 * - \b a offchip data type should be the same as \b b offchip data type when \b a and \b offchip data type is
 *   floating point.
 * - If both \b a and \b b offchip data type is integer, you do not need to set the onchip data type of \a and \b.
 *   If you set the onchip data type of \b a and b, the offchip and onchip data type of \b a must be the same,
 *   and the offchip and onchip data type of \b b must be the same too.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for \b c onchip data type should not be shorter than \b c offchip data type.
 *
 * - This function does not support offline asymmetric quantization currently.
 * - The combinations of the data types should satisfy the following extra rules on CE3226 platform:
 *   - The onchip data type of \b a and \b b must be fixed-point type.
 *   - The \b b data type must be int8 if \b a data type is int8.
 *   - The data type bitwidth of \b c onchip data type for operation computing is not shorter than \b c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   1. The \b a and \b b must be two dimensions.
 *   2. The number of \b a matrix's columns must be equal to the number of \b b matrix's rows after both inputs
 *   perform transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 * - CNNL_MATMUL_DESC_TRANSA, CNNL_MATMUL_DESC_TRANSB attributes of \b matmul_desc can be set by
 *   ::cnnlSetMatMulDescAttr function.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantizeMatMul(cnnlHandle_t handle,
                                             const cnnlMatMulDescriptor_t matmul_desc,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t a_desc,
                                             const void *a,
                                             const void *a_position,
                                             const void *a_scale,
                                             const void *a_offset,
                                             const cnnlTensorDescriptor_t b_desc,
                                             const void *b,
                                             const void *b_position,
                                             const void *b_scale,
                                             const void *b_offset,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t c_desc,
                                             void *c,
                                             cnnlMatMulAlgo_t algo,
                                             void *workspace,
                                             size_t workspace_size_in_bytes);

// Group:MatMulInference
/*!
 * @brief Returns the most suited matrix multiplication algorithm that can be used
 * in the matrix multiplication operation with quantization.
 *
 * The returned matrix multiplication is chosen from all the CNNL supported matrix
 * algorithms defined in ::cnnlMatMulAlgo_t and is based on the given matrix
 * multiplication descriptor \b matmul_desc, tensor descriptor of left matrix \b a_desc, tensor
 * descriptor of right matrix \b b_desc, tensor descriptor of output matrix \b c_desc, and
 * matrix multiplication algorithm \b preference.
 *
 * The computing performance options \b preference defined in the ::cnnlMatMulPreference_t
 * enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *  Input. The descriptor of the matrix multiplication operation. For detailed
 *  information, see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the matrix multiplication operation to get better performance.
 *   This parameter only supports CNNL_MATMUL_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of the algorithm are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMatMulInferenceAlgorithm(cnnlHandle_t handle,
                                                          cnnlMatMulDescriptor_t matmul_desc,
                                                          cnnlTensorDescriptor_t a_desc,
                                                          cnnlTensorDescriptor_t b_desc,
                                                          cnnlTensorDescriptor_t c_desc,
                                                          cnnlMatMulPreference_t preference,
                                                          cnnlMatMulAlgo_t *algo);

// Group:MatMulInference
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication operation after quantization.
 *
 * The size of extra workspace is based on the given information of the matrix multiplication
 * operation, including the matrix multiplication descriptor \b matmul_desc, input tensor
 * descriptor of left matrix \b a_desc, input tensor descriptor of right matrix \b b_desc, output
 * tensor descriptor \b c_desc, and the matrix multiplication algorithm \b algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of the algorithm are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *   You can get the best suited algorithm with the ::cnnlGetMatMulInferenceAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetMatMulInferenceAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlMatMulInference function to
 *   perform the matrix multiplication operation with quantization.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetMatMulInferenceWorkspaceSize(cnnlHandle_t handle,
                                    cnnlMatMulDescriptor_t matmul_desc,
                                    cnnlTensorDescriptor_t a_desc,
                                    cnnlTensorDescriptor_t b_desc,
                                    cnnlTensorDescriptor_t c_desc,
                                    cnnlMatMulAlgo_t algo,
                                    size_t *workspace_size);

// Group:MatMulInference
 /*!
 * @brief Converts the data type of input \b x to integer when the offchip data type of input \b x is
 * floating point, then computes the matrix mulplication operation. And converts the data type of
 * result to integer when the offchip data type of \b c is integer. For more information about
 * quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetMatMulInferenceWorkspaceSize function. The matrix multiplication is computed based
 * on the matrix multiplication algorithm set in \b algo. You can call the
 * ::cnnlGetMatMulInferenceAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] matmul_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of alpha to 1.0 and the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Attributes of the algorithm are defined in an opaque structure pointed by ::cnnlMatMulAlgo_t.
 *   You can get the best suited algorithm with the ::cnnlGetMatMulInferenceAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulInferenceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c on all hardware platforms.
 *   - the offchip data type of \b a: half, float, int8, int16.
 *   - the onchip data type of \b a: int8, int16.
 *   - the offchip data type and onchip data type of \b b: int8, int16.
 *   - the offchip data type of \b c: half, float, int8, int16.
 *   - the onchip data type of \b c: half, float.
 * - This function supports the combinations of the following data types for inpute tensor \b a, \b b and
 *   output tensor \b c on MLU300 series or above.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c : half, half, half, half, half, half.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c : half, half, half, half, half, float.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c : float, float, float, float, float, float.
 * - The combinations of the data types should satisfy the following rules:
 *   - If \b a offchip data type is integer type, it should be the same as \b a onchip data type.
 *   - The bitwidth of \b a onchip data type must be greater than or equal to the bitwidth of \b b onchip data type.
 *   - If \b a offchip data type and \b c offchip data type are both integer types, they should be the same integer type.
 *   - The \b c offchip data type should be equal to \b a onchip data type or \b c onchip data type.
 *   - The floating point data types of \b a \b b and \b c under the same combination need to be consistent.
 *   - When the offchip data type of \b c is floating point, the data type of \b bias should be the same
 *     with the offchip data type of \b c, otherwise the data type of \b bias should be the same with the
 *     onchip data type of \b c.
 *
 * @note
 * - The function supports adding bias to matrix mulplication on all platforms. You can
 *   set the pointer of \b bias in the \b matmul_desc with the interface :: cnnlSetMatMulDescAttr function.
 * - The combinations of the data types should satisfy the following extra rules on CE3226 platform:
 *   - The onchip data type of \b a and \b b must be fixed-point type.
 *   - The \b b data type must be int8 if \b a data type is int8.
 *   - The data type bitwidth of \b c onchip data type for operation computing is not shorter than \b c
 *     offchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   1. The \b a and \b b must be two dimensions.
 *   2. The number of columns in matrix \b a must be equal to the number of rows in matrix \b b after both inputs
 *   perform transpose operations according to parameters.
 *   3. The number of \b bias must be equal to the number of columns in matrix \b b after both inputs perform transpose
 *   operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a should not be transposed and matrix \b b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
     @endverbatim
 * - CNNL_MATMUL_DESC_TRANSA, CNNL_MATMUL_DESC_TRANSB attributes of \b matmul_desc can be set by
 *   ::cnnlSetMatMulDescAttr function.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=matmul#torch.matmul
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulInference(cnnlHandle_t handle,
                                              cnnlMatMulDescriptor_t matmul_desc,
                                              const void *alpha,
                                              cnnlTensorDescriptor_t a_desc,
                                              const void *a,
                                              cnnlTensorDescriptor_t b_desc,
                                              const void *b,
                                              const void *beta,
                                              cnnlTensorDescriptor_t c_desc,
                                              void *c,
                                              cnnlMatMulAlgo_t algo,
                                              void *workspace,
                                              size_t workspace_size_in_bytes);

// Group:BatchMatMul
/*!
 * @brief Creates a descriptor pointed by \b bmm_desc for a batch matrix multiplication operation,
 *        and allocates memory for holding the information about the batch matrix multiplication operation.
 *        The information is defined in ::cnnlBatchMatMulDescriptor_t.
 *
 * @param[out] bmm_desc
 *   Output. A host pointer to the batch matrix multiplication descriptor that holds information about the
 *   batch matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetBatchMatMulDescAttr function to initialize
 *   and set the information to the batch matrix multiplication descriptor.
 * - You need to call the ::cnnlBatchMatMulDescDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulDescCreate(cnnlBatchMatMulDescriptor_t *bmm_desc);

/*! The descriptor of the fusion bias info EpilogueBias used by cnnlMatMulInferenceDesc_t. This descriptor should be set
 *  with cnnlSetMatmulInferenceDesc infertface.You need to call the ::cnnlCreateEpilogueBias function to
 *  create a descriptor, and call the ::cnnlSetEpilogueBias function to set the information to the descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the
 *  ::cnnlDestroyEpilogueBias function.
 */
typedef struct EpilogueBias *cnnlEpilogueBias_t;

// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by fusion bias info \b bias_epilogue used by cnnlMatMulInferenceDesc_t,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlEpilogueBias_t.
 *
 * @param[out] bias_epilogue
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation with bias.
 *
 * @param[out] size
 *   Output. A host pointer to the size of struct of matrix multiplication inference fused operation with bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetEpilogueBias function to initialize
 *   and set the information to the matrix multiplication inference fused operation with bias.
 * - You need to call the ::cnnlDestroyEpilogueBias function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlCreateEpilogueBias(cnnlEpilogueBias_t *bias_epilogue, size_t *size);

// Group:MatMulInference
/*!
 * @brief Destroys a fusion bias info descriptor \b bias_epilogue that is previously created with the
 *  ::cnnlCreateEpilogueBias.
 *
 * @param[in] bias_epilogue
 *   Input. The descriptor of matrix multiplication inference fused operation information to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyEpilogueBias(cnnlEpilogueBias_t bias_epilogue);

// Group:MatMulInference
/*!
 * @brief Initializes the fusion bias info descriptor \b bias_epilogue that is previously created with the
 *  ::cnnlCreateEpilogueBias function, and sets the information of descriptor \b bias_epilogue.
 *  The information includes the shape of bias, and the device pointer to the bias value \b bias_ptr.
 *
 * @param[in] bias_epilogue
 *   Input. The descriptor of the matrix multiplication inference fused operation with bias. For detailed
 *   information, see ::cnnlEpilogueBias_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] bias_ptr
 *   Output. Pointer to the MLU memory that stores the bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently 4 types of bias are supported. If the matrix multiplication output shape is M * N,
     the supported shapes of bias are as follows:
 *   - 1 * 1
 *   - 1 * N
 *   - M * N
 *   - M * 1
 * - The \b dim of \b bias_desc should be 1 or 2. The bias is scalar when \b dim = 1, and bias is vector or
 *   matrix when \b dim = 2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */


cnnlStatus_t CNNL_WIN_API cnnlSetEpilogueBias(cnnlEpilogueBias_t bias_epilogue,
                                              cnnlTensorDescriptor_t bias_desc,
                                              void *bias_ptr);

/*! The descriptor of the matrix multiplication inference that holds the fused operation information.
 *
 *  You need to call the ::cnnlCreateMatMulInferenceDesc function to create a descriptor, and call the
 *  ::cnnlSetMatmulInferenceDesc function to set the information to the descriptor. Also, you need to
 *  destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulInferenceDesc function.
 */
typedef struct cnnlMatMulInferenceDesc *cnnlMatMulInferenceDesc_t;

/*! The descriptor of the matrix multiplication inference that holds the configured matrix multiplication
 *  algorithm descriptor and its runtime properties.
 *
 *  You need to call the ::cnnlCreateMatMulInferHeuristicResult function to create a descriptor.
 *  Also, you need to destroy the Cambricon CNNL context at the end with the ::cnnlDestroyMatMulInferHeuristicResult function.
 */
typedef struct cnnlMatMulInferHeuristicResult *cnnlMatMulInferHeuristicResult_t;

/*! The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulInferHeuristicResult_t
 *  configuration.
 */
typedef struct cnnlMatMulInferPrefer *cnnlMatMulInferPrefer_t;

/*!
 * @brief Creates a descriptor pointed by \b algo for a matrix multiplication inference fused operation,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlMatMulInferenceAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the struct of matrix multiplication inference algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulInferenceAlgo function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t cnnlCreateMatMulInferenceAlgo(cnnlMatMulInferenceAlgo_t *algo);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference algorithm with descriptor \b algo,
 *        that is previously created with the ::cnnlCreateMatMulInferenceAlgo.
 *
 * @param[in] algo
 *   Input. The matrix multiplication inference algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlDestroyMatMulInferenceAlgo(cnnlMatMulInferenceAlgo_t algo);

/*!
 * @brief Creates a descriptor pointed by \b result for a matrix multiplication inference heuristic result,
 *        and allocates memory for the result. The result is defined in ::cnnlMatMulInferHeuristicResult_t.
 *
 * @param[out] result
 *   Output. A host pointer to the struct of matrix multiplication inference heuristic result.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - You need to call the ::cnnlDestroyMatMulInferHeuristicResult function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlCreateMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t *result);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference heuristic result,
 *        that is previously created with the ::cnnlCreateMatMulInferenceDesc.
 *
 * @param[in] result
 *   Input. The matrix multiplication inference heuristic result to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlDestroyMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t result);

// Group:MatMulInference
/*!
 * @brief Gets matrix multiplication inference algorithm and workspace size from heuristic result
 *  that is previously selected with ::cnnlMatMulInferGetAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication inference heuristic result created with ::cnnlCreateMatMulInferHeuristicResult.

 * @param[in] algo
 *   Output. The matrix multiplication inference algorithm.

 * @param[in] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication inference operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetMatMulInferHeuristicResult(cnnlMatMulInferHeuristicResult_t result,
                                          cnnlMatMulInferenceAlgo_t algo,
                                          size_t *workspace_size);

// Group:MatMulInference
/*!
 * @brief Sets the \b inference_desc with fusion bias information. The information includes the shape of bias,
 *  the data type of bias, and the device pointer to the bias value \b bias_ptr.
 *
 * @param[out] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation.
 *
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] bias_ptr
 *   Output. Pointer to the MLU memory that stores the bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently 4 types of bias are supported. If the matrix multiplication output shape is M * N,
     the supported shapes of bias are as follows:
 *   - 1 * 1
 *   - 1 * N
 *   - M * N
 *   - M * 1
 * - The \b dim of \b bias_desc should be 1 or 2. The bias is scalar when \b dim = 1, and bias is vector or
 *   matrix when \b dim = 2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlSetMatMulEpilogueBias(cnnlMatMulInferenceDesc_t inference_desc,
                                       cnnlTensorDescriptor_t bias_desc,
                                       void *bias_ptr);

// Group:MatMulInference
/*!
 * @brief Sets the \b inference_desc with fused operations in following order:
 *  bias, scale, batch normalization and activation with following formula:
 *
 *  bn_alpha * ((scale_alpha * (matmul + bias) + scale_beta) - bn_mean) / rsqrt(bn_var + epsilon)) + bn_beta
 *
 *  Batch normalization operation will be supported in the further release.
 * @param[out] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor to be set. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] scale_alpha
 *   Input. Pointer to the MLU memory that stores the scale_alpha scalar tensor.
 * @param[in] scale_beta
 *   Input. Pointer to the MLU memory that stores the scale_beta tensor.
 * @param[in] bn_mean
 *   Input. Pointer to the MLU memory that stores the bn_mean tensor. Currently not supported and should be set to NULL.
 * @param[in] bn_var
 *   Input. Pointer to the MLU memory that stores the bn_var tensor. Currently not supported and should be set to NULL.
 * @param[in] bn_filter
 *   Input. Pointer to the MLU memory that stores the bn_filter tensor. Currently not supported and should be set to NULL.
 * @param[in] bn_beta
 *   Input. Pointer to the MLU memory that stores the bn_beta tensor. Currently not supported and should be set to NULL.
 * @param[in] epsilon
 *   Input. A float value added to the denominator for numerical stability, used in batch norm operation. Currently not
 *   supported and should be set to 0.
 * @param[in] scale_alpha_num
 *   Input. The number of scale_alpha tensor elements, used in scale operation.
 * @param[in] scale_beta_num
 *   Input. The number of scale_beta tensor elements, used in scale operation.
 * @param[in] bn_filter_num
 *   Input. The number of bn_filter tensor elements, used in batch norm operation. Currently not supported and
 *   should be set to NULL.
 * @param[in] bn_beta_num
 *   Input. The number of bn_beta tensor elements, used in batch norm operation. Currently not supported and should be set to NULL.
 * @param[in] active_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently only supports fused operation of combination of matmul, bias, scale, and activation.
 *   BN operation will be supported in the further release.
 * - The activation mode only supports relu.
 * - If the matrix multiplication output shape is M * N, the supported shape of bias is 1 * N. The length of
 *   scale_alpha_num and scale_beta_num should be 1 or N if has scale operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t cnnlSetEpilogueBiasScaleBNActive(cnnlMatMulInferenceDesc_t inference_desc,
                                              cnnlTensorDescriptor_t bias_desc,
                                              const void *bias,
                                              const void *scale_alpha,
                                              const void *scale_beta,
                                              const void *bn_mean,
                                              const void *bn_var,
                                              const void *bn_filter,
                                              const void *bn_beta,
                                              const float epsilon,
                                              const int scale_alpha_num,
                                              const int scale_beta_num,
                                              const int bn_filter_num,
                                              const int bn_beta_num,
                                              cnnlActivationDescriptor_t active_desc);
// Group:MatMulInference
/*!
 * @brief Retrieves the possible algorithms can be used in the matrix multiplication inference.
 *  The output is placed in resultArray[] in the order of increasing estimated compute time.
 *
 * @param[in] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused descriptor.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. Used in out-of-place matrix multiplication.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for ::cnnlMatMulInferHeuristicResult_t
 *   configuration. Currently not supported and should be set to NULL.
 * @param[in] requestedAlgoCount
 *   Input. The size of requested algorithm. This is the requested maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[in] resultArray
 *   Input. Array containing the algorithm heuristics and associated runtime characteristic, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[in] returnAlgoCount
 *   Input. The number of algorithms returned by this function.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlMatMulInferGetAlgoHeuristic(cnnlHandle_t handle,
                                             cnnlMatMulInferenceDesc_t inference_desc,
                                             cnnlTensorDescriptor_t a_desc,
                                             cnnlTensorDescriptor_t b_desc,
                                             cnnlTensorDescriptor_t c_desc,
                                             cnnlTensorDescriptor_t d_desc,
                                             cnnlMatMulInferPrefer_t preference,
                                             int requestedAlgoCount,
                                             cnnlMatMulInferHeuristicResult_t resultArray[],
                                             int *returnAlgoCount);

// Group:MatMulInference
/*!
 * @brief Creates a descriptor pointed by \b inference_desc for a matrix multiplication inference fused operation,
 *        and allocates memory for holding the information about the operation. The information is defined in
 *        ::cnnlMatMulInferenceDesc_t.
 *
 * @param[out] inference_desc
 *   Output. A host pointer to the struct of matrix multiplication inference fused operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMatmulInferenceDesc function to initialize
 *   and set the information to the matrix multiplication inference fused operation.
 * - You need to call the ::cnnlDestroyMatMulInferenceDesc function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMatMulInferenceDesc(cnnlMatMulInferenceDesc_t *inference_desc);

// Group:MatMulInference
/*!
 * @brief Destroys a matrix multiplication inference fused operation with descriptor \b inference_desc,
 *        that is previously created with the ::cnnlCreateMatMulInferenceDesc.
 *
 * @param[in] inference_desc
 *   Input. The matrix multiplication inference fused operation descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMatMulInferenceDesc(cnnlMatMulInferenceDesc_t inference_desc);

// Group:MatMulInference
/*!
 * @brief Initializes the matrix multiplication inference descriptor \b inference_desc
 * that is previously created with the ::cnnlCreateMatMulInferenceDesc function, and sets
 * the information about the matrix multiplication inference operation to the matrix multiplication
 *  inference descriptor \b inference_desc. The information includes the attribute defined in
 * ::cnnlMatMulDescAttribute_t \b attr, the host pointer to the attribute value \b buf, and
 * the size of buffer for verification.
 *
 * @param[in] inference_desc
 *   Input. The descriptor of the matrix multiplication inference operation. For detailed
 *   information, see ::cnnlMatMulInferenceDesc_t.
 * @param[in] attr
 *   Input. Attribute of matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You should set CNNL_MATMUL_DESC_EPILOGUE_TYPE attribute before set CNNL_MATMUL_DESC_EPILOGUE_OPERAND
 *   attribute.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMatmulInferenceDesc(cnnlMatMulInferenceDesc_t inference_desc,
                                                     cnnlMatMulDescAttribute_t attr,
                                                     const void *buf,
                                                     size_t size_in_bytes);
// Group:MatMulInference
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the matrix multiplication inference operation.
 *
 * The size of extra workspace is based on the given information of the matrix multiplication inference
 * operation, including the matrix multiplication inference descriptor \b matmul_desc, input tensor
 * descriptor of left matrix \b a_desc, input tensor descriptor of right matrix \b b_desc, output
 * tensor  descriptor \b c_desc, and the matrix multiplication algorithm \b algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inference_desc
 *   Input. The descriptor of the matrix multiplication inference operations. For detail information,
 *   see ::cnnlMatMulInferenceDesc_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlSetMatmulInferenceDesc function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlMatMulInference_v2 function to
 *   perform the matrix multiplication inference operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetMatMulInferenceV2WorkspaceSize(cnnlHandle_t handle,
                                      cnnlMatMulInferenceDesc_t inference_desc,
                                      cnnlTensorDescriptor_t a_desc,
                                      cnnlTensorDescriptor_t b_desc,
                                      cnnlTensorDescriptor_t c_desc,
                                      cnnlTensorDescriptor_t d_desc,
                                      cnnlMatMulInferenceAlgo_t algo,
                                      size_t *workspace_size);
// Group:MatMulInference
/*!
 * @brief Compute the matrix mulplication inference operation. Supports to convert the data type of
 * \b a or \b b to integer when the offchip type of \b a or \b b is float. Also supports different
 * fused operation after matrix mulplication.
 *
 * Compared with ::cnnlMatMulInference, it supports 4 types of bias.
 *
 * This function needs extra MLU memory as the workspace to improve the matrix multiplication
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetMatMulInferenceV2WorkspaceSize function.

 * The matrix mulplication algorithm \b algo is not supported currently.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   matrix multiplication inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] inference_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulInferenceDesc_t.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of alpha to 1.0 now.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of beta to 0.0 now.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor of the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor of the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor \b c, or to the MLU memory that stores the
     bias \b c tensor when c = alpha * a * b + beta * c. Used in in-place matrix multiplication.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. Used in out-of-place matrix multiplication.
     Currently not support and should be set to NULL.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output \b d when d = alpha * a * b + beta * c.
 *   Used in out-of-place matrix multiplication. Currently not supported and should be set to NULL.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication. Currently not
 *   supported.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetMatMulInferenceV2WorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c on all hardware platforms.
 *   - the offchip data type of \b a: half, float, int8, int16.
 *   - the onchip data type of \b a: int8, int16.
 *   - the offchip data type of \b b: half, float, int8, int16.
 *   - the onchip data type of \b b: int8, int16.
 *   - the offchip data type of \b c: half, float.
 *   - the onchip data type of \b c: half, float.
 *   - the onchip data type of \b d: half, float.
 * - This function supports the combinations of the following data types for inpute tensor \b a, \b b and
 *   output tensor \b c on MLU300 series or above.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c: half, half, half, half, half, half.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c: half, half, half, half, half, float.
 *   - the offchip data type and onchip data type of \b a, \b b, \b c: float, float, float, float, float, float.
 * - The combinations of the data types should satisfy the following rules:
 *   - If \b a offchip data type is integer type, it should be the same as \b a onchip data type.
 *   - The floating point data types of \b a \b b and \b c under the same combination need to be consistent.
 *   - The data type of \b bias should be the same with the offchip data type of \b c.
 *
 * @note
 * - The function supports adding bias to matrix mulplication on all platforms. You can
 *   set the pointer of \b bias in the \b inference_desc with the interface :: cnnlSetMatMulDescAttr function.
 * - The combinations of the data types should satisfy the following rules on CE3226 platform:
 *   - The onchip data type of \b a and \b b must be fixed-point type.
 *   - The \b b data type must be int8 if \b a data type is int8.
 *   - The data type bitwidth of \b c onchip data type for operation computing is not shorter than \b c
 *     offchip data type.
 * - When the fuse type is not ::CNNL_MATMUL_EPI_BIAS, the combinations of the data types should
 *   satisfy the following rules:
 *   - The \b a offchip data type should be the same with \b c onchip data type when \b a offchip data type is
 *     floating point type on MLU200 series.
 *   - The \b c offchip data type should be same with \b c onchip data type when \b c offchip data type is
 *     floating point type on MLU200 series.
 * - The function supports fused operation after matrix mulplication. The fused information should be set with
 *   cnnlMatMulInferenceDesc_t.
 * - Currently this function not supports zero element input.
 *
 * @par Scale Limitation
 * - The \b a and \b b must be  two-dimensional.
 *
 * @par API Dependency
 * - Before calling this function to implement matrix multiplication inference operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a should not be transposed and matrix \b b
 *   should be transposed.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor a:  [99, 128]
      Dimension of input tensor b:  [128, 256]
      Dimension of output tensor c: [99, 256]
      Dimension of bias tensor: [1, 256]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlMatMulInference_v2(cnnlHandle_t handle,
                                                 cnnlMatMulInferenceDesc_t inference_desc,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t a_desc,
                                                 const void *a,
                                                 const cnnlTensorDescriptor_t b_desc,
                                                 const void *b,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t c_desc,
                                                 void *c,
                                                 const cnnlTensorDescriptor_t d_desc,
                                                 void *d,
                                                 cnnlMatMulInferenceAlgo_t algo,
                                                 void *workspace,
                                                 size_t workspace_size);

// Group:BatchMatMul
/*!
 * @brief Destroys a batch matrix multiplication descriptor \b bmm_desc
 *        that is previously created with the ::cnnlBatchMatMulDescCreate.
 *
 * The batch matrix multiplication descriptor is defined in ::cnnlBatchMatMulDescriptor_t
 * and holds the information about the batch matrix multiplication operation.
 *
 * @param[in] bmm_desc
 *   Input. The batch matrix multiplication descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulDescDestroy(cnnlBatchMatMulDescriptor_t bmm_desc);

// Group:BatchMatMul
/*!
 * @brief Initializes the batch matrix multiplication descriptor \b bmm_desc
 * that is previously created with the ::cnnlBatchMatMulDescCreate function, and sets
 * the information about the batch matrix multiplication operation to the batch matrix multiplication
 * descriptor \b bmm_desc. The information includes the attribute defined in
 * ::cnnlBatchMatMulDescAttribute_t \b attr, the host pointer to the attribute value \b buf, and
 * the size of buffer for verification.
 *
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operation. For detailed
 *   information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication descriptor to be set. For detailed
 *   information, see ::cnnlBatchMatMulDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlSetBatchMatMulDescAttr(cnnlBatchMatMulDescriptor_t bmm_desc,
                                                     cnnlBatchMatMulDescAttribute_t attr,
                                                     const void *buf,
                                                     size_t size_in_bytes);

// Group:BatchMatMul
/*!
 * @brief Returns the pointer to the \b buf and size of the buffer \b size_written of the attribute
 * retrieved with the given batch matmul multiplication descriptor \b matmul_desc, attribute \b attr.
 * And \b size_in_bytes is used to check if the memory size is same with \b size_written.
 *
 * You can set the attribute in the batch matrix multiplication descriptor based on the return value
 * of this function.
 *
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operation. For detailed
 *   information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 * @param[out] size_written
 *   Output. The number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatchMatMulDescAttr(const cnnlBatchMatMulDescriptor_t bmm_desc,
                                                     cnnlBatchMatMulDescAttribute_t attr,
                                                     void *buf,
                                                     size_t size_in_bytes,
                                                     size_t *size_written);

// Group:BatchMatMul
/*!
 * @brief Creates a descriptor pointed by \b algo for a batch matrix multiplication algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlBatchMatMulAlgo_t.
 *
 * @param[out] algo
 *   Output. A host pointer to the batch matrix multiplication algorithm that holds information about
 *   the batch matrix multiplication algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeBatchMatMulAlgorithm function to initialize
 *   and set the information to the batch matrix multiplication algorithm.
 * - You need to call the ::cnnlBatchMatMulAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulAlgoCreate(cnnlBatchMatMulAlgo_t *algo);

// Group:BatchMatMul
/*!
 * @brief Destroys a batch matrix multiplication algorithm descriptor \b algo
 *        that is previously created with the ::cnnlBatchMatMulAlgoCreate.
 *
 * The batch matrix multiplication descriptor is defined in ::cnnlBatchMatMulAlgo_t
 * and holds the information about the batch matrix multiplication algorithm.
 *
 * @param[in] algo
 *   Input. The batch matrix multiplication algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulAlgoDestroy(cnnlBatchMatMulAlgo_t algo);

// Group:BatchMatMul
/*!
 * @brief Returns the most suited batch matrix multiplication algorithm that can be used
 * in the operation.
 *
 * The returned batch matrix multiplication is chosen from all the supported batch matrix
 * algorithms defined in ::cnnlBatchMatMulAlgo_t and is based on the given batch matrix
 * multiplication descriptor \b bmm_desc, tensor descriptor of left matrix \b a_desc, tensor
 * descriptor of right matrix \b b_desc, tensor descriptor of output matrix \b c_desc, and batch
 * matrix multiplication algorithm \b preference.
 *
 * The computing performance options \b preference defined in the ::cnnlBatchMatMulPreference_t
 * enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *  Input. The descriptor of the batch matrix multiplication operation. For detailed
 *  information, see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the batch matrix multiplication operation to
 * get better performance. This parameter only supports CNNL_BMM_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the batch matrix
 *   multiplication. The algorithms are defined in the ::cnnlBatchMatMulAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulAlgorithm(cnnlHandle_t handle,
                                    const cnnlBatchMatMulDescriptor_t bmm_desc,
                                    const cnnlTensorDescriptor_t a_desc,
                                    const cnnlTensorDescriptor_t b_desc,
                                    const cnnlTensorDescriptor_t c_desc,
                                    cnnlBatchMatMulPreference_t preference,
                                    cnnlBatchMatMulAlgo_t *algo);

// Group:BatchMatMul
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication operation.
 *
 * The size of extra workspace is based on the given information of the batch matrix multiplication
 * operation, including the batch matrix multiplication descriptor \b bmm_desc, input tensor
 * descriptor of left matrix \b a_desc, input tensor descriptor of right matrix \b b_desc, output
 * tensor descriptor \b c_desc, and the batch matrix multiplication algorithm \b algo. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operations. For detail information,
 *   see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication. The algorithms are defined
 *   in the ::cnnlBatchMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeBatchMatMulAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeBatchMatMul function to
 *   performs the batch matrix multiplication operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulWorkspaceSize(cnnlHandle_t handle,
                                        const cnnlBatchMatMulDescriptor_t bmm_desc,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const cnnlTensorDescriptor_t b_desc,
                                        const cnnlTensorDescriptor_t c_desc,
                                        cnnlBatchMatMulAlgo_t algo,
                                        size_t *workspace_size);

// Group:BatchMatMul
 /*!
 * @brief Quantizes data type of input tensor \b a and \b b, and computes the batch matrices
 * multiplication operation, then returns the results in the output tensor \b c.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the batch matrix multiplication
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetQuantizeBatchMatMulWorkspaceSize function. The batch matrix multiplication is computed based
 * on the batch matrix multiplication algorithm set in \b algo. You can call the
 * ::cnnlGetQuantizeBatchMatMulAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_desc
 *   Input. The descriptor of the batch matrix multiplication operations. For detail information,
 *   see ::cnnlBatchMatMulDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \b a quantization parameter \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \b b quantization parameter \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication. The algorithms are defined
 *   in the ::cnnlBatchMatMulAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   operation. You can get the size of the workspace with the ::cnnlGetQuantizeBatchMatMulWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c.
 *   - \b a offchip data type: half, float.
 *   - \b a onchip data type: int8, int16, int31.
 *   - \b b offchip data type: half, float.
 *   - \b b onchip data type: int8, int16, int31.
 *   - \b c offchip data type: half, float.
 *   - The data type for operation computing: half, float.
 * - \b a offchip data type should be the same as \b b offchip data type.
 * - \b a offchip data type can be combined with any onchip data type.
 * - \b b offchip data type can be combined with any onchip data type.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for operation computing is not shorter than \b c offchip data type.
 *   - The data type for operation computing \p must be float when onchip data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Scale Limitation
 * - The inputs \b a and \b b are multi-dimensional array, the shape must be no less than
 *   two dimensions and no more than \p CNNL_DIM_MAX dimensions.
 * - The last two dimensions of the \b a and \b b must be the number of rows and the number
 *   columns for matrix multiplication respectively.
 * - The number of columns of \b a matrix must be equal to the number of rows of \b b matrix
 *   after both inputs have performed the transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 * - If \b a and \b b are two-dimensional tensors, for best practices, it is recommended to call
 *   ::cnnlQuantizeMatMul.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor \b a:  [64, 99, 128]
      Dimension of input tensor \b b:  [64, 128, 256]
      Dimension of output tensor \b c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantizeBatchMatMul(cnnlHandle_t handle,
                                                  const cnnlBatchMatMulDescriptor_t bmm_desc,
                                                  const void *alpha,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const void *a,
                                                  const void *a_position,
                                                  const void *a_scale,
                                                  const void *a_offset,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const void *b,
                                                  const void *b_position,
                                                  const void *b_scale,
                                                  const void *b_offset,
                                                  const void *beta,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  void *c,
                                                  cnnlBatchMatMulAlgo_t algo,
                                                  void *workspace,
                                                  size_t workspace_size_in_bytes);

// Group:StridedSlice
/*!
 * @brief Slices on input tensor \b input with the begin \b begin, end \b end,
 * stride \b stride, and returns the results in the output tensor \b output.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the strided_slice operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  begin
 *   Input. An array that stores the starting position of each dimension of input to be sliced.
 * @param[in]  end
 *   Input. An array that stores the ending position of each dimension of input to be sliced.
 * @param[in]  stride
 *   Input. An array that stores the stride of each dimension of input to be sliced.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par Formula
 * - See "StridedSlice Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports data types of input and output tensors as follow, and must be the same between input
 * and output.
 *   - input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double, complex_half, complex_float.
 *   - output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double, complex_half, complex_float.
 *
 * @par Scale Limitation
 * - The input parameters should meet
 *   the following requirements (dim0 means the first dimension of input, dim0_stride
 *   means stride of dim0, dim0_input means the value of dim0 dimension of input,
 *   dim0_begin is means beginning index of dim0,dim0_end means ending index of dim0):
 *   - dim0_stride * dim1_stride * dim2_stride * dim3_stride != 0.
 *   - When dim0_stride > 0, (dim0_begin >= 0 && dim0_end > dim0_begin
 *   && dim0_input >= dim0_end) should be true.
 *   - When dim0_stride < 0, (dim0_end + 1 >= (-1) * dim0_input &&
 *   dim0_begin > dim0_end && dim0_begin <= -1) should be true.
 *   - The requirement of dim1~dim7 is same with dim0.
 *   - dim0_stride must be greater than -48.
 *
 * @par Example
 * - The example of strided_slice operation is as follows:
    @verbatim
    input: an array by 4*7 -->   [[1,2,3,4,5,6,7],
                                 [8,9,10,11,12,13,14],
                                 [15,16,17,18,19,20,21],
                                 [22,23,24,25,26,27,28]]

    begin:  an array by 1*2  --> [0,0]

    end:    an array by 1*2  --> [4,7]

    stride: an array by 1*2  --> [2,3]

    Then we will get the output:

    output: an array by 2*3 --> [[1,4,7],
                                 [15,18,21]]
    @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/strided_slice
 * - https://github.com/tensorflow/tfjs/blob/master/tfjs-core/src/ops/strided_slice.ts
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlStridedSlice(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const int begin[],
                                           const int end[],
                                           const int stride[],
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:LogicOp
/*!
 * @brief Computes the element-wise truth value (true or false) based on
 *        the logical relationship \b logic_op between two input tensors \b a
 *        and \b b, and returns the results in the output tensor \b c.
 *
 * Logic operations are wildly used in artificial intelligence as a kind of basic
 * mathematical operations. Also, they are supported in almost all common
 * frameworks, like PyTorch and TensorFlow.
 *
 * This function supports partial in-place operation, which means the first input
 * tensor \b a and the output tensor \b c can be the same one. This function also
 * supports tensor broadcasting as long as \b a, \b b, and \b c satisfy the
 * broadcast conditions. For more details about tensor broadcasting, see
 * Limitations section.
 *
 * Specially, the two input tensors \b a and \b b should be the same one when
 * the logic operation is ::CNNL_LOGIC_OP_NOT.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the logic operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] logic_op
 *   Input. The specific logic operation performed in the function. The operations
 *   are defined in the ::cnnlLogicOp_t enum.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   logic operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the logic operation. You can get the size of the workspace with
 *   the ::cnnlGetLogicOpWorkspaceSize function.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: bool, uint8, int8, int16, int32, half, float
 *   - output tensor: bool, uint8, int8, int16, int32, half, float
 *   <b>Note that the data type of the output tensor must be bool, uint8, int8,
 *   or the same as that of the input tensor.</b>
 *
 * @par Limitations
 * - For each dimension of the two input tensors, the length of the dimension
 *   should be the same or one of them equal to 1.
 * - Each dimension of the output tensor should equal to the larger one between
 *   corresponding dimensions of two input tensors.
 *
 * @par API Dependency
 * - Before calling this function to perform logic operation, you need to get the
 *   size of workspace by the ::cnnlGetLogicOpWorkspaceSize function.
 *
 * @par Performance Optimization
 * - To get better performance, set the data type of the output tensor to one bool,
 *   uint8, or int8.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc and c_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - When the logic operation is ::CNNL_LOGIC_OP_NE and input data contains NaN:
 *   - On MLU200 series, MLU300 series and CE3226:
 *     If \b a or \b b is NaN, then \b c is false or 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLogicOp(cnnlHandle_t handle,
                                      const cnnlLogicOp_t logic_op,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c);

// Group:LogicOp
/*!
 * @brief Returns in \b size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the logic operation.
 *
 * The size of extra workspace is based on the given information of the input
 * and output tensor descriptors, \b a_desc, \b b_desc, and \b c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the logic operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the logic operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLogicOp function
 *   to perform the logic operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLogicOpWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t a_desc,
                                                      const cnnlTensorDescriptor_t b_desc,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);

// Group:SoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes the softmax cross entropy loss and back propagation gradients between input
 *        tensor \b x and \b p based on the given \b mode defined in ::cnnlSoftmaxMode_t,
 *        where \b x is the features and \b p is the label, and returns the results in the
 *        output tensors \b y and \b diff_y.
 *
 * @deprecated
 *   ::cnnlSoftmaxCrossEntropyWithLogits is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlSoftmaxCrossEntropyWithLogits_v2 instead, which needs the enumeration
 *   parameter ::cnnlComputationPreference_t to choose the best suited algorithm used for implementation
 *   of this function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax_corss_entropy_with_logits operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computation procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] x_desc
 *   Input. The descriptor of input feature tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the feature tensor, which is the output of last layer of
 *   AI network.
 * @param[in] p_desc
 *   Input. The descriptor of input label tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p
 *   Input. Pointer to the MLU memory that stores the input label tensor, which is a valid
 *   probability distribution in reduction dimension.
 * @param[in] y_desc
 *   Input. The descriptor of output loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output loss tensor, which is the loss of per example.
 * @param[in] diff_y_desc
 *   Input. The descriptor of output back propagation gradients tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the output back propagation gradients tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of inputs and outputs must be the same.
 * - The supported data type of inputs and outputs are as follows:
 *   - feature tensor: half, float.
 *   - label tensor: half, float.
 *   - loss tensor: half, float.
 *   - backpropagation tensor: half, float.
 *
 * @par Scale Limitation
 * - When the data type is half, the sum of exp(xk - x_max) should be in the range of [-65504.0, 65504.0],
 *   where xk and x_max mean each element and the maximum value in the reduction dimension of the input tensor
 *   respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, reshape the reduction dimension to the lowest dimension and set
 *   the \b mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - Only supports 3-dimensional input tensor and label tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softmax_cross_entropy_with_logits operation is as follows:
     @verbatim
     input two array by 1 * 1 * 4
     --> feature: [[[1, 2, 3, 4]]]

     --> label: [[[0.2, 0.4, 0.3, 0.1]]]

     \b mode: \p CNNL_SOFTMAX_MODE_LOW_DIMENSION

     output two array by 1 * 1 * 1 and 1 * 1 * 4
     --> loss: [[[2.14019]]]

     --> backprop: [[[-0.167914, -0.312856, -0.0631172, 0.543914]]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/softmax_cross_entropy_with_logits
 * - http://www.tensorflow.org/api_doc/cc/class/tensorflow/ops/softmax_cross_entropy_with_logits
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSoftmaxCrossEntropyWithLogits(cnnlHandle_t handle,
                                  cnnlSoftmaxMode_t mode,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t p_desc,
                                  const void *p,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y,
                                  const cnnlTensorDescriptor_t diff_y_desc,
                                  void *diff_y);

// Group:SoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes the softmax cross entropy loss and back propagation gradients between input
 *        tensor \b x and \b p based on the given \b mode defined in ::cnnlSoftmaxMode_t,
 *        where \b x is the features and \b p is the label, and returns the results in the
 *        output tensors \b y and \b diff_y. The difference between this function and
 *        ::cnnlSoftmaxCrossEntropyWithLogits is that this function needs the enumeration
 *        parameter ::cnnlComputationPreference_t to choose the best suited algorithm used for
 *        implementation of this function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softmax_corss_entropy_with_logits operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The reduction dimension in the computation procedure, defined in ::cnnlSoftmaxMode_t.
 * @param[in] prefer
 *  Input. The algorithm used to compute the output. For detailed information,
 *  see ::cnnlComputationPreference_t.
 * @param[in] x_desc
 *   Input. The descriptor of input feature tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the feature tensor, which is the output of last layer of
 *   AI network.
 * @param[in] p_desc
 *   Input. The descriptor of input label tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] p
 *   Input. Pointer to the MLU memory that stores the input label tensor, which is a valid
 *   probability distribution in reduction dimension.
 * @param[in] y_desc
 *   Input. The descriptor of output loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output loss tensor, which is the loss of per example.
 * @param[in] diff_y_desc
 *   Input. The descriptor of output back propagation gradients tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the output back propagation gradients tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of inputs and outputs must be the same.
 * - The supported data type of inputs and outputs are as follows:
 *   - feature tensor: half, float.
 *   - label tensor: half, float.
 *   - loss tensor: half, float.
 *   - backpropagation tensor: half, float.
 *
 * @par Scale Limitation
 * - When the data type is half, the sum of exp(xk - x_max) should be in the range of [-65504.0, 65504.0],
 *   where xk and x_max mean each element and the maximum value in the reduction dimension of the input tensor
 *   respectively.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, reshape the reduction dimension to the lowest dimension and set
 *   the \b mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @par Note
 * - Only supports 3-dimensional input tensor and label tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softmax_cross_entropy_with_logits operation is as follows:
     @verbatim
     input two array by 1 * 1 * 4
     --> feature: [[[1, 2, 3, 4]]]

     --> label: [[[0.2, 0.4, 0.3, 0.1]]]

     \b mode: \p CNNL_SOFTMAX_MODE_LOW_DIMENSION

     output two array by 1 * 1 * 1 and 1 * 1 * 4
     --> loss: [[[2.14019]]]

     --> backprop: [[[-0.167914, -0.312856, -0.0631172, 0.543914]]]
     @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/softmax_cross_entropy_with_logits
 * - http://www.tensorflow.org/api_doc/cc/class/tensorflow/ops/softmax_cross_entropy_with_logits
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSoftmaxCrossEntropyWithLogits_v2(cnnlHandle_t handle,
                                     cnnlSoftmaxMode_t mode,
                                     cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t p_desc,
                                     const void *p,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y,
                                     const cnnlTensorDescriptor_t diff_y_desc,
                                     void *diff_y);

// Group:ListDiff
/*!
 * @brief Returns in \b size_out the size of the MLU memory that is used as an extra workspace
 * to optimize the listdiff operation.
 *
 * The size of extra workspace is determined based on the given information of the listdiff
 * operation, including the number of elements in \b input1 tensor. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] len_input1
 *   Input. The number of elements in \b input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \b input2 tensor.
 * @param[out] size_len
 *   Output. Pointer to the MLU memory that stores the returned size of the extra
 *   workspace in bytes that is used in ::cnnlListDiffGetOutLen.
 * @param[out] size_out
 *   Output. Pointer to the MLU memory that stores the returned size of the extra
 *   workspace in bytes that is used in the listdiff operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Scale Limitation
 * - \b len_input1 and \b len_input2 should be greater than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetListDiffWorkSpace(cnnlHandle_t handle,
                                                   const int len_input1,
                                                   const int len_input2,
                                                   int *size_len,
                                                   int *size_out);
// Group:ListDiff
/*!
 * @brief Computes the numbers of elements in the \b input1 but not in the \b input2
 * with the input data type \b data_type.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_type
 *   Input. Data type of \b input1 and \b input2.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the \b input1 tensor.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the \b input2 tensor.
 * @param[in] len_input1
 *   Input. The number of elements in \b input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \b input2 tensor.
 * @param[out] gsh_len
 *   Output. Pointer to the MLU memory that stores the number of elements in the \b input1
 *   but not in the \b input2 in each MLU core.
 * @param[out] output_len
 *   Output. Pointer to the MLU memory that stores the number of elements in the \b input1
 *   but not in the \b input2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Date types of tensor \b input1 and \b input2 must be the same.
 * - The supported data types of input and output are as follows:
 *   - \b data_type: \p CNNL_DTYPE_INT32, \p CNNL_DTYPE_FLOAT
 *   - \b input1: float, int32
 *   - \b input2: float, int32
 *   - \b len_input1: int32
 *   - \b len_input2: int32
 *   - \b gsh_len: int32
 *   - \b output_len: int32
 *
 * @par Scale Limitation
 * - \b len_input1 and \b len_input2 should be greater than 0.
 * - The number of dimensions of \b input1 and \b input2 must be 1.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetListDiffWorkSpace function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlListDiffGetOutLen(cnnlHandle_t handle,
                                                cnnlDataType_t data_type,
                                                const void *input1,
                                                const void *input2,
                                                const int len_input1,
                                                const int len_input2,
                                                int *gsh_len,
                                                int *output_len);
// Group:ListDiff
/*!
 * @brief Retrieves elements in the \b input1 tensor but not in the \b input2 tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the listdiff operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] data_type
 *   Input. Data type of \b input1, \b input2 and \b output_data.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the \b input1 tensor.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the \b input2 tensor.
 * @param[in] len_input1
 *   Input. The number of elements in \b input1 tensor.
 * @param[in] len_input2
 *   Input. The number of elements in \b input2 tensor.
 * @param[out] gsh_out
 *   Output. Pointer to the MLU memory that stores the number of elements which is
 *   in the \b input1 tensor but not in the \b input2 tensor.
 * @param[out] output_data
 *   Output. Pointer to the MLU memory that stores the output tensor \b output_data.
 * @param[out] output_index
 *   Output. Pointer to the MLU memory that stores the index of input1 that corresponds
 *   to each elements of \b output_data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Date types of tensor \b input1, \b input2 and \b output_data must be the same.
 * - The supported data types of input and output are as follows:
 *   - \b data_type: \p CNNL_DTYPE_INT32, \p CNNL_DTYPE_FLOAT
 *   - \b input1: float, int32
 *   - \b input2: float, int32
 *   - \b len_input1: int32
 *   - \b len_input2: int32
 *   - \b gsh_out: int32
 *   - \b output_data: float, int32
 *   - \b output_index: int32
 *
 * @par Scale Limitation
 * - \b len_input1 and \b len_input2 should be greater than 0.
 * - \b output_data can not be NULL.
 * - \b output_index can not be NULL.
 * - The number of dimensions of \b input1, \b input2, \b output_data and \b output_index must be 1.
 *
 * @par API Dependency
 * - You need to call the ::cnnlListDiffGetOutLen function and the ::cnnlGetListDiffWorkSpace
 *   function before calling this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the listdiff operation is as follows:
     @verbatim
       \b data_type: \p CNNL_DTYPE_INT32
       \b input1 array: [0, 9, 11, 4, 9, 1, 3, 5, 5, 10, 13, 8, 2, 2, 4]
       \b input2 array: [0, 9, 11, 4, 9, 1, 3, 5, 5, 10]
       \b len_input1: 15
       \b len_input2: 10
       \b gsh_out: 4
       \b output_data array: [13, 8, 2, 2]
       \b output_index array: [10, 11, 12, 13]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlListDiff(cnnlHandle_t handle,
                                       cnnlDataType_t data_type,
                                       const void *input1,
                                       const void *input2,
                                       const int len_input1,
                                       const int len_input2,
                                       int *gsh_out,
                                       void *output_data,
                                       int *output_index);

/******************************************************************************
 * Cambricon CNNL OP: InvertPermutation
 ******************************************************************************/

// Group:InvertPermutation
/*!
 * @brief Computes the inverse permutation of input tensor \b input, and returns
 *        the results in the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the invert_permutation operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The description of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "InvertPermutation Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data type of the input tensor and output tensor are as follows:
 *   - input tensor: int32.
 *   - output tensor: int32.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor must meet the following requirements:
 *   - The value 0 must be included.
 *   - No duplicate and negative values.
 *
 * @par API Dependency
 * - Before calling this function to implement invert_permutation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - The dimension of input tensor must be 1-D.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the invert_permutation operation is as follows:
 *   @verbatim
 *    input array by 5 --> input: [3, 4, 0, 2, 1]
 *    output array by 5 --> output: [2, 4, 3, 0, 1]
 *   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlInvertPermutation(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                void *output);
// Group:Unique
/*!
 * @brief Creates a descriptor pointer by \b unique_desc for a unique operator, and allocates
 *        memory for holding the information about the unique operator. The information is
 *        defined in ::cnnlUniqueDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[in] unique_desc
 *   Input. Pointer to the unique descriptor that holds information about the unique operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetUniqueDescriptor function to initialize
 *   and set the information to the unqiue descriptor.
 * - You need to call the ::cnnlDestroyUniqueDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCreateUniqueDescriptor(cnnlUniqueDescriptor_t *unique_desc);

// Group:Unique
/*!
 *  @brief Destroys a unique descriptor \b unique_desc that is previously created with the
 *         ::cnnlCreateUniqueDescriptor function.
 *
 *  The unique descriptor is defined in ::cnnlUniqueDescriptor_t and holds the information
 *  about the unique operator.
 *
 * @param[in] unique_desc
 *   Input. The unique descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlUnique.
 * - This function should be called to destroy the unique descriptor. Otherwise, the memory
 *   leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDestroyUniqueDescriptor(cnnlUniqueDescriptor_t unique_desc);

// Group:Unique
/*!
 * @brief Initializes the unqiue descriptor \b unique_desc that is previously created
 *        with the ::cnnlCreateUniqueDescriptor function, and sets the information about the
 *        unique operation to the unique descriptor \b unique_desc. The information includes
 *        the sorted mode of the unqiue \b mode, the number of the unique dimensions \b dim,
 *        whether to output index \b return_inverse, and whether to output counts \b return_counts.
 *
 * @param[in] unique_desc
 *   Input. The descriptor of the unqiue operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] mode
 *   Input. The sorted mode of unqiue operation. The sorted modes are define in the
 *   ::cnnlUniqueSort_t enum.
 * @param[in] dim
 *   Input. The number of dimensions in the input tensor of the unique operation.
 *   Currently, only the unique of the flattened input is supported.
 * @param[in] return_inverse
 *   Input. A boolean value that specifies whether to return the index of input elements that
 *   are in the returned unique elements.
 * @param[in] return_counts
 *   Input. A boolean value that specifies whether to return the number of duplicate values
 *   for each unique element.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetUniqueDescriptor(cnnlUniqueDescriptor_t unique_desc,
                                                  cnnlUniqueSort_t mode,
                                                  int dim,
                                                  bool return_inverse,
                                                  bool return_counts);
// Group:Unique
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace to store
 *        unique data.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the unique
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unqiue operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   unique operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptor \b input_desc before calling this function, and
 *   call the ::cnnlCreateUniqueDescriptor and ::cnnlSetUniqueDescriptor functions to create
 *   and set the unique descriptor \b unique_desc.
 * - The allocated extra workspace should be passed to the ::cnnlUnique function to perform the
 *   unique operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetUniqueWorkSpace(cnnlHandle_t handle,
                                                 const cnnlUniqueDescriptor_t unique_desc,
                                                 const cnnlTensorDescriptor_t input_desc,
                                                 size_t *size);
// Group:Unique
/*!
 * @brief Computes the length of unique data of input tensor, and returns the results
 *        in \b output_len.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the unique
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unqiue operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] unique_data
 *   Output. Pointer to the MLU memory that is used as an extra workspace for the unique operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[out] output_len
 *   Output. Pointer to the MLU memory that stores the length of unique data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetUniqueWorkSpace function to allocate extra workspace for
 *   \b unique_data.
 *
 * @par Data Type
 * - Date types of input tensor \b input and output tensor \b unique_data must be the same.
 * - The supported data types of input tensor \b input and output tensor \b unique_data are as follows:
 *   - input tensor: float, int32
 *   - output tensor: float, int32
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlUniqueGetOutLen(cnnlHandle_t handle,
                                              const cnnlUniqueDescriptor_t unique_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              void *unique_data,
                                              int *output_len);
// Group:Unique
/*!
 *  @brief Retrieves unique elements in the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the unique operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] unique_desc
 *   Input. The descriptor of the unqiue operation. For detailed information,
 *   see ::cnnlUniqueDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_len
 *   Input. An integer value that is the length of unique data of input tensor.
 * @param[in] unique_data
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store unique
 *   data. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[out] output_data
 *   Output. Pointer to the MLU memory that stores the output tensor \b output_data.
 * @param[out] output_index
 *   Output. Pointer to the MLU memory that stores the index of input elements that are in
 *   the returned unique elements \b output_data. This parameter only returns meaningful
 *   value when \b return_inverse is set to true. If \b return_inverse is to false, this
 *   parameter returns meaningless value. It is recommended to set this parameter to NULL
 *   if \b return_inverse is to false.
 * @param[out] output_counts
 *   Output. Pointer to the MLU memory that stores the number of duplicate values for each
 *   unique element \b output_data. This parameter only returns meaningful value when
 *   \b return_counts is set to true. If \b return_counts is to false, this parameter
 *   returns meaningless value. It is recommended to set this parameter to NULL if
 *   \b return_counts is set to false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlUniqueGetOutLen function to get the length of unique data
 *   of input tensor \b output_len and the unique data \b unique_data.
 *
 * @par Formula
 * - See "Unique Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor \b input and output tensor \b output_data must be the same.
 * - The supported data types of input tensor \b input and output tensors are as follows:
 *   - input tensor: float, int32
 *   - \b output_data: float, int32
 *   - \b output_index: int32
 *   - \b output_counts: int32
 *
 * @par Scale Limitation
 * - The input tensor \b input must meet the following requirement:
 *   - When the \b mode is set to \p CNNL_UNSORT_FORWARD, the dimension of \b input must be
 *     one-dimensional.
 *
 * @note
 * - The \b input with NaN is not supported currently, and the data range of \b input should
 *   satisfy the following conditions:
 *   - (-inf, +inf), where inf represents infinity.
 * - You need to call the ::cnnlUniqueGetOutLen function to get the scale \b output_len and
 *   the tensor \b unique_data before calling this function.
 * - The tensor \b output_index is same shape as input tensor \b input, and the tensor
 *   \b output_counts is same shape as \b output_data.
 * - When the \b mode is set to \p CNNL_UNSORT_FORWARD, the output \b output_counts is not
 *   supported yet.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the unique operation is as follows:
     @verbatim
       Example 1:
       input array:
         input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
       param:
         mode: \p CNNL_UNSORT_FORWARD
       output array:
         output_data: [1, 2, 4, 9, 7, 8]
         output_index: [0, 0, 1, 2, 2, 3, 4, 5, 5]

       Example 2:
       input array:
         input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
       param:
         mode: \p CNNL_SORT_ASCEND, return_inverse: true, return_counts: true,
       output array:
         output_data: [1, 2, 4, 7, 8, 9]
         output_index: [0, 0, 1, 2, 2, 5, 3, 4, 4]
         output_counts: [2, 1, 2, 1, 2, 1]

       Example 3:
       input array:
         input: [1, 1, 2, 4, 4, 9, 7, 8, 8]
       param:
         mode: \p CNNL_SORT_REVERSE, return_inverse: true, return_counts: true,
       output array:
         output_data: [8, 7, 9, 4, 2, 1]
         output_index: [5, 5, 4, 3, 3, 2, 1, 0, 0]
         output_counts: [2, 1, 1, 2, 1, 2]
    @endverbatim
*
* @par Reference
* - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Unique.cpp
*
*/
cnnlStatus_t CNNL_WIN_API cnnlUnique(cnnlHandle_t handle,
                                     const cnnlUniqueDescriptor_t unique_desc,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const int output_len,
                                     void *unique_data,
                                     void *output_data,
                                     int *output_index,
                                     int *output_counts);

// Group:Convolution
/*!
 * @brief Initializes the convolution descriptor \b desc that is previously created
 * with the ::cnnlCreateConvolutionDescriptor function, and sets the information
 * about the convolution forward and backward operation to the convolution descriptor
 * \b desc. The information includes the number of the convolution dimensions \b dimNb,
 * the padding size for each dimension \b pad, the stride of the sliding window for
 * each dimension \b stride, the dilation
 * factor for each dimension \b dilation, and the number of groups to be split into
 * by channel \b group_count.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the convolution operation.
 *   Currently, the value of this parameter can only be set to 4 or 5. The value of this parameter
 *   should be same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the convolution operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   start and end of that dimension. If \b dimNb is set to 4, the padding is on top, bottom, left,
 *   and right. If \b dimNb is set to 5, the padding is on front, back, top, bottom, left,
 *   and right. The value of this parameter should be greater than or equal to 0.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the convolution operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. If \b dimNb is set to 4,
 *   the stride is in height and width.  If \b dimNb is set to 5,
 *   the stride is in depth_stride, height and width.
 *   The value of this parameter should be greater than or equal
 *   to 1.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the convolution operation. For each dimension, the dilation factor represents
 *   the spacing between the kernel points. If \b dimNb is set to 4, the dilation should be set in
 *   height and width dimension. The value of this parameter
 *   should be greater than or equal to 1. If \b dimNb is set to 5, the dilation should be set in
 *   depth, height and width dimension. The value of this parameter should be greater than or equal to 1.
 * @param[in] group_count
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each group is convolved separately. The filter used for each group is
 *   the filter tensor divides \b group_count. The result of
 *   the convolution operation is the concatenation of all the group convolution results by the
 *   number of channels in the input tensor. Make sure that the number of channels in the input tensor
 *   and the output tensor are divisible by \b group_count. The value of this parameter should
 *   be greater than or equal to 1 and less than or equal to the number of channels in the input tensor,
 *   and input channels and output channels must both be divisible by group_count.
 *   - If \b group_count is set to 1, the input tensor is convolved without splitting into groups.
 *   - If \b group_count is set to the number of channels in the input tensor, the depthwise convolution
 *     is performed.
 *   - If the value of \b group_count is between 1 and the number of channels of input tensor,
 *     the operator becomes equivalent to group_count numbers of convolution operations side by side.
 *     Each convolution operation is (input_channel/group_count) input channels,
 *     and (output_channel/group_count) output channels,
 *     concat the output of each convolution operations subsequently.
 * @param[in] compute_type
 *   Input. The data type of temporary result in convolution operation, only supports
 *   floating-point type.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *
 * @note
 * - Currently, only supports 4-dimensional and 5-dimensional input tensor for convolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - The data width of compute_type must not be less than output tensor's data type.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptor(cnnlConvolutionDescriptor_t desc,
                                                       int dimNb,
                                                       const int pad[],
                                                       const int stride[],
                                                       const int dilation[],
                                                       const int group_count,
                                                       const cnnlDataType_t compute_type);

// Group:Convolution
/*!
 * @brief Initializes the convolution descriptor \b desc that is previously created
 * with the ::cnnlCreateConvolutionDescriptor function, and sets the information
 * about the convolution forward and backward operation to the convolution descriptor
 * \b desc. This function also includes the \b allow_tf32 parameter that is used to
 * control whether to enable TensorFloat-32.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] allow_tf32
 *   Input. An integer value which determines whether to enable TensorFloat-32.
 *   TensorFloat-32 is enabled by default.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, only supports 4-dimensional and 5-dimensional input tensor for convolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptorAllowTF32(cnnlConvolutionDescriptor_t desc,
                                                                const int allow_tf32);

// Group:Convolution
/*!
 * @brief Sets the reorder type of the filter and bias tensors used in the convolution operation.
 *
 * @param[in] desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] filter_reorder_type
 *   Input. The reorder type of the convolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[in] bias_reorder_type
 *   Input. The reorder type of the convolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call ::cnnlGetReorderConvDataSize and
 *   ::cnnlHostReorderConvData functions to reorder data for convolution operation.
 *
 * @note
 * - You must set filter_reorder_type to CNNL_REORDER.
 * - Due to the limitation of cnnlHostReorderConvData, only MLU Edge devices can use
 *   cnnlHostReorderConvData when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetConvolutionDescriptorReorderType(
                                                    cnnlConvolutionDescriptor_t desc,
                                                    const cnnlReorderType_t filter_reorder_type,
                                                    const cnnlReorderType_t bias_reorder_type);

// Group:Convolution
/*!
 * @brief Creates a descriptor pointed by \b desc for a convolution forward or backward
 *        operation, and allocates memory for holding the information about the convolution
 *        operation. The information is defined in ::cnnlConvolutionDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *  Input. A host pointer to the convolution descriptor that holds information about the convolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetConvolutionDescriptor function to initialize
 *   and set the information to the convolution descriptor.
 * - You need to call the ::cnnlDestroyConvolutionDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateConvolutionDescriptor(cnnlConvolutionDescriptor_t *desc);

// Group:Convolution
/*!
 * @brief Destroys a convolution descriptor \b desc that is previously created with the
 *        ::cnnlCreateConvolutionDescriptor function.
 *
 * The convolution descriptor is defined in ::cnnlConvolutionDescriptor_t
 * and holds the information about the convolution forward or backward operation.
 *
 *
 * @param[in] desc
 *   Input. The convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlConvolutionForward,
 *   ::cnnlConvolutionBackwardData, or ::cnnlConvolutionBackwardFilter function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the convolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyConvolutionDescriptor(cnnlConvolutionDescriptor_t desc);

// Group:Convolution
/*!
 * @brief Returns the shape \b dimSize of the output tensor of a convolution forward
 * operation with the given convolution descriptor \b desc, input tensor \b x, filter tensor
 * \b w, and the number of dimensions of the input tensor \b dimNb.
 *
 * You can set the shape of the output tensor in the output tensor descriptor based on
 * the return value of this function.
 *
 * @param[in] desc
 *    Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x
 *    Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *    Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] dimNb
 *    Input. The number of dimensions in the input tensor of the convolution forward operation.
 *    Currently, the value of this parameter can only be set to 4 or 5. The value of this parameter
 *    should be same as the one you set in the input tensor descriptor.
 * @param[out] dimSize
 *    Output. An array that stores the shape of the output tensor of the convolution forward
 *    operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *
 * @par Formula
 * \n
 * The shape of the output is based on the input tensor, padding, and dilation you set with
 * the ::cnnlSetConvolutionDescriptor function, and the filter you set in the filter tensor
 * descriptor.
 * - The height dimension of the output tensor is as follows:
 *
 *   \p output_height =
 *
 *   1 + (\p input_height + \p pad_top + \p pad_bottom -
 *   (((\p filter_height - 1) * \p dilation_height) + 1)) / \p stride_height;
 * - The width dimension of the output tensor is as follows:
 *
 *   \p output_width =
 *
 *   1 + (\p input_width + \p pad_top + \p pad_bottom -
 *   (((\p filter_width - 1) * \p dilation_width) + 1)) / \p stride_width;
 *
 *
 * @note
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b x and \b w before calling this function.
 *   For detailed information about, see ::cnnlTensorDescriptor_t.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API cnnlGetConvolutionForwardOutputDim(const cnnlConvolutionDescriptor_t desc,
                                                             const cnnlTensorDescriptor_t x,
                                                             const cnnlTensorDescriptor_t w,
                                                             int dimNb,
                                                             int dimSize[]);

// Group:Convolution
/*!
 * @brief Returns the most suited convolution algorithm that can be used
 * in the convolution forward operation.
 *
 * The returned convolution algorithm is chosen from all the supported convolution
 * algorithms defined in ::cnnlConvolutionForwardAlgo_t and is based on the given
 * convolution descriptor \b desc, input tensor \b x, filter tensor \b w, output tensor \b y,
 * and the computing performance preferences \b preference.
 *
 * The computing performance options \b preference defined in the
 * ::cnnlConvolutionFwdPreference_t enum, only support high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution operation to get a better performance.
 *   This parameter only supports CNNL_CONVOLUTION_FWD_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for computing the convolution. The
 *   algorithms are defined in the ::cnnlConvolutionForwardAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardAlgorithm(cnnlHandle_t handle,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   const cnnlTensorDescriptor_t x,
                                   const cnnlTensorDescriptor_t w,
                                   const cnnlTensorDescriptor_t y,
                                   const cnnlConvolutionFwdPreference_t preference,
                                   cnnlConvolutionForwardAlgo_t *algo);

// Group:Convolution
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward operation.
 *
 * The size of extra workspace is based on the given information of the convolution
 * forward operation, including the input tensor descriptor \b x, filter tensor descriptor \b w,
 * output tensor descriptor \b y, convolution descriptor \b conv_desc, and the convolution
 * algorithm \b algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgorithm function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b x, \b w, \b y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlConvolutionForward function
 *   to perform the convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x,
                                       const cnnlTensorDescriptor_t w,
                                       const cnnlTensorDescriptor_t y,
                                       const cnnlTensorDescriptor_t bias,
                                       const cnnlConvolutionDescriptor_t conv_desc,
                                       const cnnlConvolutionForwardAlgo_t algo,
                                       size_t *size);

// Group:Convolution
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward inference operation.
 *
 * The size of extra workspace is based on the given information of the convolution forward
 * inference operation, including the input tensor descriptor \b x, filter tensor descriptor \b w,
 * output tensor descriptor \b y, convolution descriptor \b conv_desc, and the convolution
 * algorithm \b algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The descriptor of the cast mode. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution forward inference operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgorithm function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b x, \b w, \b y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlConvolutionForwardInference
 *   function to perform the convolution forward inference operation.
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionForwardInferenceWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x,
                                       const cnnlTensorDescriptor_t w,
                                       const cnnlTensorDescriptor_t y,
                                       const cnnlTensorDescriptor_t bias,
                                       const cnnlConvolutionDescriptor_t conv_desc,
                                       const cnnlConvolutionCastMode_t cast_mode,
                                       const cnnlConvolutionForwardAlgo_t algo,
                                       size_t *size);

// Group:Convolution
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the convolution forward operation.
 *
 * The size of extra workspace is based on the given information of the quantize convolution
 * forward operation, including the input tensor descriptor \b x, filter tensor descriptor \b w,
 * output tensor descriptor \b y, convolution descriptor \b conv_desc, and the convolution
 * algorithm \b algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantize convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input.The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are defined in the
 *  ::cnnlConvolutionForwardAlgo_t enum. You can get the best suited algorithm
 *  with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the quantize convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionForwardAlgorithm function.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b x, \b w, \b y before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeConvolutionForward
 *   function to perform the quantize convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeConvolutionForwardWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x,
                                       const cnnlTensorDescriptor_t w,
                                       const cnnlTensorDescriptor_t y,
                                       const cnnlTensorDescriptor_t bias,
                                       const cnnlConvolutionDescriptor_t conv_desc,
                                       const cnnlConvolutionForwardAlgo_t algo,
                                       size_t *size);

// Group:Convolution
/*!
 * @brief Computes a 2-D or 3-D cross-correlation on input tensor \b x_ptr with the filter
 * \b w_ptr, and returns the results in the output tensor \b y_ptr. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \b algo.
 * You can call the ::cnnlGetConvolutionForwardAlgorithm function to get the most
 * suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   input tensor \b x_ptr, filter tensor \b w_ptr, bias tensor \b bias_ptr
 *   and output tensor \b y_ptr.
 *   <b>Note that the combinations of bias tensor and output tensor must be half-half
 *   or float-float.</b>
 *   - input tensor: int8, int16, int31.
 *   - filter tensor: int8, int16, int31.
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 * - If the group_count is equal to the number of input channels,
 *   the input tensor and filter tensor can also be half or float.
 *   The combinations of data types are shown below with the following order:
 *   \b input - \b filter - \b output
 *   The supported data type combinations are:
 *   - half-half-half.
 *   - float-float-float.
 * - This function also supports the combinations of the following data types
 *   for input tensor, filter tensor and output tensor on MLU300 series or above.
 *   - input tensor, filter tensor, output tensor: half, half, half.
 *   - input tensor, filter tensor, output tensor: float, float, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \b dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \b dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: If \b dimNb is set to 5, the following limitations must meet:
 *     - Only supports \p depth <= 7, \p height <=7, \p width <= 7.
 *     - Size of filter tensor should not be too large, must meet limitation below:
 *       \p kh * \p kw * max(128 / sizeof(input_dtype), 64 / sizeof(filter_dtype)) *
 *       sizeof(filter_dtype) <= 8192 on MLU220 and MLU290; <= 16384 on MLU270.
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \b dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, when /b dimNb is set to 4,
 *   set the layout of the input tensor, output tensor,
 *   bias tensor, and filter tensor to NHWC.
 *
 * @note
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, the type of the parameter \b bias can only be CNNL_DTYPE_HALF in 2-D convolution.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[136], [122]],
                                                  [[195], [176]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d.
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d.
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlConvolutionForward(cnnlHandle_t handle,
                                                 const cnnlConvolutionDescriptor_t conv_desc,
                                                 cnnlConvolutionForwardAlgo_t algo,
                                                 const void *alpha,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x_ptr,
                                                 const cnnlTensorDescriptor_t w_desc,
                                                 const void *w_ptr,
                                                 const cnnlTensorDescriptor_t bias_desc,
                                                 const void *bias_ptr,
                                                 void *workspace,
                                                 size_t workspace_size,
                                                 const void *beta,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 void *y_ptr);
// Group:Convolution
/*!
 * @brief Converts the floating-point data of input tensor \b x_ptr and
 * filter \b w_ptr into fixed-point numbers according to the quantization parameters,
 * then computes a 2-D or 3-D cross-correlation on them,
 * and returns the results in the output tensor \b y_ptr. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \b algo.
 * You can call the ::cnnlGetConvolutionForwardAlgorithm function to get the most
 * suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] x_position
 *   Input. Pointer to the MLU memory that stores the position factor
 *   for quantizing the input tensor.
 * @param[in] x_scale
 *   Input. Pointer to the MLU memory that stores the scale factor
 *   for quantizing the input tensor.
 * @param[in] x_offset
 *   Input. Pointer to the MLU memory that stores the offset factor
 *   for quantizing the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] w_position
 *   Input. Pointer to the MLU memory that stores the position factor
 *   for quantizing the filter tensor.
 * @param[in] w_scale
 *   Input. Pointer to the MLU memory that stores the scale factor
 *   for quantizing the filter tensor.
 * @param[in] w_offset
 *   Input. Pointer to the MLU memory that stores the offset factor
 *   for quantizing the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The offchip data types should be set by following rules:
 *   - If the offchip data types of input tensor and filter tensor are fix-point data types,
 *     this function supports any combinations of the following data types.
 *     - input tensor: int8, int16, int31.
 *     - filter tensor: int8, int16, int31.
 *   - If the offchip data types of input tensor or filter tensor are floating-point data types,
 *     they should be set by following rules:
 *     - Offchip data types of input tensor and filter tensor must be same.
 *       - input tensor: half; filter tensor: half
 *       - input tensor: float; filter tensor: float
 *     - Floating-point convolution only supports on MLU370. On other platforms, if offchip
 *       data types of input tensor and filter tensor are floating-point data types, their onchip
 *       data types must be specified, and their onchip data types must be fix-point data types.
 *       Onchip data types of input tensor and filter tensor supports any combinations below:
 *       - input tensor: int8, int16, int31
 *       - filter tensor: int8, int16, int31
 *   - The offchip data types of output tensor and bias tensor should be set by following rules:
 *     - Offchip data types of output tensor and bias tensor must be floating-point data types
 *       and must be same.
 *       - output tensor: half; bias tensor: half
 *       - output tensor: float; bias tensor: float
 *     - If offchip data types of input tensor and filter tensor are floating-point data types,
 *       offchip data types of output tensor must be same as input tensor and filter tensor.
 *   - You need to call ::cnnlSetTensorDescriptor to set \b offchip_dtype in \b x_desc and \b w_desc.
 *
 * - The onchip data types should be set by following rules:
 *   - If the onchip data types of intput tensor and filter tensor are fix-point, this function supports
 *     any combinations of the follwing data types.
 *     - input tensor: int8, int16, int31.
 *     - filter tensor: int8, int16, int31.
 *   - Floating-point convolution only supports on MLU370. Onchip data types of input tensor,
 *     filter tensor and output tensor must be the combinations below:
 *     - input tensor: half; filter tensor: half; output tensor: half
 *     - input tensor: float; filter tensor: float; output tensor: float
 *   - You need to call ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype in
 *     \b x_desc and \b w_desc.
 *     And you do not need to call ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype
 *     in \b y_desc. If you call ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype in
 *     \b y_desc, it must be same with \b compute_type in \b conv_desc.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \b dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \b dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \b dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the layout of the input
 *   tensor, output tensor, bias tensor, and filter tensor to NHWC.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, the type of the parameter \b bias can only be CNNL_DTYPE_HALF in 2-D convolution.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[136], [122]],
                                                  [[195], [176]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantizeConvolutionForward(cnnlHandle_t handle,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   cnnlConvolutionForwardAlgo_t algo,
                                   const void *alpha,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x_ptr,
                                   const void *x_position,
                                   const void *x_scale,
                                   const void *x_offset,
                                   const cnnlTensorDescriptor_t w_desc,
                                   const void *w_ptr,
                                   const void *w_position,
                                   const void *w_scale,
                                   const void *w_offset,
                                   const cnnlTensorDescriptor_t bias_desc,
                                   const void *bias_ptr,
                                   void *workspace,
                                   size_t workspace_size,
                                   const void *beta,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y_ptr);
// Group:Convolution
/*!
 * @brief Converts the floating-point data of input tensor \b x_ptr into
 * fixed-point numbers, and computes a 2-D or 3-D cross-correction on the
 * fixed-point input tensor with the filter \b w_ptr. Then converts the
 * floating-point computing results into fixed-point, and returns the
 * fixed-point results in the output tensor \b y_ptr. This function is only
 * used for inference.
 *
 * This function needs extra MLU memory as the workspace to improve the convolution
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetConvolutionForwardWorkspaceSize function. The convolution
 * operation is computed based on the convolution algorithm set in \b algo.
 * You can call the ::cnnlGetConvolutionForwardAlgorithm function to get the most
 * suited algorithm.
 *
 * To set the factors for quantization:
 * - If offline symmetric quantization with position was used, you need call the
 *   ::cnnlSetTensorDescriptorPosition function to set the position factor used in the fixed-point
 *   quantization.
 * - If offline symmetric quantization with position and scale factors was used,
 *   you need call the ::cnnlSetTensorDescriptorPositionAndScale function to set the position
 *   and scale factors used in fixed-point quantization.
 * - If offline asymmetric quantization was used, you need call the
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function to set the position, scale and offset
 *   factors used in fixed-point quantization.
 * - For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The descriptor of the cast mode. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x_ptr
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_ptr
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_ptr
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution operation. You can get the size of the workspace with
 *   the ::cnnlGetConvolutionForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Output. The descriptor of the output tensor. You can get the shape of the
 *   tensor with the ::cnnlGetConvolutionForwardOutputDim function.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y_ptr
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The \b x_desc data type should be set with the following rules:
 *   - If \b dtype in \b x_desc is fix-point type, the x_desc onchip_dtype will be automatically set
 *     to x_desc dtype, and you do not need to call ::cnnlSetTensorDescriptorOnchipDataType to set
 *     onchip_dtype in x_desc.
 *   - If \b dtype in \b x_desc is float-point type, it must be the same with \b compute_type in \b
 *     conv_desc, and you need to call ::cnnlSetTensorDescriptorOnchipDataType to set \b
 *     onchip_dtype in \b x_desc to fix-point type.
 * - The \b w_desc data type should be set with the following rules:
 *   - If \b dtype in \b w_desc is fix-point type, the w_desc onchip_dtype will be automatically set
 *     to w_desc dtype, and you do not need to call :: cnnlSetTensorDescriptorOnchipDataType to set
 *     onchip_dtype in w_desc.
 *   - If \b dtype in \b w_desc is float-point type, you need to call
 *     ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype in \b w_desc to fix-point
 *     type and ::cnnlHostReorderConvData to reorder filter data on host.
 *   - The \b width of \b onchip_dtype in \b w_desc must be less than or same with \b width of \b
 *     onchip_dtype in \b x_desc.
 * - The \b bias_desc data type should be set with the following rules:
 *   - If \b dtype in \b bias_desc is same with \b compute_type in \b conv_desc, the \b
 *     onchip_dtype in \b bias_desc will be automatically set \b dtype in \b bias_desc, and you do
 *     not need call ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype in \b bias_desc.
 *   - If \b dtype in \b bias_desc is not same with \b compute_type in \b conv_desc, you need call
 *     ::cnnlSetTensorDescriptorOnchipDataType to set \b onchip_dtype in \b bias_desc to \b
 *     compute_type in \b conv_desc and ::cnnlHostReorderConvData to reorder bias data on host.
 * - You do not need to set \b onchip_dtype in \b y_desc. When \b dtype in \b y_desc is fix-point
 *   type, it must be same with \b onchip_dtype in \b x_desc. When \b dtype in \b y_desc is
 *   float-point type, it must be same with \b compute_type in \b conv_desc.
 * - If offline asymmetric quantization was used, this function does not support int31 currently.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, filter tensor, bias tensor, and
 *   output tensor are as follows:
 * - If \b dimNb is set to 4:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_HWCN.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 * - If \b dimNb is set to 5:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC or \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count should be greater than or equal to 1 and less than or equal to
 *     the number of channels in the input tensor, input channels and output channels
 *     must both be divisible by group_count.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *   - onchip_dtype of y_desc must be the same as compute_type of conv_desc.
 *   - dtype of y_desc must be the same as compute_type of conv_desc
 *     when \b dimNb is set to 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the layout of the input
 *   tensor, output tensor, bias tensor, and filter tensor to NHWC.
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - When data type of output tensor is CNNL_DTYPE_HALF,
 *   you can set compute_type of conv_desc to CNNL_DTYPE_FLOAT to get higher precision.
 *   In this case, this feature only supports 3-D convolution.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution forward operation is as follows:
     @verbatim
      input three arrays by 1 * 3 * 3 * 2, 1 * 1 * 1 * 1 and 1 * 3 * 3 * 2 -->
          input: [[[[5, 1], [8, 1], [6, 4]],
               [[3, 8], [2,6], [0, 6]],
               [[8, 5], [7,4], [9, 6]]]]

      --> bias: [[[[1]]]]

      --> filter: [[[[1, 5], [7,5], [4, 2]],
                    [[1, 8], [3,8], [6, 2]],
                    [[4, 8], [5,0], [9, 5]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output array by 1 * 2 * 2 * 1 --> output: [[[[136], [122]],
                                                  [[195], [176]]]]
     @endverbatim
 * - For the example of how to program with the convolution functions,
 *   see the Cambricon CNNL sample code in ``samples/conv_sample/conv_sample.cc``
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv1d
 * - https://www.tensorflow.org/api_docs/python/tf/nn/conv2d
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlConvolutionForwardInference(cnnlHandle_t handle,
                                   const cnnlConvolutionDescriptor_t conv_desc,
                                   const cnnlConvolutionCastMode_t cast_mode,
                                   cnnlConvolutionForwardAlgo_t algo,
                                   const void *alpha,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x_ptr,
                                   const cnnlTensorDescriptor_t w_desc,
                                   const void *w_ptr,
                                   const cnnlTensorDescriptor_t bias_desc,
                                   const void *bias_ptr,
                                   void *workspace,
                                   size_t workspace_size,
                                   const void *beta,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y_ptr);

// Group:StridedSlice
/*!
 * @brief Computes gradients of strided slice. For detailed information, see ::cnnlStridedSlice.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the strided
 *   slice backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] begin
 *   Input. An array value which determines the start position of every dimension of \b output.
 * @param[in] end
 *   Input. An array value which determines the end position of every dimension of \b output.
 * @param[in] stride
 *   Input. An array value which determines the stride of every dimension of \b output.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "StridedSliceBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, begin, end, and stride must meet the following requirements:
 *   - For any dimension:
 *     - \b stride cannot be 0.
 *     - When \b stride is greater than 0:
 *       - 0 <= \b begin <= \b end <= output_num.
 *       - input_num * \b stride <= \b end - \b begin.
 *     - When stride is smaller than 0:
 *       - (-output_num) - 1 <= \b end <= \b begin <= -1.
 *       - (input_num - 1) * (-\b stride) <= (-\b end) - (-\b begin).
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the strided slice backward operation is as follows:
     @verbatim
     input array by 4 * 4 --> input: [[11, 12, 13, 14],
                                      [21, 22, 23, 24],
                                      [31, 32, 33, 34],
                                      [41, 42, 43, 44]]

     param:
       begin: [1,2], end: [4,8], stride: [1,2]

     output array by 5 * 9 --> output:[[0, 0,  0, 0,  0, 0,  0, 0,  0],
                                       [0, 0, 11, 0, 12, 0, 13, 0, 14]
                                       [0, 0, 21, 0, 22, 0, 23, 0, 24]
                                       [0, 0, 31, 0, 32, 0, 33, 0, 34]
                                       [0, 0, 41, 0, 42, 0, 43, 0, 44]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/StridedSliceGrad
 */
cnnlStatus_t CNNL_WIN_API cnnlStridedSliceBackward(cnnlHandle_t handle,
                                                   const int begin[],
                                                   const int end[],
                                                   const int stride[],
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   const void *input,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   void *output);

// Group:Std
/*!
 * @brief Calculates the standard deviation for each row of the input tensor in a given
 * dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the std forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of \b input to reduce.
 * @param[in] unbiased
 *   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
 *   standard-deviation will be calculated via the biased estimator.
 *   Otherwise, Bessel's correction will be used.
 * @param[in] input_desc
 *   Input. The descriptors of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptors of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for \b unbiased, \b dim,
 *   input tensor \b input and output tensor \b output. Data type of both tensors should be the
 *   same, and the inputs and outputs message are as follow:
 * - \b unbiased: bool
 * - \b dim: int32
 * - \b input: float, half
 * - \b output: float, half
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.std
 *
 * par API Dependency
 * - None.
 *
 * par Example
 *  The example of the std forward operation is as follows:
   @verbatim
   input: a tensor which shape is 4 * 4  --> [[0.2035, 1.2959, 1.8101, -0.4644],
                                               [1.5027, -0.3270, 0.5905, 0.6538],
                                               [-1.5745, 1.3330, -0.5596, -0.6548],
                                               [0.1264, -0.5080, 1.6420, 0.1992]]
    param: dim = 1, unbiased = True

    Then we will get the output:

    output: a tensor by 4 * 1             --> [1.0311, 0.7477, 1.2204, 0.9087]
   @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlStdForward(cnnlHandle_t handle,
                                         int dim,
                                         bool unbiased,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         const void *output);

// Group:Std
/*!
 * @brief Calculates the inverse gradient of the standard deviation.
 * The corresponding forward calculation of the function is ::cnnlStdForward.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the std backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of \b input to reduce.
 * @param[in] unbiased
 *   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
 *   standard-deviation will be calculated via the biased estimator.
 *   Otherwise, Bessel's correction will be used.
 * @param[in] input_desc
 *   Input. The descriptors of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptors of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output
 *   Input. Pointer to the MLU memory that stores the output tensor.
 * @param[in] diff_grad_desc
 *   Input. The descriptor of the \b diff_grad tensor, For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_grad
 *   Input. Pointer to the MLU memeory that stores the gradient tensor.
 * @param[in] diff_output_desc
 *   Input. The descriptor of the \b diff_output tensor, For detailed information.
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_output
 *   Output. Pointer to the MLU memory that stores the output gradient tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, output tensor
 *   \b output, gradient tensor \b diff_grad and output gradient tensor \b diff_output.
 *   Data type of these input tensors should be the same.
 * - \b dim: int32
 * - \b unbiased: bool
 * - \b input: float, half
 * - \b output: float, half
 * - \b diff_grad: float, half.
 * - \b diff_output: float, half.
 *
 * par Scale Limitation
 * - The value of \b output should be in the range of [0.01, 500].
 *
 * @par Reference
 * - https://pytorch.org/docs/1.0.0/torch.html?highlight=std#torch.std
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlStdBackward(cnnlHandle_t handle,
                                          int dim,
                                          bool unbiased,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t output_desc,
                                          const void *output,
                                          const cnnlTensorDescriptor_t diff_grad_desc,
                                          const void *diff_grad,
                                          const cnnlTensorDescriptor_t diff_output_desc,
                                          const void *diff_output);

// Group:Nllloss
/*!
 * @brief Returns in \b size of the MLU memory that is used as an extra workspace to optimize
 * the nllloss forward operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the nllloss forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNlllossWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t x_desc,
                                                      size_t *size);
// Group:Nllloss
/*!
 * @brief Computes a Negative-Log-Likelihood Loss, which is used to train a
 * classification problem with C class, on input tensor \b x with \b target
 * and \b filter, and returns the results in the output tensor \b y.
 *
 * This function needs extra MLU memory as the workspace to improve the nllloss forward
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetNlllossWorkspaceSize function. The nllloss operation is computed based
 * on the nllloss algorithm set in \b algorithm.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the nllloss. The algorithms are defined in the
 *   ::cnnlNlllossAlgorithm_t enum.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   nllloss forward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the nllloss forward operation. You can get the size of the workspace with the
 *   ::cnnlGetNlllossWorkspaceSize function.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] tf_desc
 *   Input. The descriptor of the total_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] total_filter
 *   Output. Pointer to the MLU memory that stores the total_filter tensor, which
 *   means the sum of the filter.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
  * @par Data Type
 * - Data types of input tensors \b x, \b filter and output tensors \b y, \b total_filter
 *   must be the same while data type of target must be int32.
 * - The supported data types are as follows:
 *   - x: float, half.
 *   - target: int32.
 *   - filter: float, half.
 *   - y: float, half.
 *   - total_filter: float, half.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, filter tensor and output tensor
 *   must meet the following requirements:
 *   - x: [N, C]
 *   - target: [N]
 *   - filter: [C]
 *   - y: [N] in NONE mode or [1] in SUM and MEAN mode
 * - The value of the target tensor should be in the range of [0, C-1] or equal to \b ignore_index.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetNlllossWorkspaceSize
 *   to get the extra workspace size needed in nllloss forward operation.
 *
 * @note
 * - This function only supports 2-D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Nllloss Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input three arrays by 2 * 3, 2 and 3
       --> x: [[1,2,3], [4,5,6]]

       --> target: [0,1]

       --> filter: [1,2,3]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output two arrays by 2 and 1
       --> y: [0,-10]
       --> total_filter: [5]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlNlllossForward(cnnlHandle_t handle,
                                             cnnlNlllossAlgorithm_t algorithm,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const cnnlTensorDescriptor_t t_desc,
                                             const void *target,
                                             const int ignore_index,
                                             const cnnlTensorDescriptor_t w_desc,
                                             const void *filter,
                                             const cnnlTensorDescriptor_t tf_desc,
                                             void *total_filter,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y);

// Group:Nllloss
/*!
 * @brief Computes a nllloss backward on input tensor \b diff_y with \b target, \b filter
 * and \b total_filter, and returns the results in the output tensor \b diff_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   nllloss backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute the nllloss backward. The algorithms are defined
 *   in the ::cnnlNlllossAlgorithm_t enum.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] t_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] ignore_index
 *   Input. An index that specifies a target value that is ignored and does not
 *   contribute to the input gradient.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] tw_desc
 *   Input. The descriptor of the total_filter tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] total_filter
 *   Input. Pointer to the MLU memory that stores the total_filter tensor, which means
 *   the sum of the filter.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Data Type
 * - Data types of input tensors \b diff_y, \b filter, \b total_filter and output tensor \b diff_x
 *   must be the same while data type of target must be int32.
 * - The supported data types are as follows:
 *   - diff_y: float, half.
 *   - target: int32.
 *   - filter: float, half.
 *   - total_filter: float, half.
 *   - diff_x: float, half.
 *
 * @par Limitations
 * - The dimension of the input tensor, target tensor, filter tensor and output tensor
 *   must meet the following requirements:
 *   - diff_y: [N] in NONE mode or [1] in SUM and MEAN mode
 *   - target: [N]
 *   - filter: [C]
 *   - diff_x: [N, C]
 * - The value of the target tensor should be in the range of [0, C-1] or equal to \b ignore_index.
 *
 * @par API Dependency
 * - Before calling this function to implement nllloss backward, you need to prepare all the
 *   input parameters from the output of ::cnnlNlllossForward.
 *
 * @note
 * - This function only supports 1-D input tensor currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Nllloss Backward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss backward operation is as follows:
     @verbatim
       input four arrays by 2, 2, 3 and 1
       --> diff_y: [1, 1]

       --> target: [0,1]

       --> filter: [1,2,3]

       --> total_filter: [5]

       param:
         ignore_index: 0, algorithm: CNNL_REDUCTION_NONE

       output array by 2 * 3
       --> diff_x: [[0,0,0], [0,-2,0]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlNlllossBackward(cnnlHandle_t handle,
                                              cnnlNlllossAlgorithm_t algorithm,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              const cnnlTensorDescriptor_t t_desc,
                                              const void *target,
                                              const int ignore_index,
                                              const cnnlTensorDescriptor_t w_desc,
                                              const void *filter,
                                              const cnnlTensorDescriptor_t tw_desc,
                                              const void *total_filter,
                                              const cnnlTensorDescriptor_t diff_x_desc,
                                              void *diff_x);

// Group:L1LossBackward
/*!
 * @brief Computes the gradient of input \b grad_input in artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction_mode
 *   Input. The reduction mode used to compute \b grad_input. The reduction mode are defined in the
 *   ::cnnlLossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of \b input tensor, which is the predict value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of \b target tensor, which is the ground truth value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \b target tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \b grad_output, which is the gradient with respect to \b output. In
 *   general, it is the output of artificial intelligence. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the \b grad_output.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \b grad_input, which is the gradient with respect to \b input.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \b grad_input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "L1 Loss Backward Out Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The scale of \b input tensor, \b target tensor, \b grad_output tensor, \b grad_input tensor
 *   must meet the following requirements:
 *   - If \b reduction_mode is set to NONE:
 *     - The shape of \b target equals to the shape of \b input.
 *     - The shape of \b grad_output equals to the shape of \b input.
 *     - The shape of \b grad_input equals to the shape of \b input.
 *   - If \b reduction_mode is set to MEAN or SUM:
 *     - The shape of \b target equals to the shape of \b input.
 *     - The shape of \b grad_output is [1], which is a scalar.
 *     - The shape of \b grad_input equals to the shape of \b input.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc, target_desc, grad_output_desc
 *   and grad_input_desc with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the l1 loss backward out operation is as follows:
     @verbatim
     input: a tensor of 2 --> [1,0,3,2]
     target: a tensor of 2 --> [0,2,4,1]
     grad_output: a tensor of 2 --> [0.1, 0.5, 0.2, 0.3]
     reduction: NONE

     Then we will get the grad_input:
     grad_input: a tensor of 2 --> [0.1, -0.5, -0.2, 0.3]
     @endverbatim
 *
 * @par Reference
 * - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlL1LossBackward(cnnlHandle_t handle,
                                             cnnlLossReduction_t reduction_mode,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t target_desc,
                                             const void *target,
                                             const cnnlTensorDescriptor_t grad_output_desc,
                                             const void *grad_output,
                                             const cnnlTensorDescriptor_t grad_input_desc,
                                             void *grad_input);
// Group:SmoothL1Loss
/*!
 * @brief Computes the smoothl1 loss of \b x and \b target in AI networks.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor, which is the predict value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \b x tensor.
 * @param[in] t_desc
 *   Input. The descriptor of \b target tensor, which is the ground truth value.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \b target tensor.
 * @param[in] algorithm
 *   Input. The reduction mode used to compute \b y. The reduction mode are defined in
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @param[in] y_desc
 *   Input. The descriptor of \b y, which is the smoothl1loss value of \b x and \b target.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the \b y.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "SmoothL1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 * @par Data Type
 * - Data types of input tensors \b x, \b target and output tensors \b y must be the same.
 * - The supported data types are as follows:
 *   - x: float, half.
 *   - target: float, half.
 *   - y: float, half.
 * @par Scale Limitation
 * - The scale of \b x tensor, \b target tensor, \b y tensor must meet the following requirements:
 *   - If \b algorithm is set to NONE:
 *     - The shape of \b target equals to the shape of \b x.
 *     - The shape of \b y equals to the shape of \b x.
 *   - If \b algorithm is set to MEAN or SUM:
 *     - The shape of \b target equals to the shape of \b x.
 *     - The shape of \b y is [1], which is a scalar.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1 loss operation is as follows:
     @verbatim
     x: a tensor of 2 * 3 --> [[1,2,3],[4,5,6]]
     target: a tensor of 2 * 3 --> [[6,5,4],[3,2,1]]
     algorithm: NONE
     Then we will get the y:
     y: a tensor of  2 * 3 --> [[4.5,2.5,0.5],[0.5,2.5,4.5]]
     @endverbatim
 *
 * @par Reference
 *   http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossForward(cnnlHandle_t handle,
                        const cnnlTensorDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t t_desc,
                        const void *target,
                        const cnnlTensorDescriptor_t y_desc,
                        void *y,
                        cnnlSmoothL1LossAlgorithm_t algorithm);
// Group:SmoothL1Loss
/*!
 * @brief Computes the gradient with respect to input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in this operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor, which is the predict value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the \b x tensor.
 * @param[in] target_desc
 *   Input. The descriptor of \b target tensor, which is the ground truth value.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the \b target tensor.
 * @param[in] dy_desc
 *   Input. The descriptor of \b dy, which is the gradient with respect to \b output. In
 *   general, it is the output of artificial intelligence. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the \b dy.
 * @param[in] dx_desc
 *   Input. The descriptor of \b dx, which is the gradient with respect to \b input.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the \b dx.
 * @param[in] algo
 *   Input. The algorithm used to compute \b grad_input. The algorithms are defined in the
 *   ::cnnlSmoothL1LossAlgorithm_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "Smooth L1 Loss Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types are as follows:

 *   - x(input): float, half.
 *   - target(input): float, half.
 *   - dy(input): float, half.
 *   - dx(output): float, half.
 *
 * @note
 * - Data type of x should be same with target.
 * - Each dimension of x should be same with target.
 * - The scale of \b x tensor, \b target tensor, \b dy tensor, \b dx tensor
 *   must meet the following requirements:
 *   - If algorithm is set to NONE:
 *     - The shape of \b target equals to the shape of \b x.
 *     - The shape of \b dy equals to the shape of \b x.
 *     - The shape of \b dx equals to the shape of \b x.
 *   - If algorithm is set to MEAN or SUM:
 *     - The shape of \b target equals to the shape of \b x.
 *     - The shape of \b dy is 1 * 1, which is a scalar.
 *     - The shape of \b dx equals to the shape of \b x.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the smoothl1loss backward operation is as follows:
     @verbatim
     x: a tensor of 2 --> [1,1]
     target: a tensor of 2 --> [0,1]
     dy: a tensor of 2 --> [1, 1]
     reduction: NONE

     Then we will get the dx:
     dx: a tensor of 2 --> [1,0]
     @endverbatim
 *
 * @par Reference
 * - http://www.pytorch.org/docs/master/_modules/torch/nn/modules/loss.html
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSmoothL1LossBackward(cnnlHandle_t handle,
                         const cnnlTensorDescriptor_t x_desc,
                         const void *x,
                         const cnnlTensorDescriptor_t target_desc,
                         const void *target,
                         const cnnlTensorDescriptor_t dy_desc,
                         const void *dy,
                         const cnnlTensorDescriptor_t dx_desc,
                         void *dx,
                         cnnlSmoothL1LossAlgorithm_t algo);

/*!
 * @brief Enumeration variables describing the type of random generator that can distinguish
 * between different random generators.
 *
 * You need to call the ::cnnlRandCreateGenerator function to set the random generator type to
 * a random generator defined in ::cnnlRandGenerator_t.
 */
typedef enum {
  CNNL_RAND_RNG_FAST,
  /*!< Generates random numbers with MLU hardware random generator. The random numbers
      cannot be reappeared on CPU. And it performs better on MLU200 series than
      MLU300 series.*/
  CNNL_RAND_RNG_MTGP32,
  /*!< Generates random numbers with MTGP32 generator. The random numbers can be
     reappeared on CPU with the same MTGP32 algorithm and seed. And it performs better on MLU300
     series than MLU200 series.*/
} cnnlRandRngType_t;

/*!
 * @brief Enumeration variables describing the sequence period of MTGP32.
 *
 * You need to call the ::cnnlRandSetMTGP32Period function to set the period to
 * a \b CNNL_RAND_RNG_MTGP32 random generator that is defined in ::cnnlRandGenerator_t.
 */
typedef enum {
  CNNL_RAND_MTGP32_P11213, /*!< The Mersene Twister sequence period of 11213 is used.*/
} cnnlRandMTGP32PeriodType_t;

/*!
 * The descriptor of the random operation that holds the random generator information including
 * seed, period, and random generator type.
 *
 * You need to call the ::cnnlRandCreateGenerator function to create a descriptor,
 * and call the ::cnnlRandSetPseudoRandomGeneratorSeed function to set a seed to the generator
 * or call the ::cnnlRandSetMTGP32Period function to set the period to the generator. Also,
 * you need to destroy the Cambricon CNNL context at the end with the ::cnnlRandDestroyGenerator function.
 */
typedef struct cnnlRandGeneratorStruct *cnnlRandGenerator_t;

/*!
 * The descriptor of the random operation that holds the state parameter information of MTGP32 including
 * period, parameter table and temper table.
 *
 * You need to call the ::cnnlRandGetMTGP32HostParam function to get the descriptor.
 */
typedef struct MTGP32FastParams *cnnlMTGP32FastParams_t;

// Group:Rand
/*!
 * @brief Creates a descriptor pointed by \b desc for a random
 *   operation, and allocates memory for holding the information about the random
 *   operation. The information is defined in ::cnnlRandGenerator_t. For more
 *   information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] generator
 *   Input. A host pointer to the random \b generator descriptor that holds information about the
 *   random operation.
 * @param[in] rng_type
 *   Input. The type of random \b generator defined in ::cnnlRandRngType_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlRandSetPseudoRandomGeneratorSeed
 *   function to set a seed to the \b generator or call the ::cnnlRandSetMTGP32Period function
 *   to set the period to the \b generator. Also, you need to destroy the Cambricon CNNL context at the end
 *   with the ::cnnlRandDestroyGenerator function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandCreateGenerator(cnnlRandGenerator_t *generator,
                                                  cnnlRandRngType_t rng_type);
// Group:Rand
/*!
 * @brief Destroys a random \b generator descriptor.
 *
 * @param[in] generator
 *   Input. The random \b generator descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function, or
 *   ::cnnlRandSetPseudoRandomGeneratorSeed function, or ::cnnlRandSetMTGP32Period function.
 *   Otherwise, ::CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the random \b generator descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandDestroyGenerator(cnnlRandGenerator_t generator);

// Group:Rand
/*!
 * @brief Sets a \b seed to a random \b generator descriptor.
 * A default \b seed will be used if this function is not called.
 *
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] seed
 *   Input. The \b seed to be set.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandSetPseudoRandomGeneratorSeed(cnnlRandGenerator_t generator,
                                                               int seed);
// Group:Rand
/*!
 * @brief Sets MTGP32 \b period to a random \b generator.
 * Only \b CNNL_RAND_MTGP32_P11213 is supported. If this function is
 * not called, a default value of \b CNNL_RAND_MTGP32_P11213 will be used.
 *
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] period
 *   Input. Period of Mersene Twister sequence. For detailed information,
 *   see ::cnnlRandMTGP32PeriodType_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandSetMTGP32Period(cnnlRandGenerator_t generator,
                                                  cnnlRandMTGP32PeriodType_t period);
// Group:Rand
/*!
 * @brief Gets MTGP32 state size in bytes when using \b CNNL_RAND_RNG_MTGP32 \b generator type.
 *
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] size
 *   Input. A host pointer to the returned size of the state in bytes that is used in
 *   the random operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlRandMakeMTGP32State function to
 *   initialize state workspace.
 * - The allocated extra workspace should be passed to perform the random operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32StateSize(cnnlRandGenerator_t generator,
                                                     size_t *size);
// Group:Rand
/*!
 * @brief Gets the size of MTGP32 kernel parameter data in bytes when using \b CNNL_RAND_RNG_MTGP32
 * \b generator type.
 *
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] size
 *   Input. A host pointer to the returned size of the state in bytes that is used in
 *   the random operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlRandMakeMTGP32Constants function to
 *   initialize kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32KernelParamSize(cnnlRandGenerator_t generator,
                                                           size_t *size);
// Group:Rand
/*!
 * @brief Gets a pointer to the MTGP32 host state parameter that is defined in
 * ::cnnlMTGP32FastParams_t when using \b CNNL_RAND_RNG_MTGP32 \b generator type.
 *
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[out] param
 *   Input. A host pointer to the returned parameter that is used in
 *   the random operation. For detailed information, see ::cnnlMTGP32FastParams_t.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlRandMakeMTGP32Constants function to
 *   initialize kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlRandGetMTGP32HostParam(cnnlRandGenerator_t generator,
                                                     cnnlMTGP32FastParams_t *param);
// Group:Rand
/*!
 * @brief Initializes the MTGP32 kernel parameter data on device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] params
 *   Input. Pointer to host memory that stores MTGP32 state parameter.
 *   For detailed information, see ::cnnlMTGP32FastParams_t.
 * @param[in] kernel_params
 *   Input. Pointer to MLU memory that stores MTGP32 state parameter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32HostParam function to
 *   get state host parameter and calling the ::cnnlRandGetMTGP32KernelParamSize function to
 *   initialize the MTGP32 kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32Constants(cnnlHandle_t handle,
                                                      const cnnlMTGP32FastParams_t params,
                                                      void *kernel_params);
// Group:Rand
/*!
 * @brief Initializes the MTGP32 state on device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in,out] state
 *   Input and output. Pointer to MLU memory that stores MTGP32 state.
 * @param[in] params
 *   Input. Pointer to host memory that stores MTGP32 state parameter.
 *   For detailed information, see ::cnnlMTGP32FastParams_t.
 * @param[in] kernel_params
 *   Input. Pointer to MLU memory that stores MTGP32 state parameter.
 * @param[in] seed
 *   Input. The \b seed to be used to initialize the MTGP32 state.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32StateSize function to
 *   initialize MTGP32 state workspace and calling the ::cnnlRandMakeMTGP32Constants function to
 *   initialize the MTGP32 kernel parameter data.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32KernelState(cnnlHandle_t handle,
                                                        void *state,
                                                        const cnnlMTGP32FastParams_t params,
                                                        const void *kernel_params,
                                                        const uint32_t seed);
// Group:Rand
/*!
 * @brief Initializes the MTGP32 state.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] state
 *   Input. Pointer to MLU memory that stores MTGP32 state.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandGetMTGP32StateSize function to
 *   initialize state workspace.
 * - The allocated extra workspace should be passed to perform the random operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlRandMakeMTGP32State(cnnlHandle_t handle,
                                                  cnnlRandGenerator_t generator,
                                                  void *state);
// Group:Rand
/*!
 * @brief Generates random numbers of uniform distribution in half or float data type
 * on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniform Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b num is greater than 0.
 * - The \b max is greater than the \b min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.


 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> min: 0;
      --> max: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between 0 and 1;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateUniform(cnnlHandle_t handle,
                                                  const cnnlRandGenerator_t generator,
                                                  cnnlDataType_t type,
                                                  void *state,
                                                  size_t num,
                                                  float min,
                                                  float max,
                                                  void *out);
// Group:Rand
/*!
 * @brief Generates random numbers of uniform distribution in integer
 * data type on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniformInt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b num is greater than 0.
 * - The \b max is greater than the \b min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data type of output is as follows:
 *  - output: int32.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> num: 2048;
       --> min: 0;
       --> max: 1;
       Then we will get the output:
       --> out: an array [2048] of integer type between 0 and 1;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateUniformInt(cnnlHandle_t handle,
                                                     const cnnlRandGenerator_t generator,
                                                     void *state,
                                                     size_t num,
                                                     int min,
                                                     int max,
                                                     void *out);
// Group:Rand
/*!
 * @brief Generates random numbers from the discrete uniform distribution over [\b min, \b max - 1]
 * on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] min
 *   Input. The minimum value of random numbers to be generated.
 * @param[in] max
 *   Input. The maximum value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomUniformInt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b num is greater than 0.
 * - The \b max is greater than the \b min.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data type of output is as follows:
 *  - output: half, float, int32.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
       --> type: CNNL_DTYPE_FLOAT;
       --> num: 2048;
       --> min: 0;
       --> max: 5;
       Then you will get the output:
       --> out: an array [2048] of float data type in the range of [0, 5);
     @endverbatim
 */

cnnlStatus_t cnnlRandGenerateDescreteUniform(cnnlHandle_t handle,
                                             const cnnlRandGenerator_t generator,
                                             cnnlDataType_t type,
                                             void *state,
                                             size_t num,
                                             int min,
                                             int max,
                                             void *out);
// Group:Rand
/*!
 * @brief Generates random numbers of normal distribution in half or float
 * data type on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomNormal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Scale Limitation
 * - The \b num is greater than 0.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type with mean equals 0 and standard deviation value
          equals 1;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateNormal(cnnlHandle_t handle,
                                                 const cnnlRandGenerator_t generator,
                                                 cnnlDataType_t type,
                                                 void *state,
                                                 size_t num,
                                                 float mean,
                                                 float stddev,
                                                 void *out);
// Group:Rand
/*!
 * @brief Generates random numbers of truncated normal distribution
 * in half or float data type on MLU device.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] type
 *   Input. Data type of output. For detailed information,
 *   see ::cnnlDataType_t.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] num
 *   Input. The total number of random numbers to be generated.
 * @param[in] mean
 *   Input. The mean value of random numbers to be generated.
 * @param[in] stddev
 *   Input. The standard deviation value of random numbers to be generated.
 * @param[out] out
 *   Output. Pointer to device output data, which is the random sequence between
 * (\b mean - 2 * \b stddev) and (\b mean + 2 * \b stddev).
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomTruncatedNormal Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 * - The \b num is greater than 0.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.

 * @par Data Type
 * - The supported data types of output are as follows:
 *   - output : half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> type: CNNL_DTYPE_FLOAT;
      --> num: 2048;
      --> mean: 0;
      --> stddev: 1;
      Then we will get the output:
      --> out: an array [2048] of float type between -2 and 2;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlRandGenerateTruncatedNormal(cnnlHandle_t handle,
                                                          const cnnlRandGenerator_t generator,
                                                          cnnlDataType_t type,
                                                          void *state,
                                                          size_t num,
                                                          float mean,
                                                          float stddev,
                                                          void *out);

// Group:RandGenerateMultinomial
/*!
 * @brief Returns in \b size of the MLU memory that is used as an extra workspace to optimize
 * the random multinomial operation.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   random multinomial operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the random multinomial operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRandGenerateMultinomialWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            size_t *workspace_size);
// Group:RandGenerateMultinomial
/*!
 * @brief Generates random numbers where each row contains indices sampled from the multinomial
 * probability distribution located in the corresponding row of input tensor.
 *
 * @deprecated
 *   ::cnnlRandGenerateMultinomial is deprecated and will be removed in the future release.
 *   It is recommended to use ::cnnlRandGenerateMultinomial_v2 instead, which supports
 *   negative \b input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] is_replacement
 *   Input. A boolean scalar. When \b is_replacement is true, the random multinomial operation is
 *   sampling with replacement. When \b is_replacement is false, the random multinomial operation is
 *   sampling without replacement.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 *   Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   random multinomial operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the random multinomial operation. You can get the size of the workspace with
 *   the ::cnnlGetRandGenerateMultinomialWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomMultinomial Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 *   - The input and output can be vectors or matrix. In other words, the input
 *     tensor and output tensor support no more than two dimensions.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.
 *
 * @par Data Layouts
 * - The supported data layout of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - input : half, float.
 *   - output : int32_t.
 *
 * @note
 * - The rows of \b input do not need to sum to one, but must be non-negative, finite
 *   and have a non-zero sum.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> input: data type of CNNL_DTYPE_FLOAT, shape of (2, 50048);
      --> output: data type of CNNL_DTYPE_INT32, shape of (2, 1);
      --> is_replacement: true;
      Then you will get the output:
      --> output: an matrix with 2 rows, each row of which is a sampled index
          from an input row with length of 50048;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlRandGenerateMultinomial(cnnlHandle_t handle,
                            const cnnlRandGenerator_t generator,
                            const cnnlTensorDescriptor_t input_desc,
                            const void *input,
                            const bool is_replacement,
                            void *state,
                            void *workspace,
                            size_t workspace_size,
                            const cnnlTensorDescriptor_t output_desc,
                            void *output);

// Group:RandGenerateMultinomial
/*!
 * @brief Generates random numbers where each row contains indices sampled from the multinomial
 * probability distribution located in the corresponding row of input tensor.
 *
 * Compared with ::cnnlRandGenerateMultinomial, this function allows \b input to be negative.
 * In this case, each row of \b input represents the unnormalized log-probabilities for all classes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] is_replacement
 *   Input. A boolean scalar. When \b is_replacement is true, the random multinomial operation is
 *   sampling with replacement. When \b is_replacement is false, the random multinomial operation is
 *   sampling without replacement.
 * @param[in] is_logits
 *   Input. A boolean scalar. When \b is_logits is true, each row of \b input represents
 *   the unnormalized log-probabilities for all classes. When \b is_logits is false, the rows of
 *   \b input represents filter, which do not need to sum to one, but must be non-negative,
 *   finite and have a non-zero sum.
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 *   Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   random multinomial operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the random multinomial operation. You can get the size of the workspace with
 *   the ::cnnlGetRandGenerateMultinomialWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RandomMultinomial Operator" section in "Cambricon CNNL User Guide" for details.
 * @par Scale Limitation
 *   - The input and output can be vectors or matrix. In other words, the input
 *     tensor and output tensor support no more than two dimensions.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandSetPseudoRandomGeneratorSeed function, the ::cnnlRandGetMTGP32StateSize
 *   function and the ::cnnlRandMakeMTGP32KernelState function before calling this function.
 *
 * @par Data Layouts
 * - The supported data layout of the input tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Data Type
 * - The supported data types of output are as follows:
 *   - input : half, float.
 *   - output : int32_t.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      --> input: data type of CNNL_DTYPE_FLOAT, shape of (2, 50048);
      --> output: data type of CNNL_DTYPE_INT32, shape of (2, 1);
      --> is_replacement: true;
      Then you will get the output:
      --> output: an matrix with 2 rows, each row of which is a sampled index
          from an input row with length of 50048;
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlRandGenerateMultinomial_v2(cnnlHandle_t handle,
                               const cnnlRandGenerator_t generator,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               const bool is_replacement,
                               const bool is_logits,
                               void *state,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output);
// Group:RNN
/*!
 * @brief Computes the backward process of RNN network in the training scenario. The specific
 *        network structure is determined by the description \b rnn_desc set by the user.
 *        Using the input data \b y, \b dy, \b hx, \b dhy, \b cx, \b dcy, \b weightspace,
 *        according to the specific network structure, writes the calculation result into the
 *        output memory \b dx, \b dhx, \b dcx.
 *
 * This function requires two additional MLU memory as the \b reservespace and the \b workspace to
 * improve the RNN network performance. You can get the size of the \b workspace \b workspace_size and
 * \b reservespace \b reservespace_size with the ::cnnlGetRNNTempSizes function, and the size of the
 * \b weightspace \b weightspace_size with the ::cnnlGetRNNWeightSpaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \b seqLengthArray set in \b x_desc or \b y_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] y_desc
 *   Input. The descriptor of input sequence data.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to y.
 * @param[in] x_desc
 *   Input. A previously initialized RNN data descriptor corresponding to the
 *   gradient of the loss function with respect to the RNN primary model input.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient of the loss function
 *   with respect to the RNN primary input x.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] dhy
 *   Input. Pointer to the MLU memory that stores gradient deltas \b dhy.
 * @param[out] dhx
 *   Output. Pointer to the MLU memory that stores gradient deltas \b dhx.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores input cell state tensor.
 * @param[in] dcy
 *   Input. Pointer to the MLU memory that stores gradient deltas \b dcy.
 * @param[out] dcx
 *   Output. Pointer to the MLU memory that stores gradient deltas \b dcx.
 * @param[in] weightspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] weightspace_size
 *   Input. Specifies the size of the buffer in bytes that stores weight.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the workspace with the ::cnnlGetRNNTempSizes function.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the reservespace with the ::cnnlGetRNNTempSizes function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data  layouts set in descriptors are as follows:
 *   - \b x_desc must be set to \p ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \b y_desc must be set to the same layout as \b x_desc.
 *   - \b hx_desc must be set to \p ::CNNL_LAYOUT_ARRAY and \b dimNb in \b hx_desc must be 3.
 *   - \b cx_desc must be set to \p ::CNNL_LAYOUT_ARRAY and \b dimNb in \b cx_desc must be 3.
 *
 * @par Scale Limitation
 * - The size of the dim C of \b x_desc should less than 3000.
 * - The size of the dim C of \b hx_desc should less than 3000.
 * - The \b rnn_mode of rnn_desc \b only supports ::CNNL_LSTM with projection layer.
 * - The \b input_mode of rnn_desc \b only supports ::CNNL_RNN_LINEAR_INPUT.
 * - The \b padding_mode of rnn_desc \b only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 * - The \b math_prec must be int16 on CE3226 and MLU200 series and it must be the same as
 *   \b data_type or int16 on MLU300 series.
 * - The \b dev_seq_lengths must be batch's sequence and descending order and the length of
 *   the \b dev_seq_lengths must be equal to x_desc->dims[0] when the \b cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED.
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \b x_desc and output sequence data \b y_desc
 *   to ::CNNL_SEQDATA_TNC.
 */
cnnlStatus_t CNNL_WIN_API cnnlRNNBackwardData(cnnlHandle_t handle,
                                              const cnnlRNNDescriptor_t rnn_desc,
                                              const int32_t dev_seq_lengths[],
                                              const cnnlSeqDataDescriptor_t y_desc,
                                              const void *y,
                                              const void *dy,
                                              const cnnlSeqDataDescriptor_t x_desc,
                                              void *dx,
                                              const cnnlTensorDescriptor_t hx_desc,
                                              const void *hx,
                                              const void *dhy,
                                              void *dhx,
                                              const cnnlTensorDescriptor_t cx_desc,
                                              const void *cx,
                                              const void *dcy,
                                              void *dcx,
                                              const void *weightspace,
                                              size_t weightspace_size,
                                              void *workspace,
                                              size_t workspace_size,
                                              void *reservespace,
                                              size_t reservespace_size);

// Group:LSTMGates
/*!
 * @brief Returns the size of the MLU memory that is used as an extra workspace to optimize
 * the ::cnnlLSTMGatesForward operation, and pass the intermediate results to
 * ::cnnlLSTMGatesBackward operation.
 *
 * For more information about the extra \b workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the LSTM operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] gates_desc
 *   Input. The descriptor of gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] reservespace_size
 *   Output. The size of the reservespace in bytes that needs to be used in
 *   the LSTM operation. The reservespace is used to pass intermediate results.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace memory should be passed to the ::cnnlLSTMGatesForward function,
 *   and ::cnnlLSTMGatesBackward function to perform the LSTM forward or backward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetLSTMGatesTempSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t gates_desc,
                                                   size_t *reservespace_size);

// Group:LSTMGates
/*!
 * @brief Computes the backward process of LSTM cell in the training scenario. The specific
 *        network structure is determined to get the gradient of hidden gates in LSTM cell.
 *        Based on input data \b cx, \b cy, \b grad_hy, \b grad_cy, and \b reservespace,
 *        it writes the calculation result into the output memory \b grad_gates and \b grad_cx.
 *
 * This function requires additional MLU memory as the \b reservespace to
 * improve the performance. You can get the size of the \b reservespace
 * \b reservespace_size with the ::cnnlGetLSTMGatesTempSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the LSTM operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_hy_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_hy
 *   Input. Pointer to the MLU memory that stores gradient of output hidden state tensor.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores input cell state tensor.
 * @param[in] cy
 *   Input. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] dcy
 *   Input. Pointer to the MLU memory that stores gradient of output cell state tensor.
 * @param[out] dcx
 *   Output. Pointer to the MLU memory that stores gradient of input cell state tensor.
 * @param[in] grad_gates_desc
 *   Input. The descriptor of gates tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_gates
 *   Output. Pointer to the MLU memory that stores gradient of gates tensor which consists of
 *   input gates, forget gates, hidden gates and output gates, and the order is \b CNNL_LSTM_IFGO.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of LSTM operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the LSTM operation.
 *   You can get the size of the reservespace with the ::cnnlGetLSTMGatesTempSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "LSTMGatesBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \b grad_hy_desc must be set to \p ::CNNL_LAYOUT_NC and \b dimNb in \b grad_hy_desc must be 2.
 *   - \b cx_desc must be set to \p ::CNNL_LAYOUT_NC and \b dimNb in \b cx_desc must be 2.
 *   - \b grad_gates_desc must be set to \p ::CNNL_LAYOUT_NC and \b dimNb in \b grad_gates_desc must be 2.
 *
 * @par Scale Limitation
 *   The \b grad_hy will be initialized to zero when \b grad_hy is set to NULL, and \b grad_cy will
 *   also be initialized to zero when \b grad_cy is set to NULL.
 *
 * @par API Dependency
 * - Before calling this function to implement LSTM operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details. And you need to call ::cnnlGetLSTMGatesForward function before calling this
 *   function.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlLSTMGatesBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t grad_hy_desc,
                                                const void *grad_hy,
                                                const cnnlTensorDescriptor_t cx_desc,
                                                const void *cx,
                                                const void *cy,
                                                const void *grad_cy,
                                                void *grad_cx,
                                                const cnnlTensorDescriptor_t grad_gates_desc,
                                                void *grad_gates,
                                                void *reservespace,
                                                size_t reservespace_size);

/*!
 * @brief Enumeration variables describing the mode that is used in the quantization strategy
 *        function.
 *
 */
typedef enum {
  CNNL_ADAPTIVE_BITWIDTH_AND_INTERVAL = 0, /*!< The adaptive bitwidth and interval is applied.*/
  CNNL_ADAPTIVE_INTERVAL = 1, /*!< The adaptive interval and constant bitwidth is applied.*/
  CNNL_ADAPTIVE_BITWIDTH = 2, /*!< The adaptive bitwidth and constant interval is applied.*/
} cnnlQuantizeStrategyMode_t;

/*!
 * @brief Enumeration variables describing the maximum output bitwidth that is used in the
 *        quantization strategy function.
 *
 */
typedef enum {
  CNNL_MAX_BITWIDTH_INT16 = 0, /*!< The output bitwidth cannot exceed 16.*/
  CNNL_MAX_BITWIDTH_INT31 = 1, /*!< The output bitwidth cannot exceed 31.*/
} cnnlQuantizeStrategyMaxBitwidth_t;

/*!
* @brief The cnnlQuantizeStrategyParam_t is a structure describing the hyper parameters
*        that is used in the quantization strategy function.
*
*/
typedef struct cnnlQuantizeStrategyParam {
  float alpha; /*!< Related to moving position with the scope of (0, 0.4).*/
  float beta;  /*!< Related to interval with the scope of (0, 1).*/
  float gamma; /*!< Related to interval with the scope of [0, 100].*/
  float delta; /*!< Related to bitwidth with the scope of (0, 1000).*/
  float th;    /*!< Related to bitwidth with the scope of (0, 0.5).*/
} cnnlQuantizeStrategyParam_t;

// Group:QuantizeStrategy
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 *        workspace to optimize the quantization strategy operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the quantization strategy operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeStrategyWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     size_t *workspace_size);
// Group:QuantizeStrategy
/*!
 * @brief Creates an operation of quantization strategy. The operation is used to
 *        update \b bitwidth, \b position, moving position, \b interval,
 *        \b is_exceed_max_bitwidth.
 *        For more information, see "Cambricon CNNL User Guide".
 *
  * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] quant_strategy_mode
 *   Input. An enum with information of adaptive quantization modes used in this operation.
 *   For detailed information, see ::cnnlQuantizeStrategyMode_t.
 * @param[in] quant_strategy_param
 *   Input. A struct with information of hyper parameters used in this operation. For detailed
 *   information, see ::cnnlQuantizeStrategyParam_t.
 * @param[in] max_bitwidth
 *   Input. An enum with information of maximum bitwidth used in this operation. For detailed
 *   information, see ::cnnlQuantizeStrategyMaxBitwidth_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   quantization strategy operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the quantization strategy operation. You can get the size of the workspace with
 *   the ::cnnlGetQuantizeStrategyWorkspaceSize function.
 * @param[in] bitwidth_input
 *   Input. Pointer to the scalar of input bitwidth.
 * @param[in] position_input
 *   Input. Pointer to the scalar of input position.
 * @param[in] moving_position_input
 *   Input. Pointer to the scalar of input moving position.
 * @param[out] bitwidth_output
 *   Output. Pointer to the scalar of output bitwidth.
 * @param[out] position_output
 *   Output. Pointer to the scalar of output position.
 * @param[out] moving_position_output
 *   Output. Pointer to the scalar of output moving position.
 * @param[out] interval
 *   Output. Pointer to the scalar of output interval.
 * @param[out] is_exceed_max_bitwidth
 *   Output. Pointer to the scalar of output. When the maximum bitwidth exceeds the
 *   input \b max_bitwidth, it will output 1. Otherwise, it will output 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - bitwidth: int32.
 * - position: int32.
 * - moving position: float.
 * - interval: int32.
 * - is_exceed_max_bitwdith: int32.
 *
 * @par API Dependency
 * - ::cnnlGetQuantizeStrategyWorkspaceSize should be called to get the workspace
 *     size before ::cnnlQuantizeStrategy.
 *
 * @note
 * - The \b bitwidth_input pointer can be the same with the \b bitwidth_output pointer.
 * - The \b position_input pointer can be the same with the \b position_output pointer.
 * - The \b moving_position_input pointer can be the same with the \b moving_position_output
 *     pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeStrategy(cnnlHandle_t handle,
                     const cnnlQuantizeStrategyMode_t quant_strategy_mode,
                     const cnnlQuantizeStrategyParam_t quant_strategy_param,
                     const cnnlQuantizeStrategyMaxBitwidth_t max_bitwidth,
                     const cnnlTensorDescriptor_t input_desc,
                     const void *input,
                     void *workspace,
                     size_t workspace_size,
                     const void *bitwidth_input,
                     const void *position_input,
                     const void *moving_position_input,
                     void *bitwidth_output,
                     void *position_output,
                     void *moving_position_output,
                     void *interval,
                     void *is_exceed_max_bitwidth);

/*!
 * @brief Enumeration variables describing the mode of quantization method.
 *
 */
typedef enum {
  CNNL_QUANTIZE_POSITION = 0,
  /*!< Quantization method with position factor and without scale factor.*/
  CNNL_QUANTIZE_POSITION_SCALE = 1,
  /*!< Quantization method with position and scale factors.*/
  CNNL_QUANTIZE_POSITION_SCALE_OFFSET = 2,
  /*!< Asymmetric quantization method with position, scale, and offset factors.*/
} cnnlQuantizeMode_t;

// Group:QuantizeParam
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 *        workspace to optimize the computing quantization parameters operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the computing quantization parameters operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 *   The allocated extra workspace should be passed to the ::cnnlQuantizeParam function
 *   to perform the computing quantization parameters operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeParamWorkspaceSize(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   size_t *workspace_size);

// Group:QuantizeParam
/*!
 * @brief Creates an operation of computing quantization parameters used in quantization. For more
 * information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] bitwidth
 *   Input. A scalar of quantization width, it supports 8, 16, 31.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   computing quantization parameter operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the computing quantization parameter operation. You can get the size of the workspace with
 *   the ::cnnlGetQuantizeParamWorkspaceSize function.
 * @param[out] position_output
 *   Output. Pointer to the scalar of position.
 * @param[out] scale_output
 *   Output. Pointer to the scalar of scale.
 * @param[out] offset_output
 *   Output. pointer to the scalar of offset.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - position-int32, scale-float, offset-int32.
 *
 * @par API Dependency
 * - ::cnnlGetQuantizeParamWorkspaceSize should be called to get the workspace
 *   size before ::cnnlQuantizeParam.
 *
 * @note
 * - When the mode is ::CNNL_QUANTIZE_POSITION, scale_output and offset_output can either be
 *   NULL or not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE, offset_output can either be NULL or
 *   not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET, position_output, scale_output and
 *   offset_output can not be NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
*/
cnnlStatus_t CNNL_WIN_API cnnlQuantizeParam(cnnlHandle_t handle,
                                            cnnlQuantizeMode_t mode,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            int bitwidth,
                                            void *workspace,
                                            size_t workspace_size,
                                            void *position_output,
                                            void *scale_output,
                                            void *offset_output);
// Group:Quantize
/*!
 * @brief Creates an operation of quantization for quantizing floating-point data to fixed-point
 *  data with the given input descriptor \b input_desc of the input data\b input, and the
 *  quantization mode \b mode. ::cnnlQuantizeV1 supports the quantization parameter inputs from
 *  host. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Input. Pointer to the MLU memory that stores the quantized data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: int8, int16, int31.
 *
 * @par API Dependency
 * - The ::cnnlSetTensorDescriptorPosition, ::cnnlSetTensorDescriptorPositionAndScale or
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset function should be called to set the
 *   quantization parameters to the output descriptor \b output_desc before this function.
 *   ::cnnlSetTensorDescriptorPosition is used in ::CNNL_QUANTIZE_POSITION mode.
 *   ::cnnlSetTensorDescriptorPositionAndScale is used in ::CNNL_QUANTIZE_POSITION_SCALE mode.
 *   ::cnnlSetTensorDescriptorPositionScaleAndOffset is used in
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET mode.
 *
 * @note
 * - When the data type of \b output_desc descriptor is CNNL_DTYPE_INT31,
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
*/
cnnlStatus_t CNNL_WIN_API cnnlQuantizeV1(cnnlHandle_t handle,
                                         cnnlQuantizeMode_t mode,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:Quantize
/*!
 * @brief Creates an operation of quantization for quantizing floating-point data to fixed-point
 *  data with the given input descriptor \b input_desc of the input data\b input, and the
 *  quantization mode \b mode. ::cnnlQuantizeV2 supports the quantization parameter inputs from
 *  MLU device. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the quantization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. An enum with information of quantization mode, see ::cnnlQuantizeMode_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] position
 *   Input. A pointer to the scalar of fixed position.
 * @param[in] scale
 *   Input. A pointer to the scalar of scale factor.
 * @param[in] offset
 *   Input. A pointer to the scalar of offset.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Input. Pointer to the MLU memory that stores the quantized data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: int8, int16, int31.
 *
 * @note
 * - When the data type of \b output_desc descriptor is CNNL_DTYPE_INT31,
 *   ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET is not supported.
 * - When the mode is ::CNNL_QUANTIZE_POSITION, scale and offset can either be NULL or
 *   not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE, offset can either be NULL or not NULL.
 * - When the mode is ::CNNL_QUANTIZE_POSITION_SCALE_OFFSET, position, scale and offset
 *   cannot be NULL.
 * - The difference between ::cnnlQuantizeV1 and ::cnnlQuantizeV2 is that:
 *   ::cnnlQuantizeV1 supports the quantization parameter inputs from host.
 *   ::cnnlQuantizeV2 supports the quantization parameter inputs from MLU device.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
*/
cnnlStatus_t CNNL_WIN_API cnnlQuantizeV2(cnnlHandle_t handle,
                                         cnnlQuantizeMode_t mode,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const void *position,
                                         const void *scale,
                                         const void *offset,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);


/******************************************************************************
 * Cambricon CNNL OP: CTC_Loss
 ******************************************************************************/
/*!
 * @brief Enumeration variables controling the input normalization mode for CTC_Loss operation.
 * This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_NONE_NORMALIZATION = 0,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the unnormalized activation,
       and the output gradients is the gradient of loss with respect to the unnormalized
       activation. Internally the probability is computed by softmax normalization.*/
  CNNL_SOFTMAX_NORMALIZATION = 1,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the normalized probability
       by softmax, and the output gradients is the gradient of loss with respect to the
       unnormalized activation.*/
  CNNL_LOG_SOFTMAX_NORMALIZATION = 2,
  /*!< The input data of ::cnnlCTCLoss() function is expected to be the normalized probability
       by log_softmax, and the output gradients is the gradient of loss with respect to the
       unnormalized activation.*/
} cnnlCTCLossNormalizationMode_t;

/*!
 * @brief Enumeration variables describing the dealing mode of loss for CTC_Loss operation.
 * This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_REDUCE_MODE_NONE = 0,
  /*!< The loss of ::cnnlCTCLoss will not be reduced, and the dimension of loss should be equal to
       the batch size of input.*/
  CNNL_REDUCE_MODE_SUM = 1,
  /*!< The loss of ::cnnlCTCLoss will be summed.*/
  CNNL_REDUCE_MODE_MEAN_BY_BATCH = 2,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the batch size of input.*/
  CNNL_REDUCE_MODE_MEAN_BY_INPUT_LENGTHS = 3,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the input lengths of ::cnnlCTCLoss.*/
  CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH = 4,
  /*!< The loss of ::cnnlCTCLoss will be summed and divided by the label lengths of ::cnnlCTCLoss
       and the batch size of input.*/
} cnnlCTCLossReduceMode_t;


/*!
 * @brief Enumeration variables describing whether to zero infinite losses and the associated
 * gradients. This type is used with ::cnnlSetCTCLossDescriptor.
 *
 */
typedef enum {
  CNNL_ZERO_INFINITY = 0,
  /*!< The type will zero infinite losses and the associated gradients.*/
  CNNL_NONE_ZERO_INFINITY = 1,
  /*!< The type will not zero infinite losses and the associated gradients.*/
  CNNL_NONE_ZERO_INFINITY_PROBS_GRADS = 2,
  /*!< The type will not zero infinite losses, and it will set the associated gradients
       with the probabilities normalized by softmax.*/
} cnnlCTCLossZeroInfinityMode_t;

// Group:CTCLoss
/*!
 * @brief Creates a tensor descriptor that holds cnnlCTCLossNormalizationMode_t,
 * cnnlCTCLossReduceMode_t, cnnlCTCLossZeroInfinityMode_t, blank, maximum input length,
 * maximum label length.
 *
 * @param[in] ctc_loss_desc
 *   Input. Pointer to the struct that holds information about the CTC_Loss descriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * par API Dependency
 *  The ::cnnlDestroyCTCLossDescriptor function needs to be called to destroy the
 * CTC_Loss descriptor later.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateCTCLossDescriptor(cnnlCTCLossDescriptor_t *ctc_loss_desc);

// Group:CTCLoss
/*!
 * @brief Initializes the CTC_Loss descriptor pointed by \b ctc_loss_desc that is previously
 * created with the ::cnnlCreateCTCLossDescriptor function.
 *
 * @param[in] ctc_loss_desc
 *   Input. The description of the CTC_Loss operation. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] norm_mode
 *   Input. The normalization mode of the input data. For detailed information,
 *   see ::cnnlCTCLossNormalizationMode_t.
 * @param[in] zero_infinity
 *   Input. The zero infinity mode whether to zero infinite losses and the associated gradients.
 *   For detailed information, see ::cnnlCTCLossZeroInfinityMode_t.
 * @param[in] blank
 *   Input. Blank label.
 * @param[in] max_input_length
 *   Input. A scalar of maximum input length.
 * @param[in] max_label_length
 *   Input. A scalar of maximum label length.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 *  - The supported combinations of \b norm_mode, \b reduce_mode, \b zero_infinity are shown below
 *    with the following order:
 *    \b norm_mode - \b reduce_mode - \b zero_infinity
 *    The supported combinations are:
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH
 *    - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH
 *    - ::CNNL_NONE_ZERO_INFINITY
 * - \b blank must be 0 or channel - 1.
 * - \b max_input_length must be less than or equal to input's first dimension.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc,
                         const cnnlCTCLossNormalizationMode_t norm_mode,
                         const cnnlCTCLossReduceMode_t reduce_mode,
                         const cnnlCTCLossZeroInfinityMode_t zero_infinity,
                         int blank,
                         int max_input_length,
                         int max_label_length);

// Group:CTCLoss
/*!
 * @brief Retrieves a CTC_Loss descriptor \b ctc_loss_desc that is previously created with the
 * ::cnnlCreateCTCLossDescriptor function, and sets the information about
 * ::cnnlCTCLossNormalizationMode_t, ::cnnlCTCLossReduceMode_t, ::cnnlCTCLossZeroInfinityMode_t,
 * blank, maximum input length, maximum label length.
 *
 * @param[in] ctc_loss_desc
 *   Input. The description of the CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[out] norm_mode
 *   Output. Pointer to the host memory that holds information about the normalization mode.
 *   For detailed information, see ::cnnlCTCLossNormalizationMode_t.
 * @param[out] reduce_mode
 *   Output. Pointer to the host memory that holds information about the reduce mode.
 *   For detailed information, see ::cnnlCTCLossReduceMode_t.
 * @param[out] zero_infinity
 *   Output. Pointer to the host memory that holds information about the zero infinity mode.
 *   For detailed information, see ::cnnlCTCLossZeroInfinityMode_t.
 * @param[out] blank
 *   Output. Pointer to the host memory that holds information about the blank label.
 * @param[out] max_input_length
 *   Output. Pointer to the host memory that holds information about the maximum input length.
 * @param[out] max_label_length
 *   Output. Pointer to the host memory that holds information about the maximum label length.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc,
                                                   cnnlCTCLossNormalizationMode_t *norm_mode,
                                                   cnnlCTCLossReduceMode_t *reduce_mode,
                                                   cnnlCTCLossZeroInfinityMode_t *zero_infinity,
                                                   int *blank,
                                                   int *max_input_length,
                                                   int *max_label_length);

// Group:CTCLoss
/*!
 * @brief Destroys a CTC_Loss descriptor that was created by
 *        ::cnnlCreateCTCLossDescriptor.
 *
 * @param[in] ctc_loss_desc
 *   Input. A CTC_Loss descriptor created by ::cnnlCreateCTCLossDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyCTCLossDescriptor(cnnlCTCLossDescriptor_t ctc_loss_desc);

// Group:CTCLoss
/*!
 * @brief Returns \b workspace_size which is the size of the MLU memory that is used as an extra
 *        workspace to optimize the CTC_Loss operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] ctc_loss_desc
 *   Input. The descriptor of CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of input. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] is_backward
 *   Input. A boolean scalar. When \b is_backward is false, the ctc_loss forward operation is performed.
 *   When \b is_backward is true, the ctc_loss backward operation is performed.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the CTC_Loss operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlCTCLoss function
 *   to perform the CTC_Loss operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlGetCTCLossWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlCTCLossDescriptor_t ctc_loss_desc,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       bool is_backward,
                                                       size_t *workspace_size);

// Group:CTCLoss
/*!
 * @brief Calculates loss between a continuous time series and a target sequence. CTC is
 * the acronym of Connectionist Temporal Classification. This function returns the CTC costs
 * and gradients, given the unnormalized activation and labels.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] CTC_Loss_desc
 *   Input. The descriptor of CTC_Loss. For detailed information,
 *   see ::cnnlCTCLossDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] labels_desc
 *   Input. The descriptor of the labels tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] labels
 *   Input. Pointer to the MLU memory that stores the labels, which is an initialized list
 *   of labels.
 * @param[in] input_lengths_desc
 *   Input. The descriptor of the input lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_lengths
 *   Input. Pointer to the MLU memory that stores the input lengths, which is an initialized
 *   list of the lengths of the timing steps in each batch.
 * @param[in] label_lengths_desc
 *   Input. The descriptor of the label lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] label_lengths
 *   Input. Pointer to the MLU memory that stores the labels data, which is an initialized
 *   list of the lengths of the label in each batch.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   CTC_Loss operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the CTC_Loss operation. You can get the size of the workspace with
 *   the ::cnnlGetCTCLossWorkspaceSize function.
 * @param[in] loss_desc
 *   Input. The descriptor of the loss tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] loss
 *   Input. Pointer to the MLU memory that stores the loss of CTC.
 * @param[in] grads_desc
 *   Input. The descriptor of the \b grads tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradients of CTC. These computed
 *   gradient outputs are with respect to the unnormalized activation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 *  @par Formula
 *  - See "CTC_Loss Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - input: float.
 * - labels: int.
 * - input_lengths: int.
 * - label_lengths: int.
 * - loss: float.
 * - grads: float.
 *
 * @par API Dependency
 * - ::cnnlGetCTCLossWorkspaceSize should be called to get the workspace
 *   size before ::cnnlCTCLoss.
 *
 * @note
 *  - The supported combinations of \b norm_mode, \b reduce_mode, \b zero_infinity are shown below
 *    with the following order:
 *    \b norm_mode - \b reduce_mode - \b zero_infinity
 *    The supported combinations are:
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_NONE_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_NONE - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_SUM - ::CNNL_NONE_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH
 *    - ::CNNL_ZERO_INFINITY
 *    - ::CNNL_LOG_SOFTMAX_NORMALIZATION - ::CNNL_REDUCE_MODE_MEAN_BY_LABEL_LENGTH_AND_BATCH
 *    - ::CNNL_NONE_ZERO_INFINITY
 *  - The blank label of ctc_loss_desc should be 0 or channel - 1.
 *  - The maximum input length of ctc_loss_desc must be less than or equal to input's
 *    highest dimension.
 *  - The layout of \b input and \b grads only support ::CNNL_LAYOUT_TNC.
 *  - The layout of \b labels only supports ::CNNL_LAYOUT_ARRAY.
 *  - The layout of \b input_lengths only supports ::CNNL_LAYOUT_ARRAY.
 *  - The layout of \b label_lengths only supports ::CNNL_LAYOUT_ARRAY.
 *  - When ::cnnlCTCLoss is the operation of CTC_Loss forward, \b grads_desc and \b grads
 *    must be NULL.
 *  - When ::cnnlCTCLoss is the operation of CTC_Loss backward, \b grads_desc and \b grads
 *    must not be NULL.
 *  - The size of each inputs and outputs should be less than the biggest number of int32_t.
 *  - Suppose the shape of \b input is [T, N, C], then T is recommended to be in the range
 *    of [1, 1000], N is recommended to be in the range [1, 128] and C is recommended to be in
 *    the range of [1, 1000] for higher precision when \b zero_infinity is ::CNNL_ZERO_INFINITY.
 *  - T is recommended to be in the range of [1, 150], N is recommended to be in the range [1, 128]
 *    and C is recommended to be in the range of [1, 500] for higher precision when \b zero_infinity
 *    is ::CNNL_NONE_ZERO_INFINITY_PROBS_GRADS.
 *  - The difference between the minimum value of \b input and the maximum value of \b input
 *    along the lowest dimension should be in the range of [-15, 5, 0] for higher precision.
 *  - When the channel of \b input is 1 or \b input contains NaN/infinity, it may cause
 *    undefined behavior.
 *  - Any elements in \b label cannot be the same with \b blank of \b ctc_loss_desc, otherwise it
 *    may cause undefined behavior.
 *  - The shape of label is 1-D(sum of label_lengths) or 2-D([batch_size,
 *    max_label_length])tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
*/
cnnlStatus_t CNNL_WIN_API cnnlCTCLoss(cnnlHandle_t handle,
                                       const cnnlCTCLossDescriptor_t ctc_loss_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t labels_desc,
                                       const void *labels,
                                       const cnnlTensorDescriptor_t input_lengths_desc,
                                       const void *input_lengths,
                                       const cnnlTensorDescriptor_t label_lengths_desc,
                                       const void *label_lengths,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t loss_desc,
                                       void *loss,
                                       const cnnlTensorDescriptor_t grads_desc,
                                       void *grads);

/******************************************************************************
 * Cambricon CNNL OP: Deformable Convolution
 ******************************************************************************/

// Group:Deformable Convolution
/*!
 * @brief Creates a descriptor pointed by \b dcn_desc for a deformable convolution forward
 *        or backward operation, and allocates memory for holding the information about the
 *        deformable convolution operation. The information is defined in
 *        ::cnnlDCNDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[in] dcn_desc
 *  Input. A host pointer to the deformable convolution descriptor that holds information about the
 *  deformable convolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetDCNDescriptor function to initialize
 *   and set the information to the deformable convolution descriptor.
 * - You need to call the ::cnnlDestroyDCNDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateDCNDescriptor(cnnlDCNDescriptor_t *dcn_desc);

// Group:Deformable Convolution
/*!
 * @brief Initializes the deformable convolution descriptor \b dcn_desc that is previously
 *        created with the ::cnnlCreateDCNDescriptor function, and sets the information about the
 *        deformable convolution forward and backward operation to the deformable convolution descriptor
 *        \b dcn_desc. The information includes the number of deformable convolution dimensions \b dimNb,
 *        the padding size for each dimension \b pad, the stride of the sliding window for each dimension
 *        \b stride, the dilation factor for each dimension \b dilation, the number of groups of input
 *        offset to be split along the input channel \b deformable_group, the number of convolution group
 *        \b conv_group, and the maximum image number per deformable convolution computing \b im2col_step.
 *
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the deformable convolution operation.
 *   Currently, the value of this parameter can only be set to 4 and should be the
 *   same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the deformable convolution operation.
 *   For each dimension, the padding size represents the number of zeros to be concatenated at the
 *   start and end of that dimension. For 2-dimensional deformable convolution, the padding is
 *   on top, bottom, left, and right.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the deformable convolution operation. For each dimension, the filter stride represents
 *   the number of elements to slide over the input tensor. For 2-dimensional deformable
 *   convolution, the stride should be set in height and width order.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the deformable convolution operation. For each dimension, the dilation factor
 *   represents the spacing between the kernel points. For 2-dimensional deformable convolution,
 *   the dilation should be set in height and width order.
 * @param[in] deformable_group
 *   Input. The number of deformable offset groups that split the input offset along the channel
 *   of input tensor. Each deformable group is deformed separately for detecting different input parts.
 * @param[in] conv_group
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each convolution group is convolved separately. The filter used for
 *   each group is the filter tensor divides \b conv_group. The result of
 *   the deformable convolution operation is the concatenation of all the group convolution results
 *   along the output channel dimension.
 * @param[in] im2col_step
 *   Input. The maximum number of images per deformable convolution computing. This parameter
 *   affects both the workspace size and the computing efficiency.
 *   A larger \b im2col_step will consume a larger workspace size and have a higher performance,
 *   while a smaller one will consume a smaller workspace size and have a lower performance.
 * @param[in] compute_type
 *   Input. The data type of temporary result in convolution operation. Only supports
 *   \b CNNL_DTYPE_FLOAT type.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateDCNDescriptor should be called.
 *
 * @note
 * - Currently, only supports 4-dimensional input tensor for deformable convolution forward
 *   and backward operation.
 * - The values of \b pad should be greater than or equal to 0.
 * - The values of \b stride should be greater than or equal to 1.
 * - The values of \b dilation should be greater than or equal to 1.
 * - The value of \b deformable_group should be greater than or equal to 1 and
 *   less than or equal to the number of channels in the input tensor, and input channel must be
 *   divisible by \b deformable_group.
 *   - If \b deformable_group is set to 1, the same input offset is applied to all channels
 *   of one pixel.
 *   - If the value of \b deformable_group is between 1 and the number of channels of input tensor,
 *     the input channel will be split into \b deformable_group parts. Each part is responsible for
 *     detecting different input parts, which results in a more flexible geometric transformation.
 * - The value of \b conv_group should be greater than or equal to 1 and less than or equal to the
 *   number of channels in the input tensor, and input channels and output channels must both be
 *   divisible by \b conv_group.
 * - The value of \b im2col_step should be greater than or equal to 1 and less than or equal to
 *   the number of batch size in the input tensor, and input batch should be divisible by
 *   \b im2col_step.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetDCNDescriptor(cnnlDCNDescriptor_t dcn_desc,
                                               int dimNb,
                                               const int pad[],
                                               const int stride[],
                                               const int dilation[],
                                               int deformable_group,
                                               int conv_group,
                                               int im2col_step,
                                               const cnnlDataType_t compute_type);

// Group:Deformable Convolution
/*!
 * @brief Destroys a deformable convolution descriptor \b dcn_desc that is previously created with
 *        the ::cnnlCreateDCNDescriptor function.
 *
 * @param[in] dcn_desc
 *   Input. The deformable convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlDCNForward, ::cnnlDCNBackwardData,
 *   or ::cnnlDCNBackwardWeight function. Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the deformable convolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyDCNDescriptor(cnnlDCNDescriptor_t dcn_desc);

// Group:Deformable Convolution
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution forward operation.
 *
 * The size of extra workspace is based on the given information of the deformable convolution
 * forward operation, including the deformable convolution descriptor \b dcn_desc,
 * input tensor descriptor \b input_desc, offset tensor
 * descriptor \b offset_desc, mask tensor descriptor \b mask_desc, filter tensor descriptor
 * \b filter_desc, bias tensor descriptor \b bias_desc, and output tensor descriptor \b output_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. Set this parameter to NULL if bias is not needed.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b input_desc, \b offset_desc, \b mask_desc (optional),
 *   \b filter_desc, \b bias_desc (optional) before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNForward function to perform
 *   the deformable convolution forward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNForwardWorkspaceSize(cnnlHandle_t handle,
                               const cnnlDCNDescriptor_t dcn_desc,
                               const cnnlTensorDescriptor_t input_desc,
                               const cnnlTensorDescriptor_t offset_desc,
                               const cnnlTensorDescriptor_t mask_desc,
                               const cnnlTensorDescriptor_t filter_desc,
                               const cnnlTensorDescriptor_t bias_desc,
                               const cnnlTensorDescriptor_t output_desc,
                               size_t *workspace_size);

// Group:Deformable Convolution
/*!
 * @brief Performs a 2-D deformable convolution forward operation. Compared with the standard
 *        convolution, the deformable convolution introduces 2-D offsets and masks to make
 *        the convolution adapt to the geometric variation of objects.
 *        Offsets act on the regular grid sampling locations, which enables a free form
 *        deformation of the sampling grid. Mask is a modulation mechanism to improve the ability
 *        to focus on pertinent image regions. Both offsets and masks are
 *        learnable parameters obtained from additional convolutional layers.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues. For
 *   detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of deformable convolution. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   filter_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL
 *   when mask is not requested.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. Set this parameter to NULL when bias is not
 *   requested. For detailed information see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor. Set this parameter to NULL when bias is not
 *   requested.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution operation. For more information about workspace, see
 *   "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the deformable
 *   convolution operation. You can get the size of the workspace with the
 *   ::cnnlGetDCNForwardWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The shape of output is the same with the
 *   shape of output in the convolution.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \b input, \b offset, \b mask, \b filter, \b bias, and \b output must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, filter, bias, output: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \b input and \b filter on MLU200 series and CE3226.
 *   - \b input onchip data type: int16, int31.
 *   - \b filter onchip data type: int16, int31.
 * - \b input offchip data type can be combined with any supported onchip data types.
 * - \b filter offchip data type can be combined with any supported onchip data types.
 * - This function also supports float-point computation on MLU300 series or above.
 *   To perform float-point computation, the onchip data type of \b input and \b filter
 *   should be \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, filter, bias tensor, and output tensor are
 *   as follows:
 *   - input, offset, mask, filter, output: \p CNNL_LAYOUT_NHWC.
 *   - bias: \p CNNL_LAYOUT_ARRAY
 *
 * @par Scale Limitation
 * - The input, offset, mask, filter, bias, output and the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch size of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor accordingly. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch size of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor accordingly.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - The value of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.
 * @par API Dependency
 * - Before calling this function to implement deformable convolution, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the im2col_step equal to the batch
 *   size of the input tensor.
 *
 * @note
 * - The alignment of \b input, \b offset, \b mask, \b filter, \b bias, \b output,
 *   should be contiguous in the MLU memory.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the deformable convolution forward operation is as follows:
     @verbatim

     input tensor by 1 * 3 * 3 * 2 --> input:
     [[[[0.7944, 0.4922], [0.2008, 0.2081], [0.9998, 0.3053]],
       [[0.1815, 0.9210], [0.8463, 0.1819], [0.9159, 0.4917]],
       [[0.6668, 0.2843], [0.8364, 0.2765], [0.7150, 0.6780]]]]
     offset tensor by 1 * 3 * 3 * 2 --> offset:
     [[[[-0.6317, -1.4928], [-0.0696,  1.1910], [ 0.8778,  0.5145]],
       [[-0.9248, -0.9889], [ 0.6157,  0.2157], [-1.1540, -0.1283]],
       [[-0.5704,  1.0237], [ 0.7956,  1.1203], [-0.0129, -0.2686]]]]
     mask tensor by 1 * 3 * 3 * 1 --> mask:
     [[[[ 0.4581], [-1.1605], [ 0.5951]],
       [[ 0.4313], [ 0.1070], [ 0.0225]],
       [[ 0.7484], [ 0.6262], [ 1.1908]]]]
     filter tensor by 2 * 1 * 1 * 2 --> filter:
     [[[[0.8928, 0.9682]]], [[[0.9301, 0.6817]]]]
     bias tensor by 2 --> bias:
     [0.4356, 0.0840]

     param:
       pad: (0, 0, 0, 0), stride: (1, 1), dilation: (1, 1)

     output tensor by 1 * 3 * 3 * 2 --> output:
     [[[[ 0.4356,  0.0840], [-0.6024, -0.9101], [ 0.8056,  0.4252]],
       [[ 0.4412,  0.0890], [ 0.5478,  0.1898], [ 0.4562,  0.1037]],
       [[ 1.1652,  0.7876], [ 0.5814,  0.2109], [ 1.8874,  1.3752]]]]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API cnnlDCNForward(cnnlHandle_t handle,
                                         const cnnlDCNDescriptor_t dcn_desc,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t offset_desc,
                                         const void *offset,
                                         const cnnlTensorDescriptor_t mask_desc,
                                         const void *mask,
                                         const cnnlTensorDescriptor_t filter_desc,
                                         const void *filter,
                                         const cnnlTensorDescriptor_t bias_desc,
                                         const void *bias,
                                         void *workspace,
                                         size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:Deformable Convolution
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution backward data operation.
 *
 * The size of extra workspace is based on the given information of the deformable convolution
 * backward data operation, including the deformable convolution descriptor \b dcn_desc,
 * input tensor descriptor \b input_desc, offset tensor
 * descriptor \b offset_desc, mask tensor descriptor \b mask_desc, filter tensor descriptor
 * \b filter_desc, gradient with respect to the output tensor \b grad_output_desc, the gradient
 * with respect to the input tensor \b grad_input_desc, the gradient with respect to the offset
 * tensor \b grad_offset, and the gradient with respect to the mask tensor \b grad_mask_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor as a filter in the deformable convolution
 *   operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the gradient with respect to the input tensor.
 *   This parameter is requested to be the same as \b input_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_offset_desc
 *   Input. The descriptor of the gradient with respect to the offset tensor.
 *   This parameter is requested to be the same as \b offset_desc.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient with respect to the mask tensor.
 *   This parameter is requested to be the same with \b mask_desc. Set this parameter to NULL when mask and
 *   grad_mask are not needed. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution backward data operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b input, \b offset, \b mask (optional), \b filter,
 *   \b grad_output, \b grad_input, \b grad_offset, \b grad_mask (optional) before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNBackwardData function to perform
 *   the deformable convolution backward data operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNBakcwardDataWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlDCNDescriptor_t dcn_desc,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const cnnlTensorDescriptor_t offset_desc,
                                    const cnnlTensorDescriptor_t mask_desc,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const cnnlTensorDescriptor_t grad_output_desc,
                                    const cnnlTensorDescriptor_t grad_input_desc,
                                    const cnnlTensorDescriptor_t grad_offset_desc,
                                    const cnnlTensorDescriptor_t grad_mask_desc,
                                    size_t *workspace_size);

// Group:Deformable Convolution
/*!
 * @brief Performs the back-propagation of a deformable convolution operation to compute
 *        the gradient with respect to input \b grad_input, offset \b grad_offset, and mask
 *        \b grad_mask based on the gradient of response \b grad_output.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetDCNBakcwardDataWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deformable convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   filter_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL when mask is not
 *   requested.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deformable convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the output.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deformable convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetDCNBakcwardDataWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the gradient with respect to the input tensor.
 *   This parameter is requested to be the same as \b input_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \b input.
 * @param[in] grad_offset_desc
 *   Input. The descriptor of the gradient with respect to the offset tensor.
 *   This parameter is requested to be the same as \b offset_desc.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_offset
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \b offset.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient with respect to the mask tensor.
 *   This parameter is requested to be the same with \b mask_desc. Set this parameter to NULL when mask and
 *   grad_mask are not needed. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_mask
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \b mask.
 *   Set this parameter to NULL when mask and grad_mask are not needed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 *
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \b input, \b offset, \b mask, \b filter, \b grad_output,
 *   \b grad_input, \b grad_offset, and \b grad_mask must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, filter, grad_output, grad_input, grad_offset, grad_mask: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \b grad_output and \b filter on MLU200 series and CE3226.
 *   - \b grad_output onchip data type: int16, int31.
 *   - \b filter onchip data type: int16, int31.
 * - \b grad_output offchip data type can be combined with any supported onchip data types.
 * - \b filter offchip data type can be combined with any supported onchip data types.
 * - This function also supports float-point computation on MLU300 series or above. To perform
 *   float-point computation, the onchip data type of \b grad_output and \b filter should be
 *   \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The data layout of the input, offset, mask, filter, grad_output, grad_input, grad_offset,
 *   and grad_mask should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input, offset, mask, filter, grad_output, grad_input, grad_offset, grad_mask and
 *   the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch size of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor accordingly. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - grad offset tensor: the data type, layout, and shape of grad offset should be equal to the
 *     offset tensor.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch size of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor accordingly.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - grad mask tensor: the data type, layout and shape of the grad mask should be equal to
 *     the mask tensor. When mask is passed NULL, grad mask must be NULL.
 *   - The data bytes of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.
 *   - When mask is not needed, \b mask, \b mask_desc, \b grad_mask and \b grad_mask_desc should be
 *     set to NULL. When it is needed, any of \b mask, \b mask_desc, \b grad_mask and
 *     \b grad_mask_desc cannot be NULL.
 *
 * @par API Dependency
 * - Before calling this function to implement deformable convolution backward data,
 *   you need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the im2col_step of to the batch size.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API cnnlDCNBackwardData(cnnlHandle_t handle,
                                              const cnnlDCNDescriptor_t dcn_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t offset_desc,
                                              const void *offset,
                                              const cnnlTensorDescriptor_t mask_desc,
                                              const void *mask,
                                              const cnnlTensorDescriptor_t filter_desc,
                                              const void *filter,
                                              const cnnlTensorDescriptor_t grad_output_desc,
                                              const void *grad_output,
                                              void *workspace,
                                              const size_t workspace_size,
                                              const cnnlTensorDescriptor_t grad_input_desc,
                                              void* grad_input,
                                              const cnnlTensorDescriptor_t grad_offset_desc,
                                              void* grad_offset,
                                              const cnnlTensorDescriptor_t grad_mask_desc,
                                              void* grad_mask);

// Group:Deformable Convolution
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deformable convolution backward filter operation.
 *
 * The size of extra workspace is based on the given information of the deformable convolution
 * backward filter operation, including the deformable convolution descriptor \b dcn_desc,
 * input tensor descriptor \b input_desc, offset tensor
 * descriptor \b offset_desc, mask tensor descriptor \b mask_desc, gradient with respect to
 * the output tensor \b grad_output_desc, the gradient with respect to the filter tensor
 * \b grad_filter_desc, and the gradient with respect to the bias tensor \b grad_bias_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide."
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   deformable convolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information, see
 *   ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mask_desc
 *   Input. The descriptor of the mask tensor. Set this parameter to NULL if mask is not needed. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_filter_desc
 *   Input. The descriptor of the gradient with respect to the filter tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the gradient with respect to the bias tensor.
 *   Set this parameter to NULL if the gradient with respect to bias is not needed.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deformable convolution backward filter operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b input, \b offset, \b mask (optional),
 *   \b grad_output, \b grad_filter, \b grad_bias (optional), before calling this
 *   function.
 * - The allocated extra workspace should be passed to the ::cnnlDCNBackwardWeight function to
 *   perform the deformable convolution backward filter operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDCNBackwardWeightWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlDCNDescriptor_t dcn_desc,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlTensorDescriptor_t offset_desc,
                                      const cnnlTensorDescriptor_t mask_desc,
                                      const cnnlTensorDescriptor_t grad_output_desc,
                                      const cnnlTensorDescriptor_t grad_filter_desc,
                                      const cnnlTensorDescriptor_t grad_bias_desc,
                                      size_t *workspace_size);

// Group:Deformable Convolution
/*!
 * @brief Performs the back-propagation of a deformable convolution operation to compute
 *        the gradient with respect to filter \b grad_filter and bias \b grad_bias
 *        based on the gradient of response \b grad_output.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetDCNBakcwardWeightWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deformable convolution backward filter operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dcn_desc
 *   Input. The descriptor of the deformable convolution operation. For detailed information,
 *   see ::cnnlDCNDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] offset_desc
 *   Input. The descriptor of the offset to be applied for each position in the convolution kernel.
 *   The shape of offset should be (batch, out_height, out_width, 2 * deformable_group *
 *   weight_height * filter_width). For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] mask_desc
 *   Input. The descriptor of the scaling factor to be applied for each position in the convolution
 *   kernel. The shape of mask should be (batch, out_height, out_width,
 *   deformable_group * filter_height * filter_width). Set this parameter to NULL when
 *   mask is not requested. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor. Set this parameter to NULL when mask is not
 *   requested.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the gradient with respect to the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deformable convolution backward filter operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deformable convolution backward filter operation. You can get the size of the workspace
 *   with the ::cnnlGetDCNBakcwardWeightWorkspaceSize function.
 * @param[in] grad_filter_desc
 *   Input. The descriptor of the gradient with respect to the filter tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_filter
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the filter tensor.
 * @param[in] grad_bias_desc
 *   Input. The descriptor of the gradient with respect to the bias tensor. Set this parameter to NULL if the
 *   gradient of the bias tensor is not needed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_bias
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the bias tensor.
 *   Set this parameter to NULL if the gradient of the bias tensor is not needed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NUMERICAL_OVERFLOW
 *
 * @par Formula
 * - See "Deformable Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Offchip data type of \b input, \b offset, \b mask, \b grad_output, \b grad_filter,
 *   and \b grad_bias must be the same.
 * - The supported offchip data types of the input tensor and output tensor are as follows:
 *   - input, offset, mask, grad_output, grad_filter, grad_bias, grad_mask: half, float.
 * - This function supports any combinations of the following onchip data types for input tensor
 *   \b grad_output and \b input on MLU200 series and CE3226.
 *   - \b grad_output onchip data type: int16, int31.
 *   - \b filter onchip data type: int16, int31.
 * - \b grad_output offchip data type can be combined with any supported onchip data types.
 * - \b input offchip data type can be combined with any supported onchip data types.
 * - This function also supports float-point computation on MLU300 series or above. To perform
 *   float-point computation, the onchip data type of \b input and \b grad_output should be
 *   \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - The data layout of the input, offset, mask, grad_output, and grad_filter
 *   should be \p CNNL_LAYOUT_NHWC.
 * - The data layout of grad_bias should be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input, offset, mask, grad_output, grad_filter, grad_bias and
 *   the deformable convolution descriptor
 *   (including pad, stride, dilation, deformable_group, conv_group, im2col_step) must meet the
 *   following requirements:
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - offset tensor: \p batch should be equal to the batch of input tensor, \p height and \p width
 *     should be equal to the height and width of output tensor. \p channel should be equal to
 *     deformable_group * filter_height * filter_width * 2.
 *   - mask tensor: When mask is needed, \p batch should be equal to the batch of input tensor,
 *     \p height and \p width should be equal to the height and width of output tensor.
 *     \p channel should be equal to deformable_group * filter_height * filter_width.
 *   - grad bias tensor: When the gradient of bias is needed, the \b grad_bias should be a
 *     one-dimensional array with the length of \p out_channel.
 *   - The value of (im2col_step * out_height * out_filter * filter_h * filter_w * input_channel)
 *     should be less than or equal to the INT_MAX defined in limits.h.

 * @par API Dependency
 * - Before calling this function to implement the backward filter of deformable convolution,
 *   you need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the im2col_step to the batch size.
 *
 * @note
 * - The alignment of \b input, \b offset, \b mask, \b grad_output, \b grad_filter, \b grad_bias
 *   should be contiguous in the MLU memory.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/msracver/Deformable-ConvNets
 * - Deformable Convolutional Networks, Jifeng Dai, et al., 2017.
 * - Deformable ConvNets v2: More Deformable, Better Results, Xizhou Zhu, et al., 2018.
 */
cnnlStatus_t CNNL_WIN_API
cnnlDCNBackwardWeight(cnnlHandle_t handle,
                      const cnnlDCNDescriptor_t dcn_desc,
                      const cnnlTensorDescriptor_t input_desc,
                      const void *input,
                      const cnnlTensorDescriptor_t offset_desc,
                      const void *offset,
                      const cnnlTensorDescriptor_t mask_desc,
                      const void *mask,
                      const cnnlTensorDescriptor_t grad_output_desc,
                      const void *grad_output,
                      void *workspace,
                      size_t workspace_size,
                      const cnnlTensorDescriptor_t grad_filter_desc,
                      void *grad_filter,
                      const cnnlTensorDescriptor_t grad_bias_desc,
                      void *grad_bias);

/******************************************************************************
 * Cambricon CNNL OP: CARAFE
 ******************************************************************************/

// Group:Carafe
/*!
 * @brief Creates a descriptor pointed by \b carafe_desc for CARAFE upsampling forward
 *        and backward operations, and allocates memory holding the configuration parameters.
 *        The information is defined in ::cnnlCarafeDescriptor_t.
 *        For more information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[in] carafe_desc
 *  Input. A host pointer to the CARAFE descriptor that holds information about the
 *  CARAFE operator.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetCarafeDescriptor function to initialize
 *   and set the information to the CARAFE descriptor.
 * - You need to call the ::cnnlDestroyCarafeDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateCarafeDescriptor(cnnlCarafeDescriptor_t *carafe_desc);

// Group:Carafe
/*!
 * @brief Initializes the CARAFE descriptor \b carafe_desc that is previously created with the
 *        ::cnnlCreateCarafeDescriptor function, and sets the information about the
 *        CARAFE forward and backward operations to the descriptor \b carafe_desc.
 *
 * @param[in] carafe_desc
 *   Input. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the CARAFE operation.
 * @param[in] kernel_size
 *   Input. The width of the upsampling kernel window.
 * @param[in] group_size
 *   Input. The number of channel groups. The channels in the same group share the same upsampling filter.
 * @param[in] scale_factor
 *   Input. The upsampling ratio by which the resolution of the input feature map will be
 *   increased, i.e., the height and width of the output feature maps would be \b scale_factor times
 *   of the height and width of the input feature maps, respectively.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, ::cnnlCreateCarafeDescriptor should be called.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetCarafeDescriptor(cnnlCarafeDescriptor_t carafe_desc,
                                                  const int dimNb,
                                                  const int kernel_size,
                                                  const int group_size,
                                                  const int scale_factor);

// Group:Carafe
/*!
 * @brief Destroys a CARAFE descriptor \b carafe_desc that is previously created by
 *        the ::cnnlCreateCarafeDescriptor function.
 *
 * The CARAFE descriptor is defined in ::cnnlCarafeDescriptor_t
 * and holds the information about the CARAFE forward or backward operations.
 *
 * @param[in] carafe_desc
 *   Input. The CARAFE descriptor to be destroyed. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlCarafeForward,
 *   or ::cnnlCarafeBackward function. Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the CARAFE descriptor.
 *   Otherwise, memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyCarafeDescriptor(cnnlCarafeDescriptor_t carafe_desc);

// Group:Carafe
/*!
 * @brief Performs the CARAFE (Content-Aware ReAssembly of FEatures) upsampling operation
 *        on the input feature maps \b input using weighted combination, where the
 *        filter are set with \b mask. The upsampled feature maps are returned in
 *        the output tensor \b output.
 *
 * CARAFE performs upsampling at each output location by weighted summation in a nearby mask
 * window around the corresponding input location. The mask filter are defined separately
 * for each output location, which offers the ability of content-aware handling.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the carafe forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] carafe_desc
 *   Input. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] input_desc
 *   Input. The tensor descriptor of the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mask_desc
 *   Input. The tensor descriptor of the mask applied to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor.
 * @param[in] output_desc
 *   Input. The tensor descriptor of the output upsampled feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "CARAFE Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - \b input: half, float.
 * - \b mask: half, float.
 * - \b output: half, float.
 * - Data types of \b input, \b mask and \b output tensors must be the same.
 *
 * @par Data Layout
 * - Data layout of the \b input, \b mask, and \b output tensors should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - Parameters specified in \b carafe_desc should satisfy:
 *   - \b dimNb = 4.
 *   - \b kernel_size should be an odd number, i.e., 2*n+1 (n>=0), and \b kernel_size <= 45.
 *   - \b group_size and \b scale_factor are positive integers, and \b scale_factor <= 5.
 * - The dimensions specified by \b input_desc, \b mask_desc and \b output_desc should
 *   be \b input[N, H, W, C], \b mask[N, Ho, Wo, Cm] and \b output[N, Ho, Wo, C], respectively,
 *   in which:
 *   - The length of all dimensions should be non-negative integers.
 *   - The dimensions denoted by the same symbol should have the same value.
 *   - \b C should be divisible by \b group_size, i.e., \b C = N * \b group_size (N>=1).
 *   - \b Cm = \b group_size * \b kernel_size * \b kernel_size.
 *   - \b Ho = \b scale_factor * \b H.
 *   - \b Wo = \b scale_factor * \b W.
 *
 * @par API Dependency
 * - Before calling this function to implement CARAFE forward operation, you need to
 *   prepare all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - If any dimension in \b input_desc, \b mask_desc, or \b output_desc is zero,
 *   which represents an empty tensor, ::CNNL_STATUS_SUCCESS is returned without
 *   any changes to the \b output tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - Example of CARAFE forward operation is as follows:
     @verbatim
      input tensor by 1 * 2 * 2 * 1 --> input:
        [[[[ 0.34064351], [-0.8246629 ]],
          [[-0.71797801], [-0.51707748]]]]

      mask tensor by 1 * 4 * 4 * 1 --> mask:
        [[[[ 0.97630979], [-0.06261992], [ 0.91232837], [-0.1598553 ]],
          [[-0.72060206], [ 0.48904262], [-0.65568251], [ 0.12801235]],
          [[-0.85134485], [-1.27589059], [ 3.00143314], [ 0.61258706]],
          [[-0.50308504], [-0.93015218], [-1.1125597 ], [ 0.67302385]]]]

      param:
        kernel_size: 3, group_size: 1, scale_factor: 2

      output tensor by 1 * 4 * 4 * 1 --> output:
        [[[[ 0.33257359], [-0.02133107], [-0.75236336], [ 0.13182674]],
          [[-0.24546842], [ 0.1665892 ], [ 0.54071704], [-0.10556703]],
          [[ 0.61124688], [ 0.91606138], [-1.55197348], [-0.31675497]],
          [[ 0.36120399], [ 0.66782881], [ 0.57527956], [-0.34800548]]]]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/carafe.py
 * - CARAFE: Content-Aware ReAssembly of FEatures, Jiaqi Wang et al., 2019.
 */
  cnnlStatus_t CNNL_WIN_API cnnlCarafeForward(cnnlHandle_t handle,
                                              const cnnlCarafeDescriptor_t carafe_desc,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t mask_desc,
                                              const void *mask,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:Carafe
/*!
 * @brief Performs the back-propagation of CARAFE (Content-Aware ReAssembly of FEatures)
 *        operator to compute the gradient with respect to input \b grad_input and
 *        mask \b grad_mask based on the gradient of response \b grad_output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the CARAFE backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] carafe_desc
 *   Input. The descriptor of the CARAFE operator. For detailed information,
 *   see ::cnnlCarafeDescriptor_t.
 * @param[in] input_desc
 *   Input. The tensor descriptor of the input feature maps. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mask_desc
 *   Input. The tensor descriptor of the mask applied to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores the mask tensor.
 * @param[in] grad_output_desc
 *   Input. The tensor descriptor of the gradient with respect to the output feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient with respect to the
 *   upsampled feature maps.
 * @param[in] grad_input_desc
 *   Input. The tensor descriptor of the gradient with respect to the input feature maps.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient with respect to the
 *   input feature maps.
 * @param[in] grad_mask_desc
 *   Input. The descriptor of the gradient tensor with respect to the \b mask tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_mask
 *   Output. Pointer to the MLU memory that stores the gradient with respect to \b mask.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "CARAFE Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - \b input: half or float.
 * - \b mask: half or float.
 * - \b output: half or float.
 * - Data types of \b input, \b mask, \b grad_output, \b grad_input and \b grad_mask
 *   tensors must be the same.
 *   For MLU200 series, it is not recommended to use half data type for tensors due to the
 *   low precision.
 *
 * @par Data Layout
 * - Data layout of the \b input, \b mask, \b grad_output, \b grad_input and \b grad_mask
 *   tensors should be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - Parameters specified in \b carafe_desc should satisfy:
 *   - \b dimNb = 4.
 *   - \b kernel_size should be an odd number, i.e., 2*n+1 (n>=0), and \b kernel_size <= 137.
 *   - \b group_size and \b scale_factor should be positive integers.
 * - The dimensions specified by \b input_desc, \b mask_desc, \b grad_output_desc,
 *   \b grad_input_desc and \b grad_mask_desc should be \b input[N, H, W, C],
 *   \b mask[N, Ho, Wo, Cm], \b grad_output[N, Ho, Wo, C], \b grad_input[N, H, W, C]
 *   and \b grad_mask[N, Ho, Wo, Cm], respectively, in which:
 *   - The length of all dimensions should be non-negative integers.
 *   - The dimensions denoted by the same symbol should have the same value.
 *   - \b C should be divisible by \b group_size, i.e., \b C = n * \b group_size (n>=1).
 *   - \b Cm = \b group_size * \b kernel_size * \b kernel_size.
 *   - \b Ho = \b scale_factor * \b H.
 *   - \b Wo = \b scale_factor * \b W.
 *
 * @par API Dependency
 * - Before calling this function to implement CARAFE backward operation, you need to
 *   prepare all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - If any dimension in \b input_desc, \b mask_desc, \b grad_output_desc, \b grad_input_desc
 *   or \b grad_mask_desc is zero, which represents an empty tensor, ::CNNL_STATUS_SUCCESS is
 *   returned without any changes to the \b grad_output and \b grad_input tensors.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://github.com/open-mmlab/mmcv/tree/master/mmcv/ops/carafe.py
 * - CARAFE: Content-Aware ReAssembly of FEatures, Jiaqi Wang, et al., 2019.
 */
cnnlStatus_t CNNL_WIN_API cnnlCarafeBackward(cnnlHandle_t handle,
                                             const cnnlCarafeDescriptor_t carafe_desc,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t mask_desc,
                                             const void *mask,
                                             const cnnlTensorDescriptor_t grad_output_desc,
                                             const void *grad_output,
                                             const cnnlTensorDescriptor_t grad_input_desc,
                                             void *grad_input,
                                             const cnnlTensorDescriptor_t grad_mask_desc,
                                             void *grad_mask);

/******************************************************************************
 * Cambricon CNNL OP: GatherV2
 ******************************************************************************/
// Group:Gather
/*!
 * @brief Gathers slices from \b params at axis \b axis according to \b indices.
 *        To gather slices in different batches, call ::cnnlBatchGatherV2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather_v2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An int value which determines the axis to gather value from.
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] params_shape
 *   Input. Pointer to the MLU memory that stores the shape tensor of \b params.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each element of
 *   \b output in corresponding dimension of input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b params, index tensor \b indices, and output tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor must be the same.</b>
 *   - input tensor: int32, half, float.
 *   - index tensor: int32.
 *   - output tensor: int32, half, float.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor and axis must meet the following requirements:
 *   - input tensor: The shape should be same as the shape of output except the \b axis dimension.
 *   - axis: It should be greater than -1 and less than the dimension of \b params.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 and 1 --> params: [[1., 2.], [3., 4.]]

     --> indices: [0]

     param:
       axis: 0, params_shape: [2, 2]

     output array by 2 --> output: [1., 2.]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
cnnlStatus_t CNNL_WIN_API cnnlGatherV2(cnnlHandle_t handle,
                                       const int axis,
                                       const cnnlTensorDescriptor_t params_desc,
                                       const void *params,
                                       const int *params_shape,
                                       const cnnlTensorDescriptor_t indices_desc,
                                       const int *indices,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
// Group:Gather
/*!
 * @brief Gathers slices in different batches from \b params at dimension \b axis
 *        according to \b indices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the gather_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An int value which determines the dimension to gather value from.
 * @param[in] batch_dims
 *   Input. An int value which determines the batch dimensions of inputs and output.
 *   The batch dimensions are from dimension 0 to dimension (batch_dims -1).
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index
 *   of each element of \b output in corresponding dimension of params tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b params, index tensor \b indices, and output tensor \b output.
 *   <b>The data type of input tensor and output tensor must be the same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - index tensor: int32, int64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The shape of input tensor should be same as the shape of output tensor in
 *   every dimension except the dimension \b axis.
 *
 * @note
 * - The \b axis must be in the range of [-rank, rank), where rank is the dimension size of the \b params.
 *   Negative \b axis refers to 'axis + rank'.
 * - The \b batch_dims must be in the range of [-rank, rank), where rank is the dimension size of the \b indices.
 *   Negative \b batch_dims refers to 'batch_dims + rank'.
 * - The data in \b indices is expected in the range of [-rank, rank), where rank is the size of \b axis dimension in
 *   \b params. If the data in \b indices is not in the range, the corresponding positions of the output will be
 *   filled with 0.
 *
 * @par Requirements
 * - \b batch_dims and \b axis should meet the following requirements:
 *   - the \b axis must be greater than or equal to \b batch_dims.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 and 2 * 3:
       --> params: [[[1., 2.], [3., 4.]],
                    [[5., 6.], [7., 8.]]]

       --> indices: [[0, 1, -1], [1, 0, 2]]

     param:
       axis: 1, batch_dims: 1

     output array by 2 * 3 * 2:
       --> output: [[[1., 2.], [3., 4.], [3., 4.]],
                    [[7., 8.], [5., 6.], [0., 0.]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchGatherV2(cnnlHandle_t handle,
                                            const int axis,
                                            const int batch_dims,
                                            const cnnlTensorDescriptor_t params_desc,
                                            const void *params,
                                            const cnnlTensorDescriptor_t indices_desc,
                                            const void *indices,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);
// Group:Gather
/*!
 * @brief Gathers slices in different batches from \b params at dimension \b axis
 *        according to \b indices. Compared with cnnlBatchGatherV2, this function add a
 *        \b mode parameter to specify whether the \b indices contain negative numbers.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the gather_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An int value which determines the dimension to gather value from.
 * @param[in] batch_dims
 *   Input. An int value which determines the batch dimensions of inputs and output.
 *   The batch dimensions are from dimension 0 to dimension (batch_dims -1).
 * @param[in] mode
 *   Input. An int value which determines the mode to use. 0 means that \b indices
 *   contain negative numbers. 1 means that \b indices not contain negative numbers.
 * @param[in] params_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] params
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] indices
 *   Input. Pointer to the MLU memory that stores the tensor used to store index
 *   of each element of \b output in corresponding dimension of \b params tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GatherV2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b params, index tensor \b indices, and output tensor \b output.
 *   <b>The data type of input tensor and output tensor must be the same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double.
 *   - index tensor: int32, int64, uint32, uint64.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float, double.
 *
 * @par Scale Limitation
 * - The element number of \b params should be smaller than 2^31.
 *
 * @note
 * - The shape of input tensor should be same as the shape of output tensor in
 *   every dimension except the dimension \b axis.
 * - The \b axis must be in the range of [-rank, rank), where rank is the dimension size of the \b params.
 *   Negative \b axis refers to 'axis + rank'.
 * - The \b batch_dims must be in the range of [-rank, rank), where rank is the dimension size of the \b indices.
 *   Negative \b batch_dims refers to 'batch_dims + rank'.
 * - The data in \b indices is expected in the range of [-rank, rank), where rank is the size of \b axis dimension in
 *   \b params. If the data in \b indices is not in the range, the corresponding positions of the output will be
 *   filled with 0.
 *
 * @par Requirements
 * - \b batch_dims and \b axis should meet the following requirements:
 *   - the \b axis must be greater than or equal to \b batch_dims.
 *
 * @par Example
 * - The example of the gather_v2 operation is as follows:
     @verbatim
     input two arrays by 2 * 2 * 2 and 2 * 3:
       --> params: [[[1., 2.], [3., 4.]],
                    [[5., 6.], [7., 8.]]]

       --> indices: [[0, 1, -1], [1, 0, 2]]

     param:
       axis: 1, batch_dims: 1

     output array by 2 * 3 * 2:
       --> output: [[[1., 2.], [3., 4.], [3., 4.]],
                    [[7., 8.], [5., 6.], [0., 0.]]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/GatherV2
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchGatherV2_v2(cnnlHandle_t handle,
                                               const int axis,
                                               const int batch_dims,
                                               const int mode,
                                               const cnnlTensorDescriptor_t params_desc,
                                               const void *params,
                                               const cnnlTensorDescriptor_t indices_desc,
                                               const void *indices,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);
// Group:Fill
/*!
 * @brief Fills the output tensor \b output with a scale \b value.
 *
 * @deprecated
 *   ::cnnlFill is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlFill_v3 instead, which supports the parameter \b pointer_mode that sets \b value
 *   to host pointer or device pointer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] value
 *   value. A scale value to fill the output tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for output tensor \b output.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - You can specify the stride of all dimensions for \b output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      param:value: 5

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlFill(cnnlHandle_t handle,
                                   float value,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Fill
/*!
 * @brief Fills the output tensor \b output with the value in \b value tensor.
 *
 * @deprecated
 *   ::cnnlFill_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlFill_v3 instead, which supports the parameter \b pointer_mode that sets \b value
 *   to host pointer or device pointer.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] value_desc
 *   Input. The descriptor of the \b value tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] value
 *   Input. Pointer to the MLU memory that stores the \b value tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for value tensor \b value and output tensor \b output.
 *   - value tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - Data type of value tensor \b value and output tensor \b output should be the same.
 * - The number of elements of value tensor \b value only supports one.
 * - You can specify the stride of all dimensions for \b output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      input array by 1 --> value: [1]

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlFill_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t value_desc,
                                      const void *value,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);
// Group:Fill
/*!
 * @brief Fills the output tensor \b output with \b value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the fill
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pointer_mode
 *   Input.  An enum value which indicates that the scalar value \b value is
 *   passed by reference on the host or device. The information is defined in ::cnnlPointerMode_t.
 * @param[in] value
 *   Input.  A pointer to scaling factor of tensor input.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_DEVICE, the \b value should be a device pointer.
 *   If the \b pointer_mode is \b CNNL_POINTER_MODE_HOST, the \b value should be a host pointer.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for \b value and output tensor \b output.
 *   - value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @note
 * - Data type of \b value and output tensor \b output should be the same.
 * - The number of elements of \b value only supports one.
 * - You can specify the stride of all dimensions for \b output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the fill operation is as follows:
     @verbatim
      param:value: 5

      output array by 2 * 3 * 2 --> output: [[[5,5],[5,5],[5,5]],
                                             [[5,5],[5,5],[5,5]]]
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Fill.cpp
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlFill_v3(cnnlHandle_t handle,
                                      const cnnlPointerMode_t pointer_mode,
                                      const void *value,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:Sin
/*!
 * @brief Computes sine of input tensor \b x, and returns the results in the output tensor \b y.
 *        To set the computing with faster algorithm or higher precision, call ::cnnlSin_v2.
 *
 * @deprecated
 *   ::cnnlSin is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSin_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sin
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Sin Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - half: [-3.14, 3.14].

 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=sin#torch.sin
 */
cnnlStatus_t CNNL_WIN_API cnnlSin(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);
// Group:Sin
/*!
 * @brief Computes sine of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlSin, this function allows you to choose whether to perform sin operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sin
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Sin Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - half: [-3.14, 3.14].

 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=sin#torch.sin
 */
cnnlStatus_t CNNL_WIN_API cnnlSin_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);

// Group:Cos
/*!
 * @brief Computes cosine of input tensor \b x, and returns the results in the output tensor \b y.
 *        To set the computing with faster algorithm or higher precision, call ::cnnlCos_v2.
 *
 * @deprecated
 *   ::cnnlCos is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlCos_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cos
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cos Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - half: [-3.14, 3.14].
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=cos#torch.cos
 */
cnnlStatus_t CNNL_WIN_API cnnlCos(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);
// Group:Cos
/*!
 * @brief Computes cosine of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlCos, this function allows you to choose whether to perform cos operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cos
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cos Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - half: [-3.14, 3.14].
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=cos#torch.cos
 */
cnnlStatus_t CNNL_WIN_API cnnlCos_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);

/*! The descriptor of the transpose operation that holds transpose information
 *  including \b dimensions and \b permute.
 *
 *  You need to call the ::cnnlCreateTransposeDescriptor function to create a descriptor,
 *  and call the ::cnnlSetTransposeDescriptor function to set the information of
 *  transpose operation to the descriptor. Also, you need to destroy the Cambricon CNNL context
 *  at the end with the ::cnnlDestroyTransposeDescriptor function.
 */
typedef struct cnnlTransposeStruct *cnnlTransposeDescriptor_t;

// Group:Transpose
/*!
 * @brief Creates a descriptor pointed by \b desc for a transpose operation,
 *        and allocated memory for holding the information about the transpose operation.
 *
 * The information is defined in ::cnnlTransposeDescriptor_t. For more information
 * about descriptor, see "Cambricon CNNL user Guide".
 *
 * @param[out] desc
 *   Input. A host pointer to the transpose descriptor that holds information about
 *   the transpose operation.
 * @par Return
 *   ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetTransposeDescriptor
 *   function to initialize and set information to the transpose descriptor.
 * - You need to call the ::cnnlDestroyTransposeDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTransposeDescriptor(cnnlTransposeDescriptor_t *desc);

// Group:Transpose
/*!
 * @brief Initializes the transpose descriptor \b desc that is previously created
 * with the ::cnnlCreateTransposeDescriptor function, and set the information
 * about the transpose operation to the transpose descriptor \b desc.
 * The information includes the permute dimensions \b dims and permute rules \b permute.
 *
 * @param[in] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[in] dims
 *   Input. The number of dimensions in the permute tensor of the transpose operation.
 *   Currently, the value of this parameter should be less than or equal to 8.
 * @param[in] permute
 *   Input. The order of transpose. Currently, for each dimension, the value of permute
 *   should be in the range of [0,...,dims -1], and should not be the same in each dimension.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTransposeDescriptor(cnnlTransposeDescriptor_t desc,
                                                     const int dims,
                                                     const int permute[]);

// Group:Transpose
/*!
 * @brief Destroys a transpose descriptor \b desc that is previously created with the
 *        ::cnnlCreateTensorDescriptor function.
 *
 * The transpose descriptor is defined in ::cnnlTransposeDescriptor_t and holds the information
 * about the transpose operation.
 *
 * @param[in] desc
 *   Input. The transpose descriptor to be destroyed. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTransposeDescriptor(cnnlTransposeDescriptor_t desc);

// Group:Transpose
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the transpose operation.
 *
 * The size of extra workspace is based on the given information of the transpose operation,
 * including the input tensor descriptor \b x_desc and transpose descriptor \b desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the transpose operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the transpose operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \b x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlTranspose_v2 function
 *   to perform the transpose operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetTransposeWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const cnnlTransposeDescriptor_t desc,
                                                        size_t *size);
// Group:Transpose
/*!
 * @brief Reorders the dimension according to the value of \b permute. To have better performance
 *        for over 4-dimension transpose with large-scale cases, call the
 *        ::cnnlTranspose_v2 function.
 *
 * @deprecated
 *   ::cnnlTranspose is deprecated and will be removed in the further release. It is recommended
 *   to use ::cnnlTranspose_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the transpose operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *          see ::cnnlTransposeDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *          see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, :CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x and
 *   output tensor \b y.
 *   <b>Note that the data type of input tensor and output tensor should be same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par Scale Limitation
 * - The \b x, \b y and \b permute have the same shape.
 * - The dimension size of \b x, \b y and \b permute should be less than or equal to
 *   CNNL_DIM_MAX.
 * - The \b permute i-th dimension is in the range [0,...n-1], where n is the rank of the \b x.
 * - The \b y i-th dimension will correspond to the \b x permute[i]-th dimension.
 * - The process of computing, the copy times of memcpy should be less than 65536.
 *
 * @par API Dependency
 * - Before calling this function to implement transpose, you need to prepare all the parameters
 *   passed to this function. See each parameter description for details.
 *
 * @note
 * - None.
 *
 * @par Example
 * - The example of the transpose operation is as follows:
     @verbatim
      input array by 3 * 2 -->
          input: [[1, 4],
                  [2, 5],
                  [3, 6]]
      param:
        dims: 2, permute: (1, 0),

      output array by 2 * 3 --> output: [[1, 2, 3],
                                         [4, 5, 6]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/transpose
 */
cnnlStatus_t CNNL_WIN_API cnnlTranspose(cnnlHandle_t handle,
                                        const cnnlTransposeDescriptor_t desc,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:Transpose
/*!
 * @brief Reorders the dimension according to the value of \b permute. Compared with
 *        ::cnnlTranspose, ::cnnlTrnapose_v2 provides better performance for above 4-dimension
 *        transpose with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetTransposeWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the transpose operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the transpose operation. For detailed information,
 *   see ::cnnlTransposeDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   transpose operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the transpose operation. You can get the size of the workspace with
 *   the ::cnnlGetReorgWorkspaceSize function.

 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The \b x, \b y and \b permute have the same shape.
 * - The dimension size of \b x, \b y and \b permute should be less than or equal to CNNL_DIM_MAX.
 * - The \b permute i-th dimension is in the range [0,...n-1], where n is the rank of the \b x.
 * - The \b y i-th dimension will correspond to \b x permute[i]-th dimension.
 * - The process of computing, the copy times of memcpy should be less than 65536.
 *
 * @par Formula
 * - See "Transpose Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x and
 *   output tensor \b y.
 *   <b>Note that the data type of input tensor and output tensor should be same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half,
 *     float, complex_half, complex_float.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.

 * @par API Dependency
 * - Before calling this function to implement transpose, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the transpose operation is as follows:
 *   @verbatim
 *    input array by 3 * 2 -->
 *         input: [[1, 4],
 *                 [2, 5],
 *                 [3, 6]]
 *     param:
 *       dims: 2, permute: (1, 0),
 *
 *     output array by 2 * 3 --> output: [[1, 2, 3],
 *                                        [4, 5, 6]]
 *    @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/transpose
 */
cnnlStatus_t CNNL_WIN_API cnnlTranspose_v2(cnnlHandle_t handle,
                                           const cnnlTransposeDescriptor_t desc,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y,
                                           void *workspace,
                                           size_t workspace_size);

// Group:Reorg
/*!
 * @brief Creates a descriptor pointed by \b desc for a reorg operation,
 *        and allocates memory for holding the information about the reorg
 *        operation. The information is defined in ::cnnlReorgDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Input. A host pointer to the reorg descriptor that holds information about the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetReorgDescriptor function to initialize
 *   and set the information to the reorg descriptor.
 * - You need to call the ::cnnlDestroyReorgDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateReorgDescriptor(cnnlReorgDescriptor_t *desc);

// Group:Reorg
/*!
 * @brief Destroys a reorg descriptor \b desc that is previously created with the
 *        ::cnnlCreateReorgDescriptor function.
 *
 * The reorg descriptor is defined in ::cnnlReorgDescriptor_t
 * and holds the information about the reorg operation.
 *
 *
 * @param[in] desc
 *   Input. The reorg descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlReorg function.
 * - This function should be called to destroy the reorg descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyReorgDescriptor(cnnlReorgDescriptor_t desc);

// Group:Reorg
/*!
 * @brief Initializes the reorg descriptor \b desc that is previously created
 * with the ::cnnlCreateReorgDescriptor function, and sets the information
 * about the reorg operation to the reorg descriptor \b desc.
 * The information includes the coefficient in the height dimension \b reorg_h and
 * in the width dimension \b reorg_w, and reorg is forward or not \b forward.
 *
 * @param[in] desc
 *   Input. The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 *  @param[in] reorg_h
 *    Input. The remodeling coefficient in the height dimension of input tensor.
 *  @param[in] reorg_w
 *    Input. The remodeling coefficient in the width dimension of input tensor.
 *  @param[in] forward
 *    Input. bool value, false is forward and true is reverse (splitting or merging).
 *    In the following formula output_channel is channel dimension of output shape,
 *    input_channel is channel dimension of input shape.
 *    If \b forward is true,
 *    output_channel = (input_channel * \b reorg_h * \b reorg_w);
 *    If \b forward is false,
 *    output_channel = (input_channel / \b reorg_h / \b reorg_w);
 *    Only supports forward = false currently.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The formula of calculating the height dimension of output is as follows, where
 *   output_height is height dimension of output tensor, input_height is height
 *   dimension of input tensor.
 *   If \b forward is false, the height dimension of the output tensor is as follows:
 *   output_height = (input_height / \b reorg_h);
 *   If \b forward is true, the height dimension of the output tensor is as follows:
 *   output_height = (input_height * \b reorg_h);
 * - The formula of calculating the width dimension of output is as follows, where
 *   output_width is width dimension of output tensor, input_width is width
 *   dimension of input tensor.
 *   If \b forward is false, the width dimension of the output tensor is as follows:
 *   output_width = (input_width / \b reorg_w);
 *   If \b forward is true, the width dimension of the output tensor is as follows:
 *   output_width = (input_width * \b reorg_w);
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetReorgDescriptor(cnnlReorgDescriptor_t desc,
                                                 int reorg_h,
                                                 int reorg_w,
                                                 bool forward);

// Group:Reorg
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the reorg operation.
 *
 * The size of extra workspace is based on the given information of the reorg operation,
 * including the input tensor descriptor \b x_desc, output tensor descriptor \b y_desc and
 * the reorg descriptor \b desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes
 *   that is used in the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 *   This function must be called after
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b x_desc, \b y_desc before
 *   calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlReorg function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReorgWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    const cnnlReorgDescriptor_t desc,
                                                    size_t *size);
// Group:Reorg
/*!
 * @brief Rearranges input tensor \b x_ptr according to reorg descriptor \b desc,
 *        and returns the results in the output tensor \b y_ptr.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetReorgWorkspaceSize function.
 *
 * @deprecated
 *   ::cnnlReorg is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlReorg_v2 instead, which needs user to allocate extra space.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the descriptor operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   reorg operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the reorg operation. You can get the size of the workspace with
 *   the ::cnnlGetReorgWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reorg Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \b input and output tensor \b output must be the same.
 * - The supported data layout of the input tensor is as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *
 * @par Scale Limitation
 * - If tensor layout is CNNL_LAYOUT_NTC, then reorg_h, reorg_w and
 *   dimension size of timing steps(T in NTC) must be 1.
 * - If tensor layout is CNNL_LAYOUT_NC, then reorg_h and reorg_w must be 1.
 *
 * @par API Dependency
 * - Before calling this function to implement reorg, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series, MLU300 series and CE3226:
 *     - If \b x is NaN, then \b output is random value.
 *     - If \b x is infinity, then \b output is random value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the reorg operation is as follows:
     @verbatim
      input array by 1 * 26 * 26 * 512

      param:
        input reorg_h = 2, reorg_w = 2, forward = false.

      output array by 1 * 13 * 13 *2048
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlReorg(cnnlHandle_t handle,
                                    const cnnlReorgDescriptor_t desc,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y,
                                    void *workspace,
                                    size_t workspace_size);
// Group:Reorg
/*!
 * @brief Returns in \b extra_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the reorg operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_size.
 *
 * The size of extra input data is based on the given information of the reorg operation,
 * including the input tensor descriptor \b x_desc, output tensor descriptor \b y_desc and
 * the reorg descriptor \b desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \b x_desc, \b y_desc.
 * - After calling this function, you need to call ::cnnlInitReorgExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlReorg_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReorgExtraInputSize(cnnlHandle_t handle,
                                                     const cnnlReorgDescriptor_t desc,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     const cnnlTensorDescriptor_t y_desc,
                                                     size_t *extra_size);
// Group:Reorg
/*!
 * @brief Initializes the extra input data space \b extra_host_input on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input.The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_host_input
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the reorg operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetReorgExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \b x_desc, \b y_desc.
 * - The allocated extra input should be passed to the ::cnnlReorg_v2 function
 *   to perform the reorg operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitReorgExtraInput(cnnlHandle_t handle,
                                                  const cnnlReorgDescriptor_t desc,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *extra_host_input);
// Group:Reorg
/*!
 * @brief Rearranges input tensor \b x_ptr according to reorg descriptor \b desc,
 *        and returns the results in the output tensor \b y_ptr. Compared with
 *        ::cnnlReorg, ::cnnlReorg_v2 provides better performance with extra input space.
 *
 * This function needs extra MLU memory as the workspace to work.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetReorgWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the reorg operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the reorg operation. For detailed information,
 *   see ::cnnlReorgDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] extra_device_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitReorgExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide.
 * @param[in] extra_input_size
 *   Input. The size of the extra input data in bytes that needs to be used in
 *   the reorg operation. You can get the size of the extra input data with
 *   the ::cnnlGetReorgExtraInputSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   reorg operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the reorg operation. You can get the size of the workspace with
 *   the ::cnnlGetReorgWorkspaceSize function.
 * @param[out] y_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reorg Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *   This function supports the combinations of input tensor and output tensor must be
 *   half-half or float-float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \b input and output tensor \b output must be the same.
 * - The supported data layout of the input tensor is as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NTC, \p CNNL_LAYOUT_NC.
 *
 * @par Scale Limitation
 * - If tensor layout is CNNL_LAYOUT_NTC, then reorg_h, reorg_w and
 *   dimension size of timing steps(T in NTC) must be 1.
 * - If tensor layout is CNNL_LAYOUT_NC, then reorg_h and reorg_w must be 1.
 *
 * @par API Dependency
 * - Before calling this function to implement reorg, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series, MLU300 series and CE3226:
 *     - If \b x is NaN, then \b output is random value.
 *     - If \b x is infinity, then \b output is random value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the reorg operation is as follows:
     @verbatim
      input array by 1 * 26 * 26 * 512

      param:
        input reorg_h = 2, reorg_w = 2, forward = false.

      output array by 1 * 13 * 13 *2048
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlReorg_v2(cnnlHandle_t handle,
                                       const cnnlReorgDescriptor_t desc,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void * x,
                                       const void * extra_device_input,
                                       size_t extra_input_size,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void * y,
                                       void * workspace,
                                       size_t workspace_size);

// Group:Shufflechannel
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * for the ::cnnlShuffleChannel function to optimize the shuffle channel operation.
 *
 * The size of extra workspace is based on the given information of the shuffle channel
 * operation, including input tensor descriptor \b input_desc. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of extra workspace in bytes that is used in
 *   the ::cnnlShuffleChannel function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetShufflechannelWorkspaceSize(const cnnlTensorDescriptor_t input_desc, size_t *size);

// Group:Shufflechannel
/*!
 * @brief Shuffles channels of input tensor \b input according to the given
 *        parameter \b group, and returns the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the shuffle channel operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. The pointer to the MLU memory that stores the input tensor.
 * @param[in] group
 *   Input. The number of groups which the channels of input tensor are divided into.
 * @param[in] workspace
 *   Input. The pointer to the MLU memory that is used as an extra workspace for
 *   shuffle channel operation. You can get the workspace with the
 *   ::cnnlGetShufflechannelWorkspaceSize function.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   shuffle channel operation. You can get the size of workspace with the
 *   ::cnnlGetShufflechannelWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. The pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of \b input and \b output must be the same.
 * - The supported data types of input tensor and output tensor are as follows:
 *   - input: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *   - output: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 * - The size of C (the shuffle dimension) should be divided by \b group.
 * - The parameter \b group must meet the requirements:
 *   - group > 0 and group <= c.
 *   - group must be an integer.
 * - C * sizeof(datatype of \b input) should not be more than 1572864 on MLU200 series.
 * - C * sizeof(datatype of \b input) should not be more than 2621440 on MLU300 series.
 * - C * sizeof(datatype of \b input) should not be more than 655360 on CE3226.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the
 *   ::cnnlGetShufflechannelWorkspaceSize function to get the \b workspace_size.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the shuffle channel operation is as follows:
     @verbatim
      input tensor with the layout of NHWC, shape is 1 * 1 * 2 * 6
      --> input: [[[1,2,3,4,5,6],
                   [7,8,9,10,11,12]]]

      input parameter group
      --> group: 3

      output tensor with the layout of NHWC, shape is 1 * 1 * 2 * 6
      --> output: [[[1,3,5,2,4,6],
                    [7,9,11,8,10,12]]]
      @endverbatim
 *
 * @par Reference
 * - https://arxiv.org/abs/1707.01083
 * - https://github.com/MG2033/ShuffleNet/blob/master/layers.py
 * - https://www.paddlepaddle.org.cn/documentation/docs/en/api/layers/shuffle_channel.html
 */
cnnlStatus_t CNNL_WIN_API cnnlShuffleChannel(cnnlHandle_t handle,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const int group,
                                             void *workspace,
                                             size_t workspace_size,
                                             const cnnlTensorDescriptor_t output_desc,
                                             void *output);

// Group:Maximum
/*!
 * @brief Returns in \b size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the maximum operation.
 *
 * The size of extra workspace is based on the given information of the input
 * and output tensor descriptors, \b a_desc, \b b_desc, and \b c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the maximum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the maximum operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlMaximum function
 *   to perform the maximum operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMaximumWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);

// Group:Maximum
/*!
 * @brief Computes the element-wise maximum value between two input tensors \b a
 *        and \b b, and returns the results in the output tensor \b c.
 *
 * This function is used to perform an operation of maximum, supporting 1 to 8
 * dimensions of maximum operation with tensor broadcast. It is used in resnet50
 * on TensorFlow framework.
 *
 * This function supports in-place operation, which means one of the input
 * tensors \b a or \b b can share the same memory address with the output tensor \b c.
 * This function also supports tensor broadcasting as long as \b a, \b b, and \b c
 * satisfy the broadcast conditions. For more details about tensor broadcasting, see
 * Limitations section.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the maximum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   maximum operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the maximum operation. You can get the size of the workspace with
 *   the ::cnnlGetMaximumWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: int32, half, float
 *   - output tensor: int32, half, float
 *   <b>Note that all data types of two input tensors and the output tensor must be
 *   the same.</b>
 *
 * @par Limitations
 * - For each dimension of the two input tensors, the length of the dimension
 *   should be the same or one of them should equal to 1.
 * - Each dimension of the output tensor should equal to the larger one of the
 *   corresponding dimension of two input tensors.
 *
 * @par API Dependency
 * - Before calling this function to perform maximum operation, you need to get the
 *   size of workspace by the ::cnnlGetMaximumWorkspaceSize.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series:
 *     - If \b a or \b b is NaN, then \b c is positive saturation value.
 *     - If \b a or \b b is positive infinity, then \b c is positive saturation value.
 *     - If \b a or \b b is negative infinity, then \b c is negative saturation value.
 *   - On MLU300 series and CE3226:
 *     - If \b a or \b b is NaN, then \b c is NaN.
 *     - Between \b a and \b b, if one is positive infinity and the other is finite value, then
 *       \b c is positive infinity.
 *     - Between \b a and \b b, if one is negative infinity and the other is finite value, then
 *       \b c is negative infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMaximum(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c,
                                      void *workspace,
                                      size_t workspace_size);
// Group:Minimum
/*!
 * @brief Returns in \b size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the minimum operation.
 *
 * The size of extra workspace is based on the given information of the input
 * and output tensor descriptors, \b a_desc, \b b_desc, and \b c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the minimum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the minimum operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlMinimum function
 *   to perform the minimum operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMinimumWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *size);
// Group:Minimum
/*!
 * @brief Computes the element-wise minimum value between two input tensors \b a
 *        and \b b, and returns the results in the output tensor \b c.
 *
 * This function is used to perform an operation of minimum, supporting 1 to 8
 * dimensions of minimum operation with tensor broadcast. It is used in resnet50
 * on TensorFlow framework.
 *
 * This function supports in-place operation, which means one of the input
 * tensors \b a or \b b can share the same memory address with the output tensor \b c.
 * This function also supports tensor broadcasting as long as \b a, \b b, and \b c
 * satisfy the broadcast conditions. For more details about tensor broadcasting, see
 * Limitations section.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the minimum operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   minimum operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the minimum operation. You can get the size of the workspace with
 *   the ::cnnlGetMinimumWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: int32, half, float
 *   - output tensor: int32, half, float
 *   <b>Note that all data types of two input tensors and the output tensor must be
 *   the same.</b>
 *
 * @par Limitations
 * - For each dimension of the two input tensors, the length of the dimension
 *   should be the same or one of them should equal to 1.
 * - Each dimension of the output tensor should equal to the larger one of the
 *   corresponding dimension of two input tensors.
 *
 * @par API Dependency
 * - Before calling this function to perform minimum operation, you need to get the
 *   size of workspace by the ::cnnlGetMinimumWorkspaceSize function.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series:
 *     If \b a or \b b is NaN, then \b c is positive saturation value.
 *     If \b a or \b b is positive infinity, then \b c is positive saturation value.
 *     If \b a or \b b is negative infinity, then \b c is negative saturation value.
 *   - On MLU300 series and CE3226:
 *     If \b a is NaN and \b b is finite value, then \b c is NaN.
 *     If \b a is finite value and \b b is NaN, then \b c is finite value.
 *     If \b a or \b b is positive infinity, then \b c is positive infinity.
 *     If \b a or \b b is negative infinity, then \b c is negative infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMinimum(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      const cnnlTensorDescriptor_t b_desc,
                                      const void *b,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c,
                                      void *workspace,
                                      size_t workspace_size);

/******************************************************************************
 * Cambricon CNNL OP: ConvolutionBackwardData
 ******************************************************************************/

// Group:ConvolutionBackwardData
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution
 *        backward data operation.
 *
 * The returned algorithm is chosen from all the supported convolution
 * backward data algorithms defined in ::cnnlConvolutionBwdDataAlgo_t and is
 * based on the given filter descriptor \b weight_desc, input descriptor
 * \b diff_y_desc, convolution descriptor \b conv_desc, output descriptor
 * \b diff_x_desc, and the computing performance preferences \b preference.
 *
 * The computing performance options \b preference defined in the
 * ::cnnlConvolutionBwdDataPreference_t enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution backward data operation. For
 *   detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the convolution backward data operation
 *   to get better performance. This parameter only supports
 *   \p CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out] algo
 *   Output. The returned algorithm that is best suited for performing the
 *   convolution backward data operation. The algorithms are defined in the
 *   ::cnnlConvolutionBwdDataAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b filter_desc, \b diff_y_desc, and \b diff_x_desc before this function.
 * - The ::cnnlCreateConvolutionDescriptor and ::cnnlSetConvolutionDescriptor
 *   functions should be called in series to create and set the convolution
 *   descriptor \b conv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardDataAlgorithm(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t filter_desc,
                                        const cnnlTensorDescriptor_t diff_y_desc,
                                        const cnnlConvolutionDescriptor_t conv_desc,
                                        const cnnlTensorDescriptor_t diff_x_desc,
                                        const cnnlConvolutionBwdDataPreference_t preference,
                                        cnnlConvolutionBwdDataAlgo_t *algo);

// Group:ConvolutionBackwardData
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 *        workspace to optimize the convolution backward data operation.
 *
 * The size of extra workspace is based on the given information of
 * the convolution backward data operation, including the filter descriptor
 * \b filter_desc, input descriptor \b diff_y_desc, convolution descriptor
 * \b conv_desc, output descriptor \b diff_x_desc, and the convolution backward
 * data algorithm \b algo. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution backward data operation. For
 *   detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum. You can
 *   get the best suited algorithm with the ::cnnlGetConvolutionBackwardDataAlgorithm
 *   function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the convolution backward data operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlGetConvolutionBackwardDataAlgorithm function must be called to get
 *   the best suited algorithm \b algo before this function.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b filter_desc, \b diff_y_desc, and \b diff_x_desc before this function.
 * - The ::cnnlCreateConvolutionDescriptor and ::cnnlSetConvolutionDescriptor
 *   functions should be called in series to create and set the convolution
 *   descriptor \b conv_desc before this function.
 * - The allocated extra workspace should be passed to the
 *   ::cnnlConvolutionBackwardData function to perform the convolution backward
 *   data operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardDataWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t filter_desc,
                                            const cnnlTensorDescriptor_t diff_y_desc,
                                            const cnnlConvolutionDescriptor_t conv_desc,
                                            const cnnlTensorDescriptor_t diff_x_desc,
                                            const cnnlConvolutionBwdDataAlgo_t algo,
                                            size_t *workspace_size);

// Group:ConvolutionBackwardData
/*!
 * @brief Performs the backpropagation of a convolution operation to compute the gradient of
 * input \b diff_x based on the gradient of response \b diff_y and the filter tensor \b filter.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 *  The algorithm of ::cnnlConvolutionBackwardData and ::cnnlDeconvolution are the same,
 *  except the supporting of bias. If bias is needed, call ::cnnlDeconvolution.
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetConvolutionBackwardDataWorkspaceSize function. The convolution backward
 * data operation is computed based on the algorithm set in \b algo. You can call
 * the ::cnnlGetConvolutionBackwardDataAlgorithm to get the most suited algorithm.
 *
 * Depthwise convolution backward data operation is performed when the \p group_count
 * parameter of the convolution descriptor \b conv_desc is set to the number of
 * channels of the output tensor \b diff_x.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetConvolutionBackwardDataAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetConvolutionBackwardDataWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor, filter tensor, and output tensor.
 *   - input tensor: int8, int16, int31.
 *   - filter tensor: int8, int16, int31.
 *   - output tensor: half, float.
 * - When \p dimNb is set to 4 or 5, this function also supports float-point computation on MLU370. To perform float-point
 *   computation, offchip data dtype combinations with the order input-filter-output should be
 *   half-half-half or float-float-float, while onchip data type should be \p CNNL_DTYPE_INVALID
 *   or the same as the corresponding offchip data type. Also, onchip data type of output tensor
 *   (for float-point output, which is actually off-chip data type of output tensor) must have the same
 *   float-point type with input tensor.
 *
 * - If the convolution operation is depthwise, the additional supported combinations of data types are
 *   shown below with the following order:
 *
 *   - input - filter - output.
 *
 *   The supported data type combinations are:
 *
 *   - half - half - half
 *   - float - float - float
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layout of the input tensor, filter tensor, and output tensor
 *   are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layout of the input tensor,
 *   filter tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, output tensor and the convolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the convolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \b diff_y and \b filter should be the multiple of \p group_count.
 *     - The number of output channels of \b filter multiplying \p group_count should be
 *       the number of channels of \b diff_x.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 * - Specially, when the operation is depthwise, the parameters should meet the following
 *   additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \b diff_y must be divisible by the number of channels
 *   of output tensor \b diff_x.
 *
 *   multiplier = \p diff_y_c / \p diff_x_c,
 *   sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *   sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *   where \p diff_y_c and \p diff_x_c are the number of channels of \b diff_y
 *   and \b diff_x respectively, \p filter_h and \p filter_w are the height and width of
 *   filter tensor \b filter, \p stride_h and \p stride_w are stride parameters in height and
 *   width set in the convolution descriptor \b conv_desc.
 *   - If multiplier < 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 2016 / int31_flag
 *   - If multiplier >= 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 126 / (half_flag * int31_flag)
  *
 *   Where /p half_flag is 2 when the data type of output tensor \b diff_x are half,
 *   otherwise /p half_flag is 1. The /p int31_flag is 2 when the data type of input tensor
 *   \b diff_y or \b filter are int31, otherwise /p int31_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 */
cnnlStatus_t CNNL_WIN_API cnnlConvolutionBackwardData(cnnlHandle_t handle,
                                                      const void *alpha,
                                                      const cnnlTensorDescriptor_t filter_desc,
                                                      const void *filter,
                                                      const cnnlTensorDescriptor_t diff_y_desc,
                                                      const void *diff_y,
                                                      const cnnlConvolutionDescriptor_t conv_desc,
                                                      const cnnlConvolutionBwdDataAlgo_t algo,
                                                      void *workspace,
                                                      size_t workspace_size,
                                                      const void *beta,
                                                      const cnnlTensorDescriptor_t diff_x_desc,
                                                      void *diff_x);

// Group:ConvolutionBackwardData
/*!
 * @brief Converts the filter tensor \b filter and the gradient of response \b diff_y_desc from
 * floating-point to fixed-point numbers through quantization parameters,
 * and then computes the gradient of input data \b diff_x by the convolution backward data operation.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetConvolutionBackwardDataWorkspaceSize function. The convolution backward
 * data operation is computed based on the algorithm set in \b algo. You can call
 * the ::cnnlGetConvolutionBackwardDataAlgorithm to get the most suited algorithm.
 *
 * Depthwise convolution backward data operation is performed when the \p group_count
 * parameter of the convolution descriptor \b conv_desc is set to the number of
 * channels of the output tensor \b diff_x.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   convolution backward data operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] filter_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the filter tensor.
 * @param[in] filter_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the filter tensor.
 * @param[in] filter_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the filter tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the input tensor.
 * @param[in] diff_y_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the input tensor.
 * @param[in] diff_y_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the input tensor.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution backward data operation.
 *   The algorithms are defined in the ::cnnlConvolutionBwdDataAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetConvolutionBackwardDataAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   convolution backward data operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the convolution backward data operation. You can get the size of the workspace
 *   with the ::cnnlGetConvolutionBackwardDataWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Convolution Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - If \p dimNb is set to 4 or 5, this function supports any combinations of the following
 *   data types for the input tensor \b diff_y, filter tensor \b filter, and
 *   output tensor \b diff_x. <b>Note that the combinations of input tensor and
 *   filter tensor must be half-half or float-float. Also, onchip data type of output
 *   tensor(for float-point output, which is actually off-chip data type of output tensor)
 *   must have the same float-point type with input tensor.</b>
 *   - input tensor: half, float.
 *   - filter tensor: half, float.
 *   - output tensor: half, float.
 *   - Specially, when the convolution backward data operation is depthwise, the data
 *     type of output tensor must be consistent with that of input tensor, which means
 *     the data type combination with the order input_tensor-filter_tensor-output_tensor
 *     should be half-half-half or float-float-float.
 *   - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *     For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 *   - This function supports any combinations of the following data types as the
 *     \p onchip_dtype for the input tensor \b diff_y and filter tensor \b filter.
 *     - input tensor: int8, int16, int31.
 *     - filter tensor: int8, int16, int31.
 *   - This function supports any combinations of the following data types as the
 *     \p dtype and \p onchip_dtype for the input tensor \b diff_y and filter tensor
 *     \b filter. The \p dtype and \p onchip_dtype should be same.
 *     - input tensor: int8, int16, int31.
 *     - filter tensor: int8, int16, int31.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layout of the input tensor,
 *   filter tensor, and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *     If the convolution backward data operation is depthwise, the layout of the filter
 *     tensor must be \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layout of the input tensor,
 *   filter tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, output tensor and the convolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the convolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \b diff_y and \b filter should be the multiple of \p group_count.
 *     - The number of output channels of \b filter multiplying \p group_count should be
 *       the number of channels of \b diff_x.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following
 *   additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \b diff_y must be divisible by the number of channels
 *   of output tensor \b diff_x.
 *
 *   multiplier = \p diff_y_c / \p diff_x_c,
 *   sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *   sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *   where \p diff_y_c and \p diff_x_c are the number of channels of \b diff_y
 *   and \b diff_x respectively, \p filter_h and \p filter_w are the height and width of
 *   filter tensor \b filter, \p stride_h and \p stride_w are stride parameters in height and
 *   width set in the convolution descriptor \b conv_desc.
 *   - If multiplier < 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 2016 / int31_flag
 *   - If multiplier >= 2 * sub_kh * sub_kw:
 *     sub_kh * sub_kw <= 126 / (half_flag * int31_flag)
  *
 *   Where /p half_flag is 2 when the data type of output tensor \b diff_x are half,
 *   otherwise /p half_flag is 1. The /p int31_flag is 2 when the data type of input tensor
 *   \b diff_y or \b filter are int31, otherwise /p int31_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Gradient-Based Learning Applied to Document Recognition, Yann Lecun, 1998.
 * - Multi-scale Context Aggregation by Dilated Convolution, Fisher Yu, 2016.
 */
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeConvolutionBackwardData(cnnlHandle_t handle,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const void *filter,
                                    const void *filter_position,
                                    const void *filter_scale,
                                    const void *filter_offset,
                                    const cnnlTensorDescriptor_t diff_y_desc,
                                    const void *diff_y,
                                    const void *diff_y_position,
                                    const void *diff_y_scale,
                                    const void *diff_y_offset,
                                    const cnnlConvolutionDescriptor_t conv_desc,
                                    const cnnlConvolutionBwdDataAlgo_t algo,
                                    void *workspace,
                                    size_t workspace_size,
                                    const void *beta,
                                    const cnnlTensorDescriptor_t diff_x_desc,
                                    void *diff_x);

// Group:Deconvolution
/*!
 * @brief Creates a descriptor pointed by \b desc for a deconvolution operation, and
 * allocates memory for holding the information about the deconvolution operation. The
 * information is defined in ::cnnlDeconvolutionDescriptor_t. For more information about
 * descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. A host pointer to the deconvolution descriptor that holds information about the
 *   deconvolution operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetDeconvolutionDescriptor function to
 *   initialize and set the information to the deconvolution descriptor.
 * - You need to call the ::cnnlDestroyDeconvolutionDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t *desc);

// Group:Deconvolution
/*!
 * @brief Initializes the deconvolution descriptor \b desc that is previously created
 * with the ::cnnlCreateDeconvolutionDescriptor function, and sets the information
 * about the deconvolution operation to the deconvolution descriptor \b desc.
 * The information includes the number of the deconvolution dimensions \b dimNb,
 * the padding size for each dimension \b pad, the stride of the sliding window for
 * each dimension \b stride, the dilation factor for each dimension \b dilation,
 * the number of groups to be split into by channel \b group_count,
 * and the data type \b compute_type that is used in the accumulation.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] dimNb
 *   Input. The number of dimensions in the input tensor of the deconvolution operation.
 *   Currently, the value of this parameter can only be set to 4 or 5.
 *   The value of this parameter should be same as the one you set in the input tensor descriptor.
 * @param[in] pad
 *   Input. An array that stores the zero-padding size for each dimension of the input tensor
 *   used in the deconvolution operation.
 *   For each dimension, the padding size controls the number of zeros to be concatenated at the
 *   start and end of that dimension. The number of zeros is the result of
 *   'dilation * (kernel_size - 1) - pad'.
 *   If \b dimNb is set to 4, the padding is on top, bottom, left, and right.
 *   If \b dimNb is set to 5, the padding is on front, back, top, bottom, left, and right.
 *   The value of this parameter should be greater than or equal to 0.
 * @param[in] stride
 *   Input. An array that stores the filter stride for each dimension of the input tensor
 *   used in the deconvolution operation. For each dimension, the filter stride represents
 *   the spacing between the input points in the convolution operator. For deltails, see the
 *   first link in the Reference section of ::cnnlDeconvolution.
 *   If \b dimNb is set to 4, the stride is in height and width.
 *   If \b dimNb is set to 5, the stride is in depth, height and width.
 *   The value of this parameter should be greater than or equal to 1.
 * @param[in] dilation
 *   Input. An array that stores the dilation factor for each dimension of the filter tensor
 *   used in the deconvolution operation. For each dimension, the dilation factor represents
 *   the spacing between the kernel points. If \b dimNb is set to 4, the dilation should be set in
 *   height and width dimension. If \b dimNb is set to 5, the dilation should be set in depth,
 *   height and width dimension. The value of this parameter should be greater than or equal to 1.
 * @param[in] group_count
 *   Input. The number of groups that the input data is split by the number of channels
 *   in the input tensor. Each group is convolved separately. The filter used for each group is
 *   the filter tensor divides \b group_count. The result of the deconvolution operation
 *   is the concatenation of all the group deconvolution results by the number of channels
 *   in the input tensor. Make sure that the number of channels in the input tensor
 *   and the output tensor are divisible by \b group_count. The value of this parameter should
 *   be greater than or equal to 1. Currently, this parameter only supports 1 or the number
 *   of channels in the output tensor.
 *   - If \b group_count is set to 1, the input tensor is convolved without splitting into groups.
 *   - If \b group_count is set to the number of channels in the output tensor, the depthwise
 *     deconvolution is performed.
 * @param[in] compute_type
 *   Input. The data type of temporary result in deconvolution operation, only supports floating-point
 *   type. Currently, this parameter can only be set to the data type of the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The value of \b dimNb can only be set to 4 or 5.
 * - The value of \b dimNb can only be set to 4, and the value of \b dilation can only be
 *   set to [1, 1], if depthwise deconvolution is preformed.
 * - The value of \b compute_type can only be set to the data type of the output tensor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t desc,
                                                         int dimNb,
                                                         const int pad[],
                                                         const int stride[],
                                                         const int dilation[],
                                                         const int group_count,
                                                         const cnnlDataType_t compute_type);

// Group:Deconvolution
/*!
 * @brief Initializes the deconvolution descriptor \b desc that is previously created
 * with the ::cnnlCreateDeconvolutionDescriptor function, and sets the information
 * about the deconvolution forward and backward operations to the deconvolution descriptor
 * \b desc. This function also includes the \b allow_tf32 parameter that is used to
 * control whether to enable TensorFloat-32.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] allow_tf32
 *   Input. An integer value that determines whether to enable TensorFloat-32.
 *   TensorFloat-32 is enabled by default.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, only supports 4-dimensional and 5-dimensional input tensor for deconvolution
 *   forward or backward operation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDeconvolutionDescriptorAllowTF32(cnnlDeconvolutionDescriptor_t desc,
                                        const int allow_tf32);

// Group:Deconvolution
/*!
 * @brief Sets the reorder type of the filter and bias tensors used in the deconvolution operation.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] filter_reorder_type
 *   Input. The reorder type of the deconvolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[in] bias_reorder_type
 *   Input. The reorder type of the deconvolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you need to call ::cnnlGetReorderDeconvolutionDataSize and
 *   ::cnnlHostReorderDeconvolutionData functions to reorder data for deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetDeconvolutionDescriptorReorderType(cnnlDeconvolutionDescriptor_t deconv_desc,
                                          const cnnlReorderType_t filter_reorder_type,
                                          const cnnlReorderType_t bias_reorder_type);

// Group:Deconvolution
/*!
 * @brief Returns the reorder type of the filter and bias tensors used in the deconvolution operation.
 *
 * @param[in] desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[out] filter_reorder_type
 *   Output. The reorder type of the deconvolution filter. For detailed information,
 *   see ::cnnlReorderType_t
 * @param[out] bias_reorder_type
 *   Output. The reorder type of the deconvolution bias. For detailed information,
 *   see ::cnnlReorderType_t
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlSetDeconvolutionDescriptorReorderType
 *   function to set the reorder type for deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionDescriptorReorderType(const cnnlDeconvolutionDescriptor_t deconv_desc,
                                          cnnlReorderType_t *filter_reorder_type,
                                          cnnlReorderType_t *bias_reorder_type);

// Group:Deconvolution
/*!
 * @brief Destroys a deconvolution descriptor \b desc that is previously created with the
 *        ::cnnlCreateDeconvolutionDescriptor function.
 *
 * The deconvolution descriptor is defined in ::cnnlDeconvolutionDescriptor_t
 * and holds the information about the deconvolution operation.
 *
 * @param[in] desc
 *   Input. The deconvolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlDeconvolution function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the deconvolution descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyDeconvolutionDescriptor(cnnlDeconvolutionDescriptor_t desc);

// Group:Deconvolution
/*!
 * @brief Returns the best suited deconvolution algorithm that can be used in the deconvolution
 *        operation.
 *
 * @deprecated
 *   ::cnnlGetDeconvolutionAlgorithm is deprecated and will be removed in the future release.
 *   It is recommended to use ::cnnlGetDeconvolutionAlgorithm_v2 instead, which allows to pass
 *   \b bias_desc as parameter.
 *
 * The returned algorithm is chosen from the supported deconvolution
 * algorithms defined in ::cnnlDeconvolutionAlgo_t and is
 * based on the given filter descriptor \b filter_desc, input descriptor
 * \b input_desc, deconvolution descriptor \b deconv_desc, output descriptor
 * \b output_desc, and the computing performance preferences \b preference.
 *
 * The computing performance options \b preference defined in the
 * ::cnnlDeconvolutionPreference_t enum, only supports ::CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For
 *   detailed information, see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the deconvolution operation
 *   to get better performance. This parameter only supports \p CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 *  @param[out] algo
 *   Output. The returned algorithm that is best suited for performing the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b input_desc, \b filter_desc and \b output_desc before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in series to create and set the deconvolution
 *   descriptor \b deconv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionAlgorithm(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t input_desc,
                              const cnnlTensorDescriptor_t filter_desc,
                              const cnnlDeconvolutionDescriptor_t deconv_desc,
                              const cnnlTensorDescriptor_t output_desc,
                              const cnnlDeconvolutionPreference_t preference,
                              cnnlDeconvolutionAlgo_t *algo);

// Group:Deconvolution
/*!
 * @brief Returns the best suited algorithm that can be used in the deconvolution
 *        operation.
 *
 * The returned algorithm is chosen from all the supported deconvolution
 * algorithms defined in ::cnnlDeconvolutionAlgo_t and is based on the given filter
 * descriptor \b filter_desc, input descriptor \b input_desc, bias descriptor
 * \b bias_desc, deconvolution descriptor \b deconv_desc, output descriptor \b output_desc,
 * and the computing performance preferences \b preference.
 *
 * The computing performance options \b preference defined in the
 * ::cnnlDeconvolutionPreference_t enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t. Bias is optional. If there
 *   is no bias, this descriptor should be set nullptr.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the deconvolution operation
 *   to get better performance. This parameter only supports
 *   \p CNNL_CONVOLUTION_BWD_DATA_FASTEST.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for performing the
 *   deconvolution operation. The algorithms are defined in the
 *   ::cnnlDeconvolutionAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b input_desc, \b filter_desc, \b bias_desc(optional) and \b output_desc
 *   before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in series to create and set the deconvolution
 *   descriptor \b deconv_desc before this function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionAlgorithm_v2(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const cnnlTensorDescriptor_t bias_desc,
                                 const cnnlDeconvolutionDescriptor_t deconv_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 const cnnlDeconvolutionPreference_t preference,
                                 cnnlDeconvolutionAlgo_t *algo);

// Group:Deconvolution
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deconvolution operation.
 *
 * @deprecated
 *   ::cnnlGetDeconvolutionWorkspaceSize is deprecated and will be removed in the future release.
 *   It is recommended to use ::cnnlGetDeconvolutionWorkspaceSize_v2 instead, which allows to pass
 *   \b bias_desc as parameter.
 *
 * The size of extra workspace is based on the given information of the deconvolution operation,
 * including the input descriptor \b input_desc, filter descriptor \b filter_desc,
 * deconvolution descriptor \b deconv_desc, output descriptor \b output_desc, and the
 * deconvolution algorithm \b algo. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are defined
 *   in the ::cnnlDeconvolutionAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgorithm_v2 function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   deconvolution operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_INTERNAL_ERROR
 *
 * @par API Dependency
 * - The ::cnnlGetDeconvolutionAlgorithm_v2 function must be called to get the best suited
 *   algorithm \b algo before this function.
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b input_desc, \b filter_desc, and \b output_desc before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in series to create and set the deconvolution
 *   descriptor \b deconv_desc before this function.
 * - The allocated extra workspace should be passed to the ::cnnlDeconvolution function to perform
 *   the deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const cnnlTensorDescriptor_t filter_desc,
                                  const cnnlDeconvolutionDescriptor_t deconv_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  const cnnlDeconvolutionAlgo_t algo,
                                  size_t *size);

// Group:Deconvolution
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 *        workspace to optimize the deconvolution operation.
 *
 * The size of extra workspace is based on the given information of
 * the deconvolution operation, including the filter descriptor
 * \b filter_desc, input descriptor \b input_desc, bias descriptor \b bias_desc,
 * deconvolution descriptor \b deconv_desc, output descriptor \b output_desc,
 * and the deconvolution data algorithm \b algo. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor of deconvolution operation. For
 *   detailed information, see ::cnnlTensorDescriptor_t. Bias is optional. If there
 *   is no bias, this descriptor should be set nullptr.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For
 *   detailed information, see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum. You can
 *   get the best suited algorithm with the ::cnnlGetDeconvolutionAlgorithm_v2
 *   function.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the deconvolution operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   should be called in series to create and set the tensor descriptors
 *   \b filter_desc, \b input_desc, \b bias_desc(optional), and \b output_desc
 *   before this function.
 * - The ::cnnlGetDeconvolutionAlgorithm_v2 function must be called to get
 *   the best suited algorithm \b algo before this function.
 * - The ::cnnlCreateDeconvolutionDescriptor and ::cnnlSetDeconvolutionDescriptor
 *   functions should be called in series to create and set the deconvolution
 *   descriptor \b deconv_desc before this function.
 * - The allocated extra workspace should be passed to the
 *   ::cnnlDeconvolution function to perform the deconvolution operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetDeconvolutionWorkspaceSize_v2(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const cnnlTensorDescriptor_t filter_desc,
                                     const cnnlTensorDescriptor_t bias_desc,
                                     const cnnlDeconvolutionDescriptor_t deconv_desc,
                                     const cnnlTensorDescriptor_t output_desc,
                                     const cnnlDeconvolutionAlgo_t algo,
                                     size_t *size);

// Group:Deconvolution
/*!
 * @brief Computes a 2-D or 3-D transposed convolution operator on input tensor \b input with
 *        the filter \b filter, and returns the results in the output tensor \b output.
 *        This function can be seen as the transpose or gradient of ::cnnlConvolutionForward.
 *        The algorithm of ::cnnlDeconvolution and ::cnnlConvolutionBackwardData are the same,
 *        except the supporting of bias and quantization of output. If \b bias is not needed
 *        and \b output is float-point, it is recommended to use ::cnnlConvolutionBackwardData.
 *
 * This function also supports quantization of input tensor \b input, filter tensor \b filter,
 * and output tensor \b output, which means converting those tensors from floating-point to
 * fixed-point numbers through quantization parameters. The parameters should be set in the
 * corresponding descriptors by ::cnnlSetTensorDescriptorPositionAndScale in advance.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * The deconvolution operation is computed based on the algorithm set in \b algo.
 * You can get the algorithm with the ::cnnlGetDeconvolutionAlgorithm_v2 function.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \b deconv_desc is set to the number of channels of the output tensor
 * \b output.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the ::cnnlGetDeconvolutionAlgorithm_v2
 *   function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for this operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the deconvolution
 *   operation. You can get the size of the workspace with the ::cnnlGetDeconvolutionWorkspaceSize_v2
 *   function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - If \p dimNb is set to 4 or 5, this function supports any combinations of the following data types
 *   for the input tensor, filter tensor, bias tensor and output tensor:
 *   - input tensor: int8, int16, int31.
 *   - filter tensor: int8, int16, int31.
 *   - output tensor: half, float, int8, int16.
 *   - bias tensor: half, float.
 *   - When the deconvolution operation is depthwise, the additional supported data type
 *     combinations with the order input-filter-bias-output are half-half-half-half
 *     and float-float-float-float.
 *
 * - To perform quantization on input tensor, filter tensor or output tensor, both offchip data
 *   type and onchip data type should be set as following combinations with the order
 *   dtype-onchip_dtype in advance. <b>Note that the offchip data type of input tensor and filter
 *   tensor must have the same float-point type: half-half or float-float, while the onchip data
 *   type can be different. Also, onchip data type of output tensor(for float-point output, which
 *   is actually off-chip data type of output tensor) must have the same float-point type with
 *   input tensor.</b>
 *   - input tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *   - filter tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *   - output tensor: int8-half, int16-half, int8-float, int16-float.
 *
 * - This function also supports float-point computation on MLU370 when \p dimNb is set to 4 or 5.
 *   To perform float-point computation, offchip data type combinations with the order
 *   input-filter-output should be half-half-half or float-float-float, while onchip data type
 *   should be \p CNNL_DTYPE_INVALID or the same as the corresponding offchip data type.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layout of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layout of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \b input and \b filter should be the multiple of \p group_count.
 *     - The number of output channels of \b filter multiplying \p group_count should be
 *       the number of channels of \b output and \b bias if \b bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \b input must be divisible by the number of channels
 *     of output tensor \b output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     where \p input_c and \p output_c are the number of channels of \b input
 *     and \b output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \b filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \b deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016 / int31_flag
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / (half_flag * int31_flag)
 *
 *     Where /p half_flag is 2 when the data type of output tensor \b output are half,
 *     otherwise /p half_flag is 1. The /p int31_flag is 2 when the data type of input tensor
 *     \b input or \b filter are int31, otherwise /p int31_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement deconvolution, you need to prepare all the
 *   parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - There is no need to quantize both the input and output tensors in this function.
 *   This function also supports quantizing one of them only.
 * - There is no need to quantize both the input and filter tensors in this function.
 *   This function also supports quantization the input tensor only. But it does not
 *   support quantizing the filter tensor only.
 * - Bias tensor does not support quantization in this function, which means the data type of
 *   \b bias should be the same as the onchip data type of \b output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDeconvolution(cnnlHandle_t handle,
                                            const void *alpha,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const cnnlTensorDescriptor_t filter_desc,
                                            const void *filter,
                                            const cnnlTensorDescriptor_t bias_desc,
                                            const void *bias,
                                            const cnnlDeconvolutionDescriptor_t deconv_desc,
                                            const cnnlDeconvolutionAlgo_t algo,
                                            void *workspace,
                                            size_t workspace_size,
                                            const void *beta,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:Deconvolution
/*!
 * @brief Computes a 2-D or 3-D transposed convolution operator on input tensor \b input with
 * the filter \b filter, and returns the results in the output tensor \b output.
 * This function is only used for inference and can be seen as the transpose or gradient of
 * ::cnnlConvolutionForwardInference. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function. The deconvolution operation is
 * computed based on the algorithm set in \b algo. You can call
 * the ::cnnlGetDeconvolutionAlgorithm_v2 to get the most suited algorithm.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \b deconv_desc is set to the number of channels of the output tensor
 * \b output.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation.
 *   For detailed information, see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgorithm_v2 function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deconvolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deconvolution operation. You can get the size of the workspace
 *   with the ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - If \p dimNb is set to 4 or 5, this function supports any combinations of the following data types
 *   for the input tensor, filter tensor, bias tensor and output tensor:
 *   - input tensor: int8, int16, int31.
 *   - filter tensor: int8, int16, int31.
 *   - output tensor: half, float, int8, int16.
 *   - bias tensor: half, float.
 * - To perform quantization on input tensor, filter tensor or output tensor, both offchip data
 *   type and onchip data type should be set as following combinations with the order
 *   dtype-onchip_dtype in advance. <b>Note that the offchip data type of input tensor and filter
 *   tensor must have the same float-point type: half-half or float-float, while the onchip data
 *   type can be different. Also, onchip data type of output tensor(for float-point output, which
 *   is actually off-chip data type of output tensor) must have the same float-point type with
 *   input tensor.</b>
 *   - input tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *   - filter tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *   - output tensor: int8-half, int16-half, int8-float, int16-float.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layout of the input tensor,
 *   filter tensor, bias_tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layout of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias_tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \b input and \b filter should be the multiple of \p group_count.
 *     - The number of output channels of \b filter multiplying \p group_count should be
 *       the number of channels of \b output and \b bias if \b bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \b input must be divisible by the number of channels
 *     of output tensor \b output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     where \p input_c and \p output_c are the number of channels of \b input
 *     and \b output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \b filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \b deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016 / int31_flag
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / (half_flag * int31_flag)
 *
 *     Where /p half_flag is 2 when the data type of output tensor \b output are half,
 *     otherwise /p half_flag is 1. The /p int31_flag is 2 when the data type of input tensor
 *     \b input or \b filter are int31, otherwise /p int31_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - Depthwise deconvolution does not support onchip quantization currently.
 * - 3D deconvolution only supports \p CNNL_NO_QUANTIZE and \p CNNL_OFFLINE_SYMMETRIC_QUANTIZE as \b cast_mode currently.
 * - There is no need to quantize both the input and output tensors in this function.
 *   This function also supports quantizing one of them only.
 * - There is no need to quantize both the input and filter tensors in this function.
 *   This function also supports quantizing the input tensor only. But it does not
 *   support quantizing the filter tensor only.
 * - Bias tensor does not support quantization in this function, which means the data type of
 *   \b bias should be the same as the onchip data type of \b output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlDeconvolutionInference(cnnlHandle_t handle,
                           const void *alpha,
                           const cnnlTensorDescriptor_t input_desc,
                           const void *input,
                           const cnnlTensorDescriptor_t filter_desc,
                           const void *filter,
                           const cnnlTensorDescriptor_t bias_desc,
                           const void *bias,
                           const cnnlDeconvolutionDescriptor_t deconv_desc,
                           const cnnlDeconvolutionCastMode_t cast_mode,
                           const cnnlDeconvolutionAlgo_t algo,
                           void *workspace,
                           size_t workspace_size,
                           const void *beta,
                           const cnnlTensorDescriptor_t output_desc,
                           void *output);

// Group:Deconvolution
/*!
 * @brief Converts the input tensor \b input and the filter tensor \b filter from float-point
 * to fixed-point numbers through quantization parameters, and then computes the output tensor
 * \b output by the deconvolution operation. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetDeconvolutionWorkspaceSize_v2 function. The deconvolution operation is
 * computed based on the algorithm set in \b algo. You can call
 * the ::cnnlGetDeconvolutionAlgorithm_v2 to get the most suited algorithm.
 *
 * Depthwise deconvolution operation is performed when the \p group_count parameter of the
 * deconvolution descriptor \b deconv_desc is set to the number of channels of the output tensor
 * \b output.
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the deconvolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved scaling parameters for future use. Set the value of
 *   these parameters to NULL.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] input_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the input tensor.
 * @param[in] input_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the input tensor.
 * @param[in] input_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor used as a filter in the
 *   deconvolution operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor.
 * @param[in] filter_position
 *   Input. Pointer to the MLU memory that stores the position factor for
 *   quantization of the filter tensor.
 * @param[in] filter_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for
 *   quantization of the filter tensor.
 * @param[in] filter_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for
 *   quantization of the filter tensor.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation.
 *   For detailed information, see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the deconvolution operation.
 *   The algorithms are defined in the ::cnnlDeconvolutionAlgo_t enum.
 *   You can get the best suited algorithm with the
 *   ::cnnlGetDeconvolutionAlgorithm_v2 function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   deconvolution operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the deconvolution operation. You can get the size of the workspace
 *   with the ::cnnlGetDeconvolutionWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * * @par Formula
 * - See "Deconvolution Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - If \p dimNb is set to 4 or 5, this function supports any combinations of the following data types for the
 *   input tensor, filter tensor, bias tensor and output tensor:
 *   - input tensor: half, float.
 *   - filter tensor: half, float.
 *   - output tensor: half, float.
 *   - bias tensor: half, float.
 *   - To perform quantization on input tensor and filter tensor, both offchip data
 *     type and onchip data type should be set as following combinations with the order
 *     dtype-onchip_dtype in advance. <b>Note that the offchip data type of input tensor
 *     and filter tensor must have the same float-point type: half-half or float-float,
 *     while the onchip data type can be different. Also, onchip data type of output tensor
 *     (for float-point output, which is actually off-chip data type of output tensor) must
 *     have the same float-point type with input tensor.</b>
 *     - input tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *     - filter tensor: half-int8, half-int16, half-int31, float-int8, float-int16, float-int31.
 *
 * @par Data Layout
 * - If \p dimNb is set to 4, the supported data layout of the input tensor, filter tensor, bias_tensor and output tensor
 *   are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, and \p CNNL_LAYOUT_NCHW.
 *
 * - If \p dimNb is set to 5, the supported data layout of the input tensor,
 *   filter tensor, bias tensor and output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NDHWC.
 *   - filter tensor: \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, filter tensor, bias_tensor, output tensor and the deconvolution descriptor
 *   should meet the following requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1. If the deconvolution operation is not depthwise, additional conditions
 *     should be satisfied for parameter \p group_count as below:
 *     - The number of channels of both \b input and \b filter should be the multiple of \p group_count.
 *     - The number of output channels of \b filter multiplying \p group_count should be
 *       the number of channels of \b output and \b bias if \b bias exists.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - filter tensor: \p height > 0, \p width > 0
 *   - bias tensor: \p batch = 1, \p height = 1, \p width = 1
 *   - output tensor: \p height > 0, \p width > 0, \p channel > 0
 *
 * - When the operation is depthwise, the parameters should meet the following additional requirements:
 *   - \p dimNb == 4
 *   - \p dilation == 1
 *   - The number of channels of input tensor \b input must be divisible by the number of channels
 *     of output tensor \b output.
 *
 *     multiplier = \p input_c / \p output_c,
 *     sub_kh = (\p filter_h + \p stride_h - 1) / \p stride_h,
 *     sub_kw = (\p filter_w + \p stride_w - 1) / \p stride_w
 *
 *     Where \p input_c and \p output_c are the number of channels of \b input
 *     and \b output respectively, \p filter_h and \p filter_w are the height and width of
 *     filter tensor \b filter, \p stride_h and \p stride_w are stride parameters in height and
 *     width set in the deconvolution descriptor \b deconv_desc.
 *     - If multiplier < 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 2016 / int31_flag
 *     - If multiplier >= 2 * sub_kh * sub_kw:
 *       sub_kh * sub_kw <= 126 / (half_flag * int31_flag)
 *
 *     Where /p half_flag is 2 when the data type of output tensor \b output are half,
 *     otherwise /p half_flag is 1. The /p int31_flag is 2 when the data type of input tensor
 *     \b input or \b filter are int31, otherwise /p int31_flag is 1.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward data, you
 *   need to prepare all the parameters passed to this function. See each parameter
 *   description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the input
 *   tensor and output tensor to NHWC, and filter tensor to HWCN.
 *
 * @note
 * - Depthwise deconvolution does not support onchip quantization currently.
 * - 3D deconvolution only supports \p CNNL_NO_QUANTIZE and \p CNNL_OFFLINE_SYMMETRIC_QUANTIZE as \b cast_mode currently.
 * - Bias tensor does not support quantization in this function, which means the data type of
 *   \b bias should be the same as the onchip data type of \b output.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/vdumoulin/conv_arithmetic/blob/mater/README.md
 * - www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html
 * - www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose
 * - www.pytorch.org/docs/stable/generated/torch.nn.ConvTranspose3d.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeDeconvolution(cnnlHandle_t handle,
                          const void *alpha,
                          const cnnlTensorDescriptor_t input_desc,
                          const void *input,
                          const void *input_position,
                          const void *input_scale,
                          const void *input_offset,
                          const cnnlTensorDescriptor_t filter_desc,
                          const void *filter,
                          const void *filter_position,
                          const void *filter_scale,
                          const void *filter_offset,
                          const cnnlTensorDescriptor_t bias_desc,
                          const void *bias,
                          const cnnlDeconvolutionDescriptor_t deconv_desc,
                          const cnnlDeconvolutionCastMode_t cast_mode,
                          const cnnlDeconvolutionAlgo_t algo,
                          void *workspace,
                          size_t workspace_size,
                          const void *beta,
                          const cnnlTensorDescriptor_t output_desc,
                          void *output);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Computes the filter gradient of convolution operation ::cnnlConvolutionForward.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetConvolutionBackwardFilterWorkspaceSize function. The convolution backward
 * filter operation is computed based on the algorithm set in \b algo.
 * You can call the ::cnnlGetConvolutionBackwardFilterAlgorithm function to get the most
 * suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor that is the gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y
 *   Input. Pointer to the MLU memory that stores the input differerntial tensor.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The algorithm used to compute the convolution backward filter. The algorithm is
 *   defined in the ::cnnlConvolutionBwdFilterAlgo_t enum. You can get the best suited algorithm
 *   with the ::cnnlGetConvolutionBackwardFilterAlgorithm function.
 * @param[in]  workspace
 *   Input. Pointer to MLU memory that is used as an extra workspace for the convolution backward
 *   filter operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the convolution
 *   backward filter operation. You can get the size of the workspce with the
 *   ::cnnlGetConvolutionBackwardFilterWorkspaceSize function.
 * @param[in]  diff_w_desc
 *   Input. The descriptor of the output differential tensor that is the gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  diff_w
 *   Output. Pointer to the MLU memory that stores the output differential tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "ConvolutionBackwardFilter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b x,
 *   input differential tensor \b diff_y, and output differential tensor \b diff_w.
 *   - \b x: int8, int16, int31.
 *   - \b diff_y: int8, int16, int31.
 *   - \b diff_w: half, float.
 * - If \p dimNb is set to 4, this function also supports float-point computation on MLU300 series.
 *   The supported combinations of float-point data type on MLU300 series are shown below with the following order:
 *   \b x - \b diff_y - \b diff_w
 *   The supported float-point data type combinations are:
 *   - half-half-half.
 *   - float-float-float.
 *
 * @par Data Layout
 * - The supported data layout or the input tensor, input differential tensor and output
 *   differential tensor are as follows:
 *   - \b x: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC
 *   - \b diff_y: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC
 *   - \b diff_w: \p CNNL_LAYOUT_NHWC , \p CNNL_LAYOUT_HWCN , \p CNNL_LAYOUT_NDHWC and \p CNNL_LAYOUT_NCDHW.
 * - If the value of \b group_count set in the convolution descriptor \b conv_desc equals to the number of
 *   channels in the input tensor, the layout of the output differential tensor can only be
 *   \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The input tensor, input differential tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count >= 1
 *   - \b x: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - \b diff_y: \p height > 0, \p width > 0, \p channel > 0
 *   - \p onchip_dtype of \b diff_w_desc must be the same as \p compute_type of \b conv_desc.
 *   - only support \b dimNb equals to 4 and 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward filter, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the output differential
 *   tensor to \p CNNL_LAYOUT_HWCN.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution backward filter operation is as follows:
     @verbatim
      input two arrays by 1 * 3 * 3 * 2 and 1 * 2 * 2 * 1
      --> \b x: [[[[1, 2], [3, 4], [5, 6]],
                  [[7, 8], [9, 10], [11, 12]],
                  [[13, 14], [15, 16], [17, 18]]]]

      --> \b diff_y: [[[[1],[2]],
                       [[3],[4]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output differential array by 1 * 3 * 3 * 2
      --> diff_w: [[[[36, 40], [65, 72], [27, 30]],
                    [[66, 72], [115, 128], [48, 52]],
                    [[18, 20], [29, 32], [9, 10]]]]
     @endverbatim
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/conv2d_backprop_filter.
 */

cnnlStatus_t CNNL_WIN_API cnnlConvolutionBackwardFilter(cnnlHandle_t handle,
                                                        const void *alpha,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const void *x,
                                                        const cnnlTensorDescriptor_t diff_y_desc,
                                                        const void *diff_y,
                                                        const cnnlConvolutionDescriptor_t conv_desc,
                                                        cnnlConvolutionBwdFilterAlgo_t algo,
                                                        void *workspace,
                                                        size_t workspace_size,
                                                        const void *beta,
                                                        const cnnlTensorDescriptor_t diff_w_desc,
                                                        void *diff_w);

// Group:ConvolutionBackwardFilter
/*!
 * @brief Converts the floating-point data of input tensor \b x and
 * input differential tensor \b diff_y into fixed-point numbers according to the quantization
 * parameters, then computes the filter gradient of the convolution
 * and returns the results in the output differential tensor \b diff_w. For more information
 * about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the performance.
 * You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetConvolutionBackwardFilterWorkspaceSize function. The convolution backward
 * filter operation is computed based on the algorithm set in \b algo.
 * You can call the ::cnnlGetConvolutionBackwardFilterAlgorithm function to get the most
 * suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameter to NULL.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  x_position
 *   Input. Pointer to the MLU memory that stores the position factor for input tensor.
 * @param[in]  x_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for input tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  x_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for input tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y
 *   Input. Pointer to the MLU memory that stores the input differerntial tensor.
 * @param[in]  diff_y_position
 *   Input. Pointer to the MLU memory that stores the position factor for input differential tensor.
 * @param[in]  diff_y_scale
 *   Input. Pointer to the MLU memory that stores the scale factor for input differential tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  diff_y_offset
 *   Input. Pointer to the MLU memory that stores the offset factor for input differential tensor.
 *   Reserved for future use. Set the value to NULL.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The algorithm used to compute the convolution backward filter. The algorithm are
 *   defined in the ::cnnlConvolutionBwdFilterAlgo_t enum. You can get the best suited algorithm
 *   with the ::cnnlGetConvolutionBackwardFilterAlgorithm function.
 * @param[in]  workspace
 *   Input. Pointer to MLU memory that is used as an extra workspace for the convolution backward
 *   filter operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the convolution
 *   backward filter operation. You can get the size of the workspce with the
 *   ::cnnlGetConvolutionBackwardFilterWorkspaceSize function.
 * @param[in]  diff_w_desc
 *   Input. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  diff_w
 *   Output. Pointer to the MLU memory that stores the output differential tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "ConvolutionBackwardFilter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b x,
 *   input differential tensor \b diff_y, and output differential tensor \b diff_w, However the
 *   data types of \b x and \b diff_y should be both floating point or both integer.
 *   if data types of \b x and \b diff_y are both floating point, the data types of these two
 *   tensor should be consistent.
 *   if data types of \b x and \b diff_y are both integer, the data type of tensor and its onchip
 *   data type should be consistent.
 *   - \b x: half, float, int8, int16 int31.
 *   - \b diff_y: half, float, int8, int16, int31.
 *   - \b diff_w: half, float.
 * - The onchip data type is set as \p onchip_dtype parameter in tensor descriptor.
 *   For more detailed information, see ::cnnlSetTensorDescriptorOnchipDataType.
 * - This function supports any combinations of the following data types as the
 *   \p onchip_dtype for the input tensor \b x and input differential tensor \b diff_y.
 *   - \b x: int8, int16, int31.
 *   - \b diff_y: int8, int16, int31.
 *   - \b diff_w: half, float.
 *   The \p onchip_dtype for the output differential must have higher bit-width than data type for
 *   output differential tensor.
 *
 * @par Data Layout
 * - The supported data layout or the input tensor, input differential tensor and output
 *   differential tensor are as follows:
 *   - \b x: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC
 *   - \b diff_y: \p CNNL_LAYOUT_NHWC and \p CNNL_LAYOUT_NDHWC
 *   - \b diff_w: \p CNNL_LAYOUT_NHWC , \p CNNL_LAYOUT_HWCN and \p CNNL_LAYOUT_NDHWC. If the
 *   value of \b group_count set in the convolution descriptor \b conv_desc equals to the number
 *   of channels in the input tensor, the layout of the output differential tensor can only be
 *   \p CNNL_LAYOUT_HWCN.
 *
 * @par Scale Limitation
 * - The input tensor, input differential tensor, and the convolution descriptor
 *   (including pad, stride, dilation, and group_count) must meet the following
 *   requirements:
 *   - \p pad >= 0
 *   - \p stride >= 1
 *   - \p dilation >= 1
 *   - \p group_count equals to 1 or the number of channels in the input tensor.
 *   - \b x: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - \b diff_y: \p height > 0, \p width > 0, \p channel > 0
 *   - \p onchip_dtype of \b diff_w_desc must be the same as \p compute_type of \b conv_desc.
 *   - \b dimNb can only be set to 4 and 5.
 *
 * @par API Dependency
 * - Before calling this function to implement convolution backward filter, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the output differential
 *   tensor to \p CNNL_LAYOUT_HWCN.
 *
 * @note
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the convolution backward filter operation is as follows:
     @verbatim
      input two arrays by 1 * 3 * 3 * 2 and 1 * 2 * 2 * 1
      --> \b x: [[[[1, 2], [3, 4], [5, 6]],
                  [[7, 8], [9, 10], [11, 12]],
                  [[13, 14], [15, 16], [17, 18]]]]

      --> \b diff_y: [[[[1],[2]],
                       [[3],[4]]]]

      param:
        pad: (1, 1, 1, 1), stride: (2, 2), dilation: (1, 1),

      output differential array by 1 * 3 * 3 * 2
      --> diff_w: [[[[36, 40], [65, 72], [27, 30]],
                    [[66, 72], [115, 128], [48, 52]],
                    [[18, 20], [29, 32], [9, 10]]]]
     @endverbatim
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/compat/v1/nn/conv2d_backprop_filter.
 */
cnnlStatus_t CNNL_WIN_API cnnlQuantizeConvolutionBackwardFilter(cnnlHandle_t handle,
                                                        const void *alpha,
                                                        const cnnlTensorDescriptor_t x_desc,
                                                        const void *x,
                                                        const void *x_position,
                                                        const void *x_scale,
                                                        const void *x_offset,
                                                        const cnnlTensorDescriptor_t diff_y_desc,
                                                        const void *diff_y,
                                                        const void *diff_y_position,
                                                        const void *diff_y_scale,
                                                        const void *diff_y_offset,
                                                        const cnnlConvolutionDescriptor_t conv_desc,
                                                        cnnlConvolutionBwdFilterAlgo_t algo,
                                                        void *workspace,
                                                        size_t workspace_size,
                                                        const void *beta,
                                                        const cnnlTensorDescriptor_t diff_w_desc,
                                                        void *diff_w);
// Group:ConvolutionBackwardFilter
/*!
 * @brief Returns the most suited algorithm that can be used in the convolution backward filter
 * operation.
 *
 * The returned algorithm is chosen from all the supported convolution backward filter
 * algorithms defined in ::cnnlConvolutionBwdFilterAlgo_t. It is chosen based on the given convolution
 * descriptor \b conv_desc, input tensor \b x_desc, input differential tensor \b diff_y_desc,
 * output differential tensor \b diff_w_desc,
 * and the computing performance preferences \b preference.
 *
 * The computing performance option \b preference defined in the
 * ::cnnlConvolutionBwdFilterPreference_t enum.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_w_desc
 *   Output. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  preference
 *   Input. The options for implementing the convolution backward filter opertion.
 * @param[out]  algo
 *   Output. The returned algorithm that is best suited for computing
 *   the convolution backward filter. The algorithm are defined in
 *   ::cnnlConvolutionBwdFilterAlgo_t enum.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardFilterAlgorithm(cnnlHandle_t handle,
                                          const cnnlConvolutionDescriptor_t conv_desc,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const cnnlTensorDescriptor_t diff_y_desc,
                                          const cnnlTensorDescriptor_t diff_w_desc,
                                          const cnnlConvolutionBwdFilterPreference_t preference,
                                          cnnlConvolutionBwdFilterAlgo_t *algo);
// Group:ConvolutionBackwardFilter
/*!
 *  @brief Returns in \b size of the MLU memory that is used as an extra workspace to optimize the
 *  convolution backward filter operation.
 *
 *  The size of extra workspace is based on the given information of the convolution backward
 *  filter operation, including the input tensor descriptor \b x_desc, input differential tensor
 *  descriptor \b diff_y_desc, output differential tensor descriptor \b diff_w_desc, convolution
 *  descriptor \b conv_desc, and the convolution backward filter algorithm \b algo. For more
 *  information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   convolution backward filter operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_y_desc
 *   Input. The descriptor of the input differential tensor, gradient with respect to the output
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  diff_w_desc
 *   Output. The descriptor of the output differential tensor, gradient with respect to the filter
 *   of the convolution. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information see
 *   ::cnnlConvolutionDescriptor_t.
 * @param[in]  algo
 *   Input. The returned algorithm that is best suited for computing
 *   the convolution backward filter. The algorithm are defined in
 *   ::cnnlConvolutionBwdFilterAlgo_t enum.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the convolution backward filter operation.
 * @par Return
 * - CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlGetConvolutionBackwardFilterAlgorithm fucntion.
 *   You also need to call
 *   the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor funcions to create and set
 *   the tensor descriptor \b x_desc, \b diff_y_desc and \b diff_w_desc before calling this
 *   function. The Allocated extra workspace should be passed to the
 *   ::cnnlConvolutionBackwardFilter function to perform the convolution backward filter operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetConvolutionBackwardFilterWorkspaceSize(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const cnnlTensorDescriptor_t diff_w_desc,
                                              const cnnlConvolutionDescriptor_t conv_desc,
                                              cnnlConvolutionBwdFilterAlgo_t algo,
                                              size_t *size);

// Group:FloorDiv
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floordiv operation.
 *
 * The size of extra workspace is based on the given information of the floordiv operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and the output
 * tensor descriptor \b output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floordiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the floordiv
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of
 *   \b input1 and \b input2, c3_dim represents the dimension of \b output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorDivWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input1_desc,
                                                       const cnnlTensorDescriptor_t input2_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:FloorDiv
/*!
 * @brief Computes floordiv on input tensor \b input1 and \b input2, and returns the results
 *        in the output tensor \b output.
 *
 * To set the computing with faster algorithm or higher precision, call ::cnnlFloorDiv_v2.
 *
 * @deprecated
 *   ::cnnlFloorDiv is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlFloorDiv_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * This function may need extra MLU memory as the workspace to improve the floordiv performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floordiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordiv operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FloorDiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b input1
 *   and \b input2, c3_dim represents the dimension of \b output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDiv function to perform the
 *   floordiv operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - This API returns -1 if the input \b input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, the accuracy problem may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorDiv(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input1_desc,
                                       const void *input1,
                                       const cnnlTensorDescriptor_t input2_desc,
                                       const void *input2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       void *workspace,
                                       size_t workspace_size);

// Group:FloorDiv
/*!
 * @brief Divides \b input1 / \b input2, rounding toward the most negative integer, and returns
 *        the results in the output tensor \b output.
 *
 * Compared with ::cnnlFloorDiv, this function allows you to choose whether to perform floordiv
 * operation with faster algorithm or higher precision.
 *
 * This function may need extra MLU memory as the workspace to improve the floordiv performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetFloorDivWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floordiv
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floordiv operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floordiv operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FloorDiv Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b input1
 *   and \b input2, c3_dim represents the dimension of \b output:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorDiv_v2 function to perform the
 *   floordiv operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 * - This API returns -1 if the input \b input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, the accuracy problem may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorDiv_v2(cnnlHandle_t handle,
                                          cnnlComputationPreference_t prefer,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void *input1,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void *input2,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          void *workspace,
                                          size_t workspace_size);

// Group:FloorMod
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floormod operation.
 *
 * The size of extra workspace is based on the given information of the floormod operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and the output
 * tensor descriptor \b output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floormod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the floormod
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorModWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input1_desc,
                                                       const cnnlTensorDescriptor_t input2_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:FloorMod
/*!
 * @brief Computes floormod on input tensor \b input1 and \b input2, and returns the results
 *        in the output tensor \b output.
 *
 * This function may need extra MLU memory as the workspace to improve the floormod performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetFloorModWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floormod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floormod operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floormod operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorModWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FloorMod Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 *  @par Data Layout
 *  - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *  - According to the rule of tensor broadcast, the parameters should satisfy the following
 *    conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b input1
 *    and \b input2, c3_dim represents the dimension of \b output:
 *
 *    min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *    max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorMod function to perform the
 *   floormod operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorMod(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input1_desc,
                                       const void *input1,
                                       const cnnlTensorDescriptor_t input2_desc,
                                       const void *input2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output,
                                       void *workspace,
                                       size_t workspace_size);

// Group:FloorModTrunc
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as
 * an extra workspace to optimize the floormodtrunc operation. The size of extra
 * workspace is based on the given information of the floormodtrunc operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and
 * the output tensor descriptor \b output_desc. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 * in the floormodtrunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 * used in the floormodtrunc operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetFloorModTruncWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input1_desc,
                                  const cnnlTensorDescriptor_t input2_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  size_t *workspace_size);

// Group:FloorModTrunc
/*!
 * @brief Computes the element-wise remainder of \b input1 tensor divided by
 * \b input2 tensor, and  returns the results in the output tensor \b output.
 * This operator is the requirement of Fmod operator under pytorch framework.
 * This function may need extra MLU memory as the workspace to improve the
 * floormodtrunc performance. You can get the size of the workspace \b
 * workspace_size with the ::cnnlGetFloorModWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 * floormodtrunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 * floormodtrunc operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floormodtrunc
 * operation. You can get the size of the workspace with ::cnnlGetFloorModTruncWorkspaceSize
 * function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FloorModTrunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 *  @par Data Layout
 *  - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 *  @par Scale Limitation
 *  - According to the rule of tensor broadcast, the parameters should satisfy the following
 *    conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions
 *    of \b input1 and \b input2, c3_dim represents the dimension of \b output:
 *
 *    min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *    max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorModTrunc function to perform
 * the floormodtrunc operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional array, supporting up to CNNL_DIM_MAX
 * dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.fmod.html
 */

cnnlStatus_t CNNL_WIN_API
cnnlFloorModTrunc(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t input1_desc,
                  const void *input1,
                  const cnnlTensorDescriptor_t input2_desc,
                  const void *input2,
                  const cnnlTensorDescriptor_t output_desc,
                  void *output,
                  void *workspace,
                  size_t workspace_size);

// Group:FloorModPro
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the FloorModPro operation.
 *
 * @deprecated
 *   ::cnnlGetFloorModProWorkspaceSize is deprecated and will be removed in the future release. It is recommended to use
 *   ::cnnlGetFloorModWorkspaceSize.
 *
 *  @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the FloorModPro
 *   operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *  @param[out]  size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the FloorModPro
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFloorModProWorkspaceSize(cnnlHandle_t handle,
                                                          const cnnlTensorDescriptor_t output_desc,
                                                          size_t *size);

// Group:FloorModPro
/*!
 * @brief Computes floormod on input tensor \b input1 and \b input2, and returns the results
 *        in the output tensor \b output.
 *
 * This function may need extra MLU memory as the workspace to improve the FloorModPro performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetFloorModProWorkspaceSize
 * function.
 *
 * @deprecated
 *   ::cnnlFloorModPro is deprecated and will be removed in the future release. It is recommended to use
 *   ::cnnlFloorMod.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the FloorModPro
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the shape of the first input tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the FloorModPro operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the FloorModPro operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorModProWorkspaceSize function.
 * @param[in]  input1_shape
 *   Input. An MLU tensor, it stores the shape of the input1.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the shape of the second input tensor.
 * @param[in]  input2_shape
 *   Input. An MLU tensor, it stores the shape of the input2.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output
 *   Output. Pointer to the MLU memory that stores the shape of the output tensor.
 * @param[in]  output_shape
 *   Input. An MLU tensor, it stores the shape of the output.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
  * @par Formula
 * - See "FloorModPro Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 *  - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - Tensors \b input1 and \b input2 support broadcasting and they should satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlFloorModPro function to perform the
 *   FloorModPro operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorModPro(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void *input1,
                                          void *workspace,
                                          size_t workspace_size,
                                          void *input1_shape,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void *input2,
                                          void *input2_shape,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          void *output_shape);
// Group:Round
/*!
 *  @brief
 *  Creates a Round operation. Rounds the values of a tensor to the nearest even integer
 *  element-wise. This operation supports any dimensions of round operation.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the round operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in]  x_desc
 *    Input. The descriptor of input data. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  x
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  y_desc
 *    Input. The descriptor of output data. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out]  y
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *
 *  @par Return
 *   - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 *  @par Data Type
 *   - The data type of the input tensor should be the same as the data type of the output tensor.
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @par Scale Limitation
 *   - The layout of the input tensor and output tensor should be same.
 *   - The shape of the input tensor and output tensor should be same.
 *   - The total number of dimensions of the input tensor and output tensor should be same.
 *
 * @note
 *  - You can specify the stride of all dimensions for input_desc and output_desc
 *      with ::cnnlSetTensorDescriptorEx.
 *  - input value must be in the range of [-2^23, 2^23], when the device is MLU200 series and data
 *      type is float.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/round
 */
cnnlStatus_t CNNL_WIN_API cnnlRound(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);
// Group:Softmax
/*!
 * @brief Computes a Softmax or Log(Softmax) on the input tensor \b x, and returns the
 * results in the output tensor \b y.
 *
 * This function is applied to a 3-dimensional input tensor only.
 * So you need to reshape the input tensor to be 3-dimensional with ::cnnlTranspose
 * before invoking this operation. Also, you need to reshape the output tensor
 * to original shape with ::cnnlTranspose after invoking this operation.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the softmax forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute softmax. The algorithms are defined
 *   in the ::cnnlSoftmaxAlgorithm_t enum.
 * @param[in] mode
 *   Input. The mode used to compute softmax. The modes are defined
 *   in the ::cnnlSoftmaxMode_t enum.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 * @par Data Type
 * - Data types of input tensor \b x and output tensor \b y must be the same.
 * - The supported data types are as follows:
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @par Limitations
 * - When \b algorithm is \p CNNL_SOFTMAX_FAST, the range of input tensor should be in [-0.5, 0.5]
 *   on MLU200 series and in [-50, 50] on other platforms.
 * - The input tensor and output tensor should be 3-dimensional and the dimension length of output tensor
 *   and input tensor must be same.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set \b mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - The algorithm \p CNNL_SOFTMAX_FAST is not recommended.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Softmax Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softmax operation is as follows:
     @verbatim
       input array by 1 * 1 * 4
       --> x: [[[1, 1, 1, 1]]]

       param:
         algorithm: CNNL_SOFTMAX_ACCURATE, mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

       output array by 1 * 1 * 4
       --> y:[[[0.25, 0.25, 0.25, 0.25]]]
     @endverbatim
 *
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxForward(cnnlHandle_t handle,
                                             cnnlSoftmaxAlgorithm_t algorithm,
                                             cnnlSoftmaxMode_t mode,
                                             const void *alpha,
                                             const cnnlTensorDescriptor_t x_desc,
                                             const void *x,
                                             const void *beta,
                                             const cnnlTensorDescriptor_t y_desc,
                                             void *y);
// Group:Softmax
/*!
 * @brief Computes a Softmax or Log(Softmax) on the input tensor \b x, and returns the
 * results in the output tensor \b y.
 *
 * This function is applied to a 3-dimensional input tensor only.
 * So you need to reshape the input tensor to be 3-dimensional with ::cnnlTranspose
 * before invoking this operation. Also, you need to reshape the output tensor
 * to original shape with ::cnnlTranspose after invoking this operation.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * Compared with ::cnnlSoftmaxForward, this function supports the parameter of\b prefer
 * that sets the computing with faster speed or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the softmax forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] algorithm
 *   Input. The algorithm used to compute softmax. The algorithms are defined
 *   in the ::cnnlSoftmaxAlgorithm_t enum.
 * @param[in] mode
 *   Input. The mode used to compute softmax. The modes are defined
 *   in the ::cnnlSoftmaxMode_t enum.
 * @param[in] prefer
 *   Input. The preference used to compute softmax. The preferences are defined
 *   in the ::cnnlComputationPreference_t enum.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 * @par Data Type
 * - Data types of input tensor \b x and output tensor \b y must be the same.
 * - The supported data types are as follows:
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *
 * @par Limitations
 * - When \b algorithm is \p CNNL_SOFTMAX_FAST, the range of input tensor should be in [-0.5, 0.5]
 *   on MLU200 series and in [-50, 50] on other platforms.
 * - The input tensor and output tensor should be 3-dimensional and the dimension length of output tensor
 *   and input tensor must be same.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set \b mode to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - The algorithm \p CNNL_SOFTMAX_FAST is not recommended.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "Softmax Forward" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softmax operation is as follows:
     @verbatim
       input array by 1 * 1 * 4
       --> x: [[[1, 1, 1, 1]]]

       param:
         algorithm: CNNL_SOFTMAX_ACCURATE, mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

       output array by 1 * 1 * 4
       --> y:[[[0.25, 0.25, 0.25, 0.25]]]
     @endverbatim
 *
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftmaxForward_v2(cnnlHandle_t handle,
                                                cnnlSoftmaxAlgorithm_t algorithm,
                                                cnnlSoftmaxMode_t mode,
                                                cnnlComputationPreference_t prefer,
                                                const void *alpha,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const void *beta,
                                                const cnnlTensorDescriptor_t y_desc,
                                                void *y);
// Group:Abs
/*!
 * @brief Computes the absolute value for every element of the input tensor \b x and returns in \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the abs operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Abs Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the abs operation is as follows:
     @verbatim
      input arrays by 1 * 3 * 3 * 2 -->
          input: [[[[5, -11], [8, 1], [6, 4]],
                  [[3, 8], [2,6], [0, 6]],
                  [[8, 5], [7,4], [-9, 6]]]]

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[5, 11], [8, 1], [6, 4]],
                   [[3, 8], [2,6], [0, 6]],
                   [[8, 5], [7,4], [9, 6]]]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/abs
 */
cnnlStatus_t CNNL_WIN_API cnnlAbs(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:LayerNorm
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the LayerNormBackward operation.
 *
 * The size of extra workspace is based on the given information of the LayerNormBackward operation,
 * including the input tensor descriptor \b x_desc and the normalized dimension \b axis
 * of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   LayerNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   LayerNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlLayerNormBackward_v2.
 *   ::cnnlLayerNormBackward does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetLayerNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      int axis,
                                      size_t *workspace_size);

/*!
 * @brief Performs the backward layer normalization operator computation. Layer normalization normalizes over
 *        the dimension of the input tensor \b x from axis to the end.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \b diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \b diff_z tensor.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input \b filter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the \b filter tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the input \b mean and \b rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Input. Pointer to the MLU memory that stores the mean of input tensor that were computed during the
 *   layer normalization forward operation.
 * @param[out] saved_rstd
 *   Input. Pointer to the MLU memory that stores the inverse of the variance of input tensor that were computed
 *   during the layer normalization forward operation.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the filter gradients in \b diff_filter tensor.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \b diff_bias tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Example
 * - The example of the layer normalization backward operation is as follows:
     @verbatim
      \b x: an array [64, 78, 1024]
      \b axis: 2 (layer normalization for the last dimension)
      \b diff_z: an array [64, 78, 1024]
      \b filter: [1024]
      \b saved_mean: [64,78]
      \b saved_rstd: [64,78]
      Then we will get the output:
      \b diff_x: an array [64, 78, 1024]
      \b diff_filter and \b diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/tree/master/caffe2/operators/layer_norm_op.cc
 */
cnnlStatus_t CNNL_WIN_API cnnlLayerNormBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                int axis,
                                                const cnnlTensorDescriptor_t diff_z_desc,
                                                const void *diff_z,
                                                const cnnlTensorDescriptor_t filter_bias_desc,
                                                const void *filter,
                                                const cnnlTensorDescriptor_t mean_rstd_desc,
                                                const void *saved_mean,
                                                const void *saved_rstd,
                                                const cnnlTensorDescriptor_t diff_x_desc,
                                                void *diff_x,
                                                void *diff_filter,
                                                void *diff_bias);

/*!
 * @brief Performs the backward layer normalization operator computation. Layer normalization normalizes over
 *        the dimension of the input tensor \b x from axis to the end.
 *
 * Compared with ::cnnlLayerNormBackward, this function requires you to allocate some extra workspace as an input
 * parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the layer normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension from which the input tensor to be normalized over.
 * @param[in] diff_z_desc
 *   Input. The descriptor of the backpropagated differential \b diff_z tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the backpropagated differential \b diff_z tensor.
 * @param[in] filter_bias_desc
 *   Input. The descriptor of the input \b filter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the \b filter tensor.
 * @param[in] mean_rstd_desc
 *   Input. The descriptor of the input \b mean and \b rstd tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] saved_mean
 *   Input. Pointer to the MLU memory that stores the mean of input tensor that were computed during the
 *   layer normalization forward operation.
 * @param[out] saved_rstd
 *   Input. Pointer to the MLU memory that stores the inverse of the variance of input tensor that were computed
 *   during the layer normalization forward operation.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlLayerNormBackward_v2.
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlLayerNormBackward_v2. You can get the size of the workspace with
 *   the ::cnnlGetLayerNormBackwardWorkspaceSize function.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the input gradients in output tensor.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the filter gradients in \b diff_filter tensor.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the bias gradients in \b diff_bias tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "LayerNormBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensors and output tensors must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Example
 * - The example of the layer normalization backward operation is as follows:
     @verbatim
      \b x: an array [64, 78, 1024]
      \b axis: 2 (layer normalization for the last dimension)
      \b diff_z: an array [64, 78, 1024]
      \b filter: [1024]
      \b saved_mean: [64,78]
      \b saved_rstd: [64,78]
      Then we will get the output:
      \b diff_x: an array [64, 78, 1024]
      \b diff_filter and \b diff_bias: [1024]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/pytorch/pytorch/tree/master/caffe2/operators/layer_norm_op.cc
 */
cnnlStatus_t CNNL_WIN_API cnnlLayerNormBackward_v2(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const void *x,
                                                   int axis,
                                                   const cnnlTensorDescriptor_t diff_z_desc,
                                                   const void *diff_z,
                                                   const cnnlTensorDescriptor_t filter_bias_desc,
                                                   const void *filter,
                                                   const cnnlTensorDescriptor_t mean_rstd_desc,
                                                   const void *saved_mean,
                                                   const void *saved_rstd,
                                                   void *workspace,
                                                   size_t workspace_size,
                                                   const cnnlTensorDescriptor_t diff_x_desc,
                                                   void *diff_x,
                                                   void *diff_filter,
                                                   void *diff_bias);

// Group:InstanceNormInference
/*!
 * @brief Computes instance normalization forward over height and filter dimensions on input tensor
 *        \b x, \b mean, \b var, \b scale and \b bias, and returns the results in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instance
 *   normalization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_var_desc
 *   Input. The descriptor of the input mean and variance tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the input mean tensor.
 * @param[in] var
 *   Input. Pointer to the MLU memory that stores the input variance tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input scale and bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the scale tensor.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the bias tensor.
 * @param[in] eps
 *    Input. A float value added to the denominator for numerical stability.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "InstanceNorm Inference Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor are as follows:
 *   - \b x and \b y: \p CNNL_LAYOUT_NHWC.
 *   - \b mean, \b var, \b scale and \b bias: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the instance normalization forward operation is as follows:
     @verbatim
      x: NHWC [2, 3, 4, 9]
      mean and var: array [2, 1, 1, 9] or [2, 9]
      scale and bias: array [1, 1, 1, 9] or [9]
      Then we will get the output:
      y: NHWC [2, 3, 4, 9] same with input
     @endverbatim
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInstanceNormInference(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const void *x,
                                                    const cnnlTensorDescriptor_t mean_var_desc,
                                                    const void *mean,
                                                    const void *var,
                                                    const cnnlTensorDescriptor_t scale_bias_desc,
                                                    const void *scale,
                                                    const void *bias,
                                                    float eps,
                                                    const cnnlTensorDescriptor_t y_desc,
                                                    void *y);
// Group:InstanceNorm
/*!
 * @brief Computes Instance Normalization for each channel and batch of data in the training
 *        scenario.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instance
 *   normalization operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to implement moving average of \b moving_mean and \b moving_var.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the \b scale and \b bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the input tensor \b scale, which is learnable
 *   parameter of size C of input_x.
 * @param[in] input_x_desc
 *   Input. The descriptor of the input tensor \b input_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input_x
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_x.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias, which is learnable
 *   parameter of size C of input_x.
 * @param[in] mean_var_desc
 *   Input. The descriptor of the \b moving_mean, \b moving_var tensors. For detailed information,
 *   see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_mean, which is
 *   the moving average of mean computed over the N*C dimensions of the input tensor
 *   \b input_x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_var, which is the
 *   moving average of variance computed over the N*C dimensions of the input tensor \b
 *   input_x. The value of this pointer can be NULL.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor of the instance normalization forward operation.
 *   For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor \b output.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_mean, which is
 *   the mean of the current sample computed over the N*C dimensions. The value of this pointer
 *   can be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_invstd, which is
 *   the reciprocal root square of the current sample computed over the N*C dimensions. The value of
 *   this pointer can be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 * @par API Dependency
 * - Before calling this function to implement instance normalization forward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 * @par Formula
 * - See "Instance Normalization Forward Operation" section in "Cambricon CNNL User Guide" for details.
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - scale_tensor - input_x_tensor - bias_tensor - moving_mean_tensor - moving_var_tensor -
 *     output_tensor - saved_mean_tensor - saved_invstd_tensor
 *     The supported data type combinations are:
 *     - half - half - half - half - half - half - half - half.
 *     - float - float - float - float - float - float - float - float.
 *
 * @par Data Layout
 * - The supported data layout of the scale tensor, input_x tensor, bias tensor, moving_mean tensor,
 *   moving_var tensor, output tensor, saved_mean tensor and saved_invstd tensor are as follows:
 *   - scale tensor: \p CNNL_LAYOUT_ARRAY.
 *   - input_x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_NC.
 *   - moving_var tensor: \p CNNL_LAYOUT_NC.
 *   - output tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input_x tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_NC.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_NC.
 *
 * @par Requirements
 * - None.
 *
 * @note
 * - For half input type, make sure the accumulate value on the interval (-65504, 65504),
 *   otherwise, there will be an overflow risk.
 *
 * @par Example
 * - The example of the instance normalization forward operation is as follows:
     @verbatim
      input_x: NDHWC [2, 3, 4, 5, 9]
      mean and var: array [2, 9]
      scale and bias: array [9]
      Then we will get the output:
      output: NDHWC [2, 3, 4, 5, 9] same with input
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm3d.html#torch.nn.InstanceNorm3d
 * - https://arxiv.org/abs/1607.08022
 */

cnnlStatus_t CNNL_WIN_API cnnlInstanceNormForward(cnnlHandle_t handle,
                                                  float eps,
                                                  float momentum,
                                                  const cnnlTensorDescriptor_t scale_bias_desc,
                                                  const void *scale,
                                                  const cnnlTensorDescriptor_t input_x_desc,
                                                  const void *input_x,
                                                  const void *bias,
                                                  const cnnlTensorDescriptor_t mean_var_desc,
                                                  void *moving_mean,
                                                  void *moving_var,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t output_desc,
                                                  void *output,
                                                  void *saved_mean,
                                                  void *saved_invstd);
// Group:InstanceNorm
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to get
 *        extra space size in instance normalization backward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the instancenorm backward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] input_x_desc
 *   Input. A descriptor of \b input_x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the InstanceNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlInstanceNormBackward function to
 *   perform the InstanceNormBackward operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetInstanceNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_x_desc,
                                         size_t *workspace_size);
// Group:InstanceNorm
/*!
 * @brief Computes gradients with respect to instance normalization in the training scenario.
 *
 * Instance Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the instancenorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] scale_desc
 *   The descriptor of the \b scale tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the input tensor \b scale, which is learnable
 *   parameter of size C of input_x.
 * @param[in] input_x_desc
 *   The descriptor of the input tensor \b input_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input_x
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_x.
 * @param[in] input_diff_z_desc
 *   The descriptor of the backpropagated differential tensor \b input_diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_diff_z.
 * @param[in] mean_var_desc
 *   The descriptor of the \b saved_invstd, \b saved_mean tensor. For detailed information,
     see ::cnnlTensorDescriptor_t.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_mean, computed during
 *   the forward phase from the ::cnnlInstanceNormForward call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_invstd, computed during
 *   the forward phase from the ::cnnlInstanceNormForward call. The value of this pointer can
 *   be NULL.
 * @param[in] output_diff_x_desc
 *   The descriptor of the result differential tensor \b output_diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b output_diff_x.
 * @param[in] diff_scale_bias_desc
 *   The descriptor of the \b output_diff_scale and \b output_diff_bias tensor.
 *   For detailed information, see::cnnlTensorDescriptor_t.
 * @param[out] output_diff_scale
 *   Output. Pointer to the MLU memory that stores the output tensor \b output_diff_scale.
 * @param[out] output_diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b output_diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - Before calling this function to implement instance normalization backward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Formula
 * - See "InstanceNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b input_x, \b input_diff_z, \b scale, \b saved_mean,
 *   \b saved_invstd, \b output_diff_x, \b output_diff_scale and \b output_diff_bias are
 *   as follows:
 *   - input_x tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - input_diff_z tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *     The layout of the diff_z tensor should be the same as x tensor.
 *   - scale tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *   - saved_mean tensor: \p CNNL_LAYOUT_NC.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_NC.
 *   - output_diff_x tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *     The layout of the output tensor should be the same as x tensor.
 *   - output_diff_scale tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *   - output_diff_bias tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *
 * @note
 * - For half input type, make sure the accumulate value on the interval (-65504, 65504),
 *   otherwise, there will be an overflow risk.
 *
 * @par Requirements
 * - None.
 *
 *
 * @par Example
 * - The example of the instancenorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> input_x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> input_diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> scale: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> output_diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> output_diff_scale: [0.0, 0.0]

      --> output_diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.InstanceNorm2d.html#torch.nn.InstanceNorm2d
 * - http://github.com/pytorch/pytorch/tree/master/torch/nn/modules/instancenorm.py
 * - http://github.com/pytorch/pytorch/tree/master/torch/nn/functional.py
 * - http://github.com/pytorch/pytorch/tree/master/caffe2/operators/instance_norm_op.cu
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlInstanceNormBackward(cnnlHandle_t handle,
                         float eps,
                         const cnnlTensorDescriptor_t scale_desc,
                         const void *scale,
                         const cnnlTensorDescriptor_t input_x_desc,
                         const void *input_x,
                         const cnnlTensorDescriptor_t input_diff_z_desc,
                         const void *input_diff_z,
                         const cnnlTensorDescriptor_t mean_var_desc,
                         const void *saved_mean,
                         const void *saved_invstd,
                         void *workspace,
                         size_t workspace_size,
                         const cnnlTensorDescriptor_t output_diff_x_desc,
                         void *output_diff_x,
                         const cnnlTensorDescriptor_t diff_scale_bias_desc,
                         void *output_diff_scale,
                         void *output_diff_bias);

// Group:BatchNorm
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to get
 *        extra space size in BatchNormForward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BatchNormForward peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BatchNormForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchNormForwardWorkspaceSize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     size_t *workspace_size);

// Group:BatchNorm
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b bias, \b saved_mean, \b saved_invstd, \b moving_mean and
 *   \b moving_var tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \b x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \b
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \b moving_mean and \b moving_var.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor \b z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \b z.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_invstd, which is
 *   rsqrt of current sample's variance computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - input_tensor - filter_tensor - bias_tensor - moving_mean_tensor - moving_var_tensor -
 *     output_tensor - saved_mean_tensor - saved_invstd_tensor
 *     The supported data type combinations are:
 *     - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float
 *     - half - float - float - float - float - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, filter tensor, bias tensor, moving_mean tensor,
 *   moving_var tensor, output tensor, saved_mean tensor and saved_invstd tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTraining(cnnlHandle_t handle,
                             const void *alpha,
                             const void *beta,
                             const cnnlTensorDescriptor_t x_desc,
                             const void *x,
                             const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                             const void *filter,
                             const void *bias,
                             void *moving_mean,
                             void *moving_var,
                             float eps,
                             float momentum,
                             const cnnlTensorDescriptor_t z_desc,
                             void *z,
                             void *saved_mean,
                             void *saved_invstd);

// Group:BatchNorm
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Compared with ::cnnlBatchNormForwardTraining, this function allows you to choose whether to
 * perform add operations and activation operations after the batch normalization when
 * \b activation_desc is not NULL.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] z_desc
 *   Input. The descriptor of the input tensor \b z.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] z
 *   Input. Pointer to the MLU memory that stores the input tensor \b z.
 *   The optional \b z_desc and \b z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \b z should have exactly the same dimensions as \b x and the final output \b y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b bias, \b saved_mean, \b saved_invstd, \b moving_mean and
 *   \b moving_var tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \b x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \b
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \b moving_mean and \b moving_var.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \b y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_invstd, which is
 *   inversed standard deviation computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 *   For variance, if the data type is half, make sure the accumulate value on the interval
 *   (-65504, 65504), otherwise, there will be an overflow risk.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlBatchNormForwardTrainingV2.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the ::cnnlBatchNormForwardTrainingV2.
 *   You can get the size of the workspace with the ::cnnlGetBatchNormForwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of x_tensor - z_tensor - filter_tensor - bias_tensor - moving_mean_tensor -
 *   moving_var_tensor - y_tensor - saved_mean_tensor - saved_invstd_tensor,
 *   the supported combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float
 *   - half - half - float - float - float - float - half - float - float
 *
 * @par Data Layouts
 * - The supported data layout of the \b x tensor, \b z tensor, \b filter tensor, \b bias tensor,
 *   \b moving_mean tensor, \b moving_var tensor, \b y tensor, \b saved_mean tensor and \b saved_invstd
 *   tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is \p CNNL_ACTIVATION_IDENTITY:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTrainingV2(cnnlHandle_t handle,
                               const cnnlActivationDescriptor_t activation_desc,
                               const cnnlBatchNormMode_t mode,
                               const cnnlBatchNormOps_t bnOps,
                               const void *alpha,
                               const void *beta,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t z_desc,
                               const void *z,
                               const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                               const void *filter,
                               const void *bias,
                               void *moving_mean,
                               void *moving_var,
                               float eps,
                               float momentum,
                               const cnnlTensorDescriptor_t y_desc,
                               void *y,
                               void *saved_mean,
                               void *saved_invstd,
                               void *workspace,
                               size_t workspace_size,
                               void *reservespace,
                               size_t reservespace_size);

// Group:BatchNorm
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the training
 *        scenario.
 *
 * Compared with ::cnnlBatchNormForwardTraining, this function allows you to choose whether to
 * perform add operations and activation operations after the batch normalization when
 * \b activation_desc is not NULL.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward training operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] z_desc
 *   Input. The descriptor of the input tensor \b z.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] z
 *   Input. Pointer to the MLU memory that stores the input tensor \b z.
 *   The optional \b z_desc and \b z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \b z should have exactly the same dimensions as \b x and the final output \b y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b bias, \b saved_mean, \b saved_invstd, \b moving_mean and
 *   \b moving_var tensors. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_mean, which is
 *   the moving average of mean computed over the (spatial+batch) dimensions of the input tensor
 *   \b x. The value of this pointer can be NULL.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_var, which is the
 *   moving average of variance computed over the (spatial+batch) dimensions of the input tensor \b
 *   x. The value of this pointer can be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \b moving_mean and \b moving_var.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \b y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 * @param[out] saved_mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_mean, which is
 *   current sample's mean computed over the (spatial+batch) dimensions. The value of this pointer can
 *   be NULL.
 * @param[out] saved_invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b saved_invstd, which is
 *   inversed standard deviation computed over the (spatial+batch) dimensions. The value of this
 *   pointer can be NULL.
 *   For variance, if the data type is half, make sure the accumulate value on the interval
 *   (-65504, 65504), otherwise, there will be an overflow risk.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlBatchNormForwardTraining_v2.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the ::cnnlBatchNormForwardTraining_v2.
 *   You can get the size of the workspace with the ::cnnlGetBatchNormForwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardTraining Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of x_tensor - z_tensor - filter_tensor - bias_tensor - moving_mean_tensor
 *   - moving_var_tensor - y_tensor - saved_mean_tensor - saved_invstd_tensor,
 *   the supported combinations of data types of this function are as follows:
 *   - half - half - half - half - half - half - half - half - half
 *   - float - float - float - float - float - float - float - float - float
 *   - half - half - float - float - float - float - half - float - float
 *
 * @par Data Layouts
 * - The supported data layout of the \b x tensor, \b z_tensor, \b filter tensor, \b bias tensor,
 *   \b moving_mean tensor, \b moving_var tensor, \b y tensor, \b saved_mean tensor and
 *   \b saved_invstd tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm forward operation is as follows when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is \p CNNL_ACTIVATION_IDENTITY:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> moving_mean: [0.5, 0.5]

      --> moving_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> saved_mean: [1.0, 1.0]

      --> saved_invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardTraining_v2(cnnlHandle_t handle,
                                const cnnlActivationDescriptor_t activation_desc,
                                const cnnlBatchNormMode_t mode,
                                const cnnlBatchNormOps_t bnOps,
                                const void *alpha,
                                const void *beta,
                                const cnnlTensorDescriptor_t x_desc,
                                const void *x,
                                const cnnlTensorDescriptor_t z_desc,
                                const void *z,
                                const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                                const void *filter,
                                const void *bias,
                                void *moving_mean,
                                void *moving_var,
                                float eps,
                                float momentum,
                                const cnnlTensorDescriptor_t y_desc,
                                void *y,
                                void *saved_mean,
                                void *saved_invstd,
                                void *workspace,
                                size_t workspace_size,
                                void *reservespace,
                                size_t reservespace_size);

// Group:BatchNorm
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data in the inference
 *        scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b bias, \b estimated_mean and \b estimated_var tensor. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter. The value of this
 *   pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias. The value of this
 *   pointer can be NULL.
 * @param[in] estimated_mean
 *   Input. Pointer to the MLU memory that stores the tensor \b estimated_mean, accumulated as \b
 *   moving_mean during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] estimated_var
 *   Input. Pointer to the MLU memory that stores the tensor \b estimated_var, accumulated as \b
 *   moving_var during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] z_desc
 *   The descriptor of the batch normalization output tensor \b z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \b z.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardInference Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b x tensor - \b filter tensor - \b bias tensor - \b estimated_mean tensor -
 *   \b estimated_var tensor - \b z tensor, the supported combinations of data types of this function
 *   are as follows:
 *
 *     - half - half - half - half - half - half
 *     - float - float - float - float - float - float
 *     - half  - float - float - float - float - half
 *     - uint8 - half - half - half - half - half
 *     - uint8 - float - float - float - float - float
 *     - uint8 - float - float - float - float - half
 *
 * @par Data Layout
 * - The supported data layouts of the \b x tensor, \b filter tensor, \b bias tensor, \b estimated_mean
 *   tensor, \b estimated_var tensor and \b z tensor are as follows:
 *     - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *     - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *     - estimated_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *     - estimated_var tensor: \p CNNL_LAYOUT_ARRAY.
 *     - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the z tensor should be the same as the x tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm inference operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> estimated_mean: [0.5, 0.5]

      --> estimated_var: [1.0, 1.0]

      param:
        eps: 0.00001

      output array by 1 * 2 * 3 * 2
      --> z: [[[[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]],
               [[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardInference(cnnlHandle_t handle,
                              const void *alpha,
                              const void *beta,
                              const cnnlTensorDescriptor_t x_desc,
                              const void *x,
                              const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                              const void *filter,
                              const void *bias,
                              const void *estimated_mean,
                              const void *estimated_var,
                              float eps,
                              const cnnlTensorDescriptor_t z_desc,
                              void *z);
// Group:BatchNorm
/*!
 * @brief Applies Batch Normalization with Relu activation for each channel across a batch of data
 * in the inference scenario. Compared with ::cnnlBatchNormForwardInference, this function allows
 * you to choose whether to perform activation operations after the batch normalization when
 * \b activation_desc is not Null.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   forward inference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b bias, \b estimated_mean and \b estimated_var tensor. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter. The value of this
 *   pointer can be NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias. The value of this
 *   pointer can be NULL.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \b y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 *   Reserved for future use. Set the value to NULL.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \b y.
 *   The optional \b y_desc and \b y are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL. When in use,
 *   \b y should have exactly the same dimensions as \b x and the final output \b z.
 * @param[in] estimated_mean
 *   Input. Pointer to the MLU memory that stores the tensor \b estimated_mean, accumulated as \b
 *   moving_mean during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] estimated_var
 *   Input. Pointer to the MLU memory that stores the tensor \b estimated_var, accumulated as \b
 *   moving_var during the training phase from the ::cnnlBatchNormForwardTraining call.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] z_desc
 *   The descriptor of the batch normalization output tensor \b z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor \b z.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormForwardInference Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b x tensor - \b filter tensor - \b bias tensor - \b y tensor -
 *   \b estimated_mean tensor - \b estimated_var tensor - \b z tensor, the supported
 *   combinations of data types of this function are as follows:
 *
 *     - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float
 *     - half - float - float - half - float - float - half
 *     - uint8 - half - half - NULL - half - half - half
 *     - uint8 - float - float - NULL - float - float - float
 *     - uint8 - float - float - NULL - float - float - half
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b filter tensor, \b bias tensor, \b y tensor,
 *   \b estimated_mean tensor, \b estimated_var tensor and \b z tensor are as follows:
 *    - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *    - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *    - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *    - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *    - estimated_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *    - estimated_var tensor: \p CNNL_LAYOUT_ARRAY.
 *    - z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the z tensor should be the same as the x tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm inference operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      --> estimated_mean: [0.5, 0.5]

      --> estimated_var: [1.0, 1.0]

      param:
        eps: 0.00001
        bnops: CNNL_BATCHNORM_OPS_BN_ACTIVATION
        mode: CNNL_ACTIVATION_RELU

      output array by 1 * 2 * 3 * 2
      --> z: [[[[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]],
               [[1.25, 1.25],[1.25, 1.25],[1.25, 1.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://docs.nvidia.com/deeplearning/cudnn/api/index.html#cudnnBatchNormalizationForwardInference
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormForwardInferenceV2(cnnlHandle_t handle,
                                const cnnlActivationDescriptor_t activation_desc,
                                const cnnlBatchNormMode_t mode,
                                const cnnlBatchNormOps_t bnops,
                                const void *alpha,
                                const void *beta,
                                const cnnlTensorDescriptor_t x_desc,
                                const void *x,
                                const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                                const void *filter,
                                const void *bias,
                                const cnnlTensorDescriptor_t y_desc,
                                const void *y,
                                const void *estimated_mean,
                                const void *estimated_var,
                                float eps,
                                const cnnlTensorDescriptor_t z_desc,
                                void *z);

// Group:SyncBatchNormStats
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the sync_batchnorm_stats operation.
 *
 * The size of extra workspace is based on the given information of the sync_batchnorm_stats
 * operation, including the input tensor descriptor \b x_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_stats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   sync_batchnorm_stats operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlSyncBatchNormStats_v2. ::cnnlSyncBatchNormStats does
 *   not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSyncBatchNormStatsWorkspaceSize(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       size_t *workspace_size);

// Group:SyncBatchNormStats
/*!
 * @brief Computes the local mean and the local inverse standard deviation for each channel
 * across a batch of data in the training scenario.
 *
 * cnnlSyncBatchNormStats_v2 is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchNormStats, this function allows you to allocate some extra
 * workspace as an input parameter. If you just set \b workspace to NULL and \b workspace_size
 * to 0, this function will perform as same as ::cnnlSyncBatchNormStats.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_stats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlSyncBatchNormStats_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlSyncBatchNormStats_v2. You can get the size of the workspace with
 *   the ::cnnlGetSyncBatchNormStatsWorkspaceSize function.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \b mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b mean, which is the
 *   local mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \b invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b invstd, which is the
 *   local inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchnormStats Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - float(x) - float(eps) - float( mean) - float(invstd).
 *   - half(x) - float(eps) - float(mean) - float(invstd).
 *
 * @par Data Layout
 * - The supported data layout of the input tensor is shown as follows:
 *   - x: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the output tensors are shown as follows:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_stats operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]
      param:
        eps: 0.00001
      output an array by 2
      --> mean: [1.0, 1.0]
      --> invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormStats_v2(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t x_desc,
                                                    const void *x,
                                                    void *workspace,
                                                    size_t workspace_size,
                                                    const float eps,
                                                    const cnnlTensorDescriptor_t mean_desc,
                                                    void *mean,
                                                    const cnnlTensorDescriptor_t invstd_desc,
                                                    void *invstd);

// Group:SyncBatchNormStats
/*!
 * @brief Computes the local mean and the local inverse standard deviation for each channel
 * across a batch of data in the training scenario.
 *
 * SyncBatchnormStats is used in CNN, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   SyncBatchnormStats operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \b mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b mean, which is the
 *   local mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \b invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b invstd, which is the
 *   local inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchnormStats Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x - eps - mean - invstd
 *   - The supported data type combinations are:
 *     - float - float - float - float.
 *     - half - float - float - float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor is shown as following:
 *   - x: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 * - The layout of the output tensors are shown as following:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_stats operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]
      param:
        eps: 0.00001
      output an array by 2
      --> mean: [1.0, 1.0]
      --> invstd: [316.221, 316.221]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormStats(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const float eps,
                                                 const cnnlTensorDescriptor_t mean_desc,
                                                 void *mean,
                                                 const cnnlTensorDescriptor_t invstd_desc,
                                                 void *invstd);

// Group:SyncBatchNormGatherStatsWithCounts
/*!
 * @brief Computes the global mean and the global inverse standard deviation across aggragation
 * of the local mean and local inverse standard deviation of multiple MLU devices.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batch_norm_gather_stats_with_counts operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] mean_all_desc
 *   Input. The descriptor of the input tensor \b mean_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean_all
 *   Input. Pointer to the MLU memory that stores the input tensor tensor \b mean_all, which is
 *   the local mean of multiple MLU devices.
 * @param[in] invstd_all_desc
 *   Input. The descriptor of the input tensor \b invstd_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd_all
 *   Input. Pointer to the MLU memory that stores the input tensor tensor \n invstd_all, which
 *   is the local inverse standard deviation of multiple MLU devices.
 * @param[in] moving_mean_desc
 *   Input. The descriptor of the input tensor \b moving_mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_mean
 *   Input and Output. Pointer to the MLU memory that stores the input tensor \b moving_mean,
 *   which is the moving average of mean computed over the dimensions of the input tensor
 *   \b mean_all. The value of this pointer can be NULL.
 * @param[in] moving_var_desc
 *   Input. The descriptor of the input tensor \b moving_var. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in,out] moving_var
 *   Input and Output. Pointer to the MLU memory that stores the tensor \b moving_var, which is
 *   the moving average of inverse standard deviation computed over the dimensions of the input
 *   tensor \b invstd_all. The value of this pointer can be NULL.
 * @param[in] momentum
 *   Input. A float value used to do moving average of \b moving_mean and \b moving_var.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] count_all_desc
 *   Input. The descriptor of the input tensor \b count_all. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] count_all
 *   Input. Pointer to the MLU memory that stores an array, which stores the total size of
 *   dimensions (except C dimension) of input for each MLU device.
 * @param[in] mean_desc
 *   Input. The descriptor of the output tensor \b mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] mean
 *   Output. Pointer to the MLU memory that stores the output tensor \b mean, which is the
 *   global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the output tensor \b invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] invstd
 *   Output. Pointer to the MLU memory that stores the output tensor \b invstd, which is the
 *   global inverse standard deviation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchnormGatherStatsWithCounts Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown as the following order:
 *   - mean_all - invstd_all - moving_mean - moving_var - momentum -  eps  - count_all - mean  - invstd
 *   -  float   -   float    -    float    -    float   -   float  - float -   float   - float -  float.
 *   -  float   -   float    -    half     -    half    -   float  - float -   half    - float -  float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors are shown as the following:
 *   - mean_all tensor: \p CNNL_LAYOUT_NC.
 *   - invstd_all tensor: \p CNNL_LAYOUT_NC.
 *   - moving_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - moving_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - momentum: Scalar.
 *   - eps: Scalar.
 *   - count_all tensor: \p CNNL_LAYOUT_N.
 * - The layout of the output tensors are shown as the following:
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The input \b mean_all and the input \b invstd_all cannot be positive infinity or negative infinity
 *   at the same time on MLU300 series or above.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_gather_stats_with_counts operation is as follows:
     @verbatim
      --> mean_all: an array [8, 1024];
      --> invstd_all: an array [8, 1024];
      --> moving_mean: an array [1024];
      --> moving_var: an array [1024];
      --> count_all: an array [8];
      param:
      --> momentum: 0.1
      --> eps: 0.00001
      output:
      --> mean: an array [1024];
      --> invstd: [1024];
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_stats
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormGatherStatsWithCounts(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t mean_all_desc,
                                       const void *mean_all,
                                       const cnnlTensorDescriptor_t invstd_all_desc,
                                       const void *invstd_all,
                                       const cnnlTensorDescriptor_t moving_mean_desc,
                                       void *moving_mean,
                                       const cnnlTensorDescriptor_t moving_var_desc,
                                       void *moving_var,
                                       float momentum,
                                       float eps,
                                       const cnnlTensorDescriptor_t count_all_desc,
                                       const void *count_all,
                                       const cnnlTensorDescriptor_t mean_desc,
                                       void *mean,
                                       const cnnlTensorDescriptor_t invstd_desc,
                                       void *invstd);
// Group:SyncBatchNormElemt
/*!
 * @brief Applies Batch Normalization for each channel across a batch of data with the given mean,
 *        inverse variance and scaling factors.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sync batchnorm
 *   element operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] mean_desc
 *   The descriptor of \b mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \b mean, which is computed over the
 *   batch and spatial dimensions by ::cnnlSyncBatchNormGatherStatsWithCounts.
 * @param[in] invstd_desc
 *   The descriptor of \b invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \b invstd, which is the inverse variance
 *   computed over the batch and spatial dimensions by ::cnnlSyncBatchNormGatherStatsWithCounts.
 * @param[in] filter_desc
 *   The descriptor of \b filter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *   The descriptor can be NULL when \b filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter for affine transformation
 *   after batch normilization. The value of this pointer can be NULL.
 * @param[in] bias_desc
 *   The descriptor of \b bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *   The descriptor can be NULL when \b bias pointer is NULL.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias for affine transformation
 *   after batch normalization. The value of this pointer can be NULL.
 * @param[in] y_desc
 *   The descriptor of the sync batch normalization output tensor \b y. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x_tensor - mean_tensor - invstd_tensor - filter_tensor - bias_tensor - y_tensor
 *   - float - float - float - float - float - float.
 *   - half - float - float - float - float - half.
 *
 * @par Data Layout
 * - The supported data layout of \b x, \b mean, \b invstd, \b filter, \b bias and \b y are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the \b y should be the same as \b x tensor.
 *
 * @note
 * - The \b mean, \b invstd, \b filter and \b \b bias must be 1-D tensors and the length of their dimensions
 *   should be the same as the the length of the lowest dimension of \b x.
 * - The length of each dimension of \b x and \b y must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> mean: [0.5, 0.5]

      --> invstd: [2.0, 2.0]

      --> filter: [0.5, 0.5]

      --> bias: [1.0, 1.0]

      output array by 1 * 2 * 3 * 2
      --> y: [[[[1.5, 1.5],[1.5, 1.5],[1.5, 1.5]],
               [[1.5, 1.5],[1.5, 1.5],[1.5, 1.5]]]]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSyncBatchNormElemt(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t x_desc,
                                                 const void *x,
                                                 const cnnlTensorDescriptor_t mean_desc,
                                                 const void *mean,
                                                 const cnnlTensorDescriptor_t invstd_desc,
                                                 const void *invstd,
                                                 const cnnlTensorDescriptor_t filter_desc,
                                                 const void *filter,
                                                 const cnnlTensorDescriptor_t bias_desc,
                                                 const void *bias,
                                                 const cnnlTensorDescriptor_t y_desc,
                                                 void *y);

// Group:BatchNorm
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in artificial intelligence, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] diff_z_desc
 *   The descriptor of the backpropagated differential tensor \b diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_z.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b saved_invstd, \b saved_mean, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTrainingV2 call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTrainingV2 call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - input_tensor - diff_z_tensor - filter_tensor - saved_mean_tensor - saved_invstd_tensor -
 *     output_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data type combinations are:
 *     - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float
 *     - half - half - float - float - float - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, diff_z tensor, filter tensor, saved_mean
 *   tensor, saved_invstd tensor, output tensor, diff_filter tensor and diff_bias tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the batchnorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackward(cnnlHandle_t handle,
                      const void *alpha_data_diff,
                      const void *beta_data_diff,
                      const void *alpha_param_diff,
                      const void *beta_param_diff,
                      const cnnlTensorDescriptor_t x_desc,
                      const void *x,
                      const cnnlTensorDescriptor_t diff_z_desc,
                      const void *diff_z,
                      const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                      const void *filter,
                      const void *saved_mean,
                      const void *saved_invstd,
                      float eps,
                      const cnnlTensorDescriptor_t diff_x_desc,
                      void *diff_x,
                      void *diff_filter,
                      void *diff_bias);

// Group:BatchNorm
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to get
 *        extra space size in BatchNormBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BatchNormBackward peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BatchNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t x_desc,
                                      size_t *workspace_size);

// Group:BatchNorm
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 * Compared with ::cnnlBatchNormBackward, this function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \b y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \b y which is the output of
 *   cnnlBatchNormForwardTraining API. The optional \b y_desc and \b y are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \b y should have exactly the same dimensions as \b x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \b diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b saved_invstd, \b saved_mean, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \b diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_z.
 *   The optional \b diff_z_desc and \b diff_z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \b diff_z should have exactly the same dimensions as \b x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBatchNormBackwardV2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   ::cnnlBatchNormBackwardV2. You can get the size of the workspace with the
 *   ::cnnlGetBatchNormBackwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x_tensor - y_tensor - diff_y_tensor - filter_tensor - saved_mean_tensor -
 *     saved_invstd_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data types combinations are:
 *     - half - half - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float - float - float
 *     - half - half - half - float - float - float - half - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b y tensor, \b diff_y tensor, \b filter tensor,
 *   \b saved_mean tensor, \b saved_invstd tensor, \b diff_z tensor, \b diff_x tensor, \b diff_filter
 *   tensor and \b diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the BatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackwardV2(cnnlHandle_t handle,
                        const cnnlActivationDescriptor_t activation_desc,
                        const cnnlBatchNormMode_t mode,
                        const cnnlBatchNormOps_t bnOps,
                        const void *alpha_data_diff,
                        const void *beta_data_diff,
                        const void *alpha_param_diff,
                        const void *beta_param_diff,
                        const cnnlTensorDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t y_desc,
                        const void *y,
                        const cnnlTensorDescriptor_t diff_y_desc,
                        const void *diff_y,
                        const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                        const void *filter,
                        const void *saved_mean,
                        const void *saved_invstd,
                        float eps,
                        const cnnlTensorDescriptor_t diff_z_desc,
                        void *diff_z,
                        const cnnlTensorDescriptor_t diff_x_desc,
                        void *diff_x,
                        void *diff_filter,
                        void *diff_bias,
                        void *workspace,
                        size_t workspace_size,
                        void *reservespace,
                        size_t reservespace_size);

// Group:BatchNorm
/*!
 * @brief Applies to compute gradients of batch normalization for the training scenario.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 * Compared with ::cnnlBatchNormBackward, this function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] alpha_data_diff, beta_data_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] alpha_param_diff, beta_param_diff
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \b y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \b y which is the output of
 *   cnnlBatchNormForwardTraining API. The optional \b y_desc and \b y are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \b y should have exactly the same dimensions as \b x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \b diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b saved_invstd, \b saved_mean, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias.
 *   Reserved for future use. Now must set the value to NULL.
 * @param[in] saved_mean
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_mean, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining_v2 call. The value of this pointer can
 *   be NULL.
 * @param[in] saved_invstd
 *   Input. Pointer to the MLU memory that stores the input tensor \b saved_invstd, computed during
 *   the forward phase from the ::cnnlBatchNormForwardTraining_v2 call. The value of this pointer can
 *   be NULL.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \b diff_z. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_z.
 *   The optional \b diff_z_desc and \b diff_z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \b diff_z should have exactly the same dimensions as \b x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBatchNormBackward_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   ::cnnlBatchNormBackward_v2. You can get the size of the workspace with the
 *   ::cnnlGetBatchNormBackwardWorkspaceSize function.
 * @param[in] reservespace
 *   Input. Reserved for future use. Set the value to NULL.
 * @param[in] reservespace_size
 *   Input. Reserved for future use. Set the value to 0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "BatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x_tensor - y_tensor - diff_y_tensor - filter_tensor - bias_tensor - saved_mean_tensor -
 *     saved_invstd_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data types combinations are:
 *     - half - half - half - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float - float - float - float
 *     - half - half - half - float - float - float - float - half - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b y tensor, \b diff_y tensor, \b filter tensor,
 *   \b bias tensor, \b saved_mean tensor, \b saved_invstd tensor, \b diff_z tensor, \b diff_x tensor,
 *   \b diff_filter tensor and \b diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_z tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - saved_invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the BatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [0.5, 0.5]

      --> saved_mean: [1.0,1.0]

      --> saved_invstd: [0.0,0.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]],
                    [[0.0, 0.0],[0.0, 0.0],[0.0, 0.0]]]]

      --> diff_filter: [0.0, 0.0]

      --> diff_bias: [12.0, 12.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchNormBackward_v2(cnnlHandle_t handle,
                         const cnnlActivationDescriptor_t activation_desc,
                         const cnnlBatchNormMode_t mode,
                         const cnnlBatchNormOps_t bnOps,
                         const void *alpha_data_diff,
                         const void *beta_data_diff,
                         const void *alpha_param_diff,
                         const void *beta_param_diff,
                         const cnnlTensorDescriptor_t x_desc,
                         const void *x,
                         const cnnlTensorDescriptor_t y_desc,
                         const void *y,
                         const cnnlTensorDescriptor_t diff_y_desc,
                         const void *diff_y,
                         const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                         const void *filter,
                         const void *bias,
                         const void *saved_mean,
                         const void *saved_invstd,
                         float eps,
                         const cnnlTensorDescriptor_t diff_z_desc,
                         void *diff_z,
                         const cnnlTensorDescriptor_t diff_x_desc,
                         void *diff_x,
                         void *diff_filter,
                         void *diff_bias,
                         void *workspace,
                         size_t workspace_size,
                         void *reservespace,
                         size_t reservespace_size);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the sync_batchnorm_backward_reduce operation.
 *
 * The size of extra workspace is based on the given information of the
 * sync_batchnorm_backward_reduce operation, including the input tensor descriptor \b desc_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the mse_loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x
 *   Input. The descriptor of the input tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   sync_batchnorm_backward_reduce operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlSyncBatchnormBackwardReduce_v2.
 *    ::cnnlSyncBatchnormBackwardReduce does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetSyncBatchnormBackwardReduceWorkspaceSize(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t desc_x,
                                                size_t *workspace_size);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Applies Syncronized Batch Normalization Reduce operator to backwardly compute grad
 * filters, grad bias, sum_dy and sum_dy_xmu on each MLU device.
 *
 * Batch Normalization is used in convolution network, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchnormBackwardReduce, this function allows you to allocate some extra
 * workspace as an input parameter. If you just set \b workspace to NULL and \b workspace_size to 0,
 * this function will perform as same as ::cnnlSyncBatchnormBackwardReduce.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_backward_reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_dz
 *   The descriptor of the input tensor \b dz. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] dz
 *   Input. Pointer to the MLU memory that stores the tensor \b dz, which denotes the partial
 *   derivative of batch normalization forward output.
 * @param[in] desc_x
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] mean_desc
 *   The descriptor of \b mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \b mean, which denotes the average
 *   result of input \b x.
 * @param[in] desc_invstd
 *   The descriptor of \b invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \b invstd, which denotes the inversed
 *   standard deviation of input \b x.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlSyncBatchnormBackwardReduce_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlSyncBatchnormBackwardReduce_v2. You can get the size of the workspace with
 *   the ::cnnlGetSyncBatchnormBackwardReduceWorkspaceSize function.
 * @param[out] desc_dfilters
 *   The descriptor of \b dfilters tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dfilters
 *   Output. Pointer to the MLU memory that stores the input tensor \b dfilters, which denotes
 *   partial derivative of filter in sync batch normalization forward training. It will be computed
 *   only if booleanvariable \b needs_input_grad1 is true.
 * @param[out] desc_dbias
 *   The descriptor of the sync batch normalization output tensor \b dbias. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] dbias
 *   Output. Pointer to the MLU memory that stores the output tensor \b dbias, which denotes partial
 *   derivative of bias in sync batch normalization forward training. It will be computed
 *   only if \b needs_input_grad2 is true.
 * @param[out] desc_sum_dy
 *   The descriptor of the sync batch normalization output tensor \b sum_dy. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy
 *   Output. Pointer to the MLU memory that stores the output tensor \b sum_dy, which denotes the
 *   summation of dz and is also an intermediate variable to compute the partial derivative of
 *   input x. Moreover, it will be computed only if boolean variable \b needs_input_grad0 is true.
 * @param[out] desc_sum_dy_xmu
 *   The descriptor of the sync batch normalization output tensor \b sum_dy_xmu. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy_xmu
 *   Output. Pointer to the MLU memory that stores the output tensor \b sum_dy_xmu, which denotes
 *   sum{dz(x-mean)}. It is also an intermediate variable to compute the partial derivative of
 *   input \b x. Moreover, it will be computed only if boolean variable \b needs_input_grad0 is
 *   true.
 * @param[in] needs_input_grad0
 *   Input. A boolean variable that determines whether to compute \b sum_dy and \b sum_dy_xmu.
 *   When \b needs_input_grad0 is true, \b sum_dy and \b sum_dy_xmu will be computed.
 *   When \b needs_input_grad0 is false, \b sum_dy and \b sum_dy_xmu will be NULL.
 * @param[in] needs_input_grad1
 *   Input. A boolean variable that determines whether to compute \b dfilters.
 *   When \b needs_input_grad1 is true, \b dfilters will be computed.
 *   When \b needs_input_grad1 is false, \b dfilter will be NULL.
 * @param[in] needs_input_grad2
 *   Input. A boolean variable that determines whether to compute \b dbias.
 *   When \b needs_input_grad2 is true, \b dbias will be computed.
 *   When \b needs_input_grad2 is false, \b dbias will be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardReduce Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - dz_tensor - x_tensor - mean_tensor - invstd_tensor - dfilter_tensor - dbias_tensor -
 *   sum_dy_tensor - sum_dy_xmu_tensor
 *   - float - float - float - float - float - float - float - float.
 *   - half - half - float - float - float - float - float - float.
 *
 * @par Data Layout
 * - The supported data layout of \b dz, \b x, \b mean, \b invstd, \b dfilter, \b dbias, \b sum_dy
 *   and \b sum_dy_xmu are as follows:
 *   - dz tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dfilter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dbias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The \b mean, \b invstd, \b dfilter, \b bias, \b sum_dy and \b sum_dy_xmu must be 1-D tensors
 *   and the length of the dimensions of these tensors should be the same as the the length of
 *   the lowest dimension of \b x.
 * - The length of each dimension of \b x and \b dz must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input four arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> dz: [[[[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]],
               [[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]]]]

      --> x: [[[[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]],
               [[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]]]]

      --> mean: [1, 1]

      --> invstd: [0.8, 0.8]

      output array by 2
      --> dfilter: [57.6, 57.6]

      --> dbias: [36.0, 36.0]

      --> sum_dy: [36.0, 36.0]

      --> sum_dy_xmu: [72.0, 72.0]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchnormBackwardReduce_v2(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t desc_dz,
                                   const void *dz,
                                   const cnnlTensorDescriptor_t desc_x,
                                   const void *x,
                                   const cnnlTensorDescriptor_t desc_mean,
                                   const void *mean,
                                   const cnnlTensorDescriptor_t desc_invstd,
                                   const void *invstd,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t desc_dfilter,
                                   void *dfilter,
                                   const cnnlTensorDescriptor_t desc_dbias,
                                   void *dbias,
                                   const cnnlTensorDescriptor_t desc_sum_dy,
                                   void *sum_dy,
                                   const cnnlTensorDescriptor_t desc_sum_dy_xmu,
                                   void *sum_dy_xmu,
                                   const bool needs_input_grad0,
                                   const bool needs_input_grad1,
                                   const bool needs_input_grad2);

// Group:SyncBatchnormBackwardReduce
/*!
 * @brief Applies Syncronized Batch Normalization Reduce operator to backwardly compute grad filters,
 * grad bias, sum_dy and sum_dy_xmu on each MLU device.
 *
 * Batch Normalization is used in CNN, including but not limited to
 * ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   sync_batchnorm_backward_reduce operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_dz
 *   The descriptor of the input tensor \b dz. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] dz
 *   Input. Pointer to the MLU memory that stores the tensor \b dz, which denotes the partial derivative of
 *   batch normalization forward output.
 * @param[in] desc_x
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] mean_desc
 *   The descriptor of \b mean tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the tensor \b mean, which denotes the average result of
 *   input \b x.
 * @param[in] desc_invstd
 *   The descriptor of \b invstd tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the tensor \b invstd, which denotes the inversed standard deviation
 *   of input \b x.
 * @param[out] desc_dfilter
 *   The descriptor of \b dfilter tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] dfilter
 *   Output. Pointer to the MLU memory that stores the input tensor \b dfilter, which denotes partial derivative
 *   of filter in sync batch normalization forward training. It will be computed only if boolean variable
 *   \b needs_input_grad1 is true.
 * @param[out] desc_dbias
 *   The descriptor of the sync batch normalization output tensor \b dbias. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] dbias
 *   Output. Pointer to the MLU memory that stores the output tensor \b dbias, which denotes partial derivative of
 *   bias in sync batch normalization forward training. It will be computed only if \b needs_input_grad2 is true.
 * @param[out] desc_sum_dy
 *   The descriptor of the sync batch normalization output tensor \b sum_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy
 *   Output. Pointer to the MLU memory that stores the output tensor \b sum_dy, which denotes the summation of dz
 *   and is also an intermediate variable to compute the partial derivative of input x. Moreover, it will be
 *   computed only if boolean variable \b needs_input_grad0 is true.
 * @param[out] desc_sum_dy_xmu
 *   The descriptor of the sync batch normalization output tensor \b sum_dy_xmu. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] sum_dy_xmu
 *   Output. Pointer to the MLU memory that stores the output tensor \b sum_dy_xmu, which denotes sum{dz(x-mean)}.
 *   It is also an intermediate variable to compute the partial derivative of
 *   input \b x. Moreover, it will be computed only if boolean variable \b needs_input_grad0 is true.
 * @param[in] needs_input_grad0
 *   Input. A boolean variable that determines whether to compute \b sum_dy and \b sum_dy_xmu.
 *   When \b needs_input_grad0 is true, \b sum_dy and \b sum_dy_xmu will be computed.
 *   When \b needs_input_grad0 is false, \b sum_dy and \b sum_dy_xmu will be NULL.
 * @param[in] needs_input_grad1
 *   Input. A boolean variable that determines whether to compute \b dfilters.
 *   When \b needs_input_grad1 is true, \b dfilters will be computed.
 *   When \b needs_input_grad1 is false, \b dfilter will be NULL.
 * @param[in] needs_input_grad2
 *   Input. A boolean variable that determines whether to compute \b dbias.
 *   When \b needs_input_grad2 is true, \b dbias will be computed.
 *   When \b needs_input_grad2 is false, \b dbias will be NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardReduce Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - dz_tensor - x_tensor - mean_tensor - invstd_tensor - dfilter_tensor - dbias_tensor - sum_dy_tensor
 *   - sum_dy_xmu_tensor
 *   - float - float - float - float - float - float - float - float.
 *   - half - half - float - float - float - float - float - float.
 *
 * @par Data Layout
 * - The supported data layout of \b dz, \b x, \b mean, \b invstd, \b dfilter, \b dbias, \b sum_dy and
 *   \b sum_dy_xmu are as follows:
 *   - dz tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - x tensor: \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dfilter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - dbias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - The \b mean, \b invstd, \b dfilter, \b bias, \b sum_dy and \b sum_dy_xmu must be 1-D tensors and the length of the dimensions
 *   of these tensors should be the same as the the length of the lowest dimension of \b x.
 * - The length of each dimension of \b x and \b dz must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync batchnorm element operation is as follows:
     @verbatim
      input four arrays by 1 * 2 * 3 * 2, 2, 2, 2 and 2
      --> dz: [[[[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]],
               [[6.0, 6.0],[6.0, 6.0],[6.0, 6.0]]]]

      --> x: [[[[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]],
               [[3.0, 3.0],[3.0, 3.0],[3.0, 3.0]]]]

      --> mean: [1, 1]

      --> invstd: [0.8, 0.8]

      output array by 2
      --> dfilter: [57.6, 57.6]

      --> dbias: [36.0, 36.0]

      --> sum_dy: [36.0, 36.0]

      --> sum_dy_xmu: [72.0, 72.0]
     @endverbatim
 *
 * @par Reference
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchnormBackwardReduce(cnnlHandle_t handle,
                                const cnnlTensorDescriptor_t desc_dz,
                                const void *dz,
                                const cnnlTensorDescriptor_t desc_x,
                                const void *x,
                                const cnnlTensorDescriptor_t desc_mean,
                                const void *mean,
                                const cnnlTensorDescriptor_t desc_invstd,
                                const void *invstd,
                                const cnnlTensorDescriptor_t desc_dfilter,
                                void *dfilter,
                                const cnnlTensorDescriptor_t desc_dbias,
                                void *dbias,
                                const cnnlTensorDescriptor_t desc_sum_dy,
                                void *sum_dy,
                                const cnnlTensorDescriptor_t desc_sum_dy_xmu,
                                void *sum_dy_xmu,
                                const bool needs_input_grad0,
                                const bool needs_input_grad1,
                                const bool needs_input_grad2);

// Group:FrozenBatchNorm
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in artificial intelligence, including but not limited
 * to Fast-RCNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batchnorm
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \b diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b pop_mean, \b pop_var, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - input_tensor - diff_y_tensor - filter_tensor - pop_mean_tensor - pop_var_tensor -
 *     output_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data type combinations are:
 *     - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float
 *     - half - half - float - float - float - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, diff_y tensor, filter tensor, pop_mean
 *   tensor, pop_var tensor, output tensor, diff_filter tensor and diff_bias tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the frozen batchnorm backward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackward(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t diff_y_desc,
                            const void *diff_y,
                            const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                            const void *filter,
                            const void *pop_mean,
                            const void *pop_var,
                            float eps,
                            const cnnlTensorDescriptor_t diff_x_desc,
                            void *diff_x,
                            void *diff_filter,
                            void *diff_bias);

// Group:FrozenBatchNorm
/*!
 * @brief Returns in \b size the size of the MLU memory that is used to get
 *        extra space size in FrozenBatchNormBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the FrozenBatchNormBackward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the FrozenBatchNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFrozenBatchNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t x_desc,
                                            size_t *workspace_size);

// Group:FrozenBatchNorm
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in convolution network, including but not limited
 * to Fast-RCNN (Regions with CNN features). Compared with ::cnnlFrozenBatchNormBackward, this
 * function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the frozen
 *   batchnorm backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \b y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \b y which is the output of
 *   cnnlBatchNormForwardInference API. The optional \b y_desc and \b y are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \b y should have exactly the same dimensions as \b x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \b diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b pop_mean, \b pop_var, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlFrozenBatchNormBackwardV2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlFrozenBatchNormBackwardV2. You can get the size of the workspace with
 *   the ::cnnlGetFrozenBatchNormBackwardWorkspaceSize function.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \b diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_z.
 *   The optional \b diff_z_desc and \b diff_z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \b diff_z should have exactly the same dimensions as \b x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x_tensor - y_tensor - diff_y_tensor - filter_tensor - pop_mean_tensor - pop_var_tensor -
 *     diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data types combinations are:
 *     - half - half - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float - float - float
 *     - half - half - half - float - float - float - half - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b y tensor, \b diff_y tensor, \b filter tensor,
 *   \b pop_mean tensor, \b pop_var tensor, \b diff_z tensor, \b diff_x tensor, \b diff_filter tensor
 *   and \b diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the FrozenBatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackwardV2(cnnlHandle_t handle,
                              const cnnlActivationDescriptor_t activation_desc,
                              const cnnlBatchNormMode_t mode,
                              const cnnlBatchNormOps_t bnOps,
                              const cnnlTensorDescriptor_t x_desc,
                              const void *x,
                              const cnnlTensorDescriptor_t y_desc,
                              const void *y,
                              const cnnlTensorDescriptor_t diff_y_desc,
                              const void *diff_y,
                              const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                              const void *filter,
                              const void *pop_mean,
                              const void *pop_var,
                              float eps,
                              void *workspace,
                              size_t workspace_size,
                              const cnnlTensorDescriptor_t diff_z_desc,
                              void *diff_z,
                              const cnnlTensorDescriptor_t diff_x_desc,
                              void *diff_x,
                              void *diff_filter,
                              void *diff_bias);

// Group:FrozenBatchNorm
/*!
 * @brief Computes gradients of batch normalization for the fine-tuning scenario.
 *
 * Frozen Batch Normalization is used in convolution network, including but not limited
 * to Fast-RCNN (Regions with CNN features). Compared with ::cnnlFrozenBatchNormBackward, this
 * function can fuse add operations and relu operations.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the frozen
 *   batchnorm backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] activation_desc
 *   Input. The descriptor of the activation operation. For detailed information,
 *   see ::cnnlActivationDescriptor_t.
 * @param[in] mode
 *   Input. The mode of normalization. The \b mode must be \p CNNL_BATCHNORM_SPATIAL now.
 *   For detailed information, see ::cnnlBatchNormMode_t enum.
 * @param[in] bnOps
 *   Input. The method of activation. The supported combinations of \b bnOps and
 *   \b activation_desc->mode are:
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_IDENTITY;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ACTIVATION and \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU;
 *   - \b bnOps is \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, \b activation_desc->mode is
 *     \p CNNL_ACTIVATION_RELU or \p CNNL_ACTIVATION_IDENTITY.
 *   For detailed information, see ::cnnlBatchNormOps_t enum.
 * @param[in] x_desc
 *   The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor \b y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor \b y which is the output of
 *   cnnlBatchNormForwardInference API. The optional \b y_desc and \b y are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION or \p CNNL_BATCHNORM_OPS_BN_ACTIVATION, otherwise should
 *   set it to NULL. When in use, \b y should have exactly the same dimensions as \b x.
 * @param[in] diff_y_desc
 *   The descriptor of the backpropagated differential tensor \b diff_y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_y.
 * @param[in] filter_bias_mean_var_desc
 *   The descriptor of the \b filter, \b pop_mean, \b pop_var, \b diff_filter and \b
 *   diff_bias tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the input tensor \b bias.
 *   Reserved for future use. Now must set the value to NULL.
 * @param[in] pop_mean
 *   Input. Pointer to the MLU memory that stores the mean of population.
 * @param[in] pop_var
 *   Input. Pointer to the MLU memory that stores the variance of population.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlFrozenBatchNormBackward_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlFrozenBatchNormBackward_v2. You can get the size of the workspace with
 *   the ::cnnlGetFrozenBatchNormBackwardWorkspaceSize function.
 * @param[in] diff_z_desc
 *   The descriptor of the result differential tensor \b diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_z
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_z.
 *   The optional \b diff_z_desc and \b diff_z are only used when \b bnOps is
 *   \p CNNL_BATCHNORM_OPS_BN_ADD_ACTIVATION, otherwise should set it to NULL.
 *   When in use, \b diff_z should have exactly the same dimensions as \b x.
 * @param[in] diff_x_desc
 *   The descriptor of the result differential tensor \b diff_x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[out] diff_filter
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_filter.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "FrozenBatchNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - x_tensor - y_tensor - diff_y_tensor - filter_tensor - bias_tensor - pop_mean_tensor -
 *     pop_var_tensor - diff_z_tensor - diff_x_tensor - diff_filter_tensor - diff_bias_tensor
 *     The supported data types combinations are:
 *     - half - half - half - half - half - half - half - half - half - half - half
 *     - float - float - float - float - float - float - float - float - float - float - float
 *     - half - half - half - float - float - float - float - half - half - float - float
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b y tensor, \b diff_y tensor, \b filter tensor,
 *   \b bias tensor, \b pop_mean tensor, \b pop_var tensor, \b diff_z tensor, \b diff_x tensor,
 *   \b diff_filter tensor and \b diff_bias tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the diff_y tensor should be the same as input tensor.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bias tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - pop_var tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_z tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *     The layout of the output tensor should be the same as input tensor.
 *   - diff_filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the FrozenBatchNormBackward operation is as follows:
     @verbatim
      input five arrays by 1 * 2 * 3 * 2, 1 * 2 * 3 * 2, 2, 2 and 2
      --> x: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
               [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_y: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> filter: [2.0, 2.0]

      --> pop_mean: [0.0,0.0]

      --> pop_var: [1.0,1.0]

      param:
        eps: 0.00001

      output three arrays by 1 * 2 * 3 * 2, 2 and 2
      --> diff_z: [[[[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]],
                    [[1.0, 1.0],[1.0, 1.0],[1.0, 1.0]]]]

      --> diff_x: [[[[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]],
                    [[2.0, 2.0],[2.0, 2.0],[2.0, 2.0]]]]

      --> diff_filter: [6.0, 6.0]

      --> diff_bias: [6.0, 6.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d
 * - https://arxiv.org/abs/1502.03167
 * - Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,
 *   Sergey Ioffe, 2015.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlFrozenBatchNormBackward_v2(cnnlHandle_t handle,
                               const cnnlActivationDescriptor_t activation_desc,
                               const cnnlBatchNormMode_t mode,
                               const cnnlBatchNormOps_t bnOps,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t y_desc,
                               const void *y,
                               const cnnlTensorDescriptor_t diff_y_desc,
                               const void *diff_y,
                               const cnnlTensorDescriptor_t filter_bias_mean_var_desc,
                               const void *filter,
                               const void *bias,
                               const void *pop_mean,
                               const void *pop_var,
                               float eps,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t diff_z_desc,
                               void *diff_z,
                               const cnnlTensorDescriptor_t diff_x_desc,
                               void *diff_x,
                               void *diff_filter,
                               void *diff_bias);

// Group:SyncBatchNormBackwardElemt
/*!
 * @brief Computes the gradients of input in the training scenario.
 *
 * This function is used in artificial intelligence, including but not limited
 * to ResNet (Deep Residual Network), Yolo (You Only Look Once) and R-CNN (Regions with CNN features).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   syncBatchNormBackwardElemt operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the backpropagated differential tensor \b diff_y. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the backpropagated differential tensor.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_desc
 *   Input. The descriptor of the input tensor \b mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the input tensor \b invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the global inverse standard deviation.
 * @param[in] filter_desc
 *   Input. The descriptor of the input tensor \b filter. For detailed information, see
 *   ::cnnlTensorDescriptor_t. The descriptor can be NULL when \b filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter for affine
 *   transformation after batch normilization. The value of this pointer can be NULL.
 * @param[in] mean_dy_desc
 *   Input. The descriptor of the input tensor \b mean_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean_dy
 *   Input. Pointer to the MLU memory that stores the mean of diff_y.
 * @param[in] mean_dy_xmu_desc
 *   Input. The descriptor of the input tensor \b mean_dy_xmu. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] mean_dy_xmu
 *   Input. Pointer to the MLU memory that stores the mean of the result of diff_y * (x - mean).
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor \b diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the derivative of input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardElemt Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - float(\b diff_y) - float(\b x) - float(\b mean) - float(\b invstd) - float(\b filter) -
 *     float(\b mean_dy) - float(\b mean_dy_xmu) - float(\b diff_x).
 *   - half(\b diff_y) - half(\b x) - float(\b mean) - float(\b invstd) - float(\b filter) -
 *     float(\b mean_dy) - float(\b mean_dy_xmu) - half(\b diff_x).
 *
 * @par Data Layout
 * - The supported data layout of \b diff_y, \b x, \b mean, \b invstd, \b filter, \b mean_dy,
 *   \b mean_dy_xmu and \b diff_x are as follows:
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mean_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mean_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 * - The layouts of the \b diff_x \b x and \b diff_y should be the same.
 *
 * @note
 * - The \b mean, \b invstd, \b filter, \b mean_dy and \b mean_dy_xmu must be 1-D tensors and the
 *   length of the dimension of these tensors should be the same as the the length of the lowest
 *   dimension of \b x.
 * - The length of each dimension of \b diff_y, \b x and \b diff_x must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batch_norm_backward_elemt operation is as follows:
     @verbatim
      input seven arrays by 1, 1, 1, 1, 1, 1, 1 and 1
      --> diff_y: [[[[1.0]]]]
      --> x: [[[[2.0]]]]
      --> mean: [3.0]
      --> invstd: [4.0]
      --> filter: [5.0]
      --> mean_dy: [6.0]
      --> mean_dy_xmu: [7.0]

      output an array by 1
      --> mean: [[[[-8960.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/jit_builtin_functions.html?highlight=batch_norm_backward_elemt
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormBackwardElemt(cnnlHandle_t handle,
                               const cnnlTensorDescriptor_t diff_y_desc,
                               const void *diff_y,
                               const cnnlTensorDescriptor_t x_desc,
                               const void *x,
                               const cnnlTensorDescriptor_t mean_desc,
                               const void *mean,
                               const cnnlTensorDescriptor_t invstd_desc,
                               const void *invstd,
                               const cnnlTensorDescriptor_t filter_desc,
                               const void *filter,
                               const cnnlTensorDescriptor_t mean_dy_desc,
                               const void *mean_dy,
                               const cnnlTensorDescriptor_t mean_dy_xmu_desc,
                               const void *mean_dy_xmu,
                               const cnnlTensorDescriptor_t diff_x_desc,
                               void *diff_x);

// Group:SyncBatchNormBackwardElemt
/*!
 * @brief Computes the gradients of input in the training scenario.
 *
 * This function is used in ResNet (Deep Residual Network), Yolo (You Only Look Once) and
 * R-CNN (Regions with CNN features).
 *
 * Compared with ::cnnlSyncBatchNormBackwardElemt, this function first computes the intermediate
 * results mean_dy and mean_dy_xmu based on \b sum_dy, \b sum_dy_xmu and \b count, and then
 * computes the gradient of \b x with the intermediate results.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   syncBatchNormBackwardElemt operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the backpropagated differential tensor \b diff_y. For
 *   detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the backpropagated differential tensor.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] mean_desc
 *   Input. The descriptor of the input tensor \b mean. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the global mean.
 * @param[in] invstd_desc
 *   Input. The descriptor of the input tensor \b invstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] invstd
 *   Input. Pointer to the MLU memory that stores the global inverse standard deviation.
 * @param[in] filter_desc
 *   Input. The descriptor of the input tensor \b filter. For detailed information, see
 *   ::cnnlTensorDescriptor_t. The descriptor can be NULL when \b filter pointer is NULL.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the input tensor \b filter for affine
 *   transformation after batch normalization. The value of this pointer can be NULL.
 * @param[in] sum_dy_desc
 *   Input. The descriptor of the input tensor \b sum_dy. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] sum_dy
 *   Input. Pointer to the MLU memory that stores the sum of diff_y.
 * @param[in] sum_dy_xmu_desc
 *   Input. The descriptor of the input tensor \b sum_dy_xmu. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] sum_dy_xmu
 *   Input. Pointer to the MLU memory that stores the sum of the result of diff_y * (x - mean).
 * @param[in] count_desc
 *   Input. The descriptor of the input tensor \b count. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] count
 *   Input. Pointer to the MLU memory that stores the number of the high dimensions (the dimensions
 *   except the lowest dimension) of the input tensor \b x on all MLU devices.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor \b diff_x. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the derivative of input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SyncBatchNormBackwardElemt_v2 Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - float(\b diff_y) - float(\b x) - float(\b mean) - float(\b invstd) - float(\b filter) -
 *     float(\b sum_dy) - float(\b sum_dy_xmu) - int32_t(\b count) - float(\b diff_x).
 *   - half(\b diff_y) - half(\b x) - float(\b mean) - float(\b invstd) - float(\b filter) -
 *     float(\b sum_dy) - float(\b sum_dy_xmu) - int32_t(\b count) - half(\b diff_x).
 *
 * @par Data Layout
 * - The supported data layouts of \b diff_y, \b x, \b mean, \b invstd, \b filter, \b sum_dy,
 *   \b sum_dy_xmu and \b diff_x are as follows:
 *   - diff_y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and \p CNNL_LAYOUT_NLC.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY.
 *   - invstd tensor: \p CNNL_LAYOUT_ARRAY.
 *   - filter tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy tensor: \p CNNL_LAYOUT_ARRAY.
 *   - sum_dy_xmu tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC, \p CNNL_LAYOUT_NC and
 *     \p CNNL_LAYOUT_NLC.
 * - The layouts of the \b diff_x \b x and \b diff_y should be the same.
 *
 * @note
 * - The \b mean, \b invstd, \b filter, \b sum_dy and \b sum_dy_xmu must be 1-D tensors and the
 *   length of the dimension of these tensors should be the same as the the length of the lowest
 *   dimension of \b x.
 * - The length of each dimension of \b diff_y, \b x and \b diff_x must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the sync_batchnorm_backward_elemt_v2 operation is as follows:
     @verbatim
      input seven arrays by 1, 1, 1, 1, 1, 1, 1 and 1
      --> diff_y: [[[[1.0]]]]
      --> x: [[[[2.0]]]]
      --> mean: [3.0]
      --> invstd: [4.0]
      --> filter: [5.0]
      --> sum_dy: [6.0]
      --> sum_dy_xmu: [7.0]
      --> count: [1]

      output an array by 1
      --> mean: [[[[-8960.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.11.0/jit_builtin_functions.html?highlight=batch_norm_backward_elemt
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlSyncBatchNormBackwardElemtV2(cnnlHandle_t handle,
                                 const cnnlTensorDescriptor_t diff_y_desc,
                                 const void *diff_y,
                                 const cnnlTensorDescriptor_t x_desc,
                                 const void *x,
                                 const cnnlTensorDescriptor_t mean_desc,
                                 const void *mean,
                                 const cnnlTensorDescriptor_t invstd_desc,
                                 const void *invstd,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const void *filter,
                                 const cnnlTensorDescriptor_t sum_dy_desc,
                                 const void *sum_dy,
                                 const cnnlTensorDescriptor_t sum_dy_xmu_desc,
                                 const void *sum_dy_xmu,
                                 const cnnlTensorDescriptor_t count_desc,
                                 const void *count,
                                 const cnnlTensorDescriptor_t diff_x_desc,
                                 void *diff_x);

// Group:Square
/*!
 * @brief Computes the square value for every element of the input tensor \b x and returns in \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the square operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Square Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the square operation is as follows:
     @verbatim
      input arrays by 1 * 3 * 3 * 2 -->
          input: [[[[5, -11], [8, 1], [6, 4]],
                  [[3, 8], [2,6], [0, 6]],
                  [[8, 5], [7,4], [-9, 6]]]]

      output array by 1 * 3 * 3 * 2 -->
          output: [[[[25, 121], [64, 1], [36, 16]],
                   [[9, 64], [4,36], [0, 36]],
                   [[64, 25], [49,16], [81, 36]]]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/square
 */
cnnlStatus_t CNNL_WIN_API cnnlSquare(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);
// Group:SquaredDifference
/*!
 * @brief Computes the square of difference between two input tensors element-wise and returns the results in the output tensor \b output.
 *
 * This function may need extra MLU memory as the workspace. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetSquaredDifferenceWorkspaceSize function.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   SquaredDifference operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the shape of the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the shape of the second input tensor.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  output
 *   Output. Pointer to the MLU memory that stores the shape of the output tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the SquaredDifference operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the SquaredDifference operation.
 *   You can get the size of the workspace with the ::cnnlGetSquaredDifferenceWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "SquaredDifference Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 *  - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 *  - Tensors \b input1 and \b input2 support broadcasting and they should satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlSquaredDifference function to perform the
 *   SquaredDifference operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlSquaredDifference(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t input1_desc,
                                                const void *input1,
                                                const cnnlTensorDescriptor_t input2_desc,
                                                const void *input2,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                void *workspace,
                                                size_t workspace_size);

// Group:SquaredDifference
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the SquaredDifference operation.
 *
 * The size of extra workspace is based on the given information of the SquaredDifference operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and the output
 * tensor descriptor \b output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
* @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the SquaredDifference
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the SquaredDifference
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSquaredDifferenceWorkspaceSize(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input1_desc,
                                           const cnnlTensorDescriptor_t input2_desc,
                                           const cnnlTensorDescriptor_t output_desc,
                                                             size_t *workspace_size);

/*!
 * @brief
 *
 * Enumeration variables describe the base that is used in the implementation
 * of the log function.
 *
 */
typedef enum {
  CNNL_LOG_E = 0,  /*!< The base e is used.*/
  CNNL_LOG_2 = 1,  /*!< The base 2 is used.*/
  CNNL_LOG_10 = 2, /*!< The base 10 is used.*/
} cnnlLogBase_t;

// Group:Log
/*!
 * @brief Computes logarithm of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * @deprecated
 *   ::cnnlLog is deprecated and will be removed in the future release. It is recommended to use
 *   ::cnnlLog_v2 instead, which supports the parameter of \b prefer that sets the computing with faster
 *   algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] base
 *    Input. A cnnlLogBase_t type value indicating which base (e, 2 or 10) to be used.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Log Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-20, 2e5].
 *   - half: [1, 60000].
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log
 */
cnnlStatus_t CNNL_WIN_API cnnlLog(cnnlHandle_t handle,
                                  const cnnlLogBase_t base,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:Log
/*!
 * @brief Computes logarithm of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlLog, this function allows you to choose whether to perform log operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] base
 *    Input. A cnnlLogBase_t type value indicating which base (e, 2 or 10) to be used.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Log Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-20, 2e5].
 *   - half: [1, 60000].
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log
 */
cnnlStatus_t CNNL_WIN_API cnnlLog_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlLogBase_t base,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);
// Group:cnnlRsqrt
/*!
 * @brief Computes the reciprocal of the square root on input tensor \b x, and returns the results
 *        in the output tensor \b y.
 *
 * @deprecated
 *   ::cnnlRsqrt is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlRsqrt_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Rsqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-15,1e6].
 *   - half: [5e-4,5000].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/rsqrt
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlRsqrt(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);
// Group:cnnlRsqrt
/*!
 * @brief Computes the reciprocal root square on input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlRsqrt, this function allows you to choose whether to perform rsqrt operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the rsqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Rsqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-15,1e6].
 *   - half: [5e-4,5000].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/rsqrt
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlRsqrt_v2(cnnlHandle_t handle,
                                       const cnnlComputationPreference_t prefer,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);
// Group:BatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, then returns the
 * results in the output tensor \b c. For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_transa
 *   Input. Boolean value indicating whether \b a matrix is transposed.
 * @param[in] is_transb
 *   Input. Boolean value indicating whether \b b matrix is transposed.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b a, \b b and output tensor \b c.
 *   - \b a offchip data type: int8, int16, int31, half, float.
 *   - \b b offchip data type: int8, int16, int31, half, float.
 *   - \b c offchip data type: half, float.
 *   - \b c onchip data type: half, float.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The \b c offchip data type for operation computing and onchip data type must be
 *     float when \b a or \b b data type is int31.
 *   - \b c offchip data type must be the same as \b c onchip data type.
 *   - \b c offchip data type and \b b offchip data type must be half when \b a offchip
 *     data type is half.
 *   - \b c offchip data type and \b b offchip data type must be float when \b a offchip
 *     data type is float.
 *   - The hardware platform must be mlu370 when \b a offchip data type is half or float.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The inputs \b a and \b b are multi-dimensional array, the shape must be no less than
 *     two dimensions and no more than \p CNNL_DIM_MAX dimensions.
 *   - The last two dimensions of the \b a and \b b must be the number of rows and the number
 *     columns for matrix multiplication respectively.
 *   - The number of columns of \b a matrix must be equal to the number of rows of \b b matrix
 *     after both inputs have performed the transpose operations according to parameters.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose
 *   and matrix \b b needs to transpose.
 * - If \b a and \b b are two-dimensional tensors, for best practices, it is recommended to call
 *   ::cnnlMatMul.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_transa:                    false
      is_transb:                    false
      Dimension of input tensor a:  [64, 99, 128]
      Dimension of input tensor b:  [64, 128, 256]
      Dimension of output tensor c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMul(cnnlHandle_t handle,
                                          const bool is_transa,
                                          const bool is_transb,
                                          const cnnlTensorDescriptor_t a_desc,
                                          const void *a,
                                          const cnnlTensorDescriptor_t b_desc,
                                          const void *b,
                                          const cnnlTensorDescriptor_t c_desc,
                                          void *c);
// Group:StrideBatchMatMul
/*!
 * @brief Computes the batch matrices multiplication of a batch of matrices, and then returns the
 * results in the output tensor \b c. Input matrices \b a, \b b and output matrix \b c for each
 * instance of the batch are located at fixed offsets in number of elements from their locations
 * in the previous instance.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   batch matrix multiplication operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] is_transa
 *   Input. Boolean value indicating whether \b a matrix is transposed.
 * @param[in] is_transb
 *   Input. Boolean value indicating whether \b b matrix is transposed.
 * @param[in] m
 *   Input. Row of \b a matrix when \b a matrix is not transposed.
 * @param[in] n
 *   Input. Column of \b b matrix when \b b matrix is not transposed.
 * @param[in] k
 *   Input. Column of \b a matrix when \b a matrix is not transposed.
 * @param[in] batch_size
 *   Input. Number of batch for matrices multiplication.
 * @param[in] alpha
 *   Input. Scalar used for mulplication.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] lda
 *   Input. Leading dimension of two-dimensional array used to store the matrix a[i] and
 *   now not supported. The default value of \b lda is 0.
 * @param[in] stride_a
 *   Input. An int value that gives the offset in number of elements between a[i] and a[i + 1].
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] ldb
 *   Input. Leading dimension of two-dimensional array used to store the matrix b[i] and
 *   now not supported. The default value of \b ldb is 0.
 * @param[in] stride_b
 *   Input. An int value that gives the offset in number of elements between b[i] and b[i + 1].
 * @param[in] beta
 *   Input. Scalar used for mulplication. Now that beta equals to 0 is only supported.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] ldc
 *   Input. Leading dimension of two-dimensional array used to store the matrix c[i] and
 *   now not supported. The default value of \b ldc is 0.
 * @param[in] stride_c
 *   Input. An int value that gives the offset in number of elements between c[i] and c[i + 1].
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "StrideBatchMatMul Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor
 *   \b a, \b b and output tensor \b c.
 *   - \b c offchip data type: half, float.
 *   - \b c onchip data type: half, float.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - \b c offchip data type must be the same as \b c onchip data type.
 *   - The hardware platform must be mlu370 or above.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - Dimension of input tensor \b a must be equal to (batch_size - 1) * stride_a + m * k.
 *   - Dimension of input tensor \b b must be equal to (batch_size - 1) * stride_b + n * k.
 *   - Dimension of output tensor \b c must be equal to (batch_size - 1) * stride_c + m * n.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose
 *   and matrix \b b needs to transpose.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      is_transa:  false
      is_transb:  false
      m:  80
      n:  80
      k:  64
      batch_size: 64
      stride_a: 15360
      stride_b: 15360
      stride_c: 19200
      alpha:  1
      beta: 0
      Dimension of input tensor a:  [972800]
      Dimension of input tensor b:  [972800]
      Dimension of output tensor c: [1216000]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlStrideBatchMatMul(cnnlHandle_t handle,
                                                const bool is_transa,
                                                const bool is_transb,
                                                const int  m,
                                                const int  n,
                                                const int  k,
                                                const int batch_size,
                                                const float alpha,
                                                const cnnlTensorDescriptor_t a_desc,
                                                const void *a,
                                                const int lda,
                                                const int64_t stride_a,
                                                const cnnlTensorDescriptor_t b_desc,
                                                const void *b,
                                                const int ldb,
                                                const int64_t stride_b,
                                                const float beta,
                                                const cnnlTensorDescriptor_t c_desc,
                                                void *c,
                                                const int ldc,
                                                const int64_t stride_c);

// Group:BatchMatMulBCast
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication with broadcasting operation.
 *
 * @deprecated
 * ::cnnlGetBatchMatMulBCastWorkspaceSize is deprecated and will be removed in the future release.
 * It is recommended to use ::cnnlGetBatchMatMulAlgoHeuristic instead, which gets the workspace size and algorithm for
 * batch matrix multiplication.
 *
 * The size of extra workspace is based on the given information of the batch matrix multiplication
 * with broadcasting operation, including input tensor descriptor of left matrix \b a_desc, input tensor descriptor of
 * right matrix \b b_desc and output tensor descriptor \b c_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBatchMatMulBCastWorkspaceSize(cnnlHandle_t handle,
                                                               const cnnlTensorDescriptor_t a_desc,
                                                               const cnnlTensorDescriptor_t b_desc,
                                                               const cnnlTensorDescriptor_t c_desc,
                                                               size_t *workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Gets the matrix multiplication algorithm and workspace size from heuristic result,
 *        that is previously selected with ::cnnlGetBatchMatMulAlgoHeuristic.
 *
 * @param[in] result
 *   Input. The matrix multiplication heuristic result obtained by ::cnnlGetBatchMatMulAlgoHeuristic.
 *
 * @param[out] algo
 *   Output. The matrix multiplication algorithm.
 *
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the matrix multiplication operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t cnnlGetBatchMatMulHeuristicResult(cnnlMatMulHeuristicResult_t result,
                                               cnnlMatMulAlgo_t algo,
                                               size_t *workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Retrieves the possible algorithms can be used in the matrix multiplication.
 *        The output is placed in result_array[] in the order of increasing estimated compute time.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the matrix multiplication operations. For detail information,
 *   see ::cnnlMatMulDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The descriptor of the matrix multiplication that holds the preferences for cnnlMatMulHeuristicResult_t
 *   configuration. Currently not supported and should be set to NULL.
 * @param[in] requested_algo_count
 *   Input. The number of requested algorithms. The maximum number of algorithms to be returned.
 *   Currently this value only supports 1.
 * @param[out] result_array
 *   Output. Array containing the algorithm heuristics and associated runtime characteristics, returned by this function,
 *   in the order of increasing estimated compute time.
 * @param[out] return_algo_count
 *   Output. A host pointer to the number of algorithms returned by this function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently the maximum number of algorithms \b requested_algo_count only supports 1.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
             cnnlGetBatchMatMulAlgoHeuristic(cnnlHandle_t handle,
                                             cnnlMatMulDescriptor_t bmm_bcast_desc,
                                             cnnlTensorDescriptor_t a_desc,
                                             cnnlTensorDescriptor_t b_desc,
                                             cnnlTensorDescriptor_t c_desc,
                                             cnnlMatMulPrefer_t preference,
                                             int requested_algo_count,
                                             cnnlMatMulHeuristicResult_t *result_array,
                                             int *return_algo_count);

// Group:BatchMatMulBCast
/*!
 * @brief Computes the batch matrix multiplication with broadcasting operation,
 * then returns the results in the output tensor \b c. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * This function may need extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetBatchMatMulAlgoHeuristic function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   with broadcasting operation. You can get the size of the workspace with the
 *   ::cnnlGetBatchMatMulBCastWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMulBCast Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c.
 *   - \b a: int8, int16, int31.
 *   - \b b: onchip data type: int8, int16, int31.
 *   - \b c: half, float.
 * - This function supports the combinations of the following data types for inpute tensor \b a, \b b and
 *   output tensor \b c on MLU300 series or above.
 *   - \b a, \b b, \b c: half, half, half.
 *   - \b a, \b b, \b c: float, float, float.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type of \b c must be float when \b a data type is int31 or \b b data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *   - \b c offchip data type must be the same as \b c onchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The \b a and \b b must have no less than two dimensions, and the last two dimensions of \b a and \b b
 *     are matrix multiplication compatible.
 *   - The number of columns of \b a matrix must be equal to the number of rows of \b b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the
 *     last two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 * - If \b a and \b b do not need broadcasting, for best practices, it is recommended to call ::cnnlBatchMatMul.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor \b a:  [99, 128]
      Dimension of input tensor \b b:  [64, 128, 256]
      Dimension of output tensor \b c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCast(cnnlHandle_t handle,
                                               const bool is_transa,
                                               const bool is_transb,
                                               const cnnlTensorDescriptor_t a_desc,
                                               const void *a,
                                               const cnnlTensorDescriptor_t b_desc,
                                               const void *b,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t c_desc,
                                               void *c);

// Group:BatchMatMulBCast
/*!
 * @brief Computes the batch matrix multiplication with broadcasting operation,
 * then returns the results in the output tensor \b c. For more information about quantization,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlBatchMatMulBCast, it supports the use of \b bmm_bcast_desc
 * to pass parameters like ::CNNL_MATMUL_DESC_TRANSA.
 *
 * The matrix mulplication algorithm \b algo, scaling factors \b alpha and \b beta,
 * and output \b d are not supported currently.
 *
 * This function may need extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetBatchMatMulAlgoHeuristic function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation.
 *   For detail information, see ::cnnlMatMulDescriptor_t.
 * @param[in] algo
 *   Input. A host pointer to the most suitable algorithm to compute the matrix multiplication.
 *   Currently not supported and should be set to NULL.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor \b a.
 *   Currently not supported and should be set to NULL.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the left matrix.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the right matrix.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor \b c.
 *   Currently not supported and should be set to NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   with broadcasting operation. You can get the size of the workspace with the
 *   ::cnnlGetBatchMatMulBCastWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BatchMatMulBCast Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - On all supported plaftorms, this function supports any combinations of the following data types
 *   for input tensor \b a, \b b and output tensor \b c.
 *   - \b a: int8, int16, int31.
 *   - \b b: onchip data type: int8, int16, int31.
 *   - \b c: half, float.
 * - On MLU300 series or above, this function supports the combinations of the following data types for
 *   inpute tensor \b a, \b b and output tensor \b c.
 *   - \b a, \b b, \b c: half, half, half.
 *   - \b a, \b b, \b c: float, float, float.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type of \b c must be float when \b a data type is int31 or \b b data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *   - \b c offchip data type must be the same as \b c onchip data type.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The \b a and \b b must have no less than two dimensions, and the last two dimensions of \b a and \b b
 *     are matrix multiplication compatible.
 *   - The number of columns of \b a matrix must be equal to the number of rows of \b b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the
 *     last two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 * - If \b a and \b b do not need broadcasting, for best practices, it is recommended to call ::cnnlBatchMatMul.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      CNNL_MATMUL_DESC_TRANSA:      false
      CNNL_MATMUL_DESC_TRANSB:      false
      Dimension of input tensor \b a:  [99, 128]
      Dimension of input tensor \b b:  [64, 128, 256]
      Dimension of output tensor \b c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCast_v2(cnnlHandle_t handle,
                                                  cnnlMatMulDescriptor_t bmm_bcast_desc,
                                                  cnnlMatMulAlgo_t  algo,
                                                  const void *alpha,
                                                  const cnnlTensorDescriptor_t a_desc,
                                                  const void *a,
                                                  const cnnlTensorDescriptor_t b_desc,
                                                  const void *b,
                                                  const void *beta,
                                                  const cnnlTensorDescriptor_t c_desc,
                                                  void *c,
                                                  void *workspace,
                                                  size_t workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Creates a descriptor pointed by \b bmm_bcast_desc for a batch matrix multiplication with
 *        broadcasting operation, and allocates memory for holding the information about the batch
 *        matrix multiplication with broadcasting operation. The information is defined in
 *        ::cnnlBatchMatMulBCastDescriptor_t. For more information about descriptor, see "Cambricon
 *        CNNL User Guide".
 *
 * @param[out] bmm_bcast_desc
 *   Output. A host pointer to the batch matrix multiplication with broadcasting descriptor that holds
 *   information about the batch matrix multiplication with broadcasting operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetBatchMatMulBCastDescAttr function to initialize
 *   and set the information to the batch matrix multiplication with broadcasting descriptor.
 * - You need to call the ::cnnlBatchMatMulBCastDescDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchMatMulBCastDescCreate(cnnlBatchMatMulBCastDescriptor_t *bmm_bcast_desc);

// Group:BatchMatMulBCast
/*!
 * @brief Destroys a batch matrix multiplication with broadcasting descriptor \b bmm_bcast_desc
 *        that is previously created with the ::cnnlBatchMatMulBCastDescCreate.
 *
 * The batch matrix multiplication with broadcasting descriptor is defined in ::cnnlBatchMatMulBCastDescriptor_t
 * and holds the information about the batch matrix multiplication with broadcasting operation.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The batch matrix multiplication with broadcasting descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBatchMatMulBCastDescDestroy(cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc);

// Group:BatchMatMulBCast
/*!
 * @brief Initializes the batch matrix multiplication with broadcasting descriptor \b bmm_bcast_desc
 * that is previously created with the ::cnnlBatchMatMulBCastDescCreate function, and sets
 * the information about the batch matrix multiplication with broadcasting operation to the batch matrix
 * multiplication with broadcasting descriptor \b bmm_bcast_desc. The information includes the attribute
 * defined in ::cnnlBatchMatMulBCastDescAttribute_t \b attr, the host pointer to the attribute value
 * \b buf, and the size of buffer for verification.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication with broadcasting descriptor to be set. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescAttribute_t.
 * @param[out] buf
 *   Output. A host pointer to the attribute value set by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetBatchMatMulBCastDescAttr(cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                cnnlBatchMatMulBCastDescAttribute_t attr,
                                const void *buf,
                                size_t size_in_bytes);

// Group:BatchMatMulBCast
/*!
 * @brief Returns the pointer to the \b buf and size of the buffer \b size_written of the attribute
 * retrieved with the given batch matmul multiplication with broadcasting descriptor \b bmm_bcast_desc,
 * attribute \b attr. And \b size_in_bytes is used to check if the memory size is same with \b size_written.
 *
 * You can set the attribute in the batch matrix multiplication with broadcasting descriptor based on
 * the return value of this function.
 *
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *   information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] attr
 *   Input. Attribute of batch matrix multiplication with broadcasting descriptor to be retrieved.
 * @param[out] buf
 *   Output. A host pointer to the attribute value to be retrieved by this function.
 * @param[in] size_in_bytes
 *   Input. Buffer in bytes for verification.
 * @param[out] size_written
 *   Output. A host pointer to the number of bytes actually written to the buffer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBatchMatMulBCastDescAttr(const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                cnnlBatchMatMulBCastDescAttribute_t attr,
                                void *buf,
                                size_t size_in_bytes,
                                size_t *size_written);

// Group:BatchMatMulBCast
/*!
 * @brief Creates a descriptor pointed by \b algo for a batch matrix multiplication with broadcasting algorithm,
 *        and allocates memory for holding the information about the algorithm.
 *        The information is defined in ::cnnlBatchMatMulBCastAlgo_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] algo
 *   Output. A host pointer to the batch matrix multiplication with broadcasting algorithm that holds information about
 *   the batch matrix multiplication with broadcasting algorithm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to initialize
 *   and set the information to the batch matrix multiplication with broadcasting algorithm.
 * - You need to call the ::cnnlBatchMatMulBCastAlgoDestroy function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCastAlgoCreate(cnnlBatchMatMulBCastAlgo_t *algo);

// Group:BatchMatMulBCast
/*!
 * @brief Destroys a batch matrix multiplication with broadcasting algorithm descriptor \b algo
 *        that is previously created with the ::cnnlBatchMatMulBCastAlgoCreate.
 *
 * The batch matrix multiplication with broadcasting descriptor is defined in ::cnnlBatchMatMulBCastAlgo_t
 * and holds the information about the batch matrix multiplication with broadcasting algorithm.
 *
 * @param[in] algo
 *   Input. The batch matrix multiplication with broadcasting algorithm descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlBatchMatMulBCastAlgoDestroy(cnnlBatchMatMulBCastAlgo_t algo);

// Group:BatchMatMulBCast
/*!
 * @brief Returns the most suited batch matrix multiplication with broadcasting algorithm that can be used
 * in the operation.
 *
 * The returned batch matrix multiplication is chosen from all the supported batch matrix with
 * broadcasting algorithms defined in ::cnnlBatchMatMulBCastAlgo_t and is based on the given batch matrix
 * multiplication with broadcasting descriptor \b bmm_bcast_desc, tensor descriptor of left matrix \b a_desc,
 * tensor descriptor of right matrix \b b_desc, tensor descriptor of output matrix \b c_desc, and batch
 * matrix multiplication with broadcasting algorithm \b preference.
 *
 * The computing performance options \b preference defined in the ::cnnlBatchMatMulBCastPreference_t
 * enum, only supports the high speed mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *  Input. The descriptor of the batch matrix multiplication with broadcasting operation. For detailed
 *  information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor descriptor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor descriptor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor descriptor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] preference
 *   Input. The options for implementing the batch matrix multiplication with broadcasting operation to
 * get better performance. This parameter only supports CNNL_BMM_BCAST_FASTEST now.
 * @param[out] algo
 *   Output. A host pointer to the returned algorithm that is best suited for computing the batch matrix multiplication
 *   with broadcasting. The algorithms are defined in the ::cnnlBatchMatMulBCastAlgo_t enum.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetQuantizeBatchMatMulBCastAlgorithm(cnnlHandle_t handle,
                                         const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                                         const cnnlTensorDescriptor_t a_desc,
                                         const cnnlTensorDescriptor_t b_desc,
                                         const cnnlTensorDescriptor_t c_desc,
                                         cnnlBatchMatMulBCastPreference_t preference,
                                         cnnlBatchMatMulBCastAlgo_t *algo);

// Group:BatchMatMulBCast
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the batch matrix multiplication with broadcasting operation.
 *
 * The size of extra workspace is based on the given information of the batch matrix multiplication
 * with broadcasting operation, including the batch matrix multiplication with broadcasting descriptor
 * \b bmm_bcast_desc, input tensor descriptor of left matrix \b a_desc, input tensor descriptor of
 * right matrix \b b_desc, output tensor descriptor \b c_desc, and the batch matrix multiplication
 * with broadcasting algorithm \b algo. For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operations. For detail information,
 *   see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the input tensor of output matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication with broadcasting. The algorithms are defined
 *   in the ::cnnlBatchMatMulBCastAlgo_t enum. You can get the best suited algorithm with the
 *   ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function.
 * @param[out] workspace_size
 *   Output. Pointer to the MLU memory that stores the returned size of the extra workspace in bytes.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function. You also need to
 *   call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions to create and set
 *   tensor descriptors \b a_desc, \b b_desc, \b c_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlQuantizeBatchMatMulBCast function to
 *   performs the batch matrix multiplication with broadcasting operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetQuantizeBatchMatMulBCastWorkspaceSize(
                          cnnlHandle_t handle,
                          cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                          const cnnlTensorDescriptor_t a_desc,
                          const cnnlTensorDescriptor_t b_desc,
                          const cnnlTensorDescriptor_t c_desc,
                          cnnlBatchMatMulBCastAlgo_t algo,
                          size_t *workspace_size);

// Group:BatchMatMulBCast
/*!
 * @brief Quantizes data type of input tensor \b a and \b b, and computes the batch matrix
 * multiplication with broadcasting operation, then returns the results in the output tensor \b c.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * This function needs extra MLU memory as the workspace to improve the batch matrix multiplication
 * with broadcasting performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetQuantizeBatchMatMulBCastWorkspaceSize function. The batch matrix multiplication with broadcasting
 * is computed based on the algorithm set in \b algo. You can call the
 * ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function to get the most suited algorithm.
 *
 * The scaling factors \b alpha and \b beta are not supported currently.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the batch
 *   matrix multiplication with broadcasting operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] bmm_bcast_desc
 *   Input. The descriptor of the batch matrix multiplication with broadcasting operations.
 *   For detail information, see ::cnnlBatchMatMulBCastDescriptor_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor of left matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] a_position
 *   Input. Pointer to the MLU memory associated tensor \b a quantization param \b position.
 * @param[in] a_scale
 *   Input. Pointer to the MLU memory associated tensor \b a quantization param \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] a_offset
 *   Input. Pointer to the MLU memory associated tensor \b a quantization param \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] b_desc
 *   Input. The descriptor of the input tensor of right matrix. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] b_position
 *   Input. Pointer to the MLU memory associated tensor \b b quantization param \b position.
 * @param[in] b_scale
 *   Input. Pointer to the MLU memory associated tensor \b b quantization param \b scale.
 *   The value of this parameter can be NULL.
 * @param[in] b_offset
 *   Input. Pointer to the MLU memory associated tensor \b b quantization param \b offset.
 *   The value of this parameter can be NULL.
 * @param[in] c_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] c
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] algo
 *   Input. The algorithm used to compute the batch matrix multiplication with broadcasting.
 *   The algorithms are defined in the ::cnnlBatchMatMulBCastAlgo_t enum. You can get the best
 *   suited algorithm with the ::cnnlGetQuantizeBatchMatMulBCastAlgorithm function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the batch matrix multiplication
 *   with broadcasting operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the batch matrix multiplication
 *   with broadcasting operation. You can get the size of the workspace with the
 *   ::cnnlGetQuantizeBatchMatMulBCastWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b a, \b b and
 *   output tensor \b c.
 *   - \b a ddr data type: half, float.
 *   - \b a onchip data type: int8, int16, int31.
 *   - \b b ddr data type: half, float.
 *   - \b b onchip data type: int8, int16, int31.
 *   - \b c ddr offchip data type: half, float.
 *   - The data type for operation computing: half, float.
 * - \b a ddr data type should be the same as \b b ddr data type.
 * - \b a ddr data type can be combined with any onchip data type.
 * - \b b ddr data type can be combined with any onchip data type.
 *
 * @note
 * - The combinations of the data types should satisfy the following rules:
 *   - The data type bitwidth for operation computing is not shorter than \b c ddr data type.
 *   - The data type for operation computing \p must be float when onchip data type is int31.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - This function does not support offline asymmetric quantization currently.
 *
 * @par Scale Limitation
 * - The input tensors and output tensor must meet the following requirements:
 *   - The \b a and \b b must have no less than two dimensions, and the last two dimensions of \b a and \b b
 *     are matrix multiplication compatible.
 *   - The number of columns of \b a matrix must be equal to the number of rows of \b b matrix after both
 *     inputs have performed the transpose operations according to parameters. With the exception of the last
 *     two dimensions, the other dimensions need to satisfy the broadcasting rules.
 *
 * @par API Dependency
 * - Before calling this function to implement batch matrix multiplication with broadcasting, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, matrix \b a does not need to transpose and matrix \b b
 *   needs to transpose.
 * - If \b a and \b b do not need broadcasting, for best practices, it is recommended to call ::cnnlQuantizeBatchMatMul.
 *
 * @par Example
 * - The example of the operation is as follows:
     @verbatim
      transa:                    false
      transb:                    false
      Dimension of input tensor \b a:  [99, 128]
      Dimension of input tensor \b b:  [64, 128, 256]
      Dimension of output tensor \b c: [64, 99, 256]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/torch.html?highlight=bmm#torch.bmm
 */
cnnlStatus_t CNNL_WIN_API
cnnlQuantizeBatchMatMulBCast(cnnlHandle_t handle,
                             const cnnlBatchMatMulBCastDescriptor_t bmm_bcast_desc,
                             const void *alpha,
                             const cnnlTensorDescriptor_t a_desc,
                             const void *a,
                             const void *a_position,
                             const void *a_scale,
                             const void *a_offset,
                             const cnnlTensorDescriptor_t b_desc,
                             const void *b,
                             const void *b_position,
                             const void *b_scale,
                             const void *b_offset,
                             const void *beta,
                             const cnnlTensorDescriptor_t c_desc,
                             void *c,
                             cnnlBatchMatMulBCastAlgo_t algo,
                             void *workspace,
                             size_t workspace_size);

// Group:OpTensor
/*!
 * @brief Creates a descriptor pointed by \b op_tensor_desc for an OpTensor operation,
 * and allocates memory for holding the information about the ::cnnlOpTensor operation.
 * The information is defined in ::cnnlOpTensorDescriptor_t. For more information about
 * descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] op_tensor_desc
 *   Output. A host pointer to the OpTensor descriptor that holds information about the
 *   OpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetOpTensorDescriptor function
 *    to initialize and set the information to the OpTensor descriptor.
 *  - You need to call the ::cnnlDestroyOpTensorDescriptor function to destroy the
 *    descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlCreateOpTensorDescriptor(cnnlOpTensorDescriptor_t *op_tensor_desc);

// Group:OpTensor
/*!
 * @brief Initializes the OpTensor descriptor \b op_tensor_desc that is previously created
 * with the ::cnnlCreateOpTensorDescriptor function, and sets the information about the
 * OpTensor operation to the OpTensor descriptor \b op_tensor_desc. The information
 * includes the operation of the OpTensor \b op_tensor_op, the data type of operation
 * \b op_tensor_comp_type, the NaN propagation policy \b op_tensor_nan_opt.
 *
 * @param[out] op_tensor_desc
 *   Output. The descriptor of the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDescriptor_t.
 * @param[in] op_tensor_op
 *   Input. The specific operation for the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDesc_t.
 * @param[in] op_tensor_comp_type
 *   Input. The data type of operation. For detailed information, see ::cnnlDataType_t.
 * @param[in] op_tensor_nan_opt
 *   Input. The NaN propagation policy. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlSetOpTensorDescriptor(cnnlOpTensorDescriptor_t op_tensor_desc,
                                                    cnnlOpTensorDesc_t op_tensor_op,
                                                    cnnlDataType_t op_tensor_comp_type,
                                                    cnnlNanPropagation_t op_tensor_nan_opt);

// Group:OpTensor
/*!
 * @brief Returns the information that is included in the OpTensor descriptor
 * \b op_tensor_desc. The operation type will be returned in the \b op_tensor_op,
 * the data type of operation will be returned in the \b op_tensor_comp_type,
 * the NaN propagation policy will be rerurned in the \b op_tensor_nan_opt.
 *
 * @param[in] op_tensor_desc
 *   Input. The descriptor of the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDescriptor_t.
 * @param[out] op_tensor_op
 *   Output. The specific operation for the OpTensor operation. For detailed information,
 *   see ::cnnlOpTensorDesc_t.
 * @param[out] op_tensor_comp_type
 *   Output. The data type of operation. For detailed information, see ::cnnlDataType_t.
 * @param[out] op_tensor_nan_opt
 *   Output. The NaN propagation policy. For detailed information,
 *   see ::cnnlNanPropagation_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlGetOpTensorDescriptor(const cnnlOpTensorDescriptor_t op_tensor_desc,
                                                    cnnlOpTensorDesc_t *op_tensor_op,
                                                    cnnlDataType_t *op_tensor_comp_type,
                                                    cnnlNanPropagation_t *op_tensor_nan_opt);

// Group:OpTensor
/*!
 * @brief Destroys a descriptor \b op_tensor_desc that is previously created with the
 * ::cnnlCreateOpTensorDescriptor.
 *
 * @param[in]  op_tensor_desc
 *   Input. The OpTensor descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API cnnlDestroyOpTensorDescriptor(cnnlOpTensorDescriptor_t op_tensor_desc);

// Group:OpTensor
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory in bytes that is used as
 * an extra workspace to optimize the ::cnnlOpTensor operation.
 *
 * The size of extra workspace is based on the given information of the input and output
 * tensor descriptors, \b a_desc, \b b_desc, and \b c_desc. This function
 * ::cnnlGetOpTensorWorkspaceSize is recommended to use.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlOpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetOpTensorWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t a_desc,
                                                       const cnnlTensorDescriptor_t b_desc,
                                                       const cnnlTensorDescriptor_t c_desc,
                                                       size_t *workspace_size);

// Group:OpTensor
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory in bytes that is used as
 * an extra workspace to optimize the ::cnnlOpTensor operation.
 *
 * @deprecated
 *   ::cnnlGetOpTensorWorkspaceSize_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGetOpTensorWorkspaceSize instead.
 *
 * The size of extra workspace is based on the given information of the input and output
 * tensor descriptors, \b a_desc, \b b_desc, and \b c_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] op_tensor_desc
 *   Input. The specific operation performed in the function. The operations are defined
 *   in the ::cnnlOpTensorDesc_t enum.
 * @param[in] alpha1
 *   Input. A host pointer to scaling factor of the tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the first input tensor a.
 * @param[in] alpha2
 *   Input. A host pointer to scaling factor of the tensor b.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. A device pointer to the MLU memory that stores the second input tensor b.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of the tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlOpTensor operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetOpTensorWorkspaceSize_v2(cnnlHandle_t handle,
                                const cnnlOpTensorDescriptor_t op_tensor_desc,
                                const void *alpha1,
                                const cnnlTensorDescriptor_t a_desc,
                                const void *a,
                                const void *alpha2,
                                const cnnlTensorDescriptor_t b_desc,
                                const void *b,
                                const void *beta,
                                const cnnlTensorDescriptor_t c_desc,
                                void *c,
                                size_t *workspace_size);

// Group:OpTensor
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = op(alpha1[0] * a, alpha2[0] * b) + beta[0] * c
 *
 * where op is the basic operation defined by ::cnnlOpTensorDesc_t enum and is indicated
 * by the descriptor ::cnnlOpTensorDescriptor_t to represent the type of \b op_tensor_desc.
 * The \b a, \b b, and \b c are tensors and \b alpha1, \b alpha2, and \b beta are the
 * scaling factors used in the operation.
 *
 * This function may need extra MLU memory as the workspace to support operation. You can
 * get the size of the workspace with the ::cnnlGetOpTensorWorkspaceSize. The interface
 * ::cnnlGetOpTensorWorkspaceSize is recommended to use. For more information about the
 * workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   optensor operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] op_tensor_desc
 *   Input. The specific operation performed in the function. The operations are defined
 *   in the ::cnnlOpTensorDesc_t enum.
 * @param[in] alpha1
 *   Input. A host pointer to scaling factor of tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the first input tensor a.
 * @param[in] alpha2
 *   Input. A host pointer to scaling factor of tensor b.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor b, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. A device pointer to the MLU memory that stores the second input tensor b.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetOpTensorWorkspaceSize
 *   function or ::cnnlGetOpTensorWorkspaceSize_v2 function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "OpTensor Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \b a, \b b and \b c must be
 *   half-half-half, float-float-float or int32-int32-int32.
 * - \b alpha1, \b alpha2, \b beta: If the data type of tensors were float or half, the
 *   data type of \b alpha1, \b alpha2, \b beta should be float*. If the data type of
 *   tensors were int32, the data type of \b alpha1, \b alpha2, \b beta should be int*.
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The corresponding dimensions of input and output tensors should satisfy the following
 *   condition:
 *   - Each dimension of the input tensor \b a must match the corresponding dimension of
 *   the tensor \b c.
 *   - Each dimension of the input tensor \b b must match the corresponding dimension of
 *   the tensor \b c.
 *   - You need to calculate the correct dimension of \b c in advance.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - When the data type of \b input is int32, the intermediate result of \b input cannot
 *   exceed the value range of the corresponding data type.
 *
 * @par API Dependency
 * - Before calling this function to perform optensor operation, you need to get the size
 *   of workspace by the ::cnnlGetOpTensorWorkspaceSize function or
 *   ::cnnlGetOpTensorWorkspaceSize_v2 function.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \b a,
 *   \b b and \b c to be the same, and the value of scaling factors \b alpha1 equals to 1,
 *   \b alpha2 equals to 1, beta equals to 0.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc and c_desc with
 *   ::cnnlSetTensorDescriptorEx.
 * - On MLU200 series, when the data type is float16, both input data and intermediate result
 *   must be within [-65504, 65504]. Input data refers to \b a, \b b and \b c, and intermediate
 *   result refers to the operation result of \b alpha1 * \b a and \b alpha2 * \b b.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor a  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       alpha1          :   1

       Input tensor b  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       alpha2          :   2

       Input tensor c  :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :    0

       Output tensor c :   [[3,  6,  9],
                            [12, 15, 18],
                            [21, 24, 27]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlOpTensor(cnnlHandle_t handle,
                                       const cnnlOpTensorDescriptor_t op_tensor_desc,
                                       const void *alpha1,
                                       const cnnlTensorDescriptor_t a_desc,
                                       const void *a,
                                       const void *alpha2,
                                       const cnnlTensorDescriptor_t b_desc,
                                       const void *b,
                                       void *workspace,
                                       size_t workspace_size,
                                       const void *beta,
                                       const cnnlTensorDescriptor_t c_desc,
                                       void *c);

// Group:AssignAdd
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAssignAdd operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAssignAdd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetAssignAddWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t a_desc,
                                                        const cnnlTensorDescriptor_t c_desc,
                                                        size_t *workspace_size);

// Group:AssignAdd
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = alpha[0] * a + beta[0] * c
 *
 * where \b a and \b c are tensors, and \b alpha and \b beta are the scaling factors used
 * in this operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAssignAddWorkspaceSize
 *   function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AssignAdd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \b a and \b c must be half-half or
 *   float-float.
 * - \b alpha, \b beta: float*.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The corresponding dimensions of input and output tensors should satisfy the following
 *   condition:
 *   - Each dimension of the input tensor \b a must match the corresponding dimension of
 *     the destination tensor \b c.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - Before calling this function to perform operation, you need to get the size
 *   of workspace by the ::cnnlGetAssignAddWorkspaceSize.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \b a,
 *   \b c to be the same, and the value of scaling factors \b alpha equals to 1, \b beta
 *   equals to 0 or 1.
 *
 * @note
 * - This function supports tensor broadcasting from \b a to \b c, and does not support that from \b c to \b a .
 * - On MLU200 series, when the data type is float16, \b a, \b c, \b a * \b alpha, \b c * \b beta and result \b c must be within [-65504, 65504].
 *   When the data type is float32, \b a, \b c, \b a * \b alpha, \b c * \b beta and result \b c must be within [-3.4 * e38, 3.4 * e38].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   2

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[3, 6,  9],
                            [6, 9,  12],
                            [9, 12, 15]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlAssignAdd(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        void *workspace,
                                        size_t workspace_size,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c);

// Group:cnnlAssignSub
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAssignSub operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAssignSub operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetAssignSubWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t a_desc,
                                                        const cnnlTensorDescriptor_t c_desc,
                                                        size_t *workspace_size);

// Group:cnnlAssignSub
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = beta[0] * c - alpha[0] * a
 *
 * where \b a and \b c are tensors, and \b alpha and \b beta are the scaling factors
 * used in the operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor a. The data type of alpha should be
 *   float*.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAssignSubWorkspaceSize
 *   function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c. The data type of beta should be
 *   float*.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AssignSub Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \b a and \b c must be half-half or
 *   float-float.
 * - \b alpha, \b beta: float*.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The corresponding dimensions of input and output tensors should satisfy the following
 *   condition:
 *   - Each dimension of the input tensor \b a must match the corresponding dimension of
 *     the destination tensor \b c.
 *   - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @par API Dependency
 * - Before calling this function to perform this operation, you need to get the size of
 *   workspace by the ::cnnlGetAssignSubWorkspaceSize.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the dimensions of tensor \b a,
 *   \b c to be the same, and the value of scaling factors \b alpha equals to 1, \b beta
 *   equals to 0 or 1.
 *
 * @note
 * - This function supports tensor broadcasting from \b a to \b c, and does not support that from \b c to \b a .
 * - On 200 series, when the data type is float16, \b a, \b c, \b a * \b alpha, \b c * \b beta and result \b c must be within [-65504,65504].
 *   When the data type is float32, \b a, \b c, \b a * \b alpha, \b c * \b beta and result \b c must be within [-3.4 * e38, 3.4 * e38].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   1

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[0, 0, 0],
                            [3, 3, 3],
                            [6, 6, 6]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAssignSub(cnnlHandle_t handle,
                                        const void *alpha,
                                        const cnnlTensorDescriptor_t a_desc,
                                        const void *a,
                                        void *workspace,
                                        size_t workspace_size,
                                        const void *beta,
                                        const cnnlTensorDescriptor_t c_desc,
                                        void *c);

/******************************************************************************
 * Cambricon CNNL OP: AssignTo
 ******************************************************************************/

// Group:AssignTo
/*!
 * @brief Assigns input tensor \b input to output tensor \b output starting from the
 * position \start along a dimension \b axis, and the output tensor \b output is reused as
 * the assigned tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the assignto operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. The dimension of input tensor along which to be assigned.
 * @param[in] start
 *   Input. The starting position of assigned dimension of the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The parameters should satisfy the following conditions:
 *
 *   - The number of input dimensions must be equal to the number of output dimensions.
 *   - All dimensions expect \b axis must be equal in assignment tensor and assigned tensor.
 *     The input is assignment tensor, the output is assigned tensor.
 *   - The dimension of assignment tensor on \b axis plus \b start must be less than or equal to
 *     the dimension of assigned tensor on \b axis.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set \b axis to 0.
 *
 * @note
 * - The data type of input should be same with output.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the assignto operation is as follows:
   @verbatim
   input: assignment tensor shape (2,3)
          --> [[2,3,4],[5,6,7]]

   axis: 0

   start: 1

   output: assigned tensor shape (3,3)
           --> [[1,1,1],[1,1,1],[1,1,1]]

   Then we will get the output:

   output: tensor shape (3,3) --> [[1,1,1],[2,3,4],[5,6,7]]
   @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAssignTo(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const size_t axis,
                                       const unsigned start,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:AddN
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAddN with broadcasting operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in ::cnnlAddN_v2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_descs[]
 *   Input. An array of descriptors for all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAddN operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAddNWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_descs[],
                                                   const uint32_t input_num,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   size_t *workspace_size);
// Group:AddN
/*!
 * @brief Compute the sum of input tensors.
 *
 * AddN operation is wildly used in artificial intelligence as a kind of basic mathematical
 * operations. Also, this operation is supported in almost all common frameworks, like
 * PyTorch and TensorFlow.
 * Compared with ::cnnlAddN, this function supports multidirectional broadcasting of input tensors.
 *
 * This function may need extra MLU memory as the workspace to support multidirectional broadcasting.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetAddNWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_descs[]
 *   Input. An array of descriptors for all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the input tensors.
 * @param[in] input_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetAddNWorkspaceSize
 *   function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *
 * @par Formula
 * - See "AddN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par API Dependency
 * - Before calling this function to perform AddN operation, you need to get the size
 *   of workspace by the ::cnnlGetAddNWorkspaceSize function.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: float, half, int32, int16, int8, uint8.
 *   - output tensor: float, half, int32, int16, int8, uint8.
 *   <b>Note that the data type of output should be same with inputs.</b>
 *
 * @par Scale Limitation
 * - The maximum dimensions of both input and output tensors are 8D.
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3]]


       Input tensor  2 :   [[1],
                            [4],
                            [7]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[3,  5,  7],
                            [9, 11, 13],
                            [15,17, 19]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAddN_v2(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_descs[],
                                      const void *const inputs[],
                                      const uint32_t input_num,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      void * workspace,
                                      size_t workspace_size);
// Group:AddN
/*!
 * @brief Implements the equation:
 *
 * c = sum(inputs)
 *
 * AddN operation is wildly used in artificial intelligence as a kind of basic mathematical
 * operations. Also, this operation is supported in almost all common frameworks, like
 * PyTorch and TensorFlow.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_descs[]
 *   Input. An array of descriptor for the all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the all input tensors.
 * @param[in] input_num
 *   Input. The num of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "AddN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *   <b>Note that the data type of output should be same with inputs.</b>
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The dimensions of input tensors and output tensor must be the same.
 * - The shape of input tensors and output tensor must be the same.
 * - The number of input tensors must be greater than or equal to one.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  2 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[3,  6,  9],
                            [12, 15, 18],
                            [21, 24, 27]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlAddN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_descs[],
                                   const void *const inputs[],
                                   uint32_t input_num,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);
// Group:MulN
/*!
 * @brief Implements the equation:
 *
 * c = product(inputs)
 *
 * MulN operation is wildly used in artificial intelligence as a kind of basic mathematical
 * operations. Also, this operation is supported in almost all common frameworks, like
 * PyTorch and TensorFlow.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_descs[]
 *   Input. An array of descriptor for the all input tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] inputs[]
 *   Input. An array of device pointers to the MLU memory for the all input tensors.
 * @param[in] input_num
 *   Input. The number of tensors in array inputs[].
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "MulN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: float, half.
 *   - output tensor: float, half.
 *   <b>Note that the data type of output should be same with inputs.</b>
 *
 * @par Data Layout
 * - Data layouts of all input tensors and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The dimensions of input tensors and output tensor must be the same.
 * - The shape of input tensors and output tensor must be the same.
 * - The number of input tensors must be greater than or equal to one.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  1 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  2 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input tensor  3 :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       Input num       :   3

       Output tensor   :   [[1,   8,   27],
                            [64,  125, 216],
                            [343, 512, 729]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlMulN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_descs[],
                                   const void *const inputs[],
                                   uint32_t input_num,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Floor
/*!
 * @brief Computes floor on input tensor \b x, and returns the results in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the floor
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. The pointer of output data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Floor Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/floor
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc with
 *   ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloor(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:WeightNorm
/*!
 *  @brief Performs the backward filter normalization operator computation.
 *
 *  filter normalization is a reparameterization that decouples the magnitude of
 *  the filter tensor from its direction. The parameter filter in network layer is
 *  replaced by magnitude (g) and direction (v).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_w_desc
 *   Input. Descriptor of \b diff_w that is gradient of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_w
 *   Input. Pointer to the MLU memory that stores the \b diff_w tensor.
 * @param[in] v_desc
 *   Input. Descriptior of \b v that is direction of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] v
 *   Input. Pointer to the MLU memory that stores the \b v tensor.
 * @param[in] g_desc
 *   Input. Descriptior of \b g that is magnitude of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] g
 *   Input. Pointer to the MLU memory that stores the \b g tensor.
 * @param[in] norm_recip_desc
 *   Input. Descriptior of \b norm_recip that is intermediate data from
 *   weightnorm forward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] norm_recip
 *   Input. Pointer to the MLU memory that stores the \b norm_recip tensor.
 * @param[in] axis
 *   Input.  The dimension of filter to normalize in network layer.
 * @param[in] diff_v_desc
 *   Input. Descriptior of \b diff_v that gradient of \b v.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_v
 *   Output. Pointer to the MLU memory that stores the \b diff_v tensor.
 * @param[in] diff_g_desc
 *   Input. Descriptior of \b diff_g that is gradient of \b g.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_g
 *   Output. Pointer to the MLU memory that stores the \b diff_g tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the filter normalization backward operation is as follows:
     @verbatim
       \b diff_w : tensor with dimension of  [2, 3, 4, 9]
       \b v : tensor with dimension of  [2, 3, 4, 9]
       \b g : tensor with dimension of  [2, 1, 1, 1]
       \b norm_recip : tensor with dimension of  [2, 1, 1, 1]
       \b axis: 0
       Then we will get the output:
       \b diff_v : tensor with dimension of  [2, 3, 4, 9]
       \b diff_g : tensor with dimension of  [2, 1, 1, 1]
      @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/WeightNorm.cpp
 * - http://arxiv.org/abs/1602.07868v3
 */
cnnlStatus_t CNNL_WIN_API
cnnlWeightNormBackward(cnnlHandle_t handle,
                       const cnnlTensorDescriptor_t diff_w_desc,
                       const void *diff_w,
                       const cnnlTensorDescriptor_t v_desc,
                       const void *v,
                       const cnnlTensorDescriptor_t g_desc,
                       const void *g,
                       const cnnlTensorDescriptor_t norm_recip_desc,
                       const void *norm_recip,
                       const int axis,
                       const cnnlTensorDescriptor_t diff_v_desc,
                       void *diff_v,
                       const cnnlTensorDescriptor_t diff_g_desc,
                       void *diff_g);

// Group:Exp
/*!
 * @brief Computes exponetial of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * @deprecated
 *   ::cnnlExp is deprecated and will be removed in the future release. It is recommended to use
 *   ::cnnlExp_v2 instead, which supports the parameter of \b prefer that sets the computing with faster
 *   algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the exp
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Exp Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor \b x and output tensor \b y have the same shape.
 * - The input tensor \b x should be in the following range to guarantee the accuracy of output \b y:
 *   - On MLU 200 series:
 *     - float: [-90.0,50.0].
 *     - half: [-5.0,5.0].
 * - The range of \b x is recommended to be in [-10.0,10.0] for float data type for higher precision on
 *   MLU 300 series.
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/exp
 */
cnnlStatus_t CNNL_WIN_API cnnlExp(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  void *y);

// Group:Exp
/*!
 * @brief Computes exponetial of input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlExp, this function allows you to choose whether to perform exp operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the exp
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Exp Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor \b x and output tensor \b y have the same shape.
 * - The input tensor \b x should be in the following range to guarantee the accuracy of output \b y:
 *   - On MLU 200 series:
 *     - float: [-90.0,50.0].
 *     - half: [-5.0,5.0].
 * - The range of \b x is recommended to be in [-10.0,10.0] for float data type for higher precision on
 *   MLU 300 series.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/exp
 */
cnnlStatus_t CNNL_WIN_API cnnlExp_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     void *y);
// Group:Sqrt
/*!
 * @brief Computes the square root on input tensor \b x, and returns the results in the output tensor \b y.
 *
 * @deprecated
 *   ::cnnlSqrt is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSqrt_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Sqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same, except when intput tensor is int32, output tensor can be float.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-10,1e10].
 *   - half: [1e-3,1e-2] & [1e-1,60000].
 *   - int32: [-2^23, 2^23 - 1] on MLU200 series.
 *   - int32: [-2^31, 2^31 - 1] on MLU300 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.

 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/sqrt
 *
 * @note
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlSqrt(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:Sqrt
/*!
 * @brief Computes square root on input tensor \b x, and returns the results in the output tensor \b y.
 *
 * Compared with ::cnnlSqrt, this function allows you to choose whether to perform sqrt operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the sqrt
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Sqrt Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same, except when intput tensor is int32, output tensor can be float.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [1e-10,1e10].
 *   - half: [1e-3,1e-2] & [1e-1,60000].
 *   - int32: [-2^23, 0) & (0, 2^23 - 1] on MLU200 series.
 *   - int32: [-2^31, 2^31 - 1] on MLU300 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.

 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/sqrt
 *
 * @note
 * - The \b prefer mode ::CNNL_COMPUTATION_ULTRAHIGH_PRECISION is not supported currently.
 * - Stride is supported when the input type is half or float, but not supported when the input type is int32.
 * - You can specify the stride of all dimensions for x_desc and y_desc
 *   with ::cnnlSetTensorDescriptorEx.
 */
cnnlStatus_t CNNL_WIN_API cnnlSqrt_v2(cnnlHandle_t handle,
                                      const cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      void *y);
// Group:Reciprocal
/*!
 * @brief Computes the reciprocal of the elements of input tensor \b input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   reciprocal operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Reciprocal Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the data type of input \b input is float, the range of input is [-2e6, -0.00391] or [0.00391, 2e6].
 *   When the data type of input \b input is half, the range of input is [-65504, -0.00391] or [0.00391, 65504].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the reciprocal operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[5, 2], [3, 4]]

     output array by 2 * 2 --> output: [[0.2, 0.5], [0.3333, 0.25]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/reciprocal
 */
cnnlStatus_t CNNL_WIN_API cnnlReciprocal(cnnlHandle_t handle,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void * input,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void * output);

/******************************************************************************
 * Cambricon CNNL OP: SoftsignForward
 ******************************************************************************/

// Group:Softsign
/*!
 * @brief Applys the softsign function on input.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softsignforward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftsignForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - When the data type of \b input is float and platforms is MLU200 series,
 *   the range of input is [-2e6, 2e6].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the softsign_forward operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[-4., 1.], [0., 3.]]

     output array by 2 * 2 --> output: [[-0.8, 0.5], [0., 0.75]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/nn/softsign
 */

cnnlStatus_t CNNL_WIN_API cnnlSoftsignForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const cnnlTensorDescriptor_t y_desc,
                                              void *y);

/******************************************************************************
 * Cambricon CNNL OP: ComplexAbs
 ******************************************************************************/
// Group:ComplexAbs
/*!
 * @brief Computes the modulus of complex numbers.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ComplexAbs operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor of complex numbers.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ComplexAbs Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: complex_float.
 *   - output tensor: float.
 *
 * @par Scale Limitation
 * - On MLU200 series, when the data type of \b input is complex_float, The modulus range of
 *   input is [1e-5, 800].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the complex_abs operation is as follows:
     @verbatim
     input one array by 2 * 2 --> input: [[(-4.,-3.), (1.,0.)], [(0.,0.), (-3.,4.)]]

     output array by 2 * 2 --> output: [[5., 1.], [0., 5.]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/ComplexAbs
 */

cnnlStatus_t cnnlComplexAbs(const cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t x_desc,
                            const void *x,
                            const cnnlTensorDescriptor_t y_desc,
                            void *y);

/******************************************************************************
 * Cambricon CNNL OP: SoftsignGrad
 ******************************************************************************/
// Group:Softsign
/*!
 * @brief Computes the softsign gradients of softsign operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softsignGrad operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gradients_desc
 *   Input. The descriptor of the gradients tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] gradients
 *   Input. Pointer to the MLU memory that stores the gradients tensor.
 * @param[in] features_desc
 *   Input. The descriptor of the features tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] features
 *   Input. Pointer to the MLU memory that stores the features tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "SoftsignGrad Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of gradients tensor and output tensor must be the same.
 * - The supported data types of gradients and output tensors are as follows:
 *   - gradients tensor: half, float.
 *   - features tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the gradients tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The range of inputs are [-1.41e3, 1.41e3] on the MLU200.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of the SoftsignGrad operation is as follows:
     @verbatim
     gradients array by 2 * 2 --> input: [[1, -2], [3, -4]]
     features array by 2 * 2 --> input: [[-4, 1], [0, 3]]

     output array by 2 * 2 --> output: [[0.04, -0.5], [3, -0.25]]
     @endverbatim
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/python/tf/nn/softsigngrad
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftsignGrad(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t gradients_desc,
                                           const void *gradients,
                                           const cnnlTensorDescriptor_t features_desc,
                                           const void *features,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:Div
/*!
 * @brief Computes division on input tensor \b x and \b y, and returns the results
 *        in the output tensor \b output.
 *
 * @deprecated
 *   ::cnnlDiv is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlDiv_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * This function may need extra MLU memory as the workspace to improve the division performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetDivWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the division operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the division operation.
 *   You can get the size of the workspace with the ::cnnlGetDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Div Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b x
 *   and \b y, c3_dim represents the dimension of \b z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlDiv function to perform the division operation.
 *
 * @note
 * - The inputs \b x and \b y are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 * - On MLU200 series, when input \b y data type is float, \b y data range is [-1e10,-1e-20] & [1e-20,1e10],
 *   and when \b y data type is half, \b y data range is [-65504,-1e-4] & [1e-4,65504].
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with ::cnnlSetTensorDescriptorEx.
 * - When the data type of input tensors is half, if the calculation result exceeds the representation range of half,
 *   the value of \b c will be saturation value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 */
cnnlStatus_t CNNL_WIN_API cnnlDiv(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  const void *y,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t z_desc,
                                  void *z);

// Group:Div
/*!
 * @brief Computes division on input tensor \b x and \b y, and returns the results
 *        in the output tensor \b output.
 *
 * Compared with ::cnnlDiv, this function allows you to choose whether to perform div
 * operation with faster algorithm or higher precision.
 *
 * This function may need extra MLU memory as the workspace to improve the division performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetDivWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the division operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the division operation.
 *   You can get the size of the workspace with the ::cnnlGetDivWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Div Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b x
 *   and \b y, c3_dim represents the dimension of \b z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlDiv_v2 function to perform the division operation.
 *
 * @note
 * - The inputs \b x and \b y are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 * - On MLU200 series, when the \b prefer is CNNL_COMPUTATION_FAST and input \b y data type is float, \b y data range
 *   is [-1e10,-1e-20] & [1e-20,1e10].
 * - On MLU200 series, when the \b prefer is CNNL_COMPUTATION_HIGH_PRECISION and \b y data type is half, \b y data
 *   range is [-65504,-1e-4] & [1e-4,65504].
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with ::cnnlSetTensorDescriptorEx.
 * - On MLU300 series, \b prefer does not work currently. No matter what \b prefer is, the calculation method for
 *   half and float is same.
 * - When the data type of input tensors is half, if the calculation result exceeds the representation range of half,
 *   the value of \b c will be saturation value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 */
cnnlStatus_t CNNL_WIN_API cnnlDiv_v2(cnnlHandle_t handle,
                                     cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t y_desc,
                                     const void *y,
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t z_desc,
                                     void *z);

// Group:Div
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the division operation.
 *
 * The size of extra workspace is based on the given information of the division operation,
 * including the input tensor descriptors \b x_desc and \b y_desc, and the output
 * tensor descriptor \b z_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the division
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the division
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example, c1_dim and c2_dim represent the dimensions of \b x
 *   and \b y, c3_dim represents the dimension of \b z:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDivWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  const cnnlTensorDescriptor_t z_desc,
                                                  size_t *workspace_size);

// Group:DivNoNan
/*!
 * @brief Computes a division of \b x and \b y, and returns zero if input \b y is zero.
 *
 * @deprecated
 *   ::cnnlDivNoNan is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlDivNoNan_v2 instead, which supports parameter of \b prefer that determines
 *   to compute with faster algorithm or higher precision.
 *
 * This function needs extra MLU memory as the workspace to improve the
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetDivNoNanWorkspaceSize function. The input and output tensors
 * are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the divnonan operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor which is a dividend.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor which is a divisor.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   divnonan operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the divnonan operation. You can get the size of the workspace with
 *   the ::cnnlGetDivNoNanWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @note
 * - The inputs \b x and \b y are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - When input \b y data type is float, \b y data range is [-1e10, -1e-10] & [1e-10,1e10].
 *   when \b y data type is half, the range is [-500, -1e-3] & [1e-3, 500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example:
 *
 *   min(x_dim, y_dim) == 1 or x_dim == y_dim
 *
 *   max(x_dim, y_dim) == z_dim
 *
 *   Where x_dim, and y_dim are the number of dimensions in the input tensor \b x and
 *   \b y of the divnonan operation.
 *
 * @par API Dependency
 * - Before calling this function to implement the divnonan operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDivNoNan(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       const void *y,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t z_desc,
                                       void *z);

// Group:DivNoNan
/*!
 * @brief Computes a division of \b x and \b y, and returns zero if input \b y is zero.
 *
 * Compared with ::cnnlDivNoNan, this function allows you to choose whether to perform divnonan
 * operation with faster algorithm or higher precision.
 *
 * This function needs extra MLU memory as the workspace to improve the
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetDivNoNanWorkspaceSize function. The input and output tensors
 * are multi-dimensional arrays, supporting up to CNNL_DIM_MAX dimensions.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the divnonan operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor which is a dividend.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor which is a divisor.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   divnonan operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the divnonan operation. You can get the size of the workspace with
 *   the ::cnnlGetDivNoNanWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @note
 * - The inputs \b x and \b y are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - When input \b y data type is float, \b y data range is [-1e10, -1e-10] & [1e-10,1e10].
 *   when \b y data type is half, the range is [-500, -1e-3] & [1e-3, 500].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. Take the lowest dimension for example:
 *
 *   min(x_dim, y_dim) == 1 or x_dim == y_dim
 *
 *   max(x_dim, y_dim) == z_dim
 *
 *   Where x_dim, and y_dim are the number of dimensions in the input tensor \b x and
 *   \b y of the divnonan operation.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/divide
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDivNoNan_v2(cnnlHandle_t handle,
                                          cnnlComputationPreference_t prefer,
                                          const cnnlTensorDescriptor_t x_desc,
                                          const void *x,
                                          const cnnlTensorDescriptor_t y_desc,
                                          const void *y,
                                          void *workspace,
                                          size_t workspace_size,
                                          const cnnlTensorDescriptor_t z_desc,
                                          void *z);

// Group:DivNoNan
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the divnonan operation.
 *
 * The size of extra workspace is based on the given information of the divnonan
 * operation, including the input tensor descriptor \b x_desc, \b y_desc, output tensor
 * descriptor \b z_desc, For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor which is a dividend. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the divnonan operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetDivNoNanWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t x_desc,
                                                       const cnnlTensorDescriptor_t y_desc,
                                                       const cnnlTensorDescriptor_t z_desc,
                                                       size_t *workspace_size);
// Group:PowR
/*!
 * @brief Computes the power of each element in input tensor \b x with exponent tensor
 *        \b y, and returns the result in the output tensor \b z.
 *
 * This function may need extra MLU memory as the workspace to improve the powr
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetPowRWorkspaceSize function.
 *
 * @deprecated
 *   ::cnnlPowR is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlPow instead, which supports the parameter of \b prefer that sets the computing with
 *   faster algorithm or higher precision and supports value of \b x less than 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the powr
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the powr
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowRWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowRWorkspaceSize function to allocate extra
 *   workspace for \b workspace.
 *
 * @par Formula
 * - See "PowR Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b x, exponent tensor \b y and output tensor \b z must be
 *   the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor: half, float.
 *   - exponent tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input, exponent and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \b x and exponent tensor /b y should satisfy the
 *   following conditions:
 *   - On MLU 200 series:
 *     - input tensor \b x >= 0.
 *     - -15.5 < y * log(x) < 15.5.
 *   - On MLU 300 and CE3226 series:
 *     - input tensor \b x >= 0.
 *     - if \b x = inf or -inf, \b y != 0, where inf represents infinity.
 *     - if \b x = 1, \b y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \b input1, \b input2, \b output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The input tensor \b x and exponent tensor \b y are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPowR(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t z_desc,
                                   void *z);
// Group:PowR
/*!
 * @brief Computes the power of each element in input tensor \b x with exponent tensor
 *        \b y, and returns the result in the output tensor \b z.
 *
 * This function may need extra MLU memory as the workspace to improve the powr
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetPowRWorkspaceSize function.
 *
 * @deprecated
 *   ::cnnlPowR_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlPow instead, which supports value of \b x less than 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   \p CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the powr
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the powr
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowRWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowRWorkspaceSize function to allocate extra
 *   workspace for \b workspace.
 *
 * @par Formula
 * - See "PowR Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b x, exponent tensor \b y and output tensor \b z must be
 *   the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor: half, float.
 *   - exponent tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input, exponent and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \b x and exponent tensor /b y should satisfy the
 *   following conditions:
 *   - On MLU 200 series:
 *     - input tensor \b x >= 0.
 *     - -15.5 < y * log(x) < 15.5.
 *   - On MLU 300 and CE3226 series:
 *     - input tensor \b x >= 0.
 *     - if \b x = inf or -inf, \b y != 0, where inf represents infinity.
 *     - if \b x = 1, \b y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \b input1, \b input2, \b output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The input tensor \b x and exponent tensor \b y are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPowR_v2(cnnlHandle_t handle,
                                      cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      void *workspace,
                                      size_t workspace_size,
                                      const cnnlTensorDescriptor_t z_desc,
                                      void *z);
// Group:PowR
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the powr operation.
 *
 * The size of extra workspace is based on the given information of the powr operation,
 * including the input tensor descriptors \b x_desc and \b y_desc, and the output tensor
 * descriptor \b z_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the powr operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used
 *   in the powr operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPowRWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t x_desc,
                                                   const cnnlTensorDescriptor_t y_desc,
                                                   const cnnlTensorDescriptor_t z_desc,
                                                   size_t *workspace_size);
// Group:Pow
/*!
 * @brief Computes the power of each element in input tensor \b x with exponent tensor
 *        \b y, and returns the result in the output tensor \b z.
 *
 * Compared with ::cnnlPowR and ::cnnlPowR_v2, this funtion has no limitations on the
 * \b x value. Compared with ::cnnlPowN and ::cnnlPowN_v2, the data type of \b y is
 * float or half.
 *
 * This function may need extra MLU memory as the workspace to improve the power
 * performance. You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetPowWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the power operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   \p CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the power
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the power
 *   operation. You can get the size of the workspace with the
 *   ::cnnlGetPowWorkspaceSize function.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetPowWorkspaceSize function to allocate extra
 *   workspace for \b workspace.
 *
 * @par Formula
 * - See "Pow Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor /b x and output tensor /b z must be the same.
 * - This function supports the combinations of the following data types for input tensor \b x,
 *   exponent tensor \b y and output tensor \b z on all hardware platforms.
 *   - \b x, \b y, \b z data type: half, half, half.
 *   - \b x, \b y, \b z data type: float, float, float.
 *   - \b x, \b y, \b z data type: half, int16, half.
 *   - \b x, \b y, \b z data type: float, int16, float.
 *
 * @par Data Layout
 * - The supported data layout of the input, exponent and output tensors must be
 *   \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \b x and exponent tensor /b y should satisfy the
 *   following conditions:
 *   - On MLU 200 series:
 *     - -15.5 < y * log(|x|) < 15.5.
 *   - On MLU 300 and CE3226 series:
 *     - -32767 < y < 32767.
 *     - if \b x = inf or -inf, \b y != 0, where inf represents infinity.
 *     - if \b x = 1, \b y != nan.
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \b input1, \b input2, \b output respectively:
 *   - min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim.
 *   - max(c1_dim, c2_dim) == c3_dim.
 *
 * @note
 * - The input tensor \b x and exponent tensor \b y are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPow(cnnlHandle_t handle,
                                  cnnlComputationPreference_t prefer,
                                  const cnnlTensorDescriptor_t x_desc,
                                  const void *x,
                                  const cnnlTensorDescriptor_t y_desc,
                                  const void *y,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t z_desc,
                                  void *z);

// Group:Pow
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the pow operation.
 *
 * The size of extra workspace is based on the given information of the pow operation,
 * including the input tensor descriptors \b x_desc and \b y_desc, and the output tensor
 * descriptor \b z_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   the pow operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used
 *   in the pow operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPowWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  const cnnlTensorDescriptor_t z_desc,
                                                  size_t *workspace_size);
// Group:Scatter
/*!
 * @brief Reduces all values from \b src tensor into \b input at the indices specified in the \b
 * index tensor along a given axis \b dim.  For each value in \b src, its output index is specified
 * by its index in \b src for dimensions outside of \dim and by the corresponding value in \b index
 * for dimension \b dim. The applied reduction is defined via the ::cnnlScatterMode_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the scatter operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The axis along which to index. Different value of \b dim corresponds to different formula. The axis can be negative.
 *   See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data. \b input supports up to \p CNNL_DIM_MAX dimensions.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index data. The index value of each output data in one of the dimensions is equal to the
 *   corresponding value from \b index tensor. \b index supports up to \p CNNL_DIM_MAX dimensions.
 *   Dimension is determined by the different \b dim value.
 * @param[in] src_desc
 *   Input. The descriptor of the \b src tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] src
 *   Input. Pointer to the MLU memory that stores the src data. All values from tensor \b src will be used to replace or add input value according to \b index.
 *   \b src data supports up to \p CNNL_DIM_MAX dimensions.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output data.
 * @param[in] mode
 *   Input. Scatter mode. Whether to perform add or replace operation defined in ::cnnlScatterMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "Scatter Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The replace operation support the following data types for input tensor \b input, \b src, \b index,
 * and output tensor \b output. The data type of \b input and \b src and \b output must be the same.
 * - input: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - index: int32, int64.
 * - src: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - output: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 * - The add operation support the following data types for input tensor \b input, \b src, \b index,
 * and output tensor \b output. The data type of \b input and \b src and \b output must be the same.
 * - input: int32, half, float.
 * - index: int32, int64.
 * - src: int32, half, float.
 * - output: int32, half, float.
 *
 * @par Scale Limitation
 * - The \b input, \b index, \b src (if it is a tensor) and \b output should all have the same number of dimensions.
 * - The shape of \b src (if it is a tensor) tensor and \b index tensor must be the same.
 * - The shape of \b input tensor and \b output tensor must be the same.
 * - The size of \b index tensor is equal to or less than \b input tensor for all dimensions except for the \b dim dimension.
 * - The \b index value must be greater than or equal to 0.
 *
 * @note
 * - Currently, \b input, \b index, \b src and \b output support up to \p CNNL_DIM_MAX dimensions.
 * When indices are not unique, the behavior is non-deterministic (one of the values from \b src will be picked arbitarily) and
 * the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index).
 * - You can specify the stride of all dimensions for input_desc, index_desc,
 *   src_desc, and output_desc with ::cnnlSetTensorDescriptorEx.
 * - The \b src can be either a tensor or a scalar, the \b src is treated as a tensor when the total element number of \b src
 *   is more than 1, it is treated as a scalar when the total element number of \b src is equal to 1.
 * - In add operation, accuracy problem may occur when the inputs are accumulated on the same coordinate too many times (eg.more
 *   than 800 times).
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scatter or scatter_add operation is as follows:
   @verbatim

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]

   src: [[0., 1., 2., 3., 4.], [5., 6., 7., 8., 9.]]

   dimension: 0

   mode: CNNL_SCATTER

   output: [[0., 6., 7., 3., 4.], [0., 1., 0., 8., 0.], [5., 0., 2., 0., 9.]]

   input: [[0., 0., 0., 0.], [0., 0., 0., 0.]]

   index: [[2], [3]]

   src: 1.23

   dimension: 1

   mode: CNNL_SCATTER

   output: [[0., 0., 1.23, 0.], [0., 0., 0., 1.23]]

   input: [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   index: [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]

   src: [[0.3747, 0.2888, 0.5314, 0.5101, 0.5372], [0.1934, 0.0481, 0.7520, 0.8824, 0.3281]]

   dimension: 0

   mode: CNNL_SCATTER_ADD

   output: [[0.5681, 0.3368, 1.2834, 1.3943, 0.8653], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]

   @endverbatim

 * @par Reference
 * - https://pytorch.org/docs/stable/tensors.html?highlight=scatter_#torch.Tensor.scatter_
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlScatter(cnnlHandle_t handle,
                                      const int dim,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t index_desc,
                                      const void *index,
                                      const cnnlTensorDescriptor_t src_desc,
                                      const void *src,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output,
                                      cnnlScatterMode_t mode);

// Group:BiasAdd
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlBiasAdd operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output, For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlBiasAdd operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlGetBiasAddWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t a_desc,
                                                      const cnnlTensorDescriptor_t c_desc,
                                                      size_t *workspace_size);

// Group:BiasAdd
/*!
 * @brief Implements the basic operation that are widely used in artificial intelligence with the
 * following formula:
 *
 * c = alpha[0] * a + beta[0] * c
 *
 * where \b a and \b c are tensors, and \b alpha and \b beta are the scaling factors
 * used in the operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha
 *   Input. A host pointer to scaling factor of tensor a.
 * @param[in] a_desc
 *   Input. The descriptor of the input tensor a, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. A device pointer to the MLU memory that stores the input tensor a.
 * @param[in] workspace
 *   Input. A device pointer to the MLU memory that is used as an extra workspace for this
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in this
 *   operation. You can get the size of workspace with the ::cnnlGetBiasAddWorkspaceSize
 *   function.
 * @param[in] beta
 *   Input. A host pointer to scaling factor of tensor c.
 * @param[in] c_desc
 *   Input. The descriptor of the tensor c used as input and output. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in, out] c
 *   Input, Output. A device pointer to the MLU memory that stores the tensor c.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "BiasAdd Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The combinations of the data types for all tensors \b a and \b c must be half-half,
 *   float-float, int32-int32.
 * - \b alpha, \b beta: If the data type of tensors were float or half, the data type of
 *   \b alpha, \b beta should be float*. If the data type of tensors were int32, the
 *   data type of \b alpha, \b beta should be int*.
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - The input \b a tensor only has one dimension and the length of this dimension is the
 *   same with the lowest dimension of \b c. In short, A_DIM[0] = C_DIM[C_DIM_NUM - 1].
 *   A_DIM[0] means the length of the only one dimension of \b a tensor,
 *   C_DIM[C_DIM_NUM - 1] means the length of the last dimension of \b c tensor.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 * - When the data type of \b input is int32, the intermediate result of \b input cannot
 *   exceed the value range of the corresponding data type.
 *
 * @par API Dependency
 * - Before calling this function to perform this operation, you need to get the size of
 *   workspace by the ::cnnlGetBiasAddWorkspaceSize function.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the value of scaling factors
 *   \b alpha equals to 1, \b beta equals to 0 or 1.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  a :   [[1, 2, 3]]

       alpha           :   1

       Input tensor  c :   [[1, 2, 3],
                            [4, 5, 6],
                            [7, 8, 9]]

       beta            :   1

       Output tensor   :   [[2, 4, 6],
                            [5, 7, 9],
                            [8, 10, 12]]
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlBiasAdd(cnnlHandle_t handle,
                                      const void *alpha,
                                      const cnnlTensorDescriptor_t a_desc,
                                      const void *a,
                                      void *workspace,
                                      size_t workspace_size,
                                      const void *beta,
                                      const cnnlTensorDescriptor_t c_desc,
                                      void *c);

// Group:BiasAdd
/*!
 * @brief Computes the bias add gradient with respect to the bias, which is the sum
 * of every element belonging to the same feature map across all of the images of
 * the input tensor. Therefore, the number of elements produced is equal to the
 * number of features maps of the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::BiasAddBackward peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. Descriptor of input tensor \b diff_y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \b diff_y tensor.
 * @param[in] axis
 *   Input. \b axis is used to determine the channel dimension of the feature map.
 * @param[in] diff_bias_desc
 *   Input. Descriptor of output tensor \b diff_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the \b diff_bias tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BiasAddBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - Input tensor (\b diff_y) only supports 2D~5D.
 * - \b axis must be in the range of [1, diff_y_desc.dim-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/bias-add-grad
 */
cnnlStatus_t CNNL_WIN_API cnnlBiasAddBackward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t diff_y_desc,
                                              const void *diff_y,
                                              int axis,
                                              const cnnlTensorDescriptor_t diff_bias_desc,
                                              void *diff_bias);
// Group:BiasAdd
/*!
 * @brief Computes the bias add gradient with respect to the bias, which is the sum
 *        of every element belonging to the same feature map across all of the images
 *        of the input tensor. Therefore, the number of elements produced is equal to
 *        the number of features maps of the input tensor.
 *
 * Compared with ::cnnlBiasAddBackward, ::cnnlBiasAddBackward_v2 needs extra
 * workspace. You can get the size of the workspace \b workspace_size with
 * the ::cnnlGetBiasAddBackwardWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBiasAddBackward_v2 peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. Descriptor of input tensor \b diff_y.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the \b diff_y tensor.
 * @param[in] axis
 *   Input. \b axis is used to determine the channel dimension of the feature map.
 * @param[in] diff_bias_desc
 *   Input. Descriptor of output tensor \b diff_bias.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the \b diff_bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBiasAddBackward_v2 operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBiasAddBackward_v2 operation. You can get the size of the workspace with
 *   the ::cnnlGetBiasAddBackwardWorkspaceSize function.

 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BiasAddBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - Input tensor (\b diff_y) only supports 2D~5D.
 * - \b axis must be in the range of [1, diff_y_desc.dim-1].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/bias-add-grad
 */
cnnlStatus_t CNNL_WIN_API cnnlBiasAddBackward_v2(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t diff_y_desc,
                                                 const void *diff_y,
                                                 int axis,
                                                 const cnnlTensorDescriptor_t diff_bias_desc,
                                                 void *diff_bias,
                                                 void *workspace,
                                                 size_t workspace_size);
// Group:BiasAdd
/*!
 * @brief Returns in \b workspace_size_inbytes the size of the MLU memory that
 *        is used to get extra space size in BiasAddBackward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BiasAddBackward peration.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] diff_y_desc
 *   Input. The descriptor of input tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_bias_desc
 *   Input. The descriptor of output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] axis
 *   Input. \b axis is used to determine the channel dimension of the feature map.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is
 *   used in the BiasAddBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBiasAddBackwardWorkspaceSize(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t diff_y_desc,
                                    const cnnlTensorDescriptor_t diff_bias_desc,
                                    const int axis,
                                    size_t *workspace_size);

// Group:Gather
/*!
 * @brief Gathers values along an axis specified by \b dim.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the gather
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. An int value which determines the axis to gather value along.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of each element of \b output in
 *   corresponding dimension of input tensor \b input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Gather Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b input,
 *   index tensor \b index, and output tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor must be the same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *   - index tensor: int32.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, index tensor, and output tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - index tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor, index tensor and dimension must meet the following requirements:
 *   - The input tensor, index tensor and output tensor should have the same number of dimensions.
 *   - input tensor: The shape of input tensor should be same as output tensor's except dimension
 *     \b dim.
 *   - index tensor: The shape of index tensor should be same as output tensor.
 *   - dim: It should be greater than -1 and less than the dimension size of \b index.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the gather operation is as follows:
     @verbatim
     input two arrays both by 1 * 2 * 2 --> input: [[[1., 2.], [3., 4.]]]

     --> index: [[[0, 0], [1, 0]]]

     param:
       dim: 2

     output array by 1 * 2 * 2 --> output: [[[1., 1.], [4., 3.]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.gather.html
 */
cnnlStatus_t CNNL_WIN_API cnnlGather(cnnlHandle_t handle,
                                     const int dim,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t index_desc,
                                     const void *index,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Cast
/*!
 *  @brief Converts \b input from original type to target type,
 *  and returns in \b output. This operation supports data type conversion in any dimensions,
 *  The supported data type conversions are defined in ::cnnlCastDataType_t. The shape of \b input
 *  and \b output must be consistent.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the cast operation. For detailed information,
 *    see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the input tensor. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] cast_type
 *    Input. The cast_type used to specify the conversion data types from one to another.
 *    The \b cast_type are defined the ::cnnlCastDataType_t.
 *  @param[in] output_desc
 *    Input. The descriptor of the output tensor. For detailed information, see
 *    ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @par Return
 *    - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 *  @par Scale Limitation
 *    - The input tensor and output tensor have the same shape.
 *    - The total number of dimensions of input tensor should be less than CNNL_DIM_MAX
*       when tensor is discontinuous.
 *
 *  @note
 *    - This operation only supports conversion data types defined in ::cnnlCastDataType_t.
 *    You can specify the stride of all dimensions for input_desc and output_desc with
 *    ::cnnlSetTensorDescriptorEx. The transformation between the floating point and
 *    the integer type must be in the range of [-2^23, 2^23-1], when the device is
 *    MLU200 series.
 *    The input should be in range of [-2^23, 2^23-1] on MLU220 when ::cnnlCastDataType_t
 *    is CNNL_CAST_FLOAT322INT32, CNNL_CAST_FLOAT322INT64, CNNL_CAST_INT322FLOAT32 or
 *    CNNL_CAST_INT642FLOAT32.
 *    The CNNL_CAST_DOUBLE2FLOAT32 in cnnlCastDataType_t is not supported on MLU220.
 *    The CNNL_CAST_INT642HALF in cnnlCastDataType_t are not supported on MLU200 series.
 *    The tensor number will be limited to 2*10^9 if the input/output tensor contains
 *    double/int64/uint64 data type, and 2^31-1 in other cases.
 *
 *  @par Requirements
 *  - None.
 *  @par Reference
 *  - https://tensorflow.org/api_docs/cc/class/tensorflow/ops/cast
 */
cnnlStatus_t CNNL_WIN_API cnnlCastDataType(cnnlHandle_t handle,
                                           const cnnlTensorDescriptor_t input_desc,
                                           const void *input,
                                           const cnnlCastDataType_t cast_type,
                                           const cnnlTensorDescriptor_t output_desc,
                                           void *output);

// Group:PowN
/*!
 * @brief Computes the power of each element in input tensor \b x with exponent tensor
 *        \b y, and returns the result in the output tensor \b z.
 *
 * @deprecated
 *   ::cnnlPowN is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlPow instead, which supports the parameter of \b prefer to set the computing with
 *   faster algorithm or higher precision and supports data type of \b y to set float or half.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   pown operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PowN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor /b x and output tensor /b z must be the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor /b x: half, float.
 *   - exponent tensor /b y: int16.
 *   - output tensor /b z: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor \b x, exponent tensor \b y and
 *   output tensor \b z must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \b x and exponent tensor /b y should satisfy the
 *   following condition:
 *   - On MLU 200 series:
 *     -15.5 < y*log(|x|) < 15.5.
 *   - On MLU 300 and CE3226 series:
 *     - if \b x = inf or -inf, \b y != 0, where inf represents infinity.
 *
 * @note
 * - The input tensor \b x and exponent tensor \b y do not support broadcast.
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPowN(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   const void *y,
                                   const cnnlTensorDescriptor_t z_desc,
                                   void *z);

// Group:PowN
/*!
 * @brief Computes the power of each element in input tensor \b x with exponent tensor
 *        \b y, and returns the result in the output tensor \b z.
 *
 * @deprecated
 *   ::cnnlPowN_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlPow instead, which supports data type of \b y to set float or half.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   pown operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   \p CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the exponent tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the exponent tensor.
 * @param[in] z_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] z
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "PowN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor /b x and output tensor /b z must be the same.
 * - The supported data types of input, exponent and output tensors are as follows:
 *   - input tensor /b x: half, float.
 *   - exponent tensor /b y: int16.
 *   - output tensor /b z: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor \b x, exponent tensor \b y and
 *   output tensor \b z must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The data range of the input tensor \b x and exponent tensor /b y should satisfy the
 *   following condition:
 *   - On MLU 200 series:
 *     -15.5 < y*log(|x|) < 15.5.
 *   - On MLU 300 and CE3226 series:
 *     - if \b x = inf or -inf, \b y != 0, where inf represents infinity.
 *
 * @note
 * - The input tensor \b x and exponent tensor \b y do not support broadcast.
 * - You can specify the stride of all dimensions for x_desc, y_desc and z_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/pow
 */
cnnlStatus_t CNNL_WIN_API cnnlPowN_v2(cnnlHandle_t handle,
                                      cnnlComputationPreference_t prefer,
                                      const cnnlTensorDescriptor_t x_desc,
                                      const void *x,
                                      const cnnlTensorDescriptor_t y_desc,
                                      const void *y,
                                      const cnnlTensorDescriptor_t z_desc,
                                      void *z);

// Group:BitCompute
/*!
 * @brief  Returns in \b workspace_size the size of the MLU memory in bytes that is
 *         used as an extra workspace to optimize the ::cnnlBitCompute_v2 operation.
 *
 * The size of extra workspace is based on the given information of the input
 * and output tensor descriptors, \b a_desc, \b b_desc, and \b c_desc. For more
 * information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlBitCompute_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlBitCompute_v2 function
 *   to perform the bitwise operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBitComputeWorkspaceSize(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t input1_desc,
                                                         const cnnlTensorDescriptor_t input2_desc,
                                                         const cnnlTensorDescriptor_t output_desc,
                                                         size_t *workspace_size);

// Group:BitCompute
/*!
 * @brief Performs bitwise operation \b optype between two input tensors \b input1 and
 *        \b input2, and returns the results in the output tensor \b output.
 *
 * Bitwise operation is widely used in machine learning and image processing. It is
 * supported in Pytorch and Tensorflow.
 *
 * The ::cnnlBitCompute has the following limitations on input shapes.
 * For each dimension of the input1 and input2, the size of the dimension can be the same,
 * and these same dimensions should be consecutive, other dimensions of input2 should be 1.
 * This function has less limitations on input shapes. For each dimension of the two input
 * tensors, the size of the dimension should be the same or one of them should equal to 1.
 *
 * This function supports partial in-place operation, which means the first input
 * tensor \b input1 and the output tensor \b output can be the same one.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BitCompute operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] optype
 *   Input. The specific bitwise operation performed in the function. The operations are
 *   defined in the ::cnnlBitComputeOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   logic operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   The size of the extra workspace in bytes that needs to be used in the logic operation.
 *   You can get the size of the workspace with the ::cnnlGetBitComputeWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Bit Compute" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor.
 *   - input: uint8, bool, int8, uint16, int16, uint32, int32, half, float.
 *   - output: uint8, bool, int8, uint16, int16, uint32, int32, half, float.
 *   <b>Note that the data type of output tensor must be same with the input tensor.</b>
 *   <b>Note that half and float are only supported when optype is CNNL_BLEFT_SHIFT_OP or CNNL_BRIGHT_SHIFT_OP.</b>
 *   <b>Note that bool is not supported when optype is CNNL_BLEFT_SHIFT_OP or CNNL_BRIGHT_SHIFT_OP.</b>
 *
 * @par Scale Limitation
 * - The input optype CNNL_BLEFT_SHIFT_OP and CNNL_BRIGHT_SHIFT_OP are only supported on 300 plaform.

 * @par API Dependency
 * - Before calling this function to perform Bitwise operation, you need to get the
 *   size of workspace by the ::cnnlGetBitComputeWorkspaceSize function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bitwise operation is as follows:
     @verbatim
      bitcompute_op: CNNL_CYCLE_BAND_OP
      input two arrays by 2 * 3 * 3, 2 * 1 * 3
      --> input1: [[[1, 2, 3], [2, 3, 4], [3, 4, 5]],
                  [[4, 5, 6], [5, 6, 7], [6, 7, 8]]]

      --> input2: [[[-1, -2, -1]], [[-3, -4, -5]]]

      output array by 2 * 3 * 3
      --> output: [[[1, 2, 3], [2, 2, 4], [3, 4, 5]],
                  [[4, 4, 2], [5, 4, 3], [4, 4, 8]]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlBitCompute_v2(cnnlHandle_t handle,
                                          const cnnlBitComputeOp_t optype,
                                          const cnnlTensorDescriptor_t input1_desc,
                                          const void * input1,
                                          const cnnlTensorDescriptor_t input2_desc,
                                          const void * input2,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          void * workspace,
                                          size_t workspace_size);

// Group:BitCompute
/*!
 * @brief Performs cycle bitwise operation \b optype between two input tensors \b input1 and
 *        \b input2, and returns the results in the output tensor \b output.
 *
 * @deprecated
 *   ::cnnlBitCompute is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlBitCompute_v2 instead. This function has the following limitations on input
 *   shapes: for each dimension of the input1 and input2, the size of the dimension can be the
 *   same, and these same dimensions should be consecutive, other dimensions of input2 should be 1.
 *   The new function ::cnnlBitCompute_v2 has less limitations on input shapes. For each dimension
 *   of the two input tensors, the size of the dimension should be the same or one of them should
 *   equal to 1.
 *
 * Bitwise operation is widely used in machine learning and image processing. It is
 * supported in Pytorch.
 *
 * This function supports partial in-place operation, which means the first input tensor \b input1 and
 * the output tensor \b output can be the same one.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the BitCompute_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] optype
 *   Input. The specific bitwise operation performed in the function. The operations are
 *   defined in the ::cnnlBitComputeOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Bit Compute" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor.
 *   - input: uint8, bool, int8, int16, int32.
 *   - output: uint8, bool, int8, int16, int32.
 *   <b>Note that the data type of output tensor must be same with the input tensor.</b>
 *
 * @par Scale Limitation
 * - The input optype CNNL_BLEFT_SHIFT_OP and CNNL_BRIGHT_SHIFT_OP are not supported.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the bitwise operation is as follows:
     @verbatim
      bitcompute_op: CNNL_CYCLE_BAND_OP
      input two arrays by 2 * 3 * 3, 2 * 1 * 1
      --> input1: [[[1, 2, 3], [1, 2, 3], [1, 2, 3]],
                  [[2, 3, 4], [2, 3, 4], [2, 3, 4]]]

      --> input2: [[[2]], [[3]]]

      output array by 2 * 3 * 3
      --> output: [[[0, 2, 2], [0, 2, 2], [0, 2, 2]],
                  [[2, 3, 0], [2, 3, 0], [2, 3, 0]]]
     @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlBitCompute(cnnlHandle_t handle,
                                         const cnnlBitComputeOp_t bitcompute_op,
                                         const cnnlTensorDescriptor_t descinput1,
                                         const void *input1,
                                         const cnnlTensorDescriptor_t descinput2,
                                         const void *input2,
                                         const cnnlTensorDescriptor_t descoutput,
                                         void *output);

// Group:CycleOp
/*!
 * @brief Performs cycle element-wise operation based on the operation type defined in
 *        ::cnnlCycleOp_t between two input \b input1 and \b input2, and returns the results
 *        in the output tensor \b output.
 *
 * Cycle operations are wildly used in artificial intelligence as a kind of basic mathematical operations.
 * Also, they are supported in almost all common frameworks, like PyTorch and TensorFlow.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cycle operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  cycle_optype
 *   Input. The specific cycle operation performed in the function. The operations are defined in
 *   the ::cnnlCycleOp_t enum.
 * @param[in]  input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input1
 *   Input. Pointer to the MLU memory that stores the first input tensor. \b Input1 is the larger
 *   input of two inputs, the size of \b input1 should be multi times of the size of \b input2
 * @param[in]  input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input2
 *   Input. Pointer to the MLU memory that stores the second input tensor. \b Input2 is the smaller
 *   input of two inputs.
 * @param[out]  output_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *  Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input: float, half, int32, bool.
 *   - output: float, half, int32, bool.
 *   <b>Note that the data type of output tensor must be same with the input tensor expected
 *   when out type is bool.</b>
 * - Data type of input as bool only for \p CNNL_CYCLE_OR, \p CNNL_CYCLE_AND, \p CNNL_CYCLE_XOR,
 *   \p CNNL_CYCLE_EQUAL and \p CNNL_CYCLE_NEQUAL.
 * - Data type of output as bool only for \p CNNL_CYCLE_OR, \p CNNL_CYCLE_AND, \p CNNL_CYCLE_XOR,
 *   \p CNNL_CYCLE_LESS, \p CNNL_CYCLE_LESS_EQUAL, \p CNNL_CYCLE_EQUAL, \p CNNL_CYCLE_NEQUAL,
 *   \p CNNL_CYCLE_GREATER and \p CNNL_CYCLE_GREATER_EQUAL.
 *
 * @par Limitations
 * - For each dimension of the \b input1 and \b input2, the size of the dimension can be the same,
 * and these same dimensions should be consecutive, other dimensions of \b input2 should be 1.
 *
 * @note
 * - When \b input1 or \b input2 contains NaN:
 *   If \b cycle_optype is \p CNNL_CYCLE_NEQUAL, \b output is positive saturation value on MLU200 series
 *   and \b output is 0 on MLU300 series and CE3226.
 *   If \b cycle_optype is \p CNNL_CYCLE_MIN_EQUAL or \p CNNL_CYCLE_MAX_EQUAL,
 *   \b output is saturation value on MLU200 series and \b output is NaN on MLU300 series and CE3226.
 *
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the cycle operation is as follows:
     @verbatim
      optype: CNNL_CYCLE_ADD

      input two arrays by 2 * 3 * 3, 2 * 1 * 1
      --> input1: [[[1, 2, 3], [1, 2, 3], [1, 2, 3]],
                  [[2, 3, 4], [2, 3, 4], [2, 3, 4]]]

      --> input2: [[[2]], [[3]]]

      output array by 2 * 3 * 3
      --> output: [[[3, 4, 5], [3, 4, 5], [3, 4, 5]],
                  [[5, 6, 7], [5, 6, 7], [5, 6, 7]]]
     @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlCycleOp(cnnlHandle_t handle,
                                      const cnnlCycleOp_t cycle_optype,
                                      const cnnlTensorDescriptor_t input1_desc,
                                      const void *input1,
                                      const cnnlTensorDescriptor_t input2_desc,
                                      const void *input2,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:Erf
/*!
 * @brief Computes gauss error function (erf) of input tensor \b input, and returns the results in the
 *        output tensor \b output. To set the computing with faster algorithm or higher precision,
 *        call ::cnnlErf_v2.
 *
 * @deprecated
 *   ::cnnlErf is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlErf_v2 instead, which supports parameters of \b prefer to set the computing
 *   with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the erf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Erf Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [-3.4e+38,3.4e+38].
 *   - half:  [-65504,-1e-3] || [1e-3,65504].

 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/erf
 */
cnnlStatus_t CNNL_WIN_API cnnlErf(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Erf
/*!
 * @brief Computes gauss error function (erf) of input tensor \b input, and returns the results in the
 *        output tensor \b output.
 *
 * Compared with ::cnnlErf, this function allows you to choose whether to perform erf operation
 * with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the erf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. The default value of this parameter is
 *   \p CNNL_COMPUTATION_HIGH_PRECISION.
 *   When the type of input data is float, it is recommended to set /b prefer to
 *   \p CNNL_COMPUTATION_FAST; when the type of input data is half, it is recommended to set
 *   /b prefer to \p CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor have the same shape, and the input tensor must meet
 *   the following input data range:
 *   - float: [-3.4e+38, 3.4e+38].
 *   - half:  [-65504,-1e-3] || [1e-3,65504].

 * @note
 * - You can specify the stride of all dimensions for input_desc and output_desc with
 *   ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/erf
 */
cnnlStatus_t CNNL_WIN_API cnnlErf_v2(cnnlHandle_t handle,
                                     const cnnlComputationPreference_t prefer,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group: Gru
/*!
 * @brief Executes the gru network described by \b gru_desc with inputs \b input, \b state, filter
 *        \b filter, biases \b bias, output \b output, and \b state_output. \b workspace is
 *        required for intermediate results. You can get the size of the workspace
 *        \b workspace_size by the ::cnnlGetGruWorkspaceSize function.
 *
 * @deprecated
 *   ::cnnlGru is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlGru_v2 instead, which supports input sequence data and more parameters in
 *   \b gru_desc to set direction, layer number, filter layout and input layout.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input. The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state
 *   Input. Pointer to the MLU memory that stores the state tensor, which stores the hidden state.
 * @param[in] filter_desc
 *   Input. An array that stores the descriptors of filter tensor, and the number of descriptors is
 *   determined by the information described in \b gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the host memory that stores an array of pointers to the MLU memory of filter
 *   tensors. The element order of this filter array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] bias_desc
 *   Input. An array that stores the descriptors of bias tensor, and the number of descriptors is
 *   determined by the information described in \b gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the host memory that stores an array of pointers to the MLU memory of bias
 *   tensors. The element order of this bias array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that stores the workspace tensor, which is used as an extra
 *   workspace for the gru operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the gru operation. You can get the size of the workspace with
 *   the ::cnnlGetGruWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] state_output_desc
 *   Input. The descriptor of the state_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] state_output
 *   Output. Pointer to the MLU memory that stores the state_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GRU Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor, state tensor,
 *   filter tensor, bias tensor, output tensor, and state_output tensor:
 *   - input tensor: half, float.
 *   - state tensor: half, float.
 *   - filter tensor: int8, int16.
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 *   - state_output tensor: half, float.
 * - The data types of \b input, \b state, \b bias, \b output, and \b state_output must be the
 *   same.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor, and state tensor must meet the following requirements:
 *   - input tensor: \p batch > 0, \p time > 0, \p input_size > 0.
 *   - state tensor: \p batch > 0, \p hidden_size > 0.
 * - The batch number of input tensor must be the same as the batch number of state tensor.
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions
 *   to create and set the gru descriptors \b gru_desc before calling this function.
 * - Before performing the operation, you need to get the size of workspace by the
 *   ::cnnlGetGruWorkspaceSize function and pass the allocated extra workspace to the
 *   ::cnnlGru function.
 *
 * @note
 * - The shape of the input tensor, state tensor, filter tensor, bias tensor, output tensor and
 *   state_output tensor are as follows:
 *   - input tensor: When \p dim is equal to 3, shape is [\p batch, \p time, \p input_size].
 *   - state tensor: When \p dim is equal to 2, shape is
 *     [\p batch * \p num_layer in \b gru_desc, \p hidden_size].
 *     - If \p is_bidirectional in \b gru_desc is true, \p hidden_size needs to be doubled.
 *   - filter tensor: When \p dim is equal to 2, for the filter of input, the shape is
 *     [\p hidden_size, \p input_size]; for the filter of state, the shape is
 *     [\p hidden_size, \p hidden_size].
 *   - bias tensor: When \p dim is equal to 2, the shape is [1, \p hidden_size].
 *   - output tensor: When \p dim is equal to 3, the shape is [\p batch, \p time, \p hidden_size].
 *     - If \p is_bidirectional in \b gru_desc is true, \p hidden_size needs to be doubled.
 *   - state_output tensor: When \p dim is equal to 2, the shape is [\p batch, \p hidden_size].
 *     - If \p is_bidirectional in \b gru_desc is true, \p hidden_size needs to be doubled.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#gru
 */
cnnlStatus_t CNNL_WIN_API cnnlGru(cnnlHandle_t handle,
                                  const cnnlGruDescriptor_t gru_desc,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t state_desc,
                                  const void *state,
                                  const cnnlTensorDescriptor_t filter_desc[],
                                  const void **filter,
                                  const cnnlTensorDescriptor_t bias_desc[],
                                  const void **bias,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output,
                                  const cnnlTensorDescriptor_t state_output_desc,
                                  void *state_output);

// Group: Gru
/*!
 * @brief Executes the gru network described by \b gru_desc with inputs \b input, \b state,
 *        sequence lengths \b sequence_lens, filter \b filter, biases \b bias, output \b output
 *        and \b state_output. \b workspace is required for intermediate results. Users can get the
 *        size of the workspace \b workspace_size by the ::cnnlGetGruWorkspaceSize_v2 function.
 *
 * Compared with ::cnnlGru, this function allows users to set direction, filter layout,
 * input layout, and layer number.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input. The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \b seqLengthArray set in \b input_desc or \b output_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] extra_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitGruExtraInput.
 * @param[in] input_desc
 *   Input. The descriptor of the input sequence tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input sequence tensor.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state
 *   Input. Pointer to the MLU memory that stores the state tensor, which stores the hidden state.
 * @param[in] sequence_lens_desc
 *   Input. The descriptor of the sequence_lens tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] sequence_lens
 *   Input. Pointer to the MLU memory that stores the sequence_lens tensor,
 *   which stores the sequence lengths of input tensor.
 * @param[in] filter_desc
 *   Input. An array that stores the descriptors of filter tensor, and the number of descriptors is
 *   determined by the information described in \b gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the host memory that stores an array of pointers to the MLU memory of filter
 *   tensors. The element order of this filter array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] bias_desc
 *   Input. An array that stores the descriptors of bias tensor, and the number of descriptors is
 *   determined by the information described in \b gru_desc. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the host memory that stores an array of pointers to the MLU memory of bias
 *   tensors. The element order of this bias array must be consistent with the element order of the
 *   ::cnnlTensorDescriptor_t array.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that stores the workspace tensor, which is used as an extra
 *   workspace for the gru operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the gru operation. You can get the size of the workspace with
 *   the ::cnnlGetGruWorkspaceSize_v2 function.
 * @param[in] output_desc
 *   Input. The descriptor of the output sequence tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output sequence tensor.
 * @param[in] state_output_desc
 *   Input. The descriptor of the state_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] state_output
 *   Output. Pointer to the MLU memory that stores the state_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GRU Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for the input tensor, state tensor, filter
 *   tensor, bias tensor, output tensor, and state_output tensor:
 *   - input tensor: half, float.
 *   - state tensor: half, float.
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *   - output tensor: half, float.
 *   - state_output tensor: half, float.
 * - The data types of \b input, \b state, \b bias, \b output, and \b state_output must be the
 *   same.
 * - The data types of \b input, \b state, \b filter, \b bias, \b output, and \b state_output must
 *   be the same when data type of \b filter is half or float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, and output tensor are as follows:
 *   - input sequence data: \b ::CNNL_SEQDATA_NTC, \b ::CNNL_SEQDATA_TNC, \b ::CNNL_SEQDATA_TNC_PACKED.
 *   - output sequence data: \b ::CNNL_SEQDATA_NTC, \b ::CNNL_SEQDATA_TNC, \b ::CNNL_SEQDATA_TNC_PACKED.
 *
 * @par Scale Limitation
 * - The input tensor, and state tensor must meet the following requirements:
 *   - input tensor: \p batch > 0, \p time > 0, \p input_size > 0.
 *   - state tensor: \p batch > 0, \p hidden_size > 0.
 *   - If input tensor data type is half, then \p hidden_size < 128.
 *   - If \p hidden_size > 1024, then \p time <= 3.
 *   - On MLU200 series, if \p hidden_size > 256, then \p time <= 10.
 *   - On MLU200 series, if \p hidden_size > 128, then \p time <= 16.
 * - The input and sequence_lens tensor, \b num_layer, \b state_layout, \b algo,
 *   \b and multi_layer_mode must meet the following requirements:
 *   - If \b algo is \b CNNL_GRU_ALGO_V1, then
 *	   layout of input and output tensor can not be \b ::CNNL_SEQDATA_TNC_PACKED,
 *	   sequence_lens tensor must be NULL,
 *	   \b direction can not be \b ::CNNL_GRU_BACKWARD,
 *	   and \b multi_layer_mode can not be \b ::CNNL_GRU_MODE_V2.
 *	 - If \b layout of input and output tensor is \b ::CNNL_SEQDATA_TNC_PACKED, then
 *	   sequence_lens tensor must be NULL,
 *	   \b multi_layer_mode can not be \b ::CNNL_GRU_MODE_V1.
 *	   and \b state_layout can not be CNNL_GRU_LNDC.
 *	 - If sequence_lens tensor is not NULL, then
 *	   \b num_layer must be 1.
 * - The batch number of input tensor must be the same as the batch number of state tensor.
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor_v2 functions
 *   to create and set the gru descriptors \b gru_desc before calling this function.
 * - Before performing the operation, you need to get the size of workspace by the
 *   ::cnnlGetGruWorkspaceSize_v2 function and pass the allocated extra workspace to the
 *   ::cnnlGru_v2 function.
 *
 * @note
 * - The shape of the input tensor, state tensor, filter tensor, bias tensor, output tensor and
 *   state_output tensor are as follows:
 *   - input sequence data: When \p dim is equal to 3, the shape is as follows:
 *     - If \p version in \b gru_desc is 1, the shape is [\p batch, \p time, \p input_size].
 *     - If \p version in \b gru_desc is 2 and layout is ::CNNL_SEQDATA_NTC, the shape
 *       is [\p batch, \p time, \p input_size].
 *     - If \p version in \b gru_desc is 2 and layout is ::CNNL_SEQDATA_TNC, the shape
 *       is [\p time, \p batch, \p input_size].
 *   - state tensor: When \p dim is equal to 4, the shape is
 *     [\p num_layer, \p batch, 1, \p hidden_size].
 *     - If \p version in \b gru_desc is 1 and \p is_bidirectional in \b gru_desc is true, the
 *       third dim value needs to be 2.
 *     - If \p version in \b gru_desc is 2 and \p direction in \b gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, the third dim value needs to be 2.
 *   - filter tensor: When \p dim is equal to 2, for the filter of input, the shape is
 *     [\p hidden_size, \p input_size]; for the filter of state, the shape is
 *     [\p hidden_size, \p hidden_size].
 *   - bias tensor: When \p dim is equal to 2, the shape is [1, \p hidden_size].
 *   - output sequence data: When \p dim is equal to 3, the shape is as follows:
 *     - If \p version in \b gru_desc is 1, the shape is [\p batch, \p time, \p hidden_size].
 *     - If \p version in \b gru_desc is 2 and layout is ::CNNL_SEQDATA_NTC, the shape
 *       is [\p batch, \p time, \p hidden_size].
 *     - If \p version in \b gru_desc is 2 and layout is ::CNNL_SEQDATA_TNC, the shape
 *       is [\p time, \p batch, \p hidden_size].
 *     - If \p version in \b gru_desc is 1 and \p is_bidirectional in \b gru_desc is true,
 *       \p hidden_size needs to be doubled.
 *     - If \p version in \b gru_desc is 2 and \p direction in \b gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, \p hidden_size needs to be doubled.
 *   - state_output tensor: When \p dim is equal to 4, the shape is
 *     [\p num_layer, \p batch, 1, \p hidden_size].
 *     - If \p version in \b gru_desc is 1 and \p is_bidirectional in \b gru_desc is true, the
 *       third dim value needs to be doubled.
 *     - If \p version in \b gru_desc is 2 and \p direction in \b gru_desc is
 *       ::CNNL_GRU_BIDIRECTIONAL, the third dim value needs to be doubled.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.html#gru
 */
cnnlStatus_t CNNL_WIN_API cnnlGru_v2(cnnlHandle_t handle,
                                     const cnnlGruDescriptor_t gru_desc,
                                     const int dev_seq_lengths[],
                                     const void *extra_input_ptr,
                                     const cnnlSeqDataDescriptor_t input_desc,
                                     const void * input,
                                     const cnnlTensorDescriptor_t state_desc,
                                     const void * state,
                                     const cnnlTensorDescriptor_t sequence_lens_desc,
                                     const void * sequence_lens,
                                     const cnnlTensorDescriptor_t filter_desc[],
                                     const void ** filter,
                                     const cnnlTensorDescriptor_t bias_desc[],
                                     const void ** bias,
                                     void * workspace,
                                     size_t workspace_size,
                                     const cnnlSeqDataDescriptor_t output_desc,
                                     void * output,
                                     const cnnlTensorDescriptor_t state_output_desc,
                                     void * state_output);

// Group: Gru
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlGru function.
 *
 * The size of extra workspace is based on the given information of the gru operation,
 * including the input tensor descriptor \b input_desc, state tensor descriptor \b state_desc,
 * and gru descriptor \b gru_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGruWorkspaceSize_v2, this function accepts input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b input_desc and \b state_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions
 *   to create and set the gru descriptors \b gru_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlGru function
 *   to perform the gru operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGruWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const cnnlTensorDescriptor_t state_desc,
                                                  const cnnlGruDescriptor_t gru_desc,
                                                  size_t *size);

// Group: Gru
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlGru_v2 funtion.
 *
 * The size of extra workspace is based on the given information of the gru operation,
 * including the input tensor descriptor \b input_desc, state tensor descriptor \b state_desc,
 * and gru descriptor \b gru_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * Compared with ::cnnlGetGruWorkspaceSize, this function accepts input sequence data.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] state_desc
 *   Input. The descriptor of the state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the sequence data descriptors \b input_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions
 *   to create and set the tensor descriptors \b state_desc
 *   before calling this function.
 * - You need to call the ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor_v2 functions
 *   to create and set the gru descriptors \b gru_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlGru_v2 function
 *   to perform the gru operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGruWorkspaceSize_v2(cnnlHandle_t handle,
                                                     const cnnlSeqDataDescriptor_t input_desc,
                                                     const cnnlTensorDescriptor_t state_desc,
                                                     const cnnlGruDescriptor_t gru_desc,
                                                     size_t *size);

// Group: Gru
/*!
 * @brief Creates a descriptor pointed by \b desc for a gru operation,
 *        and allocates memory for holding the information about the gru operation.
 *        The information is defined in ::cnnlGruDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Input. A host pointer to the gru descriptor that holds information about the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGruDescriptor function to initialize
 *   and set the information to the gru descriptor.
 * - You need to call the ::cnnlDestroyGruDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateGruDescriptor(cnnlGruDescriptor_t *desc);

// Group: Gru
/*!
 * @brief Initializes the gru descriptor \b gru_desc that is previously created
 *        by the ::cnnlCreateGruDescriptor function, and sets the information
 *        about the gru operation to the gru descriptor \b gru_desc.
 *        The information includes the algorithm \b algo defined in ::cnnlGruAlgo_t,
 *        \b is_bidirectional to determine whether the gru operation is bidirectional,
 *        and \b num_layer the number of layer.
 *
 * Compared with ::cnnlSetGruDescriptor_v2, this function only supports
 * to set \b algo, \b is_bidirectional, and \b num_layer parameters.
 *
 * @param[in] desc
 *  Input. The descriptor of the gru operation. For detailed information,
 *  see ::cnnlGruDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute gru, defined in ::cnnlGruAlgo_t.
 * @param[in] is_bidirectional
 *   Input. Determines whether the gru is bidirectional or not.
 * @param[in] num_layer
 *   Input. The number of layers of the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The \b num_layer must be larger than 0.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGruDescriptor(cnnlGruDescriptor_t gru_desc,
                                               cnnlGruAlgo_t algo,
                                               bool is_bidirectional,
                                               int num_layer);

// Group: Gru
/*!
 * @brief Initializes the gru descriptor \b gru_desc that is previously created
 *        by the ::cnnlCreateGruDescriptor_v2 function, and sets the information
 *        about the gru operation to the gru descriptor \b gru_desc.
 *        The information includes the algorithm \b algo defined in ::cnnlGruAlgo_t,
 *        the recurrence pattern \b direction, and \b num_layer the number of layer.
 *
 * Compared with ::cnnlSetGruDescriptor, this function use \b direction
 * instead of \b is_bidirectional, support to set more parameters,
 * include \b direction, \b filter_order, \b activation_zr, \b activation_n,
 * \b linear_before_reset, \b alpha_zr, \b alpha_n, \b beta_zr, \b beta_n, and \b clip .
 *
 * @param[in] desc
 *  Input. The descriptor of the gru operation. For detailed information,
 *  see ::cnnlGruDescriptor_t.
 * @param[in] algo
 *   Input. The algorithm used to compute gru, defined in ::cnnlGruAlgo_t.
 * @param[in] multi_layer_algo
 *   Input. The mode used to compute multi-layer bidirectional gru, defined in ::cnnlGruMode_t.
 * @param[in] direction
 *   Input. Indicate the recurrence pattern.
 * @param[in] filter_order
 *   Input. Indicate the filter order of the gates.
 * @param[in] activation_zr
 *   Input. An array that is used to specify activation function for update and reset gates.
 *   The function must be one of supported activation functions: Sigmoid, Tanh and Relu.
 *   If \b direction is set to \b ::CNNL_GRU_FORWARD or \b ::CNNL_GRU_BACKWARD, one
 *   activation function is needed. If \b direction is set to \b ::CNNL_GRU_BIDIRECTIONAL,
 *   two activation functions are needed.
 *   If Tanh or Relu is used, the precision will be low when sequence time is too large.
 *   When it is null, it means that default activation Sigmoid is used.
 * @param[in] activation_n
 *   Input. An array that is used to specify activation function for hidden gate.
 *   The function must be one of supported activation functions: Sigmoid, Tanh and Relu.
 *   If \b direction is set to \b ::CNNL_GRU_FORWARD or \b ::CNNL_GRU_BACKWARD, one
 *   activation function is needed. If \b direction is set to \b ::CNNL_GRU_BIDIRECTIONAL,
 *   two activation functions are needed.
 *   If Relu is used, the precision will be low when sequence time is too large.
 *   When it is null, it means that default activation Tanh is used.
 * @param[in] state_layout
 *   Input. The layout of \b state and \b state_output in cnnlGru_v2.
 * @param[in] math_prec
 *   Input. This parameter is not supported currently.
 * @param[in] linear_before_reset
 *   Input. A boolean value indicating whether to apply linear transformation before multiplying
 *   reset gate when computing output of hidden gate.
 * @param[in] alpha_zr
 *   Input. Not implemented.
 * @param[in] alpha_n
 *   Input. Not implemented.
 * @param[in] beta_zr
 *   Input. Not implemented.
 * @param[in] beta_n
 *   Input. Not implemented.
 * @param[in] clip
 *   Input. An array with one value that is used to define a value range to clip the elements of an
 *   input tensor within the defined range[-clip, clip] before performing activation.
 *   When it is null, it means that \b clip is not enabled.
 * @param[in] num_layer
 *   Input. The number of layers of the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - If \b algo is ::CNNL_GRU_AOGO_V1, \b direction must be ::CNNL_GRU_FORWARD,
 *   \b filter_order must be ::CNNL_GRU_RZN, \b num_layer must be 1,
 *   and other parameters will be ignored except these three parameters.
 * - If \b direction is ::CNNL_GRU_BACKWARD, \b num_layer must be 1.
 * - If \b activation_zr is not Sigmoid or \b activation_n is not Tanh, filter data type
 *   can not be half.
 * - If \b linear_before_reset is false, the size of input channel or output channel
 *   should not be too large, e.g. For example, the size of input channel or output channel should
 *   be less than 209 when filter data type is float on MLU370x4.
 * - If \b linear_before_reset is false, only float-point data type is supported in filter tensor.
 * - The \b clip must be larger than 0.
 * - The \b num_layer must be larger than 0.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetGruDescriptor_v2(cnnlGruDescriptor_t gru_desc,
                                                  cnnlGruAlgo_t algo,
                                                  cnnlGruMode_t multi_layer_mode,
                                                  cnnlGruDir_t direction,
                                                  cnnlGruWeightOrder_t filter_order,
                                                  cnnlActivationMode_t activation_zr[],
                                                  cnnlActivationMode_t activation_n[],
                                                  cnnlGruLayout_t state_layout,
                                                  cnnlDataType_t math_prec,
                                                  bool linear_before_reset,
                                                  float alpha_zr[],
                                                  float alpha_n[],
                                                  float beta_zr[],
                                                  float beta_n[],
                                                  float clip[],
                                                  int num_layer);

// Group: Gru
/*!
 * @brief Destroys a gru descriptor \b desc that is previously created with the
 *        ::cnnlCreateGruDescriptor function.
 *
 * The gru descriptor is defined in ::cnnlGruDescriptor_t
 * and holds the information about the gru operation.
 *
 *
 * @param[in] desc
 *   Input. The gru descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlGru function.
 * - This function should be called to destroy the gru descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyGruDescriptor(cnnlGruDescriptor_t desc);

// Group: Gru
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the gru operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_input_size.
 *
 * The size of extra input data is based on the given information of the gru
 * operation, including Gru descriptor \b gru_desc.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[out] extra_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_size can not be nullptr.
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions to create and
 *   set up the gru operation descriptors \b gru_desc.
 * - After calling this function, you need to call ::cnnlInitGruExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlGru_v2 function
 *   to perform the gru operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetGruExtraInputSize(cnnlHandle_t handle,
                                                   cnnlGruDescriptor_t gru_desc,
                                                   size_t *extra_input_size);
// Group: Gru
/*!
 * @brief Initializes the extra input data space \b extra_input_host_ptr on host.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the gru operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] gru_desc
 *   Input.The descriptor of the gru operation. For detailed information,
 *   see ::cnnlGruDescriptor_t.
 * @param[in] filter_desc[]
 *   Input. The descriptor of the filter tensor array. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the gru operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_host_ptr can not be nullptr.
 *
 * @par API Dependency
 * - Before calling this function,
 *   The ::cnnlCreateGruDescriptor and ::cnnlSetGruDescriptor functions are used to create and
 *   set up a complete gru operation descriptors \b gru_desc.
 *   The ::cnnlCreateTensorDescriptor, ::cnnlInitTensorDescriptor,
 *   and ::cnnlInitTensorDescriptorPositionAndScale functions are used to create and set
 *   the tensor descriptor array \b filter_desc[].
 * - The allocated extra input should be passed to the ::cnnlGru_v2 function
 *   to perform the gru operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitGruExtraInput(cnnlHandle_t handle,
                                                cnnlGruDescriptor_t gru_desc,
                                                const cnnlTensorDescriptor_t filter_desc[],
                                                void *extra_input_host_ptr);

// Group:LSTM
/*!
 * @brief Computes the forward process of LSTM network in the inference scenario. This function uses
 *        the input data \b input, \b state, \b control, \b filter, \b bias, according to the specific
 *        network structure, and writes the calculation result into the output memory \b output,
 *        \b state_output, \b control_output.
 *
 * @deprecated
 *   ::cnnlLSTM is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlRNNForwardInference instead.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *          For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of input tensor.
 * @param[in]  input
 *   Input. Pointer to MLU memory that stores input data.
 * @param[in]  state_desc
 *   Input. The descriptor of state tensor.
 * @param[in]  state
 *   Input. Pointer to MLU memory that stores state data.
 * @param[in]  control_desc
 *   Input. The descriptor of control tensor.
 * @param[in]  control
 *   Input. Pointer to MLU memory that stores control data.
 * @param[in]  filter_desc
 *   Input. The descriptor array of filter tensor.
 * @param[in]  filter
 *   Input. Pointer to MLU memory that stores filter data.
 * @param[in]  bias_desc
 *   Input. The descriptor array of bias tensor.
 * @param[in]  bias
 *   Input. Pointer to MLU memory that stores bias data.
 * @param[in]  forget
 *   Input. The value of forget.
 * @param[in]  workspace
 *   Input. Pointer to MLU workspace memory.
 * @param[in]  workspace_size
 *   Input. The size of the workspace in bytes.
 * @param[in]  output_desc
 *   Input. The descriptor of output tensor.
 * @param[out]  output
 *   Input. Pointer to MLU memory that stores output data.
 * @param[in]  state_output_desc
 *   Input. The descriptor of output state tensor.
 * @param[out]  state_output
 *   Input. Pointer to MLU memory that stores output state data.
 * @param[in]  control_output_desc
 *   Input. The descriptor of output control tensor.
 * @param[out]  control_output
 *   Input. Pointer to the MLU memory that stores output control data.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input, filter
 *   and output tensors.
 *   - input tensor: float, half
 *   - filter tensor: int8, int16
 *   - output tensor: float, half
 *
 * @par Scale Limitation
 * - The sum of \b hidden size and \b input size should be smaller than 2048 in int16 mode and
 *   should be smaller than 4096 in int8 mode.
 * - The sum of \b hidden size and \b state size should be smaller than 2048 in int16 mode and
 *   should be smaller than 4096 in int8 mode.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLSTM(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void *input,
                                   const cnnlTensorDescriptor_t state_desc,
                                   const void *state,
                                   const cnnlTensorDescriptor_t control_desc,
                                   const void *control,
                                   const cnnlTensorDescriptor_t filter_desc[],
                                   void **filter,
                                   const cnnlTensorDescriptor_t bias_desc[],
                                   void **bias,
                                   const float forget,
                                   void *workspace,
                                   size_t workspace_size,
                                   cnnlTensorDescriptor_t output_desc,
                                   void *output,
                                   cnnlTensorDescriptor_t state_output_desc,
                                   void *state_output,
                                   cnnlTensorDescriptor_t control_output_desc,
                                   void *control_output);

// Group:Attention
/*!
 * @brief A function.
 *
 * @deprecated
 *   ::cnnlAttention is deprecated and will be removed in the future release.
 *
 *  This function is used to create attention operation.
 *  The input, output are tensors.
 *
 *  **Example**
 *
 *    Dimension of input1: [n, 1, c]
 *
 *    Dimension of input2: [n, t, c]
 *
 *    Dimension of input3: [2, n, t]
 *
 *    Dimension of input3: [n, t, units]
 *
 *    Dimension of output: [n, 1, c]
 *
 *    Dimension of output_score: [2, n, t]
 *
 *  **DataType**
 *
 *    - input: half, float.
 *
 *    - state: half, float.
 *
 *    - filter: int8, int16
 *
 *    - bias: half, float.
 *
 *    - output: half, float.
 *
 *  **Attention**
 *
 *   - Datatype of output should be the same with input.
 *   - Layout of filter is NHWC
 *
 *  **Scale Limitation**
 *
 *   batch > 0, time > 0, channel > 0, units > 0
 *
 *  @param[in]  handle
 *    Input. A handle struct with information that all you need in this operation.
 *  @param[in] attention_desc
 *    Input. Description of attention operation, containing algorithm.
 *  @param[in]  input_desc
 *    Input. The descriptor array of input tensor, containing dimensions and data type of all input data.
 *  @param[in]  input
 *    Input. The pointer to data address of input.
 *  @param[in]  filter_desc
 *    Input. The descriptor array of filter tensor, containing dimensions and data type of all
 * filter.
 *  @param[in]  filter
 *    Input. The pointer to data address of filter.
 *  @param[in]  bias_desc
 *    Input. The descriptor array of filter tensor, containing dimensions and data type of all bias.
 *  @param[in]  bias
 *    Input. The pointer to data address of bias.
 *  @param[in]  workspace
 *    Input. The pointer to data address of workspace.
 *  @param[in]  workspace_size
 *    Input. The value of workspace_size.
 *  @param[in]  output_desc
 *    Input. The descriptor of output tensor, containing dimensions and data type of output data.
 *  @param[in]  output
 *    Input. The pointer to data address of output.
 *  @param[in]  output_score_desc
 *    Input. The descriptor of state tensor, containing dimensions and data type of state output
 * data.
 *  @param[in]  output_score
 *    Input. The pointer to data address of output_score.
 *  @retval  CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval  CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - Handle is empty.
 *    - Input is empty.
 *    - filter is empty.
 *    - bias is empty.
 *    - Output is empty.
 *    - Dims are invalid.
 */
cnnlStatus_t CNNL_WIN_API cnnlAttention(cnnlHandle_t handle,
                                        const cnnlAttentionDescriptor_t attention_desc,
                                        const cnnlTensorDescriptor_t input_desc[],
                                        const void **input,
                                        const cnnlTensorDescriptor_t filter_desc[],
                                        void **filter,
                                        const cnnlTensorDescriptor_t bias_desc[],
                                        void **bias,
                                        void *workspace,
                                        size_t workspace_size,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output,
                                        const cnnlTensorDescriptor_t output_score_desc,
                                        void *output_score);

// Group:Attention
/*!
 *  @brief A function.
 *  This function is used to get extra space size needed in cnnlAttention operation, according to the
 * input param.
 *
 *  **Scale Limitation**
 *
 *  @param[in]  handle
 *    Input.A handle struct holding all the information used in this operation.
 *  @param[in]  input_desc
 *    Input. The descriptor array of input tensor, containing dimensions and data type of all input data.
 *  @param[in] attention_desc
 *    Input. Description of attention operation, containing algorithm.
 *  @param[out]  size
 *    Output. Extra space size needed in the cnnlAttention operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - handle is empty.
 *    - size is empty.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAttentionWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t *input_desc,
                                                   const cnnlAttentionDescriptor_t attention_desc,
                                                   size_t *size);

// Group:Attention
/*!
 *  @brief A function.
 *
 *  This function is used to create a descriptor of attention and allocate memory for it.
 *
 *  @param[in] desc
 *    Input. Description of attention operation, containing algorithm.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_ALLOC_FAILED
 *    The function malloc memory space failed.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateAttentionDescriptor(cnnlAttentionDescriptor_t *desc);

// Group:Attention
/*!
 *  @brief A function.
 *
 *  This function is used to assign attention descriptor with params.
 *
 *  **API Dependency**
 *
 *  Some functions need to be called before this function:
 *
 *    1.Create the descriptor of attention operation by cnnlCreateAttentionDescriptor() and get
 *      the handle, then set its params by cnnlSetAttentionDescriptor().
 *
 *  @param[in] desc
 *    Input. Description of attention operation, containing algorithm.
 *  @param[in] algo
 *    Input. The algorithm to use to compute attention.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_NOT_INITIALIZED
 *    The desc is not allocated.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetAttentionDescriptor(cnnlAttentionDescriptor_t attention_desc,
                                                     cnnlAttentionAlgo_t algo);

// Group:Attention
/*!
 *  @brief A function.
 *
 *  This function is used to destroy attention descriptor.
 *
 *  This function need to be called after cnnlAttention().
 *
 *  **API Dependency**
 *
 *  Some functions need to be called before this function:
 *
 *    1.Create the descriptor of attention operation by cnnlCreateAttentionDescriptor() and get
 *      the handle, then set its params by cnnlSetAttentionDescriptor().
 *
 *  @param[in] desc
 *    Input. Description of attention operation, containing algorithm.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_EXECUTION_FAILED
 *    Destroy attention descriptor failed.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyAttentionDescriptor(cnnlAttentionDescriptor_t desc);

/*****************************************************************************
 * Cambricon CNNL OP: MultiHeadAttn
 *****************************************************************************/
/*!
 * @brief Enumeration variables describing the attention mode of multi-head attention in CNNL.
 * NOBIAS and BIAS modes are used to distinguish whether extra biases are needed after the linear
 * projection of queries, keys, values and outputs. When extra biases are configured, biases are
 * broadcasted along the column dimension and added to the projection results.
 */
typedef enum {
  ALL_TO_ONE_BIAS = 0,
  /*!< All beams of queries map to a single beam of keys-values with extra projection biases.
   * This is used when the beam size of keys and values are equal to one and the beam size of
   * queries is greater than one.*/
  ALL_TO_ONE_NOBIAS = 1,
  /*!< All beams of queries map to a single beam of keys-values without projection extra biases.
   * This is used when the beam size of keys and values are equal to one and the beam size of
   * queries is greater than one.*/
  ONE_TO_ONE_BIAS = 2,
  /*!< All beams of queries map to the corresponding beams of keys and values with extra projection
   * biases. The beam size of keys and values is equal to the beam size of queries.*/
  ONE_TO_ONE_NOBIAS = 3,
  /*!< All beams of queries map to the corresponding beams of keys and values without extra
   * projection biases. The beam size of keys and values is equal to the beam size of queries.*/
} cnnlAttnMode;

// Group:MultiHeadAttn
 /*!
 * @brief Creates a descriptor pointed by \b desc for a multi-head attention forward or backward
 * operation, and allocates memory for holding the information about the multi-head attention
 * operation. The information is defined in ::cnnlMultiHeadAttnDescriptor_t.
 *
 * @param[in] desc
 *    Input. A host pointer to the multi-head attention descriptor that containing information about
 *  the multi-head attention operation. For detailed information, see ::cnnlMultiHeadAttnDescriptor_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetMultiHeadAttnDescriptor function to
 * initialize and set the information to the multihead_attn descriptor.
 * - You need to call the ::cnnlDestroyMultiHeadAttnDescriptor function to destroy the descriptor.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t *desc);

// Group:MultiHeadAttn
/*!
 * @brief Initializes the multi-head attention descriptor \b desc that is previously created with
 * ::cnnlCreateMultiHeadAttnDescriptor function, and sets the information about the multi-head
 * attention forward and backward operation to the multi-head attention descriptor \b desc.
 * The information includes \b attn_mode, \b nheads, \b sm_scaler, \b dtype,
 * \b compute_type, \b attn_dropout, \b post_dropout,
 * \b q_size, \b k_size, \b v_size, \b q_proj_size, \b k_proj_size,
 * \b v_proj_size, \b o_proj_size,
 * \b qo_max_seq_length, \b kv_max_seq_length, \b max_batch_size and \b max_beam_size.
 *
 * @param[in] desc
 *   Input. The descriptor of multi-head attention operation, For detailed information,
 *   see ::cnnlMultiHeadAttnDescriptor_t.
 * @param[in] attn_mode
 *   Input. Attention mode including the query map and extra biases. See ::cnnlAttnMode.
 * @param[in] nheads
 *   Input. The number of attention heads.
 * @param[in] sm_scaler
 *   Input. The coefficient for softmax smoothing.
 * @param[in] dtype
 *   Input. The data type of inputs, outputs and filter. See ::cnnlDataType_t.
 * @param[in] compute_type
 *   Input. The data type for computing all matrix multiplications in the
 *   multi-head attention operation.
 * @param[in] attn_dropout
 *   Input. The dropout rate of the dropout layer after softmax output.
 *   Dropout is used only in the training mode,
 *   and it is should be zero in the inferencing mode.
 * @param[in] post_dropout
 *   Input. The dropout rate of the dropout layer after multi-head attention output
 *   and before residual connection. Dropout is used only in the training mode,
 *   and it is should be zero in the inferencing mode.
 * @param[in] q_size, k_size, v_size
 *   Input. The embedding length of queries, keys, and values.
 * @param[in] q_proj_size, k_proj_size, v_proj_size, o_proj_size
 *   Input. The embedding length of queries, keys, values and outputs after linear projection.
 * @param[in] qo_max_seq_length
 *   Input. The largest sequence length of queries and outputs.
 * @param[in] kv_max_seq_length
 *   Input. The largest sequence length of keys and values.
 * @param[in] max_batch_size
 *   Input. The maximum batch size of queries, keys, values and outputs.
 * @param[in] max_beam_size
 *   Input. The maximum beam size of queries and outputs.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateMultiHeadAttnDescriptor function to create the descriptor
 *   before calling this function.
 * - You need to call the ::cnnlDestroyMultiHeadAttnDescriptor function to destroy the descriptor
 *   after calling this function.
 *
 * @note
 * - The coefficient \b sm_scaler in the range of (0.0, 1.0) for smoothing the softmax, and
 * \b sm_scaler > 1.0 for sharpening the softmax, \b sm_scaler = 1.0 for an ordinary softmax.
 *
 * @par Requirements
 * - \b sm_scaler > 0
 * - \b q_size >= 1, k_size >= 1, v_size >= 1
 * - \b q_proj_size >= 1, k_proj_size >= 1, v_proj_size >= 1, o_proj_size >= 1
 * - \b q_proj_size = k_proj_size
 * - \b qo_max_seq_length >= 1, kv_max_seq_length >= 1
 * - \b max_batch_size >= 1, max_beam_size >= 1

 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t desc,
                                                         cnnlAttnMode attn_mode,
                                                         int nheads,
                                                         double sm_scaler,
                                                         cnnlDataType_t dtype,
                                                         cnnlDataType_t compute_type,
                                                         float attn_dropout,
                                                         float post_dropout,
                                                         int q_size,
                                                         int k_size,
                                                         int v_size,
                                                         int q_proj_size,
                                                         int k_proj_size,
                                                         int v_proj_size,
                                                         int o_proj_size,
                                                         int qo_max_seq_length,
                                                         int kv_max_seq_length,
                                                         int max_batch_size,
                                                         int max_beam_size);
// Group:MultiHeadAttn
/*!
 * @brief Destroys a multi-head attention descriptor \b desc that is previously created with the
 * ::cnnlCreateMultiHeadAttnDescriptor function.
 *
 * @param[in] desc
 *   Input. The multi-head attention descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @par API Dependency
 *  - This function need to be called after ::cnnlCreateMultiHeadAttnDescriptor and
 *    ::cnnlSetMultiHeadAttnDescriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyMultiHeadAttnDescriptor(cnnlMultiHeadAttnDescriptor_t desc);

// Group:MultiHeadAttn
/*!
 * @brief Assigns the quantization parameters of the filter used in multi-head attention
 * operation. Including \b wq_position, \b wk_position, \b wv_position, \b wo_position,
 * \b wq_scale, \b wk_scale, \b wv_scale, \b wo_scale. It is only used when the data type of
 * filter are fixed-data. If the \b attn_mode described in \desc is set to \p ALL_TO_ONE_BIAS or
 * \p ONE_TO_ONE_BIAS, the data type of bias should be aligned to the \b dtype described in \desc.
 * For more information about quantization, see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] wtype
 *   Input. The data type of queries, keys, values, outputs of filter,
 *   Set this parameter to CNNL_DTYPE_HALF or CNNL_DTYPE_FLOAT in training
 *   mode. Set this parameter to CNNL_DTYPE_HALF, CNNL_DTYPE_FLOAT or
 *   CNNL_DTYPE_INT16, CNNL_DTYPE_INT8 in inference mode.
 * @param[in] wq_position
 *   Input. The position of queries filter in fixed-point data type.
 * @param[in] wk_position
 *   Input. The position of keys filter in fixed-point data type.
 * @param[in] wv_position
 *   Input. The position of values filter in fixed-point data type.
 * @param[in] wo_position
 *   Input. The position of outputs filter in fixed-point data type.
 * @param[in] wq_scale
 *   Input. The scale of queries filter in fixed-point data type.
 * @param[in] wk_scale
 *   Input. The scale of keys filter in fixed-point data type.
 * @param[in] wv_scale
 *   Input. The scale of values filter in fixed-point data type.
 * @param[in] wo_scale
 *   Input. The scale of outputs filter in fixed-point data type.
 * @param[in] reserve_size_bytes
 *   Input. The size of reserved space in bytes, pass NULL in the inference mode.
 *
 * @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateMultiHeadAttnDescriptor to create the
 *   descriptor of multi-head attention operation, and ::cnnlSetMultiHeadAttnDescriptor to set
 *   descriptor.
 *
 * @note
 * - It can only be set in inference mode.
 * - The data type of filters are in fixed type and the data type of bias are in float or half
 *   type.
 * - The fixed-point data type is not supported in ::cnnlMultiHeadAttnForward, this function
 *   is not recommended to use.
 *
 * @par Requirements
 * - 2^position*scale should not be larger than 127 when \b wtype is
 *   \p CNNL_DTYPE_INT8.
 * - 2^position*scale should not be larger than 32767 when \b wtype is
 *   \p CNNL_DTYPE_INT16.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t cnnlSetMultiHeadAttnWeightsQuantifyInfo(cnnlMultiHeadAttnDescriptor_t desc,
                                                     cnnlDataType_t wtype,
                                                     int wq_position,
                                                     int wk_position,
                                                     int wv_position,
                                                     int wo_position,
                                                     float wq_scale,
                                                     float wk_scale,
                                                     float wv_scale,
                                                     float wo_scale,
                                                     size_t *reserve_size_bytes);

// Group:MultiHeadAttn
/*!
 * @brief Gets the size of MLU memory buffers for multi-head attention, including
 * \b filter_size_bytes, \b workspace_size_bytes and \b reserve_size_bytes.
 * The size of extra workspace is based on the given information of the multi-head attention
 * descriptor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the get multi-head attetnion buffers operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[out] filter_size_bytes
 *   Output. The minimum size in bytes required for multi-head attention trainable parameters.
 * @param[out] workspace_size_bytes
 *   Output. The minimum buffer size in bytes required to hold the ::cnnlMultiHeadAttnForward,
 *   ::cnnlMultiHeadAttnBackwardData (training mode) and
 *   ::cnnlMultiHeadAttnBackwardWeights (training mode).
 * @param[out] reserve_size_bytes
 *   Output. The minimum buffer size in bytes required to store the intermediate results of
 *   ::cnnlMultiHeadAttnForward, ::cnnlMultiHeadAttnBackwardData and
 *   ::cnnlMultiHeadAttnBackwardWeights.
 *   Set this parameter to NULL in the inference mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after ::cnnlCreateMultiHeadAttnDescriptor and
 *   ::cnnlSetMultiHeadAttnDescriptor.
 *
 * @note
 * All filter and bias tensors are aggregated in a single filter buffer. The layout of filter
 * inside the filter buffer may be different from your inputs for performance optimization. If
 * \b reserve_size_bytes is set to NULL, the inference mode is used and only ::cnnlMultiHeadAttnForward
 * is invoked. Otherwise, ::cnnlMultiHeadAttnForward, ::cnnlMultiHeadAttnBackwardData and
 * ::cnnlMultiHeadAttnBackwardWeights are invoked.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMultiHeadAttnBuffers(cnnlHandle_t handle,
                                                      const cnnlMultiHeadAttnDescriptor_t desc,
                                                      size_t *filter_size_bytes,
                                                      size_t *workspace_size_bytes,
                                                      size_t *reserve_size_bytes);

/*! @brief Enumeration variables describing the filter and biases of multi-head attention.
 *  The shapes of the filter of queries, keys, values are [1, nheads, proj_size, embedding_size] and
 *  the shape of outputs filter is [1, nheads, o_proj_size, v_proj_size].
 *  If extra biases are configured in the multi-head attention descriptor, the queries, keys,
 *  values biases are in the shape of [1, 1, nheads, proj_size] and outputs bias is in the shape of
 *  [1, 1, 1, o_proj_size].
 *  The enumeration is only used for ::cnnlGetMultiHeadAttnWeights.
 */
typedef enum {
  CNNL_MH_ATTN_Q_WEIGHT, /*!< Multi-head attention filter for queries. */
  CNNL_MH_ATTN_K_WEIGHT, /*!< Multi-head attention filter for keys. */
  CNNL_MH_ATTN_V_WEIGHT, /*!< Multi-head attention filter for values. */
  CNNL_MH_ATTN_O_WEIGHT, /*!< Multi-head attention filter for outputs. */
  CNNL_MH_ATTN_Q_BIAS, /*!< Multi-head attention bias for queries. */
  CNNL_MH_ATTN_K_BIAS, /*!< Multi-head attention bias for keys. */
  CNNL_MH_ATTN_V_BIAS, /*!< Multi-head attention bias for values. */
  CNNL_MH_ATTN_O_BIAS, /*!< Multi-head attention bias for outputs. */
} cnnlMultiHeadAttnWeightKind_t;

// Group:MultiHeadAttn
/*!
 *  @brief Gets the multi-head attention filter and biases with the given filter or bias
 *  tensor. The function returns the start address of the filter/biases tensor stored
 *  in the filter buffer. See ::cnnlMultiHeadAttnWeightKind_t for details.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the get multi-head attention filter operation. For detailed information,
 *   see ::cnnlHandle_t.
 *  @param[in] desc
 *    Input. Descriptor of multi-head attention operation.
 *  @param[in] wkind
 *    Input. The requested filter/bias kind. See ::cnnlMultiHeadAttnWeightKind_t for the
 *    description of the enumeration type.
 *  @param[in] filter_size_bytes
 *    Input. The size of filter size in bytes, whose size can be derived through
 *    ::cnnlGetMultiHeadAttnBuffers.
 *  @param[in] filter
 *    Input. The start address of the filter buffer on host or device memory.
 *  @param[out] w_desc
 *    Output. The tensor descriptor of the requested filter or bias. For filter, the shape is
 *    [1, nheads, proj_size, original_size]. For biases, the shape of queries, keys, values biases
 *    are [1, 1, nheads, proj_size] and the shape of output biases is [1, 1, 1, o_proj_size].
 *  @param[out] w_addr
 *    Output. The start address of the requested filter or bias on host or device memory.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 *  - This function need to be called after ::cnnlCreateMultiHeadAttnDescriptor,
 *    ::cnnlSetMultiHeadAttnDescriptor, and ::cnnlGetMultiHeadAttnBuffers.
 *
 * @note
 * - When requested tensor is one of {\p CNNL_MH_ATTN_Q_BIAS, \p CNNL_MH_ATTN_K_BIAS,
 *  \p CNNL_MH_ATTN_V_BIAS, \p CNNL_MH_ATTN_O_BIAS}, and ::cnnlAttnMode is set to \p ALL_TO_ONE_NOBIAS or
 *  \p ONE_TO_ONE_NOBIAS, the element size of requested tensor is 0, the function will return
 * an defaulted descriptor and a NULL \b w_addr. Meanwhile, the function returns ::CNNL_STATUS_SUCCESS.
 *
 * @par Example
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMultiHeadAttnWeights(cnnlHandle_t handle,
                                                      const cnnlMultiHeadAttnDescriptor_t desc,
                                                      cnnlMultiHeadAttnWeightKind_t wkind,
                                                      size_t filter_size_bytes,
                                                      const void *filter,
                                                      const cnnlTensorDescriptor_t w_desc,
                                                      void **w_addr);

// Group:MultiHeadAttn
/*!
 * @brief Computes the forward process of multi-head attention operation in both inference and
 * training mode.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the multi-head attention forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] curr_idx.
 *   Input. Time-step in \b queries to process. When curr_idx greater than or equal to 0,
 *   this function will process the selected time-step only.
 *   When curr_idx is less than 0, all \b qo_max_seq_length time-steps will be processed automatically.
 * @param[in] padding_mask_desc
 *   Input. Descriptor of \b padding_mask.
 *   Pass NULL when padding mask is not requested.
 * @param[in] padding_mask
 *   Input. Pointer to \b padding_mask on the device memory.
 *   Pass NULL when padding mask is not requested.
 * @param[in] attn_mask_desc
 *   Input. Descriptor of \b attn_mask.
 *   Pass NULL when attention mask is not requested.
 * @param[in] attn_mask
 *   Input. Pointer to attention mask data on the device memory.
 *   Pass NULL when attention mask is not requested.
 * @param[in] q_desc
 *   Input. Descriptor for the queries and residuals. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] queries,
 *   Input. Pointer to queries sequence data on the device memory.
 * @param[in] residuals
 *   Input. Pointer to residuals sequence data on the device memory.
 *   Pass NULL when residual connection is not requested.
 * @param[in] k_desc
 *   Input. Descriptor for the keys. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] keys
 *   Input. Pointer to the filter on the device memory, which contains attention bias
 *   and filter for queries, keys, values and outputs. Parameters of attention filter,
 *   i.e. filter types, positions and scales, can be set via
 *   ::cnnlSetMultiHeadAttnWeightsQuantifyInfo in the inference mode.
 * @param[in] v_desc
 *   Input. Descriptor for the values. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] values
 *   Input. Pointer to value sequence data on the device memory.
 * @param[in] o_desc
 *   Input. Descriptor for the outputs. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[out] outputs
 *   Output. Pointer to outputs data on the device memory.
 * @param[in] filter_size_bytes
 *   Input. Size of filter in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. Pointer to the filter on the device memory.
 * @param[in] workspace_size_bytes.
 *   Input. Size of workspace buffer in bytes used for temporary storage, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] workspace
 *   Input/Output. Pointer to the workspace buffer used for temporary storage on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. Size of reserved space in bytes. The parameter should be 0 in the inference mode.
 *   In the training mode, the size can be derived through ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] reservespace
 *   Input/Output. Pointer to the reserved space buffer used for data exchange between
 *   forward function ::cnnlMultiHeadAttnForward, and backward function
 *   ::cnnlMultiHeadAttnBackwardData
 *   and ::cnnlMultiHeadAttnBackwardWeights.
 *   Pass NULL in the inference mode and non-NULL in the training mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @note
 * - The padding mask is a three-dimensional tensor adding before performing softmax operation.
 *   Typically, the values of padding mask should be near negative infinity in the masked part
 *   and 0 in the unmasked part.
 *   When the attention mode is set \b ALL_TO_ONE_BIAS or \b ALL_TO_ONE_NOBIAS, the dimensions of
 *   padding mask are required to be [max_batch_size, 1, kv_max_seq_length].
 *   When the attention mode is set \b ONE_TO_ONE_BIAS or \b ONE_TO_ONE_NOBIAS, the dimensions of
 *   padding mask are required to be [max_batch_size, max_beam_size, kv_max_seq_length].
 * - The attention mask is a two-dimensional tensor adding before softmax.
 *   The attention mask is used in the masked self attention on the transformer's
 *   decoder side. Typically, it is an upper triangular matrix with all upper triangular values
 *   negative infinity.
 *   The shape of attention mask is required to be [qo_max_seq_length, kv_max_seq_length].
 * - If \b reservespace is set to NULL and \b reservespace_size_bytes is 0, no intermediate data is
 *   stored and this function is implemented in the inference mode.
 *   Otherwise, this function is implemented in the training mode. Meanwhile,
 *   intermediate data are passed through ::cnnlMultiHeadAttnForward,
 *   ::cnnlMultiHeadAttnBackwardData
 *   and ::cnnlMultiHeadAttnBackwardWeights on the \b reservespace.
 * - In the inference mode, \b curr_idx is greater than or equal to 0, specifies the time-step of the embedding
 *   vectors to be processed. In this mode, you can perform adaptive masking through:
 *   1. Start from a single time-steps forward, \b curr_idx is set to 0.
 *   2. Update \b queries, \b keys, \b values and the \b attn_mask dynamically before the next
 *      time-step iteration.
 *   3. Repeat the two steps above until all time-steps are processed.
 * - When all \b qo_max_seq_length time-steps are available (for example, in the training mode or
 *   in the inference mode on the encoder side in self-attention), the user can pass a
 *   negative \b curr_idx(i.e. -1).
 *   The ::cnnlMultiHeadAttnForward will automatically go through all time-steps and mask
 *   according to the \b padding_mask and \b attn_mask given.
 * - The descriptors of \b queries and \b outputs should match while the descriptors of \b keys and
 *   \b values should match. The \b queries and \b residuals share the same \b q_desc.
 *   Note that when \b residual is activated (\b residuals != NULL), the \b o_proj_size and
 *   \b q_size recorded in the \b attn_desc should be the same, so that the residual connection
 *   is feasible. Set \b residual to NULL when residual connection is not requested.
 * - Aside from \b residuals, \b queries, \b keys and \b values are not allowed to set NULL even
 *   if any two or three of them are the same. If it is the self-attention situation, pass them the
 *   same device address. And if it is the encoder-decoder attention situation, \b keys and \b values are
 *   the same device address.

 * @par API Dependency
 * - Before calling this function to implement multi-head attention, you need to create the
 * ::cnnlSeqDataDescriptor_t of \b queries, \b keys, \b values, and \b outputs by
 * ::cnnlCreateSeqDataDescriptor, and set the data type, layout, dimensions, sequence lengths,
 * padding fill for the descriptors using ::cnnlSetSeqDataDescriptor. Then, create a multi-head
 * attention descriptor using ::cnnlCreateMultiHeadAttnDescriptor and set the multi-head
 * attention descriptor using ::cnnlSetMultiHeadAttnDescriptor.
 * - Get MLU device filter size, workspace size and reserved space size using
 * ::cnnlGetMultiHeadAttnBuffers and allocate memory for them. Copy in the filter data to the
 * address given by ::cnnlGetMultiHeadAttnWeights.
 * - Allocate MLU device memory and copy in for all inputs or outputs.
 *
 * @par Performance Optimization
 * - The traditional multi-head attention with 8, 12, 16 heads is optimized with fused algorithm.
 * - To get better performance, set \b q_proj_size, \b k_proj_size, \b v_proj_size to 64, and
 *   \b set \b q_size, \b k_size, \b v_size and \b o_proj_size to nheads * 64.
 * - The addresses of \b queries, \b keys, and \b values are the same, which is the
 *   self-attention scenario.
 * - The address of \b keys and \b values are the same, \b padding_mask is NULL, which is the
 *   encoder-decoder attention scenario.
 *
 * @par Example
 * - The example of padding mask. It prevents attention to the padding part of sequences.
 *   Suppose two 5-word sequence arrays with the real sequence length of 3 and 4.
 *   The padding lengths of the first and second array are 1 and 2, separately.
 *   The padding mask are as follows:
     @verbatim
      padding_mask = [[[0, 0, 0, -inf, -inf]], [[0, 0, 0, 0, -inf]]]
      where inf represents infinity
     @endverbatim
     The padding mask usually occurs in self-attention on the encoder side and
     encoder-decoder attention on the decoder side.
 * - The example of a typical attention mask in the training scenario. The \b attn_mask is an attention
 *   mask prevents attention to certain positions. The attention mask for a 5-word sequence array is
 *   as follows:
     @verbatim
       attn_mask = [[   0, -inf, -inf, -inf, -inf],
                    [   0,    0, -inf, -inf, -inf],
                    [   0,    0,    0, -inf, -inf],
                    [   0,    0,    0,    0, -inf],
                    [   0,    0,    0,    0,    0]].
     @endverbatim
 *   The attention mask usually occurs in masked self-attention on the decoder side.
 * - Multi-head attention, layer norm and residual connections are highly coupled
 *   in the transformers networks. This function does not include layer norm. You
 *   can set any data data as \b residuals to realize residual connection,
 *   as long as it has the same shape and data type with \b queries.
 *   Following are two typical cases indicating the residual connection.
     @verbatim
       1. Residual connection before layernorm.
       - layernorm_out = LayerNormForward(queries)
       - residuals = queries
       - out = cnnlMultiHeadAttnForward(layernorm_out, keys, values) call.
       2. Residual connection outside the layernorm.
       - residuals = queries
       - out = cnnlMultiHeadAttnForward(queries, keys, values) call.
       - layernorm_out = LayerNormForward(out)
     @endverbatim
 *
 * @par Data Layout
 * - \b Queries, \b keys, \b values, \b outputs, and \b residuals only support \p CNNL_SEQDATA_NBTC
 *  layout.
 *
 * @par Data Type
 * - Note that the data type of queries, keys, values, outputs, residuals and filter should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same with the data type on MLU300 series or above.
 * - The data types supported are as follows:
 *   - queries, keys, values, outputs, residuals: float, half.
 *   - filter: attention bias in filter should be float and half, attention filter for queries, keys, values
 *     and outputs can be int8, int16, float and half.
 *   - computing type: int8, int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Scale Limitation
 * - Inference mode (curr_idx >= 0) is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
cnnlStatus_t CNNL_WIN_API cnnlMultiHeadAttnForward(cnnlHandle_t handle,
                                                   const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                                   int curr_idx,
                                                   const cnnlTensorDescriptor_t padding_mask_desc,
                                                   const void *padding_mask,
                                                   const cnnlTensorDescriptor_t attn_mask_desc,
                                                   const void *attn_mask,
                                                   const cnnlSeqDataDescriptor_t q_desc,
                                                   const void *queries,
                                                   const void *residuals,
                                                   const cnnlSeqDataDescriptor_t k_desc,
                                                   const void *keys,
                                                   const cnnlSeqDataDescriptor_t v_desc,
                                                   const void *values,
                                                   const cnnlSeqDataDescriptor_t o_desc,
                                                   void *outputs,
                                                   size_t filter_size_bytes,
                                                   const void *filter,
                                                   size_t workspace_size_bytes,
                                                   void *workspace,
                                                   size_t reservespace_size_bytes,
                                                   void *reservespace);
// Group:MultiHeadAttn
/*!
 * @brief Computes the first-order derivatives of the multi-head attention \b dqueries, \b keys,
 *  \b dvalues, \b doutputs with respect to the queries, keys, values, and forward outputs.
 *  Meanwhile, this function computes intermediate data and saves to the reserved space for
 *  ::cnnlMultiHeadAttnBackwardWeights to derive the derivatives of filter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the multi-head attention backward data operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] padding_mask_desc
 *   Input. Descriptor of \b padding_mask.
 *   Pass NULL when padding mask is not requested.
 * @param[in] padding_mask
 *    Input. Pointer to \b padding_mask on the device memory.
 *    Pass NULL when padding mask is not requested.
 * @param[in] attn_mask_desc
 *    Input. Descriptor of \b attn_mask.
 *    Pass NULL when attention mask is not requested.
 * @param[in] attn_mask
 *    Input. Pointer to attention mask data on the device memory.
 *    Pass NULL when attention mask is not requested.
 * @param[in] do_desc
 *   Input. The descriptor of \b doutputs. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] doutputs
 *   Input. Pointer to \b doutputs on the device memory.
 * @param[in] dq_desc
 *   Input. The descriptor of output tensor \b dqueries. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dqueries
 *   Output. Pointer to dqueries on the device memory.
 * @param[in] dk_desc
 *   Input. The descriptor of output tensor \b dkeys. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dkeys
 *   Output. Pointer to \b dkeys on the device memory.
 * @param[in] dv_desc
 *   Input. The descriptor of output tensor \b dvalues. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] dvalues
 *   Output. Pointer to \b dvalues on the device memory.
 * @param[in] filter_size_bytes
 *   Input. The size of filter in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. Pointer to filter on the device memory. Note that the filter should be the same as
 *   the filter in the ::cnnlMultiHeadAttnForward.
 * @param[in] workspace_size_bytes
 *   Input. The size of workspace in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] workspace
 *   Input. Pointer to the workspace on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. The size of reserved space in bytes.
 *   In the training mode, the size can be derived through ::cnnlGetMultiHeadAttnBuffers.
 * @param[in,out] reservespace
 *   Input, Output. Pointer to the reserved space on the device memory. Note that the reserve space
 *   should be the same as the \b reservespace in the ::cnnlMultiHeadAttnForward.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 *  - The ::cnnlMultiHeadAttnBackwardData should be invoked after ::cnnlMultiHeadAttnForward, and
 *   before ::cnnlMultiHeadAttnBackwardWeights.
 *  - \b padding_mask, \b attn_mask, \b weights and \b reservespace are same as them in
 *   the ::cnnlMultiHeadAttnForward.
 *
 * @note
 * - The ::cnnlMultiHeadAttnBackwardData function does not compute the partial derivatives of
 *   the residual connections, since the result is simply \b doutputs. If there is a residual
 *   connection between the queries and outputs, you need to add \b doutputs to \b dqueries to get
 *   the correct result.
 *
 * @par Data Layout
 * - \b dqueries, \b dkeys, \b dvalues, \b doutputs, only support \p CNNL_SEQDATA_NBTC layout.
 *
 * @par Data Type
 * - Note that the data type of dqueries, dkeys, dvalues, doutputs and filters should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same with the data type on MLU300 series or above.
 * - The data types supported are as follows:
 *   - dqueries, dkeys, dvalues, doutputs: float, half.
 *   - filter: float, half.
 *   - computing type: int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Performance Optimization
 * - To get better performance, set \b q_proj_size, \b k_proj_size, \b v_proj_size to 64, and
 *   \b set \b q_size, \b k_size, \b v_size and \b o_proj_size to nheads * 64.
 *
 * @par Scale Limitation
 * - Device of CE3226 is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
cnnlStatus_t cnnlMultiHeadAttnBackwardData(cnnlHandle_t handle,
                                           const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                           const cnnlTensorDescriptor_t padding_mask_desc,
                                           const void *padding_mask,
                                           const cnnlTensorDescriptor_t attn_mask_desc,
                                           const void *attn_mask,
                                           const cnnlSeqDataDescriptor_t do_desc,
                                           const void *doutputs,
                                           const cnnlSeqDataDescriptor_t dq_desc,
                                           void *dqueries,
                                           const cnnlSeqDataDescriptor_t dk_desc,
                                           void *dkeys,
                                           const cnnlSeqDataDescriptor_t dv_desc,
                                           void *dvalues,
                                           const size_t filter_size_bytes,
                                           const void *filter,
                                           const size_t workspace_size_bytes,
                                           void *workspace,
                                           const size_t reservespace_size_bytes,
                                           void *reservespace);
/*!
 * @brief An enum.
 *
 *  Enumeration variables describe how buffers that hold gradients of the loss function
 *  with respect to trainable parameters, are updated. Currently, this type is
 *  used by ::cnnlMultiHeadAttnBackwardWeights function only.
 */
typedef enum {
    CNNL_WGRAD_MODE_ADD = 0,
    /*!< Add filter gradients to the output buffers. Note that the dfilters
     * should be initialized to zero before using this mode. Alternatively, use CNNL_WGRAD_MOD_SET in
     * the first time ::cnnlMultiHeadAttnBackwardWeights or ::cnnlRNNBackwardWeights is called.*/
    CNNL_WGRAD_MODE_SET = 1, /*!< Write partial gradients to dfilters buffers directly.*/
} cnnlWgradMode_t;

// Group:MultiHeadAttn
/*!
 * @brief Computes the gradient of the multi-head attention block with respect
 * to the filter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the multi-head attention backward weights operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] attn_desc
 *   Input. Descriptor of multi-head attention operation.
 * @param[in] add_grad
 *   Input. The requested filter/bias kind. Refer to ::cnnlMultiHeadAttnWeightKind_t for detail.
 * @param[in] q_desc
 *   Input. Descriptor of \b queries sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] queries
 *   Input. Pointer to the \b queries on the device memory.
 * @param[in] k_desc
 *   Input. Descriptor of \b keys sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] keys
 *   Input. Pointer to the \b keys on the device memory.
 * @param[in] v_desc
 *   Input. Descriptor of \b values sequence data. For detailed information, see
 *   ::cnnlSeqDataDescriptor_t.
 * @param[in] values
 *   Input. Pointer to the \b values on the device memory.
 * @param[in] doutputs_desc
 *   Input. Descriptor of delta outputs sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] doutputs
 *   Input. Pointer to the delta outputs on the device memory.
 * @param[in] filter_size_bytes
 *   Input. The size of filter size in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] filter
 *   Input. The start address of the filter/biases buffers.
 * @param[out] dfilters
 *   Output. The start address of the gradient of filter/biases buffers.
 * @param[in] workspace_size_bytes
 *   Input. The size of workspace in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[out] workspace
 *   Output. The start address of the workspace on the device memory.
 * @param[in] reservespace_size_bytes
 *   Input. The size of reserved space in bytes, which can be derived through
 *   ::cnnlGetMultiHeadAttnBuffers.
 * @param[in] reservespace
 *   Input. The start address of the reserved space on the device memory. Note that the reserve
 *   space should be the same as the \b reservespace in the ::cnnlMultiHeadAttnForward and
 *   ::cnnlMultiHeadAttnBackwardData.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 *  - The ::cnnlMultiHeadAttnBackwardWeights should be invoked after ::cnnlMultiHeadAttnForward and
 *    ::cnnlMultiHeadAttnBackwardData.
 *  - The \b reservespace and \b filter should be the same as them in the
 *    ::cnnlMultiHeadAttnForward and ::cnnlMultiHeadAttnBackwardData.
 *  - The \b queries, \b keys, \b values should be the same as them in the
 *    ::cnnlMultiHeadAttnForward.
 *  - The \b doutputs should be the same as the one in the ::cnnlMultiHeadAttnBackwardData.
 *
 * @note
 * - None.
 *
 * @par Data Layout
 * - \b dqueries, \b dkeys, \b dvalues, \b doutputs, only support \p CNNL_SEQDATA_NBTC layout.
 *
 * @par Data Type
 * - Note that the data type of queries, keys, values, doutputs, filter and dfilters should be the same.
 *   The computing type is used for all matrix multiplications in the multi-head attention
 *   operation. The computing type can be the same with the data type on MLU300 series.
 * - The data types supported are as follows:
 *   - queries, keys, values, doutputs: float, half.
 *   - filter, dfilter: float, half.
 *   - computing type: int16, float (MLU300 series), half (MLU300 series).
 *
 * @par Performance Optimization
 * - To get better performance, set \b q_proj_size, \b k_proj_size, \b v_proj_size to 64, and
 *   \b set \b q_size, \b k_size, \b v_size and \b o_proj_size to nheads * 64.
 *
 * @par Scale Limitation
 * - Device of CE3226 is not supported.
 *
 * @par Reference
 * - Attention is All You Need, vaswani ashish, et, al., 2017.
 * - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Jacob
 * Devlin, et al., 2019.
 */
cnnlStatus_t CNNL_WIN_API
cnnlMultiHeadAttnBackwardWeights(cnnlHandle_t handle,
                                 const cnnlMultiHeadAttnDescriptor_t attn_desc,
                                 cnnlWgradMode_t add_grad,
                                 const cnnlSeqDataDescriptor_t q_desc,
                                 const void *queries,
                                 const cnnlSeqDataDescriptor_t k_desc,
                                 const void *keys,
                                 const cnnlSeqDataDescriptor_t v_desc,
                                 const void *values,
                                 const cnnlSeqDataDescriptor_t dout_desc,
                                 const void *doutputs,
                                 size_t filter_size_bytes,
                                 const void *filter,
                                 void *dfilter,
                                 size_t workspace_size_bytes,
                                 void *workspace,
                                 size_t reservespace_size_bytes,
                                 void *reservespace);
/*****************************************************************************
 * Cambricon CNNL OP: Interp
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing the interpolation algorithms used to implement the
 *        interpolation operation.
 *
 */
typedef enum {
  CNNL_INTERP_NEAREST = 0,
  /*!< The interp mode is Nearest-Neighbor,
   * which means using nearest pixel for interpolation.*/
  CNNL_INTERP_BILINEAR = 1,
  /*!< The interp mode is Bilinear,
   * which means using four corner pixels with bilinear algorithm for interpolation.*/
  CNNL_INTERP_LINEAR = 2,
  /*!< The interp mode is Linear,
   * which means using two corner pixels with linear algorithm for interpolation.*/
  CNNL_INTERP_TRILINEAR = 3,
  /*!< The interp mode is Trilinear,
   * which means using eight corner pixels with trilinear algorithm for interpolation.*/
  CNNL_INTERP_BICUBIC = 4,
  /*!< The interp mode is Bicubic,
   * which means using sixteen corner pixels with bicubic algorithm for interpolation.*/
} cnnlInterpMode_t;

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, bicubic and trilinear methods.
 * The input tensors and output tensor need to have the same number of dimensions. Supports 3-Dimension,
 * 4-Dimension and 5-Dimension.
 *
 * @deprecated
 *   ::cnnlInterp is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlInterp_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *          and original input images.
 *          Generally, pixels of input and output images are considered to be squares.
 *          If \b align_corners is set to true, the input and output images are aligned by the
 *          center points of 4 corner pixels.
 *          Otherwise, the input and output images are aligned by the upper-left corner points of
 *          the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable determines whether to use center or corner coordinates of each pixel.
 *          If this parameter is true, the center coordinates are used to represent each pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when align_center is set to true,
 *          the coordinate of the first pixel is (0, 0).
 *          When align_center is set to false, the coordinate of the first pixel is (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of input tensor can only be \p CNNL_LAYOUT_NHWC.
 *     If \b mode is set to CNNL_INTERP_NEAREST, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of output tensor can only be \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_NEAREST, the layout of output tensor can only be \p CNNL_LAYOUT_NLC or
 *     \p CNNL_LAYOUT_NHWC. If \b mode is set to CNNL_INTERP_TRILINEAR,
 *     the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - \b align_corners and \b align_center can not be True at the same time.
 * - If \b mode is set to CNNL_INTERP_NEAREST and the layout of input is \p CNNL_LAYOUT_NLC,
 *   \b align_corners and \b align_center can only be False.
 * - On MLU200 series, \b input containing NaN/infinity is not supported. On MLU300 series and CE3226,
 *   if \b input contains NaN/infinity, it may cause undefined behavior.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInterp(cnnlHandle_t handle,
                                     bool align_corners,
                                     bool align_center,
                                     cnnlInterpMode_t mode,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, bicubic and trilinear methods.
 * The input tensors and output tensor need to have the same number of dimensions. Supports 3-Dimension,
 * 4-Dimension and 5-Dimension.
 *
 * @deprecated
 *   ::cnnlInterp_v2 is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlInterp_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *          and original input images.
 *          Generally, pixels of input and output images are considered to be squares.
 *          If \b align_corners is set to true, the input and output images are aligned by the
 *          center points of 4 corner pixels.
 *          Otherwise, the input and output images are aligned by the upper-left corner points of
 *          the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable determines whether to use center or corner coordinates of each pixel.
 *          If this parameter is true, the center coordinates are used to represent each pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when align_center is set to true,
 *          the coordinate of the first pixel is (0, 0).
 *          When align_center is set to false, the coordinate of the first pixel is (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpMode_t enum.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scale factors. When mode is set to
 *          CNNL_INTERP_LINEAR, the length of \b scale_factors is one, scale_factors[0] is equal to
 *          the width of output images divided by the width of input images. When mode is set to
 *          CNNL_INTERP_BILINEAR, CNNL_INTERP_NEAREST or CNNL_INTERP_BICUBIC, the length of \b scale_factors is two,
 *          scale_factors[0] is equal to the height of output images divided by the height of input
 *          images, scale_factors[1] is equal to the width of output images divided by the width of
 *          input images. When mode is set to CNNL_INTERP_TRILINEAR, the length of \b scale_factors
 *          is three, scale_factors[0] is equal to the depth of output images divided by the depth
 *          of input images, scale_factors[1] is equal to the height of output images divided by
 *          the height of input images, scale_factors[2] is equal to the width of output images
 *          divided by the width of input images. The elements of scale factors should be greater
 *          than zero.
 * @param[in] recompute_scale_factor
 *   Input. Boolean variable determines whether to recompute the scale factors in the
 *          interpolation calculation. If \b recompute_scale_factor is false, the passed-in
 *          \b scale_factors will be used in the interpolation computation. Otherwise, a new
 *          scale factor will be computed based on the output and input sizes for use in the
 *          interpolation computation.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NHWC. If \b mode is set to CNNL_INTERP_NEAREST, the layout of
 *     input tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NHWC.If \b mode is set to CNNL_INTERP_NEAREST, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - \b align_corners and \b align_center can not be True at the same time.
 * - ::cnnlInterp_v2 is recommended to use since ::cnnlInterp does not support the input
 *   of \b recompute_scale_factor and \b scale_factors.
 * - When \b recompute_scale_factor is true, \b scale_factors is not neccessary, so \b scale_factors
 *   must be NULL.
 * - If \b mode is set to CNNL_INTERP_NEAREST and the layout of input is \p CNNL_LAYOUT_NLC,
 *   \b align_corners and \b align_center can only be False.
 * - Given valid \b scale_factors, the shapes of \b input and \b output should satisfy the following
 *   requirements.
 *   - For 5D \b input and \b output, the shapes are [n, di, hi, wi, c] and [n, do, ho, wo, c],
 *     respectively. Written in C style, the relationship between \p di and \p do should follow equation:
 *     \p do = (int)((double)\p \p scale_factor_d * \p di), where \p scale_factor_d is the scale factor
 *     for d-dimension. The same equation applies to other dimensions except \p n and \p c.
 *   - For 3D and 4D \b input and \b output, the equation described above is also used to map the dimension
 *     value between \b input and \b output.
 * - On MLU200 series, \b input containing NaN/infinity is not supported. On MLU300 series and CE3226,
 *   if \b input contains NaN/infinity, it may cause undefined behavior.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       align_corners          : false
       align_center           : false
       mode                   : CNNL_INTERP_NEAREST
       scale_factors[]        : [2.3, 2.3]
       recompute_scale_factor : false

       Input tensor           : [[[[1.0], [2.0]],
                                  [[3.0], [4,0]]]]

       Output tensor          : [[[[1.0], [1.0], [1.0], [2.0]],
                                  [[1.0], [1.0], [1.0], [2.0]],
                                  [[1.0], [1.0], [1.0], [2.0]],
                                  [[3.0], [3.0], [3.0], [4.0]]]]

     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlInterp_v2(cnnlHandle_t handle,
                                     bool align_corners,
                                     bool align_center,
                                     cnnlInterpMode_t mode,
                                     const float scale_factors[],
                                     bool recompute_scale_factor,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

/*!
 * @brief Enumeration variables describing the coordinate transformation algorithms used
 * to implement the interp operation.
 *
 */
typedef enum {
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0 = 0,
  /*!< A type of algorithm.The formula is:
       ``x_original = length_resized > 1 ? (x_resized + 0.5) / (length_resized / length_original) - 0.5 : 0``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1 = 1,
  /*!< A type of algorithm.The formula is:
       ``x_original = length_resized > 1 ? (x_resized + 0.5) / (length_resized / length_original) - 0.5 : 0``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 = 2,
  /*!< A type of algorithm.The formula is:
       ``x_original = length_resized > 1 ? x_resized * (length_original - 1) / (length_resized - 1) : 0``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 = 3,
  /*!< A type of algorithm.The formula is:
       ``x_original = x_resized / (length_resized / length_original)``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO4 = 4,
  /*!< A type of algorithm.The formula is:
       ``x_original = (x_resized + 0.5) / (length_resized / length_original)``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO5 = 5,
  /*!<A mode to have different result with ALGO0, especially for bilinear mode when align_corners = false, align_center = true
      The formula is:
       ``x_original = (x_resized + 0.5) / (length_resized / length_original) - 0.5``.*/
  CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO6 = 6,
  /*!<A mode to have different result with ALGO2, especially for bilinear mode when align_corners = true, align_center = false
      The formula is:
       ``x_original = length_resized > 1 ? x_resized * (length_original - 1) / (length_resized - 1) : x_resized * (length_original / length_resized)``.*/
} cnnlInterpCoordinateTransformationMode_t;

/*!
 * @brief Enumeration variables describing the rounding mode of interp operation.
 *
 */
typedef enum {
  CNNL_INTERP_ROUND_PERFER_CEIL = 0,
  /*!< The rounding mode to round prefer ceil as known as round half ceil for
   nearest interpolation.*/
  CNNL_INTERP_ROUND_PERFER_FLOOR = 1,
  /*!< The rounding mode to round prefer floor as known as round half floor for
   nearest interpolation.*/
  CNNL_INTERP_CEIL = 2,
  /*!< The rounding mode to ceil for nearest interpolation.*/
  CNNL_INTERP_FLOOR = 3,
  /*!< The rounding mode to floor for nearest interpolation.*/
} cnnlInterpRoundMode_t;

// Group:Interp
/*!
 * @brief Creates an interp descriptor that holds cnnlInterpMode_t,
 * cnnlInterpCoordinateTransformationMode_t, cnnlInterpRoundMode_t, scale_factors,
 * pad, cubic_coeff_a and exclude_outside.
 *
 * @param[in] interp_desc
 *   Input. Pointer to the interp descriptor that holds information about interp.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - None
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateInterpDescriptor(cnnlInterpDescriptor_t *interp_desc);

// Group:Interp
/*!
 * @brief Initializes the interp descriptor pointed by \b interp_desc that is previously
 * created with the ::cnnlCreateInterpDescriptor function.
 *
 * @param[in] interp_desc
 *   Input. The descriptor of the interp operation. For detailed information,
 *   see ::cnnlInterpDescriptor_t.
 * @param[in] mode
 *   Input. The interp mode. For detailed information, see ::cnnlInterpMode_t.
 * @param[in] coordinate_trans_mode
 *   Input. The coordinate transformation algorithms of interp.
 *   For detailed information, see ::cnnlInterpCoordinateTransformationMode_t.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0 means align_corners is false and align_center
 *   is true.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 means align_corners is true and align_center
 *   is false.
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3 means align_corners is false and align_center
 *   is false.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - \b coordinate_trans_mode only supports ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0,
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2 and
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlSetInterpDescriptor(cnnlInterpDescriptor_t interp_desc,
                                                   const cnnlInterpMode_t mode,
                                                   const cnnlInterpCoordinateTransformationMode_t
                                                   coordinate_trans_mode);

// Group:Interp
/*!
 * @brief Set the extra parameters to the interp descriptor pointed by \b interp_desc
 * that is previously created with the ::cnnlCreateInterpDescriptor function.
 *
 * @param[in] interp_desc
 *   Input. The descriptor of the interp operation. For detailed information,
 *   see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input. For detailed information,
 *   see ::cnnlTnesorDescriptor_t.
 * @param[in] nearest_round_mode
 *   Input. The rounding mode of nearest. For detailed information,
 *   see ::cnnlInterpRoundMode_t.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scaling factors. When mode is set to
 *   ::CNNL_INTERP_LINEAR, the length of \b scale_factors is one, scale_factors[0] is equal to
 *   the width of output images divided by the width of input images. When mode is set to
 *   CNNL_INTERP_BILINEAR, CNNL_INTERP_NEAREST or CNNL_INTERP_BICUBIC, the length of \b scale_factors is two,
 *   scale_factors[0] is equal to the height of output images divided by the height of input
 *   images, scale_factors[1] is equal to the width of output images divided by the width of
 *   input images. When mode is set to CNNL_INTERP_TRILINEAR, the length of \b scale_factors
 *   is three, scale_factors[0] is equal to the depth of output images divided by the depth
 *   of input images, scale_factors[1] is equal to the height of output images divided by
 *   the height of input images, scale_factors[2] is equal to the width of output images
 *   divided by the width of input images. The elements of scale factors should be greater
 *   than zero.
 * @param[in] pad
 *   Input. Pointer to the host memory that stores the pads. It contains four pad parameters.
 *   pad[0] is the top pad of height, pad[1] is the bottom pad of height, pad[2] is the left
 *   pad of width, pad[3] is the right pad of width.
 * @param[in] cubic_coeff_a
 *   Input. The coefficient 'a' used in cubic interpolation. Two common choices are
 *   -0.5 (in some cases of TensorFlow) and -0.75 (in PyTorch). Check out Equation (4) in
 *   https://ieeexplore.ieee.org/document/1163711 for the details.
 *   This attribute is valid only if interp mode is cubic.
 * @param[in] exclude_outside
 *   Input. It is only used in cubic mode. If set to true, the filter of sampling locations outside
 *   the input will be set to 0 and the filter will be renormalized so that their sum is 1.0.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - When \b pad is not NULL, interp mode only supports ::CNNL_INTERP_BILINEAR, the layout of
 *   input and output should be ::CNNL_LAYOUT_NHWC.
 * - \b nearest_round_mode is only used when interp mode is set to nearest.
 * - When \b scale_factors is not used, it must be set to NULL, when \b scale_factors is used,
 *   it can not be NULL.
 * - \b cubic_coeff_a only supports -0.75.
 * - \b exclude_outside only supports false.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t  CNNL_WIN_API cnnlSetInterpDescriptorEx(cnnlInterpDescriptor_t interp_desc,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const cnnlInterpRoundMode_t nearest_round_mode,
                                                     const float scale_factors[],
                                                     const int pad[],
                                                     float cubic_coeff_a,
                                                     bool exclude_outside);

// Group:Interp
/*!
 * @brief Destroys an interp descriptor that was created by ::cnnlCreateInterpDescriptor.
 *
 * @param[in] interp_desc
 *   Input. The interp descriptor created by ::cnnlCreateInterpDescriptor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyInterpDescriptor(cnnlInterpDescriptor_t interp_desc);

// Group:Interp
/*!
 * @brief Performs interpolation operation with nearest, linear, bilinear, trilinear, and bicubic methods.
 * The input tensors and output tensor need to have the same number of dimensions. Supports 3-Dimension,
 * 4-Dimension and 5-Dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] interp_desc
 *   Input. The descriptor of interp.
 *   For detailed information, see ::cnnlInterpDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor of input images.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input images.
 * @param[in] output_desc
 *   Input. The descriptor of output tensor of output images.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @par Data Layout
 * - The supported of data layout of input tensor and output tensor are as follows:
 *   - input tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of input tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC,
 *     the layout of input tensor can only be \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_NEAREST, the layout of input tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC.
 *     If \b mode is set to CNNL_INTERP_TRILINEAR, the layout of input tensor can only be \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: If \b mode is set to CNNL_INTERP_LINEAR, the layout of output tensor can only
 *     be \p CNNL_LAYOUT_NLC. If \b mode is set to CNNL_INTERP_BILINEAR or CNNL_INTERP_BICUBIC, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NHWC. If \b mode is set to CNNL_INTERP_NEAREST, the layout of
 *     output tensor can only be \p CNNL_LAYOUT_NLC or \p CNNL_LAYOUT_NHWC. If \b mode is set to
 *     CNNL_INTERP_TRILINEAR, the layout of output tensor can only be \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - coordinate_trans_mode in \b interp_desc do not support ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO1
 *   and ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO4.::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0
 *   means \b align_corners is false and \b align_center is true. ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2
 *   means \b align_corners is true and \b align_center is false. ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3
 *   means \b align_corners is false and \b align_center is false.
 * - If \b mode is set to ::CNNL_INTERP_NEAREST, when coordinate_trans_mode is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO0, the rounding mode in \b interp_desc must be
 *   ::CNNL_INTERP_CEIL. When coordinate_trans_mode in \b interp_desc is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO2, the rounding mode in \b interp_desc must be
 *   ::CNNL_INTERP_ROUND_PERFER_CEIL. When coordinate_trans_mode is
 *   ::CNNL_INTERP_COORDINATE_TRANSFORMATION_ALGO3, the rounding mode in \b interp_desc must be
 *   ::CNNL_INTERP_FLOOR.
 * - Given valid \b scale_factors, the shapes of \b input and \b output should satisfy the following
 *   requirements.
 *   For 5D \b input and \b output, the shapes are [n, di, hi, wi, c] and [n, do, ho, wo, c],
 *   respectively. Written in C style, the relationship between \p di and \p do should follow equation:
 *   \p do = (int)((double)\p \p scale_factor_d * \p di), where \p scale_factor_d is the scale factor
 *   for d-dimension. The same equation applies to other dimensions except \p n and \p c.
 *   For 3D and 4D \b input and \b output, the equation described above is also used to map the dimension
 *   value between \b input and \b output.
 * - On MLU200 series, \b input containing NaN/infinity is not supported. On MLU300 series and CE3226,
 *   if \b input contains NaN/infinity, it may cause undefined behavior.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInterp_v3(cnnlHandle_t handle,
                                       const cnnlInterpDescriptor_t interp_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void* input,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void* output);

// Group:Lerp
/*!
 * @brief Implements a linear interpolation of two tensors \b a and \b b based on
 *        a scalar or tensor \b w and returns the results in \b d tensor.
 *
 * The shapes of a and b must be broadcastable. If w is a tensor, then the shapes
 * of w, a, and b must be broadcastable.
 *
 * This function supports partial in-place operation, which means the first input
 * tensor \b a and the output tensor \b d can be the same one. This function also
 * supports tensor broadcasting as long as \b a, \b b, \b w, and \b d satisfy the
 * broadcasting conditions.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the lerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] a
 *   Input. Pointer to the MLU memory that stores the first input tensor.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b
 *   Input. Pointer to the MLU memory that stores the second input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the input filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w
 *   Input. Pointer to the MLU memory that stores the input filter scalar or tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   lerp operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the lerp operation. You can get the size of the workspace with
 *   the ::cnnlGetLerpWorkspaceSize function.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] d
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "cnnlLerp" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output.
 *   - \b a: half, float
 *   - \b b: half, float
 *   - \b w \b scalar: half, float
 *   - \b w \b tensor: half, float
 *   - \b d: half, float
 *   <b>Note that the data type of the input and output must be the same.</b>
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - Before calling this function to perform lerp operation, you need to get the
 *   size of workspace by the ::cnnlGetLerpWorkspaceSize function.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - You can specify the stride of all dimensions for a_desc, b_desc, w_desc and
 *   d_desc with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlLerp(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t a_desc,
                                   const void *a,
                                   const cnnlTensorDescriptor_t b_desc,
                                   const void *b,
                                   const cnnlTensorDescriptor_t w_desc,
                                   const void *w,
                                   void *workspace,
                                   size_t workspace_size,
                                   const cnnlTensorDescriptor_t d_desc,
                                   void * d);

// Group:Lerp
/*!
 * @brief Returns in \b size the size of the MLU memory in bytes that is used as
 *        an extra workspace to optimize the lerp operation.
 *
 * The size of extra workspace is based on the given information of the input
 * and output tensor descriptors, including \b a_desc, \b b_desc, \b w_desc,
 * and \b d_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the lerp operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] a_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] f_desc
 *   Input. The descriptor of the input filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] d_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in the lerp operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlLerp function
 *   to perform the lerp operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLerpWorkspaceSize(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t a_desc,
                                                   const cnnlTensorDescriptor_t b_desc,
                                                   const cnnlTensorDescriptor_t w_desc,
                                                   const cnnlTensorDescriptor_t d_desc,
                                                   size_t *size);

/*****************************************************************************
 * Cambricon CNNL OP: CropAndResize
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing the algorithms used to implement
 *        the CropAndResize operation.
 */
typedef enum {
  CNNL_CROP_AND_RESIZE_NEAREST = 0,
  /*!< The crop and resize mode is Nearest-Neighbor,
   * which means using the nearest pixel for interpolation.
   * The value of each pixel in crops is determined by its nearest neighbor in the input images.*/
  CNNL_CROP_AND_RESIZE_BILINEAR = 1,
  /*!< The crop and resize mode is Bilinear,
   * which means using the four corner pixels with bilinear algorithm for interpolation.
   * The value of each pixel in crops is determined by its 4 nearest corner pixels
   * in the input images with scaling factors calculated with bilinear algorithm.*/
} cnnlCropAndResizeMode_t;

// Group:CropAndResize
/*!
 * @brief Performs crop and resize operation with nearest and bilinear interpolation methods.
 *        The input tensors and output tensor only support 4-Dimension.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input data.
 * @param[in] boxes_desc
 *   Input. The descriptor of the boxes tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 *          The shape of bounding boxes tensor should be [boxes_num, 4],
 *          in which \b boxes[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *          specifying 4 corner coordinates of the corresponding box in the i-th image.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the boxes data. The boxes data contains the coordinates
 *          of box which are [y1,x1,y2,x2]. The coordinate of [y1,x1] is beginning position,
 *          the coordinate of [y2,x2] is ending position.
 * @param[in] box_index_desc
 *   Input. The descriptor of the boxes tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] box_index
 *   Input. Pointer to the MLU memory that stores box index data.The box index data contains the
 *          index of input images, and the index should belong to the scope of [0, batch). Batch is
 *          the dimension N of input. The data type of box_index should be ::CNNL_DTYPE_INT32.
 * @param[in] mode
 *   Input. The specific algorithm used to resize the crops extracted from the images.
 *          The algorithms are defined in ::cnnlCropAndResizeMode_t enum.
 * @param[in] extrapolation_value
 *   Input. Scalar. When the coordinate of boxes is outside the scope of [0, 1], it will use
 *          extrapolation_value to extrapolate the input image values.
 * @param[out] output_desc
 *   Output. The descriptor of output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores \b output data.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - input: float, half.
 * - output: float, half.
 *
 * @note
 * - The data layout of input and output should be ::CNNL_LAYOUT_NHWC.
 * - The supported combinations of data types are shown below with the following order:
 *   \b input - \b boxes- \b output
 *   The supported data type combinations are:
 *   - float - float - float
 *   - half - half -half
 *   - half - float - float
 * - The first dimension of boxes equals the first dimension of box_index and the first
 *   dimension of output.
 * - When \b boxes contains NaN/infinity:
 *   - On MLU200 series:
 *    - If \b boxes contains NaN/infinity, then \b output is \b extrapolation_value.
 *   - On MLU300 series and CE3226:
 *    - If \b boxes contains NaN/infinity, it may cause undefined behavior such as core
 *      dump and so on.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCropAndResize(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t boxes_desc,
                                     const void *boxes,
                                     const cnnlTensorDescriptor_t box_index_desc,
                                     const void *box_index,
                                     cnnlCropAndResizeMode_t mode,
                                     float extrapolation_value,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:CropAndResizeBackwardImage
/*!
 * @brief Computes the gradients of images \b grads_image based on the gradients of crops \b grads,
 * bounding boxes \b boxes and index of each box referring to the corresponding images \b box_idx
 * to perform the backpropagation of ::cnnlCropAndResize operation.
 *
 * This function supports algorithms defined in ::cnnlCropAndResizeMode_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlCropAndResizeBackwardImage operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor of the crops in the backpropagation process.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor of the crops.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] box_idx_desc
 *   Input. The descriptor of box indices tensor.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] box_idx
 *   Input. Pointer to the MLU memory that stores the box indices tensor.
 *          \b box_idx[i] indicates the specific batch of image which the i-th box refers to.
 * @param[in] mode
 *   Input. The specific algorithm used to resize the crops extracted from the images.
 *          The algorithms are defined in ::cnnlCropAndResizeMode_t enum.
 * @param[out] grads_image_desc
 *   Output. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "CropAndResizeBackwardImage Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 *    This function supports the combinations of data types with the following order:
 *    - \b grads - \b boxes - \b box_idx - \b grads_image
 *      The supported combinations of data types are:
 *      - float - float - int32 - float
 *      - half - half - int32 - half
 *      - float - float - int32 - half
 *
 * @par Data Layout
 * - The supported data layout of \b grads, \b boxes, \b box_idx, \b grads_images tensor
 *   are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - boxes_idx tensor: \p CNNL_LAYOUT_ARRAY, only supports 1-D tensor.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The shape of \b grads should be [boxes_num, crop_height, crop_width, channels].
 * - The shape of \b boxes should be [boxes_num, 4].
 * - The shape of \b box_idx should be [boxes_num].
 * - The shape of \b grads_image should be [batch_num, image_height, image_width, channels].
 * - The value of \b box_idx should be in the range of [0, batch_num).
 * - Half data type is not recommended due to low precision
 *   and high possibility of numerical overflow, especially in any of the following cases:
 *   - \p boxes_num is big while \p batch_num is small.
 *   - |y2 - y1| is small.
 *   - |x2 - x1| is small.
 * - For half data type, the data value should be in the range of [-65504.0, 65504.0].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   \p channels is recommended to be aligned to 128 Bytes.
 *
 * @note
 * - \b boxes[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *   specifying 4 corner coordinates of the corresponding box in the i-th image.
 * - The [0, 1] interval of boxes coordinates y and x is mapped to [0, image_height -1]
 *   and [0, image_width - 1] in image coordinates respectively.
 * - y1 > y2 or x1 > x2 is allowed, indicating the crop is an up-down or
 *   left-right flipped version of the original image.
 * - Normalized coordinates outside [0, 1] interval are also allowed.
 * - Coordinates in \b box should not be NaN or infinity, otherwise the results of \b output are
 *   unpredictable.
 * - When \b input contains infinity:
 *   - On MLU300 series and CE3226:
 *     - \b output will contain infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/crop-and-resize-grad-image
 * - https://www.tensorflow.org/api_docs/cc/class/tensorflow/ops/crop-and-resize
 */
cnnlStatus_t CNNL_WIN_API cnnlCropAndResizeBackwardImage(cnnlHandle_t handle,
                                                         const cnnlTensorDescriptor_t grads_desc,
                                                         const void *grads,
                                                         const cnnlTensorDescriptor_t boxes_desc,
                                                         const void *boxes,
                                                         const cnnlTensorDescriptor_t box_idx_desc,
                                                         const void *box_idx,
                                                         cnnlCropAndResizeMode_t mode,
                                                         const cnnlTensorDescriptor_t
                                                               grads_image_desc,
                                                         void *grads_image);

/*****************************************************************************
 * Cambricon CNNL OP: CropAndResizeBackwardBoxes
 * ***************************************************************************/

// Group:CropAndResizeBackwardBoxes
/*!
 * @brief A function.
 *
 * This function is one of the backpropagation operations of CropAndResize operation,
 * with which the grads_boxes is computed. The other backpropagation operation is
 * cnnlCropAndResizeBackwardImage() which calculates grads_image.
 *
 * @param[in]  handle
 *  Input. A handle struct with information that all you need in this operation.
 * @param[in]  input_desc
 *  Input. A descriptor of 4-D input tensor,
 *         containing dimension, data type and layout information.
 *         The data type of input tensor should be half / float.
 * @param[in]  input
 *  Input. A pointer to the 4-D MLU input tensor on device.
 * @param[in]  image_desc
 *  Input. A descriptor of 4-D image tensor,
 *         containing dimension, data type and layout information.
 *         The data type of image tensor should be half / float.
 * @param[in]  image
 *  Input. A pointer to the 4-D MLU image tensor on device.
 * @param[in]  box_desc
 *  Input. A descriptor of 2-D box tensor,
 *         containing dimension, data type and layout information.
 *         The data type of box tensor should be the same with input's data type.
 *         in which box[i] is composed of 4 normalized coordinates [y1, x1, y2, x2],
 *         specifying 4 corner coordinates of the corresponding box in the i-th image.
 *         Both hight and width of images are normalized to [0, 1].
 *         Notice that y1 > y2 and x1 > x2 are allowed, indicating the resized crop is a flipped
 *         version of the original image.
 *         Besides, normalized coordinates outside [0, 1] interval are allowed.
 * @param[in]  box
 *  Input. A pointer to the 2-D MLU box tensor on device.
 * @param[in]  index_desc
 *  Input. A descriptor of 1-D index tensor,
 *         containing dimension, data type and layout information.
 *         The data type of index should be int32.
 *         The values of index should be in the range of [0, num_boxes),
 *         For instance, index[i] indicates the specific batch of image
 *         which the i-th box refers to.
 * @param[in]  index
 *   Input. A pointer to the 1-D MLU index tensor on device.
 * @param[in]  mode
 *   Input. The specific algorithm used in resize stage.
 *          This is defined in ::cnnlCropAndResizeMode_t enum.
 *          Only "Bilinear" is supported:
 *          CNNL_CROP_AND_RESIZE_BILINEAR stands for the Bilinear algorithm,
 *          where each pixel value of crops is determined by the 4 nearest corner points
 *          in original images with scaling factors calculated with bilinear algorithm.
 * @param[out]  output_desc
 *  Output. A descriptor of 4-D output tensor,
 *          containing dimension, data type and layout information.
 *          The data type of output tensor should be half / float.
 * @param[out]  output
 *  Output. A pointer to the 4-D MLU output tensor on device.
 *
 * @par Data Type
 * - By the order of input tensor - image tensor - boxes tensor - index tensor - output tensor,
 *   the supported data type combinations are as follows:
 *   - float - float - float - int32 - float
 *   - half - half - half - int32 - half
 *   - float - half - float - int32 - float
 * @par Data Layout
 *  - The support data layout of the input tensor, image tensor, box tensor, index tensor and
 *    output tensor are as follows:
 *    - input  tensor: \p CNNL_LAYOUT_NHWC
 *    - image  tensor: \p CNNL_LAYOUT_NHWC
 *    - box    tensor: \p CNNL_LAYOUT_ARRAY
 *    - index  tensor: \p CNNL_LAYOUT_ARRAY
 *    - output tensor: \p CNNL_LAYOUT_ARRAY
 *
 * @par Scale Limitation
 * - Data type of half is not recommended due to low precision
 *   and high possibility of numerial overflow.
 * - For half data type, the data value should be in the range of [-65504.0, 65504.0].
 *
 * @par Performance Optimization
 * - The size of C dimension of grads is suggested to be aligned to 128 bytes
 *   for better performance.
 *
 * @note
 *  - The support data shape of the input tensor, image tensor, box tensor, index tensor and
 *    output tensor are as follows:
 *    - input tensor: \p [num_boxes, crop_height, crop_width, channels]
 *    - image tensor: \p [batch_size, image_height, image_width, channels]
 *    - box tensor: \p [num_boxes, 4]
 *    - index tensor: \p [num_boxes]
 *    - output tensor: \p [num_boxes, 4]
 * @retval  CNNL_STATUS_SUCCESS
 *  The function ends normally.
 * @retval  CNNL_STATUS_BAD_PARAM
 *  Any one of the following conditions is satisfied:
 *   - Handle is NULL.
 *   - Input is NULL.
 *   - Image is NULL.
 *   - Box is NULL.
 *   - Index is NULL.
 *   - Output is NULL.
 *   - Dimension of input, image or output is not 4.
 *   - Dimension of box is not 2.
 *   - Dimension of index is not 1.
 *   - Shape of output is different with box's.
 *   - N dimension of input, box, index and output is not the same with each other.
 *   - Data type of each input and output tensor does not satisfy the above data type combinations.
 *   - Layout of input, image and output is not NHWC.
 *   - Layout of box and box_idx is not ARRAY.
 *   - Mode is not CNNL_CROP_AND_RESIZE_BILINEAR.
 */
cnnlStatus_t CNNL_WIN_API
cnnlCropAndResizeBackwardBoxes(cnnlHandle_t handle,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               const cnnlTensorDescriptor_t image_desc,
                               const void *image,
                               const cnnlTensorDescriptor_t box_desc,
                               const void *box,
                               const cnnlTensorDescriptor_t index_desc,
                               const void *index,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output,
                               cnnlCropAndResizeMode_t algorithm);

// Group:LSTM
/*!
 *  @brief A function.
 *  This function is used to get extra space size needed in cnnllstm operation, according to the
 * input param.
 *
 *  **Scale Limitation**
 *
 *    ci and co should be greater than 0.
 *
 *  @param[in]  handle
 *    Input. A struct with information that all you need in this operation.
 *  @param[in]  ci
 *    Input. The number of columns of the input.
 *  @param[in]  co
 *    Input. The number of columns of the state input.
 *  @param[out]  size
 *    Output. Extra space size needed in the cnnllstm operation.
 *  @retval CNNL_STATUS_SUCCESS
 *    The function ended normally.
 *  @retval CNNL_STATUS_BAD_PARAM
 *    One or more of the following conditions are met:
 *    - handle is empty.
 *    - size is empty.
 *    - ci or co less than or equal to 0.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetLstmWorkspaceSize(cnnlHandle_t handle,
                                                   const int ci,
                                                   const int co,
                                                   size_t *size);

// Group:Cumsum
/*!
 * @brief Gets cumulative sum of input tensor \b input according to \b axis.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumsum
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An int number that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A boolean value which determines whether the first row of \b axis of \b output is set
 *   to zero. If true, the first row of \b axis of \b output is set to zero, and the second row of
 *   \b axis of \b output equals to the first row of \b input. Otherwise, the first row of \b axis
 *   of \b output equals to the first row of \b input.
 * @param[in] reverse
 *   Input. A boolean value which determines whether to compute from the last row of \b axis of
 *   \b input to the fist row.
 * @param[in] nan_propagation
 *   Input. Enumeration variable which describes whether to propagate NaN elements. For detailed
 *   information, see ::cnnlNanPropagation_t. Only supports CNNL_NOT_PROPAGATE_NAN now.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Cumsum Operator" section in Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, half, float.
 *   - output tensor: int8, int16, int32, half, float.
 *
 * @par Scale Limitation
 * - The input tensor, axis, and output tensor must meet the following requirements:
 *   - input tensor: If the data type is int32, each element in \b input should be in the range
 *     of (-2^23, 2^23). If the data type is int8, int16 or int32, the intermediate results
 *     cannot exceed the value range of the corresponding data type.
 *   - \b axis: \b axis >= 0 and \b axis < \p input_dims.
 *   - output tensor: If the data type is int32, each element in \b output should be in the range
 *     of (-2^23, 2^23).
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumsum operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1, exclusive: false, reverse: false

     output array by 2 * 3 --> output: [[1,2,3],[5,7,9]]

     2.param:
       axis: 1, exclusive: true, reverse: false

     output array by 2 * 3 --> output: [[0,0,0],[1,2,3]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumsum.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCumsum(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     int axis,
                                     bool exclusive,
                                     bool reverse,
                                     cnnlNanPropagation_t nan_propagation,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Cumprod
/*!
 * @brief Gets the cumulative product of input tensor \b input along the given \b axis.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the cumprod
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] axis
 *   Input. An integer that specifies the dimension to compute over.
 * @param[in] exclusive
 *   Input. A boolean value which determines whether the first row of \b axis of \b output is set
 *   to one. If true, the first row of \b axis of \b output is set to one, and the second row of
 *   \b axis of \b output equals to the first row of \b input. Otherwise, the first row of \b axis
 *   of \b output equals to the first row of \b input. Currently, \b exclusive is not supported and needs to be set as false.
 * @param[in] reverse
 *   Input. A boolean value that specifies whether to compute from the last row of \b axis of
 *   \b input to the first row. Currently, \b reverse is not supported and needs to be set as false.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Cumprod Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Types
 * - This function supports the following data types for input tensor and output tensor.
 *   Data type of input tensor and output tensor should be the same.
 *   - input tensor: int8, int16, int32, half, float, bool.
 *   - output tensor: int8, int16, int32, half, float, bool.
 *
 * @par Scale Limitation
 *   - If the data type is int32, each element in \b input should be in the range
 *     of (-2^23, 2^23). If the data type is int8, int16 or int32, the intermediate results
 *     cannot exceed the value range of the corresponding data type.
 *   - The value of \b axis should be in the range of [-input_dims, input_dims -1].
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The examples of the cumprod operation are as follows:
     @verbatim
     input array by 2 * 3 --> input: [[1,2,3],[4,5,6]]

     1.param:
       axis: 1

     output array by 2 * 3 --> output: [[1,2,3],[4,20,120]]

     2.param:
       axis: 0

     output array by 2 * 3 --> output: [[1,2,6],[4,10,18]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.cumprod.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCumprod(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      int axis,
                                      bool exclusive,
                                      bool reverse,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:GroupNorm
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the GroupNormForward operation.
 *
 * The size of extra workspace is based on the given information \b group_num of the GroupNormForward
 * operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   GroupNormForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   GroupNormForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlGroupNormForward_v2.
 *   ::cnnlGroupNormForward does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormForwardWorkspaceSize(cnnlHandle_t handle,
                                     int group_num,
                                     const cnnlTensorDescriptor_t x_desc,
                                     size_t *workspace_size);

/*!
 * @brief Applies group normalization over a mini-batch of inputs. Group normalization divides the channels
 *        into groups and computes the mean and variance for normalization within each group. Computation
 *        of group normalization is independent of batch sizes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input \b scale and \b bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \b scale tensor.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the \b bias tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GroupNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below：
 *   - half(input_tensor) - half(scale_tensor) - half(bias_tensor) - half(output_tensor).
 *   - float(input_tensor) - float(scale_tensor) - float(bias_tensor) - float(output_tensor).
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be
 *   \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - C dimension of tensor \b x is dividable by \b group_num.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the group normalization forward operation is as follows:
     @verbatim
      \b eps: 0.00001
      \b group_num: 3
      \b x: NHWC [2, 3, 4, 9]
      \b scale and \b bias: array [9]
      \b y: NHWC [2, 3, 4, 9] same with input
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py
 */
cnnlStatus_t CNNL_WIN_API cnnlGroupNormForward(cnnlHandle_t handle,
                                               float eps,
                                               int group_num,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t scale_bias_desc,
                                               const void *scale,
                                               const void *bias,
                                               const cnnlTensorDescriptor_t y_desc,
                                               void *y);

/*!
 * @brief Applies group normalization over a mini-batch of inputs. Group normalization divides the channels
 *        into groups and computes the mean and variance for normalization within each group. Computation
 *        of group normalization is independent of batch sizes.
 *
 * Compared with ::cnnlGroupNormForward, this function requires you to allocate some extra workspace
 * as an input parameter.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] eps
 *   Input. A float value added to the denominator for numerical stability.
 * @param[in] group_num
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] scale_bias_desc
 *   Input. The descriptor of the input \b scale and \b bias tensors. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \b scale tensor.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores the \b bias tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGroupNormForward_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlGroupNormForward_v2. You can get the size of the workspace with
 *   the ::cnnlGetGroupNormForwardWorkspaceSize function.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "GroupNorm Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below:
 *   - half(input_tensor) - half(scale_tensor) - half(bias_tensor) - half(output_tensor).
 *   - float(input_tensor) - float(scale_tensor) - float(bias_tensor) - float(output_tensor).
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensors must be
 *   \p CNNL_LAYOUT_NHWC or \p CNNL_LAYOUT_NCHW.
 *
 * @par Scale Limitation
 * - C dimension of tensor \b x is dividable by \b group_num.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the group normalization forward operation is as follows:
     @verbatim
      \b eps: 0.00001
      \b group_num: 3
      \b x: NHWC [2, 3, 4, 9]
      \b scale and \b bias: array [9]
      \b y: NHWC [2, 3, 4, 9] same with input
     @endverbatim
 *
 * @par Reference
 * - http://github.com/pytorch/pytorch/blob/master/torch/nn/modules/normalization.py
 */
cnnlStatus_t CNNL_WIN_API cnnlGroupNormForward_v2(cnnlHandle_t handle,
                                                  float eps,
                                                  int group_num,
                                                  const cnnlTensorDescriptor_t x_desc,
                                                  const void *x,
                                                  const cnnlTensorDescriptor_t scale_bias_desc,
                                                  const void *scale,
                                                  const void *bias,
                                                  void *workspace,
                                                  size_t workspace_size,
                                                  const cnnlTensorDescriptor_t y_desc,
                                                  void *y);

// Group:GroupNorm
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used
 *   to store the temporary result of the ::cnnlGroupNormBackward operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlGroupNormBackward
 * operation. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ::cnnlGroupNormBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] NC
 *   Input. The product of batch and channel. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGroupNormBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlGroupNormBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetGroupNormBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const int32_t NC,
                                      size_t *workspace_size);
// Group:GroupNorm
/*!
 * @brief Performs the backward group normalization computation. Group normalization divides the channels
 *   into groups and computes the mean and variance for normalization in every group, and the backward group
 *   normalization normalizes over the dimension of the input tensor \b x within each group.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the group normalization
 *   backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   The descriptor of the \b x tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x. The shape of \b x
 *   is [batch, channel, height, width].
 * @param[in] diff_z_desc
 *   The descriptor of the input tensor \b diff_z. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] diff_z
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_z. The shape of \b diff_z
 *   is [batch, channel, height, width].
 * @param[in] gamma_desc
 *   The descriptor of the input tensor \b gamma. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] gamma
 *   Input. Pointer to the MLU memory that stores the input tensor \b gamma.
 * @param[in] mean_desc
 *   The descriptor of the \b mean tensor. For detailed information,
     see ::cnnlTensorDescriptor_t.
 * @param[in] mean
 *   Input. Pointer to the MLU memory that stores the input tensor \b mean, which is computed during
 *   the forward phase from the ::cnnlGroupNormForward call. The shape of \b mean
 *   is [batch, G].
 * @param[in] rstd_desc
 *   The descriptor of the result differential tensor \b rstd. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] rstd
 *   Input. Pointer to the MLU memory that stores the input tensor \b rstd, which is computed during
 *   the forward phase from the ::cnnlGroupNormForward call. The shape of \b rstd is [batch, G].
 * @param[in] G
 *   Input. An integer value that specifies the number of groups to separate the channels (C) into.
 * @param[in] diff_x_desc
 *   Input. Pointer to the MLU memory that stores the input tensor \b diff_x. For detailed information,
 *   see::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_x.
 * @param[in] diff_scale_desc
 *   The descriptor of the \b diff_scale tensor.
 *   For detailed information, see::cnnlTensorDescriptor_t.
 * @param[out] diff_scale
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_scale.
 * @param[in] diff_bias_desc
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias. For detailed
 *   information, see::cnnlTensorDescriptor_t.
 * @param[out] diff_bias
 *   Output. Pointer to the MLU memory that stores the output tensor \b diff_bias.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGroupNormBackward.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlGroupNormBackward. You can get the size of the workspace with
 *   the ::cnnlGetGroupNormForwardWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - Before calling this function to implement group normalization backward, you need to prepare
 *   all the parameters passed to this function. See each parameter description for details.
 *
 * @par Formula
 * - See "GroupNormBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensors should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b x, \b diff_z, \b gamma, \b mean,
 *   \b rstd, \b diff_x, \b diff_scale and \b diff_bias are
 *   as follows:
 *   - x tensor: \p CNNL_LAYOUT_NCHW.
 *   - diff_z tensor: \p CNNL_LAYOUT_NCHW.
 *     The layout of the diff_z tensor should be the same as \b x, \b diff_z.
 *   - gamma tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *   - mean tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 2-D.
 *   - rstd tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 2-D.
 *   - diff_x tensor: \p CNNL_LAYOUT_NCHW.
 *     The layout of the \b diff_x tensor should be the same as \b x tensor.
 *   - diff_scale tensor  tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *   - diff_bias tensor: \p CNNL_LAYOUT_ARRAY, and the dimension must be 1-D.
 *
 * @note
 * - This function is only supported on MLU370 series.
 * - All the inputs contain NaN or infinity are not supported.
 *
 * @par Requirement
 * - None.
 *
 * @par Example
 * - The example of the groupnorm backward operation is as follows:
     @verbatim
      input :
      --> x = [[[[1.0, 2.0]],[[3.0, 4.0]],[[5.0, 6.0]],[[7.0, 8.0]]]]]

      --> diff_dz = [[[[1000.0, 2000.0]],[[5000.0, 6000.0]],
                      [[1100.0, 1200.0]], [1500.0, 1600.0]]]]

      --> scale= [1, 2, 4, 8]

      --> mean= [2.5,6.5]

      --> saved_invstd: [0.8944236755372, 0.8944236755372]

      param:
        G: 2

      output :
      --> diff_x = [[[[  804.9370, -1967.7463]],
                     [[ 1520.5354,  -357.7249]],
                     [[  679.7275, -1860.4131]],
                     [[ 1681.5283,  -500.8418]]]]

      --> diff_scale = [-4472.1177, 20571.7422, -4024.9060,  5634.8682]

      --> diff_bias = [ 6000., 22000.,  4600.,  6200.]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.com/pytorch/pytorch/blob/v1.6.0/torch/nn/functional.py#L2052
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlGroupNormBackward(cnnlHandle_t handle,
                      const cnnlTensorDescriptor_t x_desc,
                      const void *x,
                      const cnnlTensorDescriptor_t diff_z_desc,
                      const void *diff_z,
                      cnnlTensorDescriptor_t gamma_desc,
                      const void *gamma,
                      const cnnlTensorDescriptor_t mean_desc,
                      const void *mean,
                      const cnnlTensorDescriptor_t rstd_desc,
                      const void *rstd,
                      const int64_t G,
                      const cnnlTensorDescriptor_t diff_x_desc,
                      void *diff_x,
                      const cnnlTensorDescriptor_t diff_scale_desc,
                      void *diff_scale,
                      const cnnlTensorDescriptor_t  diff_bias_desc,
                      void *diff_bias,
                      void *workspace,
                      size_t workspace_size);
// Group:Softplus
/*!
 * @brief Computes softplus on input tensor \b x, and returns the results in the output
 *        tensor \b y.
 *
 * Softplus Forward is used in activation operation to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   softplus_forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. The β value used in the softplus_forward operation. For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide". The default value of
 *   beta is 1.
 * @param[in] threshold
 *   Input. The threshold value used in the softplus_forward operation. For detailed information, see
 *   "SoftplusForward Operation" section in "Cambricon CNNL User Guide". The default value of
 *   threshold is 20.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftplusForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Scale Limitation
 * - The value of \b beta should not equal to 0.
 * - The value of \b threshold should not be greater than 11 when data type is half, and should not be greater than 88
 *   when data type is float on MLU200 series.
 * - The value of \b threshold should not be greater than 11 when data type is half on MLU300 series.
 * - The element value of \b x should be greater than -7.75 to get high-precision result on MLU200 series.
 *
 * @note
 * - When the element value of \b x is less than -7.75 for the data type of float,
 *   the result is 6e-8; When the element value of \b x is less than -7.75 for the
 *   data type of half, the result is 5.96e-8 on MLU200 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the softplus forward operation is as follows:
     @verbatim
      input one array by 2 * 3
      --> input: [[0.0, 3.4, 30],[-4.3, -1.2, -0.5]]

      param:
        beta: 1
        threshold: 20

      output array by 2 * 3
      --> output: [[0.69194, 3.43291, 30],[0.01348, 0.26330, 0.47297]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftplusForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t x_desc,
                                              const void *x,
                                              const cnnlTensorDescriptor_t y_desc,
                                              void *y,
                                              const int beta,
                                              const int threshold);

// Group:Prelu
/*!
 * @brief Computes PReLu on input tensor \b x with alpha tensor \b alpha,
 *        and returns the results in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the prelu operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_alpha
 *   Input. The descriptor of the alpha tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the learned coefficient alpha tensor.
 * @param[in] desc_y
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Prelu Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor, alpha tensor and output tensor must be the same.
 * - The supported data types of input tensor, alpha tensor and output tensor are as follows:
 *   - input tensor: half, float.
 *   - alpha tensor: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - Shape of input tensor and output tensor must be the same.
 * - Dim of input tensor and output tensor must be the same.
 * - The supported shape of input tensor and alpha tensor are as follows:
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., 1, ..., 1]
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., 1, ..., c]
 *   - input shape: [a, ..., b, ..., c], alpha shape: [1, ..., b, ..., 1]
 *
 * @par Example
 * - The example of the select operation is as follows:
     @verbatim
      input two arrays by 2 * 3 and 1 * 3
      --> input: [[-0.3, 0.8, -0.5], [0.6, -0.4, -0.7]]

      --> alpha: [0.1, 0.2, 0.3]

      output array by 2 * 3
      --> output: [[-0.03, 0.8, -0.15], [0.6, -0.08, -0.21]]

     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/keras/layers/PReLu
 */
cnnlStatus_t CNNL_WIN_API cnnlPrelu(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t desc_x,
                                    const void *x,
                                    const cnnlTensorDescriptor_t desc_alpha,
                                    const void *alpha,
                                    const cnnlTensorDescriptor_t desc_y,
                                    void *y);

// Group:PreluBackward
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used
 *   to store the temporary result of the ::cnnlPreluBackward operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlPreluBackward
 * operation. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of alpha_grad tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlPreluBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The input \b alpha_grad_desc should not be null.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPreluBackward function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetPreluBackwardWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t alpha_grad_desc,
                                  size_t *workspace_size);

// Group:PreluBackward
/*!
 * @brief Implements the backward propagation for the prelu function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlPreluBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \b grad_output tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the \b grad_output tensor,
 *          \b grad_output  is the input gradient of ReplicationPad.
 * @param[in] alpha_desc
 *   Input. The descriptor of \b alpha tensor. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the learned coefficient alpha tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlPreluBackward
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlPreluBackward operation. You can get the size of the workspace with the
 *          ::cnnlGetPreluBackwardWorkspaceSize function.
 * @param[in] x_grad_desc
 *   Input. The descriptor of \b x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] x_grad
 *   Output. Pointer to the MLU memory that stores the output tensor,
 *           the gradient calculation result of \b x.
 * @param[in] alpha_grad_desc
 *   Input. The descriptor of \b alpha_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] alpha_grad
 *   Output. Pointer to the MLU memory that stores the \b alpha_grad tensor.
 *           \b alpha_grad is the calculated gradient result of \b alpha.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,:: CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Prelu Backward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \b x, \b grad_output, \b alpha, \b x_grad and \b alpha_grad
 *     should be the same.
 * - The supported data types of \b x, \b grad_output, \b alpha, \b x_grad and
 *     \b alpha_grad are as follows:
 *   - x: half, float.
 *   - grad_output: half, float.
 *   - alpha: half, float.
 *   - x_grad: half, float.
 *   - alpha_grad: half, float.
 *
 * @par Scale Limitation
 *   - None.
 *
 * @par API Dependency
 * - Before calling this function you need to call ::cnnlGetPreluBackwardWorkspaceSize
 *   to get the extra space size needed in ::cnnlPreluBackward operation.
 *
 * @note
 * - The shape of the \b x, \b grad_output and \b x_grad must be the same.
 * - The shape of the \b alpha and \b alpha_grad must be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cuda/Activation.cu
*/
cnnlStatus_t CNNL_WIN_API
cnnlPreluBackward(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t x_desc,
                  const void *x,
                  const cnnlTensorDescriptor_t grad_output_desc,
                  const void *grad_output,
                  const cnnlTensorDescriptor_t alpha_desc,
                  const void *alpha,
                  void *workspace,
                  size_t workspace_size,
                  const cnnlTensorDescriptor_t x_grad_desc,
                  void *x_grad,
                  const cnnlTensorDescriptor_t alpha_grad_desc,
                  void *alpha_grad);
// Group:Trigon
/*!
 * @brief Computes trigonometric functions on input tensor \b x, and returns the results in the
 *        output tensor \b y.
 *
 * Trigon Forward is used in activation operation to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   trigon operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  trigon_desc
 *   Input. The descriptor of the trigon operation. It contains trigon function mode
 *   which is defined in ::cnnlTrigonFunctionMode_t and computation function mode
 *   which is defined in ::cnnlComputationPreference_t.
 * @param[in]  x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Trigon Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensor and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Scale Limitation
 * - None
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \b trigon_desc and to call the ::cnnlSetTrigonDescriptor
 *   function to set the information.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 *
 * - Now only asin, acos and atan support ultrahigh precision mode.
 *
 * - On MLU200 series:
 *
 *   - Value range limit of input data:
 *
 *     - CNNL_COMPUTATION_FAST and CNNL_COMPUTATION_HIGH_PRECISION
 *
 *       - Sin and Cos: [-2π, 2π]
 *       - Tan: [-1.0, 1.0]
 *       - Asin and Acos:
 *         - half: [-0.9, 0.9]
 *         - float: [-1.0. 1.0]
 *       - Atan:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Sinh:
 *         - half: [-4.1815, 4.1815]
 *         - float: [-42.7536, 42.7536]
 *       - Cosh:
 *         - half: [-4.8749, 4.8749]
 *         - float: [-42.7460, 42.7460]
 *       - Tanh:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Asinh: [-100.0, 100.0]
 *       - Acosh: (1.0, 100.0]
 *       - Atanh: [-0.8, 0.8]
 *
 *   - When input data contains NaN/infinity:
 *
 *     - SIN: If \b x is NaN, then \b y is 1.
 *            If \b x is infinity, then \b y is 1.
 *     - COS: If \b x is NaN, then \b y is 0.
 *            If \b x is infinity, then \b y is 1.
 *     - TAN: If \b x is NaN, then \b y is 0.
 *            If \b x is infinity, then \b y is 0.
 *     - ASIN: If \b x is NaN, then \b y is positive saturation value.
 *             If \b x is infinity, then \b y is positive saturation value.
 *     - ACOS: If \b x is NaN, then \b y is positive saturation value.
 *             If \b x is infinity, then \b y is positive saturation value.
 *     - ATAN: If \b x is NaN, then \b y is 0.
 *             If \b x is positive infinity, then \b y is π/2.
 *             If \b x is negative infinity, then \b y is -π/2.
 *     - SINH: If \b x is NaN, then \b y is -0.00755828.
 *             If \b x is positive infinity, then \b y is 1.84778e18.
 *             If \b x is negative infinity, then \b y is -1.84778e18.
 *     - COSH: If \b x is NaN, then \b y is 1.
 *             If \b x is positive infinity, then \b y is -1.84778e18.
 *             If \b x is negative infinity, then \b y is 1.84778e18.
 *     - TANH: If \b x is NaN, then \b y is 1.
 *             If \b x is positive infinity, then \b y is 1.
 *             If \b x is negative infinity, then \b y is -1.
 *     - ASINH: If \b x is NaN, then \b y is -0.00757841.
 *              If \b x is positive infinity, then \b y is 23.7978.
 *              If \b x is negative infinity, then \b y is -23.5978.
 *     - ACOSH: If \b x is NaN, then \b y is 23.7648.
 *              If \b x is positive infinity, then \b y is 23.5978.
 *              If \b x is negative infinity, then \b y is 0.123092.
 *     - ATANH: If \b x is NaN, then \b y is 0.
 *              If \b x is positive infinity, then \b y is 0.
 *              If \b x is negative infinity, then \b y is 0.
 *
 * - On MLU300 series and CE3226:
 *
 *   - Value range limit of input data:
 *
 *     - CNNL_COMPUTATION_FAST
 *
 *       - Sin and Cos: [-2π, 2π]
 *       - Tan: [-1.0, 1.0]
 *       - Asin and Acos:
 *         - half: [-0.9, 0.9]
 *         - float: [-1.0. 1.0]
 *       - Atan:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Sinh:
 *         - half: [-4.1815, 4.1815]
 *         - float: [-42.7536, 42.7536]
 *       - Cosh:
 *         - half: [-4.8749, 4.8749]
 *         - float: [-42.7460, 42.7460]
 *       - Tanh:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Asinh: [-100.0, 100.0]
 *       - Acosh: (1.0, 100.0]
 *       - Atanh: [-0.8, 0.8]
 *
 *     - CNNL_COMPUTATION_HIGH_PRECISION
 *
 *       - Tan: obtains low precision result when input is around π/2+kπ.
 *       - Asin and Acos:
 *         - half: [-0.9, 0.9]
 *         - float: [-1.0. 1.0]
 *       - Atan:
 *         - half: [-65504, 65504]
 *         - float: [-3.4e38, 3.4e38]
 *       - Sinh:
 *         - float: [-∞, -89.41599] and [-88.72283, 88.72283] and [89.41599, +∞]
 *       - Cosh:
 *         - float: [-∞, -89.41599] and [-88.72283, 88.72283] and [89.41599, +∞]
 *       - Asinh
 *         - float: [-18446742974197923840, 18446742974197923840]
 *       - Acosh:
 *         - float: [-∞, 18446742974197923840]
 *
 *   - When input data contains NaN/infinity:
 *
 *     - SIN: If \b x is NaN, then \b y is NaN.
 *            If \b x is infinity, then \b y is NaN.
 *     - COS: If \b x is NaN, then \b y is NaN.
 *            If \b x is infinity, then \b y is NaN.
 *     - TAN: If \b x is NaN, then \b y is NaN.
 *            If \b x is infinity, then \b y is NaN.
 *     - ASIN: If \b x is NaN, then \b y is NaN.
 *             If \b x is infinity, then \b y is NaN.
 *     - ACOS: If \b x is NaN, then \b y is NaN.
 *             If \b x is infinity, then \b y is NaN.
 *     - ATAN: If \b x is NaN, then \b y is NaN.
 *             If \b x is infinity, then \b y is NaN.
 *     - SINH: If \b x is NaN, then \b y is NaN.
 *             If \b x is positive infinity, then \b y is positive infinity.
 *             If \b x is negative infinity, then \b y is negative infinity.
 *     - COSH: If \b x is NaN, then \b y is NaN.
 *             If \b x is positive infinity, then \b y is positive infinity.
 *             If \b x is negative infinity, then \b y is positive infinity.
 *     - TANH: If \b x is NaN, then \b y is NaN.
 *             If \b x is positive infinity, then \b y is 1.
 *             If \b x is negative infinity, then \b y is -1.
 *     - ASINH: If \b x is NaN, then \b y is NaN.
 *              If \b x is positive infinity, then \b y is positive infinity.
 *              If \b x is negative infinity, then \b y is negative infinity.
 *     - ACOSH: If \b x is NaN, then \b y is NaN.
 *              If \b x is positive infinity, then \b y is positive infinity.
 *              If \b x is negative infinity, then \b y is NaN.
 *     - ATANH: If \b x is NaN, then \b y is NaN.
 *              If \b x is positive infinity, then \b y is NaN.
 *              If \b x is negative infinity, then \b y is NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the trigon forward operation is as follows:
     @verbatim
      input one array by 2 * 3
      --> input: [[1.0, 1.2, 1.4],[-0.8, -1.2, -1.1]]

      param:
        mode: TRIGON_SIN
        prefer: CNNL_COMPUTATION_HIGH_PRECISION

      output array by 2 * 3
      --> output: [[0.84147, 0.93204, 0.98545],[-0.71736, -0.93204, -0.89121]]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/sin
 * - https://www.tensorflow.org/api_docs/python/tf/cos
 * - https://www.tensorflow.org/api_docs/python/tf/tan
 * - https://www.tensorflow.org/api_docs/python/tf/asin
 * - https://www.tensorflow.org/api_docs/python/tf/acos
 * - https://www.tensorflow.org/api_docs/python/tf/atan
 * - https://www.tensorflow.org/api_docs/python/tf/sinh
 * - https://www.tensorflow.org/api_docs/python/tf/cosh
 * - https://www.tensorflow.org/api_docs/python/tf/tanh
 * - https://www.tensorflow.org/api_docs/python/tf/asinh
 * - https://www.tensorflow.org/api_docs/python/tf/acosh
 * - https://www.tensorflow.org/api_docs/python/tf/atanh
 */
cnnlStatus_t CNNL_WIN_API cnnlTrigonForward(cnnlHandle_t handle,
                                            const cnnlTrigonDescriptor_t trigon_desc,
                                            const cnnlTensorDescriptor_t x_desc,
                                            const void *x,
                                            const cnnlTensorDescriptor_t y_desc,
                                            void *y);
// Group:Trigon
/*!
 * @brief Creates a descriptor pointed by \b trigon_desc for a trigon forward operator.
 *
 * @param[in] trigon_desc
 *   Input. A host pointer to the trigon forward descriptor that holds the trigonometric
 *   function mode which is defined in ::cnnlTrigonFunctionMode_t and computation
 *   preference mode which is defined in ::cnnlComputationPreference_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetTrigonDescriptor function to initialize
 *   and set the information to trigon descriptor.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateTrigonDescriptor(cnnlTrigonDescriptor_t *trigon_desc);

// Group:Trigon
/*!
 * @brief Initializes the trigon descriptor \b trigon_desc that is previously created with
 *        the ::cnnlCreateTrigonDescriptor function, and sets the trigonometric function mode
 *        \b mode which is defined in ::cnnlTrigonFunctionMode_t.
 *
 * @deprecated
 *   ::cnnlSetTrigonDescriptor is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlSetTrigonDescriptor_v2 instead, which supports parameter of \b prefer
 *   that sets computation preference.
 *
 * @param[in] trigon_desc
 *   Input. The descriptor of the trigon operation. It contains trigonometric function mode which
 *   is defined in ::cnnlTrigonFunctionMode_t.
 * @param[in] mode
 *   Input. A trigonometric function mode which is defined in ::cnnlTrigonFunctionMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \b trigon_desc.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - The default value of prefer is set to CNNL_COMPUTATION_HIGH_PRECISION.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTrigonDescriptor(cnnlTrigonDescriptor_t trigon_desc,
                                                  cnnlTrigonFunctionMode_t mode);
// Group:Trigon
/*!
 * @brief Initializes the trigon descriptor \b trigon_desc that is previously created with
 *        the ::cnnlCreateTrigonDescriptor function, and sets the trigonometric function mode
 *        \b mode which is defined in ::cnnlTrigonFunctionMode_t and computation preference mode
 *        \b prefer which is defined in ::cnnlComputationPreference_t.
 *
 * @param[in] trigon_desc
 *   Input. The descriptor of the trigon operation. It contains trigonometric function mode which
 *   is defined in ::cnnlTrigonFunctionMode_t and computation preference mode which is defined
 *   in ::cnnlComputationPreference_t.
 * @param[in] mode
 *   Input. A trigonometric function mode which is defined in ::cnnlTrigonFunctionMode_t.
 * @param[in] prefer
 *   Input. A computation preference mode which is defined in ::cnnlComputationPreference_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlCreateTrigonDescriptor
 *   function to create the \b trigon_desc.
 * - You need to call the ::cnnlDestroyTrigonDescriptor function to destroy the descriptor.
 *
 * @note
 * - Only asin, acos and atan support ultrahigh precision computation mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetTrigonDescriptor_v2(cnnlTrigonDescriptor_t trigon_desc,
                                                     cnnlTrigonFunctionMode_t mode,
                                                     cnnlComputationPreference_t prefer);

// Group:Trigon
/*!
 * @brief Destroys a trigon descriptor \b trigon_desc that is previously created with
 *        the ::cnnlCreateTrigonDescriptor function.
 *
 * The trigon descriptor is defined in ::cnnlTrigonDescriptor_t and holds the information
 * about trigonometric function mode and computation preference mode.
 *
 * @param[in] trigon_desc
 *   Input. The trigon descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - You need to call this function after calling the ::cnnlTrigonForward function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the trigon descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyTrigonDescriptor(cnnlTrigonDescriptor_t trigon_desc);

// Group:Nms
/*!
 *  @brief Creates a descriptor pointed by \b desc for Nms operation, and allocates
 *         memory for holding the information about the Nms operation. The information
 *         is defined in ::cnnlNmsDescriptor_t. For more information about descriptor,
 *         see "Cambricon CNNL User Guide"
 *
 *  @param[out] desc
 *   Output. A host pointer to the Nms descriptor that holds information about the Nms operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlSetNmsDescriptor, ::cnnlSetNmsDescriptor_v2,
 *    ::cnnlSetNmsDescriptor_v3 or ::cnnlSetNmsDescriptor_v4 function to initialize and set the
 *    information to Nms descriptor.
 *  - You need to call the ::cnnlDestroyNmsDescriptor function to destroy the descriptor.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateNmsDescriptor(cnnlNmsDescriptor_t *desc);

// Group:Nms
/*!
 *  @brief Destroys a Nms descriptor \b desc that is previously created with the
 *         ::cnnlCreateNmsDescriptor function.
 *
 *  The Nms descriptor is defined in ::cnnlNmsDescriptor_t and holds the information
 *  about the Nms operation.
 *
 *  @param[in] desc
 *    Input. The Nms descriptor to be destroyed.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED, ::CNNL_STATUS_BAD_PARAM
 *
 *  @note
 *  - You need to call this function after calling the ::cnnlNms_v2 function.
 *    Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 *  - This function should be called to destroy the Nms descriptor. Otherwise, the
 *    memory leak may occur.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyNmsDescriptor(cnnlNmsDescriptor_t desc);

// Group:Nms
/*!
 * @brief Initializes the Nms descriptor \b desc that is previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the Nms operation
 * to the Nms descriptor \b desc. The information includes the output mode \mode, the
 * intersection over union threshold \b iou_threshold, the maximum output size \b max_output_size,
 * the score threshold \b confidence_threshold, the data layout of input \b input_layout,
 * the running mode of MLU \b run_mode.
 *
 * @deprecated
 *   ::cnnlSetNmsDescriptor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetNmsDescriptor_v2, ::cnnlSetNmsDescriptor_v3 or ::cnnlSetNmsDescriptor_v4 instead.
 *   Compared with ::cnnlSetNmsDescriptor, the parameter of \b run_mode is deprecated in ::cnnlSetNmsDescriptor_v2.
 *   ::cnnlSetNmsDescriptor_v3 supports the parameters of \b algo and \b offset. ::cnnlSetNmsDescriptor_v4
 *   supports the parameters of \b box_mode, \b method_mode and \b soft_nms_sigma, and the \b algo is deprecated.
 *
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] mode
 *   Input. The output mode of Nms descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union(iou) threshold used in Nms computation.
 *   Boxes would be filtered out if the intersection over union is greater than \b iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in Nms computation.
 *   Boxes would be filtered out directly if the confidence of boxes are less than this
 *   threshold.
 * @param[in] input_layout
 *   Input. The input boxes layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4] and 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 * @param[in] run_mode
 *   Input. The supported core version kernel. 0 reepresents block and 1 represents union1.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \b confidence_threshold, it should be in the range of [0, 1].
 * - For the \b max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor(cnnlNmsDescriptor_t desc,
                                               const cnnlNmsOutputMode_t mode,
                                               const float iou_threshold,
                                               const int max_output_size,
                                               const float confidence_threshold,
                                               const int input_layout,
                                               const int run_mode);

// Group:Nms
/*!
 * @brief Initializes the Nms descriptor \b desc that is previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the Nms operation
 * to the Nms descriptor \b desc. The information includes the output mode \mode, the
 * intersection over union threshold \b iou_threshold, the maximum output size \b max_output_size,
 * the score threshold \b confidence_threshold, the data layout of input \b input_layout. The
 * difference between this function and ::cnnlSetNmsDescriptor is that this function removes
 * the \b run_mode. To set computation algorithm and the boundary offset in the Nms operation,
 * call ::cnnlSetNmsDescriptor_v3 function.
 *
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] mode
 *   Input. The output mode of Nms descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in Nms computation.
 *   Boxes would be filtered out if the intersection over union is greater than \b iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in Nms computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4] and 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \b confidence_threshold, it should be in the range of [0, 1].
 * - For the \b max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v2(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsOutputMode_t mode,
                                                  const float iou_threshold,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Initializes the Nms descriptor \b desc that is previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the Nms operation
 * to the Nms descriptor \b desc. The information includes the output mode \mode, the
 * intersection over union threshold \b iou_threshold, the computation algorithm \algo,
 * the maximum output size \b max_output_size, the score threshold \b confidence_threshold,
 * the offset of boundary \offset, the data layout of input \b input_layout. The main
 * difference between this function and ::cnnlSetNmsDescriptor_v2 is that this function
 * supports setting computation algorithm and the offset of boundary.
 *
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] mode
 *   Input. The output mode of Nms descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] algo
 *   Input. The computation algorithm of Nms operation. For detailed information,
 *   see ::cnnlNmsAlgo_t.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in Nms computation.
 *   Boxes would be filtered out if the intersection over union is greater than or equal to \b iou_threshold.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in Nms computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] offset
 *   Input. The offset size of boundary used in Nms computation.
 *   This value would be used when \algo is ::CNNL_NMS_ALGO_INCLUDE_BOUNDARY.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4] and 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - For the \b confidence_threshold, it should be in the range of [0, 1].
 * - For the \b offset, it should be 0.0 or 1.0.
 * - For the \b max_output_size, it should not be less than 0.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v3(cnnlNmsDescriptor_t desc,
                                                  const cnnlNmsOutputMode_t mode,
                                                  const cnnlNmsAlgo_t algo,
                                                  const float iou_threshold,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const float offset,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Initializes the Nms descriptor \b desc that is previously created with the
 * ::cnnlCreateNmsDescriptor function, and sets the information about the Nms operation
 * to the Nms descriptor \b desc. The information includes the box data structure mode
 * \b box_mode, the output mode \b mode, the confidence update method mode \b method_mode,
 * the intersection over union threshold \b iou_threshold, the parameter sigma for soft Nms
 * with Gaussian method \b soft_nms_sigma, the maximum output size \b max_output_size, the
 * score threshold \b confidence_threshold, the offset of boundary \b offset, and the data
 * layout of input \b input_layout. The main difference between this function and the
 * ::cnnlSetNmsDescriptor_v3 is that this function supports box data structure mode, the
 * confidence update method mode, and the soft Nms parameter sigma. Besides, \b algo is
 * deprecated in this function. One important thing is that when the dimension of input box
 * is 2 ([boxes_num, 4] or [4, boxes_num]) rather than 3 ([batches_num, boxes_num, 4] or
 * [batches_num, 4, boxes_num]), the new features above will not work, and it is recommended
 * to use ::cnnlSetNmsDescriptor_v3. In addition, because of the different implementation,
 * when the dimension of input box is 2, it has better performance.
 *
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in] box_mode
 *   Input. The box data sturcture mode of Nms descriptor to be set. For detailed information,
 *   see ::cnnlNmsBoxPointMode_t.
 * @param[in] mode
 *   Input. The output mode of Nms descriptor to be set. For detailed information,
 *   see ::cnnlNmsOutputMode_t.
 * @param[in] method_mode
 *   Input. The confidence update method mode. For detailed information,
 *   see ::cnnlNmsMethodMode_t.
 *   Please note that method mode 1 and 2 are not supported in current version,
 *   and it will be supported in the future.
 * @param[in] iou_threshold
 *   Input. The intersection over union (iou) threshold used in Nms computation.
 *   Boxes would be filtered out if the intersection over union is greater than or equal to \b iou_threshold.
 * @param[in] soft_nms_sigma
 *   Input. The parameter used in soft Nms with Gaussian method.
 *   This value would be used when method_mode is ::CNNL_NMS_SOFT_NMS_GAUSSIAN.
 * @param[in] max_output_size
 *   Input. The maximum number of output boxes. If the dimension of input box is 3, i.e.
 *   [batches_num, boxes_num, 4] or [batches_num, 4, boxes_num], this parameter indicates
 *   the maximum number of output boxes per class.
 * @param[in] confidence_threshold
 *   Input. The confidence threshold used in Nms computation.
 *   Boxes would be filtered out directly if the confidence of boxes are no more than this
 *   threshold.
 * @param[in] offset
 *   Input. The offset size of boundary used in Nms computation.
 *   This value would be used when \algo is ::CNNL_NMS_ALGO_INCLUDE_BOUNDARY.
 * @param[in] input_layout
 *   Input. The input data layout. Supported values are 0 and 1. 0 represents
 *   [boxes_num, 4], [boxes_num, 7] or [batches_num, boxes_num, 4] and 1 represents
 *   [4, boxes_num], [7, boxes_num] or [batches_num, 4, boxes_num].
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - \b confidence_threshold should be in the range of [0, 1].
 * - \b soft_nms_sigma should not be less than 0.0.
 * - \b offset should be 0.0 or 1.0.
 * - \b input_layout should be 0 or 1.
 * - \b max_output_size should not be less than 0.
 *
 * @note
 * - If the dimension of input box is 3 ([batches_num, boxes_num, 4] or
 *   [batches_num, 4, boxes_num]), the dimension of input confidence is
 *   3 ([batches_num, classes_num, boxes_num]). If the dimension of input box
 *   is 2 ([boxes_num, 4], [4, boxes_num], [boxes_num, 7] or [7, boxes_num]),
 *   the dimension of input confidence is 1 ([boxes_num]).
 * - When the dimension of input confidence is 3, the box mode 1 is supported.
 * - When the dimension of input confidence is 3, the output mode of 0, 1 and 2
 *   are supported only in the case of batches_num = 1 and classes_num = 1.
 * - Method mode 1 and 2 are not supported in current version,
 *   and they will be supported in the future.
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   only parameters iou_threshold and layout are valid and other parameters
 *   can be arbitrary. Besides, the mode is set as 0 during the computation.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetNmsDescriptor_v4(cnnlNmsDescriptor_t nms_desc,
                                                  const cnnlNmsBoxPointMode_t box_mode,
                                                  const cnnlNmsOutputMode_t mode,
                                                  const cnnlNmsMethodMode_t method_mode,
                                                  const float iou_threshold,
                                                  const float soft_nms_sigma,
                                                  const int max_output_size,
                                                  const float confidence_threshold,
                                                  const float offset,
                                                  const int input_layout);

// Group:Nms
/*!
 * @brief Computes the subset of input tensor \b boxes with the scores of \b confidence, and returns
 *        the results in the output tensor \b output.
 *
 * @deprecated
 *   ::cnnlNms is deprecated and will be removed in the future release, and there are memory copy operation
 *   from device to host, which may result in problems. It is recommended to use ::cnnlNms_v2 instead.
 *
 * NMS(Non-Maximum Suppression) operation is a necessary procedure in detection networks. And this
 * operation selects no more than \b max_output_size targets with high confidence, based on their
 * intersection over union. This function needs extra MLU memory, and you can get the size of workspace
 * \b workspace_size with the ::cnnlGetNmsWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Nms operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in]  boxes_desc
 *   Input. The descriptor of the input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  boxes
 *   Input. Pointer to the MLU memory that stores the input boxes tensor.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  confidence
 *   Input. Pointer to the MLU memory that stores the input confidence tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the Nms operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the Nms operation. You can
 *   get the size of the workspace with the ::cnnlGetNmsWorkspaceSize function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor, including the information of dimension, data type and layout
 *   of output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Nms Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This operation supports combinations of the following data types for input boxes tensor \b boxes,
 *   input confidence tensor \b confidence, and output tensor \b output.
 *   - input boxes tensor: half, float.
 *   - input confidence tensor: half, float
 *   - output tensor: half, float, int32, uint32.
 * - If the output is the indices of boxes, the output data type should be uint32, otherwise
 *   the output data type should the same as input data type.
 *   <b> Note that the combinations of input boxes tensor and input confidence tensor must be half-half
 *   or float-float.</b>
 *
 * @par Data Layout
 * - The combination of the shape of input boxes with confidence can be [boxes_num, 4] with [boxes_num], [4, boxes_num] with
 *   [boxes_num], [boxes_num, 7] with [boxes_num], [7, boxes_num] with [boxes_num], [batches_num, boxes_num, 4] with
 *   [batches_num, classes_num, boxes_num] and [batches_num, 4, boxes_num] with [batches_num, classes_num, boxes_num].
 * - The output tensor is a 1-dimensional tensor if the output result is the indices of boxes, otherwise it is a 2-dimensional
 *   tensor, which containing the coordinates and confidence of output boxes.
 *
 * @par Scale Limitation
 *  - For the input boxes tensor, if the shape is [boxes_num, 4], the order of coordinates is x_01, y_01, x_02, y_02,
 *    x_11, y_11, x_12, y_12, ...  x_n1, y_n1, x_n2, y_n2. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 * -  For the input boxes tensor, if the shape is  [4, boxes_num], the order of coordinates is x_01, x_11, ... x_n1,
 *    y_01, y_11, ... y_n1, x_02, x_12, ... x_n2, x_01, x_11, ... x_n1. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 *
 * @par API Dependency
 *  - Before calling this function to implement Nms, you need to prepare all the parameters passed to this function.
 *    See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the shape of input boxes tensor to [4, num_boxes].
 *   The num_boxes represent the number of input boxes.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/
 */
cnnlStatus_t CNNL_WIN_API cnnlNms(cnnlHandle_t handle,
                                  const cnnlNmsDescriptor_t desc,
                                  const cnnlTensorDescriptor_t boxes_desc,
                                  const void *boxes,
                                  const cnnlTensorDescriptor_t confidence_desc,
                                  const void *confidence,
                                  void *workspace,
                                  size_t workspace_size,
                                  cnnlTensorDescriptor_t output_desc,
                                  void *output);

// Group:Nms
/*!
 * @brief Computes the subset of input tensor \b boxes with the scores of \b confidence, and returns
 *        the results in the output tensor \b output and \output_size.
 *
 * NMS(Non-Maximum Suppression) operation is a necessary procedure in detection networks. And this
 * operation selects no more than \b max_output_size targets with high confidence, based on their
 * intersection over union. This function needs extra MLU memory, and you can get the size of workspace
 * \b workspace_size with the ::cnnlGetNmsWorkspaceSize_v3 function. The difference between this
 * function and ::cnnlNms is that this function returns the valid output size in \b output_size.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Nms operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] desc
 *   Input. The descriptor of the Nms operation. For detailed information, see ::cnnlNmsDescriptor_t.
 * @param[in]  boxes_desc
 *   Input. The descriptor of the input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  boxes
 *   Input. Pointer to the MLU memory that stores the input boxes tensor.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  confidence
 *   Input. Pointer to the MLU memory that stores the input confidence tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the Nms operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the Nms operation. You can
 *   get the size of the workspace with the ::cnnlGetNmsWorkspaceSize_v3 function.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor, including the information of dimension, data type and layout
 *   of output. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] output_size
 *   Output. Pointer to the MLU memory that stores the number of output boxes.
 *
 * @par Returns
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Nms Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This operation supports combinations of the following data types for input boxes tensor \b boxes,
 *   input confidence tensor \b confidence, and output tensor \b output.
 *   - input boxes tensor: half, float.
 *   - input confidence tensor: half, float
 *   - output tensor: half, float, int32, uint32.
 *   - output size: uint32.
 * - If the output is the indices of boxes, the output data type should be uint32, otherwise
 *   the output data type should the same as input data type. The data type of output size is uint32.
 *   <b> Note that the combinations of input boxes tensor and input confidence tensor must be half-half
 *   or float-float.</b>
 *
 * @par Data Layout
 * - The input boxes tensor should be a 2-dimensional tensor, and the input confidence tensor should be a 1-dimensional tensor.
 * - The output tensor is a 1-dimensional tensor if the output result is the indices of boxes, otherwise it is a 2-dimensional
 *   tensor, which containing the coordinates and confidence of output boxes.
 *
 * @par Scale Limitation
 *  - For the input boxes tensor, if the shape is [boxes_num, 4], the order of coordinates is x_01, y_01, x_02, y_02,
 *    x_11, y_11, x_12, y_12, ...  x_n1, y_n1, x_n2, y_n2. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 * -  For the input boxes tensor, if the shape is  [4, boxes_num], the order of coordinates is x_01, x_11, ... x_n1,
 *    y_01, y_11, ... y_n1, x_02, x_12, ... x_n2, x_01, x_11, ... x_n1. And x_i1 must be less than x_i2, y_i1 must be less than y_i2.
 *    The (x_i1, y_i1) and (x_i2, y_i2) represent the top left corner and bottom right corner coordinates, respectively.
 *
 * @par API Dependency
 *  - Before calling this function to implement Nms, you need to prepare all the parameters passed to this function.
 *    See each parameter description for details.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the shape of input boxes tensor to [4, num_boxes].
 *   The num_boxes represent the number of input boxes.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   both of confidence_desc and confidence should be provided as null pointer.
 * - In NMS3D mode, when finding the point with minimum y and minimum x in convex-hull-graham,
 *   it performs min-pooling operation. If the input data of pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN.
 *      Otherwise, the \b output value is the minimum value after the last NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/
 */
cnnlStatus_t CNNL_WIN_API cnnlNms_v2(cnnlHandle_t handle,
                                  const cnnlNmsDescriptor_t desc,
                                  const cnnlTensorDescriptor_t boxes_desc,
                                  const void *boxes,
                                  const cnnlTensorDescriptor_t confidence_desc,
                                  const void *confidence,
                                  void *workspace,
                                  size_t workspace_size,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output,
                                  void *output_size);

// Group:Nms
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * needed in Nms operation.
 *
 * @deprecated
 *   ::cnnlGetNmsWorkspaceSize is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlGetNmsWorkspaceSize_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the Nms operation. For detailed information, see ::cnnlHandle_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the Nms operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms function to perform the Nms operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize(cnnlHandle_t handle, size_t *size);

// Group:Nms
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * needed in Nms operation.
 *
 * @deprecated
 *   ::cnnlGetNmsWorkspaceSize_v2 is deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlGetNmsWorkspaceSize_v3 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the Nms operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the Nms operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms_v2 function to perform the Nms operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize_v2(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t confidence_desc,
                                                     size_t *size);

// Group:Nms
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * needed in Nms operation. Compared with ::cnnlGetNmsWorkspaceSize_v2, this function adds an
 * additional parameter \b boxes_desc and supports the \b confidence_desc to be null pointer
 * in the case of NMS3D where \b confidence is unnecessary.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the Nms operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] boxes_desc
 *   Input. The descriptor of input boxes tensor, including the information of dimension, data type and
 *   layout of input boxes. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] confidence_desc
 *   Input. The descriptor of input confidence tensor, including the information of dimension, data type and
 *   layout of input confidence. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used
 *   in the Nms operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlNms_v2 function to perform the NMS operation.
 *
 * @note
 * - When the input boxes are in NMS3D format([boxes_num, 7] or [7, boxes_num]),
 *   the confidence_desc must be provided with null pointer.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNmsWorkspaceSize_v3(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t boxes_desc,
                                                     const cnnlTensorDescriptor_t confidence_desc,
                                                     size_t *size);

/******************************************************************************
 * Cambricon CNNL OP: ScaledTanh
 ******************************************************************************/

// Group:ScaledTanh
/*!
 * @brief Performs a scaled hyperbolic tangent activation fuction with the alpha
 *        \b alpha and \b beta scale factors on every element of input tensor
 *        \b input by the dimension axis \b axis, and places the result into
 *        the corresponding element of output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the scaledtanh operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input. An int number that specifies which dimension of input tensor is used in
 *   the operation. Users can use the specified axis value or vector to perform
 *   scaledtanh calculations across the whole input tensor.
 *   The axis value is in the range of [0, total_number_of_input_dimensions -1]
 * @param[in] desc_input
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_alpha
 *   Input. The descriptor of the alpha tensor that is used to scale the input tensor.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the alpha tensor.
 * @param[in] desc_beta
 *   Input. The descriptor of the beta tensor that is used to scale the result of
 *   tanh(alpha * input).
 * @param[in] beta
 *   Input. Pointer to the MLU memory that stores the beta tensor.
 * @param[in] desc_output
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "ScaledTanh Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data Type of input tensor \b input, alpha tensor \b alpha, beta tensor \b
 *   beta and output tensor \b output must be the same.
 * - The supported data types of input, alpha, beta and output tensors are as follows:
 *   - input tensor: half, float.
 *   - alpha tensor: half, float.
 *   - beta tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - This function supports multi-dimension.
 * - The supported data layout of input, alpha, beta, output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have the same shape.
 *
 * @note
 * - If the value of alpha tensor and beta tensor are set to 1.0, the Tanh function is performed.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the scaledtanh operation is as follows:
     @verbatim
      input: an array[dim0, dim1, dim2, dim3, ...]
      axis: 2 (assume axis is 2, which marks operating 3th dimension)
      alpha: [1, 1, 1, 1, ...] or [1, 1, dim2, 1, ...], dim2 is marked dimension.
      beta: [1, 1, 1, 1, ...] or [1, 1, dim2, 1, ...], dim2 is marked dimension.
      output: an array[dim0, dim1, dim2, dim3, ...] same with input and
              output[dim0, dim1, dim2, dim3, ...] = tanh(alpha[1, 1, dim2, 1, ...] *
              input[dim0, dim1, dim2, dim3, ...]) * beta[1, 1, dim2, 1, ...]
     @endverbatim
 *
 * @par Reference
 * - https://lasagne.readthedocs.io/en/latest/modules/nonlinearities.html
 *   #lasagne.nonlinearites.ScaledTanH
 * - https://docs.microsoft.com/en-us/windows/win32/api/directml/
 *   ns-directml-dml_active_scaled_tanh_operator_desc
 */
cnnlStatus_t CNNL_WIN_API cnnlScaledTanh(cnnlHandle_t handle,
                                         const int axis,
                                         const cnnlTensorDescriptor_t desc_input,
                                         const void *input,
                                         const cnnlTensorDescriptor_t desc_alpha,
                                         const void *alpha,
                                         const cnnlTensorDescriptor_t desc_beta,
                                         const void *beta,
                                         const cnnlTensorDescriptor_t desc_output,
                                         void *output);
// Group:UnarySelect
/*!
 *  @brief Selects elements in one tensor according to the corresponding values in another tensor. The
 *   elements in \b input will be selected if the corresponding elements in \b index are not equal to zero.
 *   The result composed of two parts. The first part is the number of selected elements which stored
 *   in \b number, and the second part is the selected elements which stored in \b output.
 *
 *  @deprecated
 *    ::cnnlUnarySelect is deprecated and will be removed in the future release. It is recommended
 *    to use ::cnnlMasked_v3 instead. The masked_mode should be set to \p CNNL_MASKED_SELECT.
 *    ::cnnlMasked_v3 supports parameter of masked_mode that determines how to compute.
 *
 *  @param[in]  handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in]  input_desc
 *    Input. The descriptors of the \b index tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in]  index_desc
 *    Input. The descriptors of the \b index tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  index
 *    Input. Pointer to the MLU memory that stores the index data.
 *  @param[in]  output_desc
 *    Input. The descriptors of the \b output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in]  output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @param[in]  number
 *    Input. Pointer to the MLU memory that stores the length of output data.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 *  @par Data Type
 *  - \b input: float, half, int8, int16, int32.
 *  - \b index: bool, float, half.
 *  - \b output: float, half, int8, int16, int32.
 *  - \b number: int32.
 *  - When the data type of index tensor is not bool, the data type of input tensor must
 *    be same with the data type of the index tensor.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - The output memory size should keep same with input data size. The selected output data
 *    keep the same sequence with the input data.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the index select operation is as follows:
      @verbatim
      input array
        input = [[0, 1, 2, 3],
                 [4, 5, 6, 7],
                 [8, 9, 10, 11]]
      index array
        index = [[0, 1, 0, 0],
                 [0, 0, 0, 1],
                 [1, 0, 0, 0]]
      output array
        output = [1, 7, 8]
      number value
        number = 3
      @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnarySelect(cnnlHandle_t handle,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void *input,
                                          const cnnlTensorDescriptor_t index_desc,
                                          const void *index,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void *output,
                                          uint32_t *number);

// Group:IsFinite
/*!
 *  @brief Judges every element in \b input tensor is finite number or not, the element
 *    in \b output tensor is boolean value.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] x_desc
 *    Input. The descriptors of the \b input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] x
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] y_desc
 *    Input. The descriptors of the \b output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] y
 *    Output. Pointer to the MLU memory that stores the output tensor. The element
 *    in \b output tensor is boolean value.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.

 *  @par Data Type
 *  - \b input: float, half.
 *  - \b output: bool.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par Reference
 *  https://pytorch.org/docs/master/generated/torch.isfinite.html
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The example of the index select operation is as follows:
      @verbatim
      input array
        input = [0, 1, 2, 100000000000],
      output array
        output = [true, true, true, false],
      @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIsFinite(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:IsInf
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *   workspace to optimize the ::cnnlIsInf operation.
 *   The size of extra workspace is based on the given information of the Isinf operation.
 *   For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the IsInf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The detection and output mode of isinf.
 *   See ::cnnlIsInfMode_t for details.
 * @param[in] reduce
 *   Input. A boolean value indicating whether to reduce the dimension of output.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   IsInf operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIsinfWorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlIsInfMode_t mode,
                                                    const bool reduce,
                                                    size_t *workspace_size);

// Group:IsInf
/*!
 *  @brief Judges every element in \b input tensor is negative/positive infinite number or not,
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the isinf operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptor of the \b input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] mode
 *    Input. The detection and output mode of isinf.
 *    See ::cnnlIsInfMode_t for details.
 *  @param[in] reduce
 *    Input. A boolean value indicating whether to reduce the dimension of output.
 *    If \b reduce is false, the shape of \b output is the same as input; If \b reduce is
 *    true, reduce the dimensions of \b output to 1D with one element.
 *      - When \b mode=CNNL_POS_INF and \b reduce= true: outputs 1D tensor containing only
 *        one "true" element if all elements in \b input are positive infinity. Otherwise,
 *        return 1D tensor containing one "false" element.
 *      - When \b mode=CNNL_NEG_INF and \b reduce= true: outputs 1D tensor containing only
 *        one "true" element if all elements in \b input are negative infinity. Otherwise,
 *        return 1D tensor containing one "false" element.
 *      - When \b mode = NNL_INF_INDICATOR and \b reduce= true:
 *        Output "1" if all elements in \b input are negative infinity;
 *        Output "2" if all elements in \b input are positive infinity;
 *        Output "3" if the \b input contains both negative and positive infinity;
 *        Output "0" if the \b input contains neither negative infinity nor positive infinity.
 *  @param[in]  workspace
 *    Input. Pointer to the MLU memory that is used as an extra workspace for the IsInf
 *    operation.
 *    only when \b mode is CNNL_INF_INDICATOR and \b reduce is true, workspace is required.
 *  @param[in] workspace_size
 *    Input. The size of the extra workspace in bytes that needs to be used in the IsInf
 *    operation when \b reduce is true. You can get the size of the workspace with the
 *    ::cnnlGetIsinfWorkspaceSize function.
 *  @param[in] output_desc
 *    Input. The descriptor of the \b output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the \b output tensor. The element
 *    in \b output tensor is boolean or indicator value.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *  @par Data Type
 *   - This function supports any combinations of the following data types for the \b input, \b reduce and \b output tensors:
 *     - \b input: float, half.
 *     - \b reduce: bool.
 *     - \b output: bool, int8.
 *
 *  @par Scale Limitation
 *  - None.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to get the size of workspace by the ::cnnlGetIsinfWorkspaceSize function
 *  and pass the allocated extra workspace to the function.
 *
 *  @note
 *  - Inplace operation is not supported.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - The examples of the isinf operation are as follows:
      @verbatim
        input：{9, -inf, 2, nan, +inf} mode = CNNL_NEG_INF, reduce = false; ouput：{false, true, false, false, false}
        input：{9, -inf, 2, nan, +inf} mode = CNNL_POS_INF, reduce = false; ouput：{false, false, false, false, true}
        input：{9, -inf, 2, nan, +inf} mode = CNNL_INF, reduce = false; ouput：{false, true, false, false, true}
        input：{9, -inf, 2, nan, +inf} mode = CNNL_NEG_INF, reduce = true; ouput：true
        input：{9, -inf, 2, nan, +inf} mode = CNNL_POS_INF, reduce = true; ouput：true
        input：{9, -inf, 2, nan, +inf} mode = CNNL_INF, reduce = true;  ouput：true
        input：{9, -inf, 2, nan, +inf} mode = CNNL_INF_INDICATOR, reduce = false; ouput：{0, 1, 0, 0, 2}
        input：{9, -inf, 2, nan, +inf} mode = CNNL_INF_INDICATOR, reduce = true;  ouput：3
      @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/1.9.0/generated/torch.isinf.html?highlight=isinf#torch.isinf
 *  - https://tensorflow.google.cn/api_docs/python/tf/math/is_inf
 */
cnnlStatus_t CNNL_WIN_API cnnlIsInf(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t input_desc,
                                    const void *input,
                                    const cnnlIsInfMode_t mode,
                                    const bool reduce,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:IndexFill
/*!
 * @brief Fills the elements of the input with value by selecting the indices in the order
 * given by index. This function supports in-place operation when the pointer to the input
 * and output tensors are the same tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The data of int type to indicate which dimension to be selected to fill.
 * @param[in] value
 *   Input. The data of float type to indicate the value to fill with.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A device pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. A device pointer to the MLU memory that stores the index tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor, For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A device pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,
 *   ::CNNL_STATUS_ARCH_MISMATCH, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "IndexFill Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensors.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, bool, half, float.
 *   <b>Note that the data type of output should be same with input.</b>
 *
 * @par Data Layout
 * - Data layouts of input tensor and output tensor must be the same.
 *
 * @par Scale Limitation
 * - Each dimension of the input tensor must be the same with the output tensor.
 * - The shape of input tensor and output tensor must be the same.
 * - The index tensor must be 1-D tensor.
 * - The data type of \b index must be CNNL_DTYPE_INT32.
 * - The value of \b dim should be in the range of [-input_dim_size, input_dim_size -1]
 *   where \b input_dim_size is the size of input dimension.
 * - The value of each element in \b index should be in the range of the value of the
 *   dimension indicated by the \b dim. Negative dim is not supported.
 *   Because the \b index is a device pointer to the MLU memory, this operation does not
 *   do the check function on CPU. Users need to guarantee that the value of each element
 *   in \b index in the correct range.
 * - When the data type of input tensor and output tensor is \b CNNL_DTYPE_HALF, there may
 *   be a precision error. This error is caused by data type conversion from float32 to
 *   float16 on param \b value.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set this operation to be in-place.
 *
 * @note
 * - You can specify the stride of all dimensions for input_desc, index_desc and
 *   output_desc with ::cnnlSetTensorDescriptorEx.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
       Input tensor  :   [[1, 2, 3],
                          [4, 5, 6],
                          [7, 8, 9]]

       Dim           :   1

       Value         :   -1

       Index tensor  :   [0,2]

       Output tensor :   [[-1, 2, -1],
                          [-1, 5, -1],
                          [-1, 8, -1]]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexFill(cnnlHandle_t handle,
                                        const int dim,
                                        const float value,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t index_desc,
                                        const void *index,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:Hardtanh
/*!
 * @brief Computes hardtanh on input tensor \b x by the maximum value \b max_val and minimum value
 *        \b min_val of the linear range, and returns the result in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the hardtanh
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] max_val
 *   Input. A float value used as the maximum value of the linear range.
 * @param[in] min_val
 *   Input. A float value used as the minimum value of the linear range.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Handtanh Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor \b x and output tensor \b y must be the same. The supported data types of
 *   input tensor \b x and output tensor \b y are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://pytroch.org/docs/master/nn.functional.html#torch.nn.functional.hardtanh
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHardtanh(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t x_desc,
                                       const void *x,
                                       const float max_val,
                                       const float min_val,
                                       const cnnlTensorDescriptor_t y_desc,
                                       void *y);

// Group:Hardtanh
/*!
 * @brief Computes gradient of hardtanh on input tensor \b x and \b diff_y by the maximum value
 *        \b max_val and minimum value \b min_val of the linear range, and returns the result
 *        in the output tensor \b diff_x.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   hardtanh_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] max_val
 *   Input. A float value used as the maximum value of the linear range.
 * @param[in] min_val
 *   Input. A float value used as the minimum value of the linear range.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "HandtanhBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensors \b x and \b diff_y must be same with the data type output tensor
 *   \b diff_x. The supported data types of input tensors and output tensor are as follows:
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://pytroch.org/docs/master/n.functional.html#torch.nn.functional.hardtanh_backward
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHardtanhBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t diff_y_desc,
                                               const void *diff_y,
                                               const float max_val,
                                               const float min_val,
                                               const cnnlTensorDescriptor_t diff_x_desc,
                                               void *diff_x);

// Group:WeightNorm
/*!
 *  @brief Performs the filter normalization operator computation.
 *
 *  filter normalization is a reparameterization that decouples the magnitude of
 *  the filter tensor from its direction. The parameter filter in network layer is
 *  replaced by magnitude (g) and direction (v).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] axis
 *   Input.  The dimension of filter to normalize in network layer.
 * @param[in] v_desc
 *   Input. Descriptior of \b v that is direction of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] v
 *   Input. Pointer to the MLU memory that stores the \b v tensor.
 * @param[in] g_desc
 *   Input. Descriptior of \b g that is magnitude of filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] g
 *   Input. Pointer to the MLU memory that stores the \b g tensor.
 * @param[in] w_desc
 *   Input. Descriptor of \b w that is filter in network layer.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] w
 *   Output. Pointer to the MLU memory that stores the \b w tensor.
 * @param[in] norm_recip_desc
 *   Input. Descriptior of \b norm_recip that is intermediate data for
 *   weightnorm backward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] norm_recip
 *   Output. Pointer to the MLU memory that stores the \b norm_recip tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Data Type
 * - Data type of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - On MLU200 series, the value of input data should be in the following range:
 *    float: [1e-10,1e5]
 *    half: [5e-4, 1000].
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - http://arxiv.org/abs/1602.07868v3
 */

cnnlStatus_t CNNL_WIN_API cnnlWeightNorm(cnnlHandle_t handle,
                                         const int axis,
                                         const cnnlTensorDescriptor_t v_desc,
                                         const void *v,
                                         const cnnlTensorDescriptor_t g_desc,
                                         const void *g,
                                         const cnnlTensorDescriptor_t w_desc,
                                         void *w,
                                         const cnnlTensorDescriptor_t norm_recip_desc,
                                         void *norm_recip);

// Group:SparseSoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes sparse softmax cross-entropy and the gradients with inputs
 * and index of sparse labels.
 *
 * @deprecated
 * ::cnnlSparseSoftmaxCrossEntropyWithLogits is deprecated and will be removed in the future
 * release. It is recommended to use ::cnnlSparseSoftmaxCrossEntropyWithLogits_v2 instead, which
 * allows users to perform the operation with faster algorithm or higher precision.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlSparseSoftmaxCrossEntropyWithLogits operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The specific algorithm used to determine which dimension the operation would be
 *   performed.
 *   The algorithms are defined in ::cnnlSoftmaxMode_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of logits tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the logits tensor.
 *   The value of \b x is per-label activation, indicating the probabilities of the
 *   corresponding output from the previous layer in a artificial intelligence.
 * @param[in] idx_desc
 *   Input. The descriptor of index tensor that determines the sparse label.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] idx
 *   Input. Pointer to the MLU memory that stores the index tensor.
 *   For the i-th batch, the probability of the idx[i]-th class is 1.0, while the probabilities of
 *   other classes are 0.0.
 * @param[in] y_desc
 *   Input. The descriptor of softmax cross-entropy tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the softmax cross-entropy tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the gradients tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the gradients tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "SparseSoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 *  - This function supports the following data types for \b x, \b idx, \b y, and \b diff_y:
 *    - \b x: half, float.
 *    - \b idx: int32.
 *    - \b y: half, float.
 *    - \b diff_y: half, float.
 * - Data type of \b x, \b y, and \b diff_y should be the same.
 *
 * @par Data Layout
 * - The supported data layout of \b x, \b idx, \b y, \b diff_y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_ARRAY.
 *   - idx tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - Only \p CNNL_SOFTMAX_MODE_LOW_DIMENSION of \b mode is supported.
 * - The shape of \b x should be [high_dim, mid_dim, low_dim] or [high_dim, low_dim].
 * - If \b mode is set to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION:
 *   - If the shape of \b x is [high_dim, mid_dim, low_dim],
 *     the shape of \b idx should be [high_dim, mid_dim].
 *   - If the shape of \b x is [high_dim, low_dim], the shape of \b idx should be [high_dim].
 * - The shape of \b y and \b idx should be the same.
 * - The shape of \b diff_y and \b x should be the same.
 * - |x - x_max| of \b x is recommended to be in the range of [-15.5, 0] for higher precision,
 *   where x represents each value of \b x and x_max represents the maximum value of \b x along the
 *   specified dimension in \b mode.
 * - Half data type is not recommended as the precision cannot be ensured for large amount of data.
 * - For half data type, the value of \b x is recommended to be in the range of [-1, 1], and the size
 *   of \p low_dim of \b x is recommended to be less than 128 bytes for higher precision.
 * - The sum of \p exp(x - x_max) along the specified dimension in \b mode should be in the value
 *   range of the corresponding data type.
 * - When \p low_dim = 1, the value of \b idx should be in the range of [-2^23, 2^23] on MLU200 series.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   the \b mode is recommended to be \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - When \b x contains NaN/infinity:
 *   - On MLU200 series:
 *     - NaN in \b x is considered as positive saturation value.
 *     - Positive infinity in \b x is considered as positive saturation value.
 *     - Negative infinity in \b x is considered as negative saturation value.
 *     - If the lowest dimension of \b x is 1, then \b y and \b diff_y are 0.
 *   - On MLU300 series and CE3226:
 *     - If the lowest dimension of \b x is 1, then \b y and \b diff_y are 0.
 *
 * @par Requirements
 * - The value of \b idx should be in the range of [0, low_dim), otherwise the corresponding \b y and
 *   \b diff_y will be NaN.
 *
 * @par Example
 * - The example of the SparseSoftmaxCrossEntropyWithLogits operation is as follows:
     @verbatim
      input two arrays by 3 * 4 and 3 -->
          x: [[1, 2, 3, 4],
              [4, 3, 2, 1],
              [1, 4, 3, 2]]

      --> idx: [0, 1, 2]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

      output two arrays by 3 and 3 * 4 -->
          y: [3.44019, 1.44019, 2.44019]

      --> diff_y: [[-0.96794, 0.08714, 0.23688, 0.64391],
                   [0.64391, -0.76312, 0.08714, 0.03206],
                   [0.03206, 0.64391, -0.76312, 0.08714]]
      @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/sparse_softmax_cross_entropy_with_logits
 */

cnnlStatus_t CNNL_WIN_API
cnnlSparseSoftmaxCrossEntropyWithLogits(cnnlHandle_t handle,
                                        cnnlSoftmaxMode_t mode,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const cnnlTensorDescriptor_t idx_desc,
                                        const void *idx,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y,
                                        const cnnlTensorDescriptor_t diff_y_desc,
                                        void *diff_y);

// Group:SparseSoftmaxCrossEntropyWithLogits
/*!
 * @brief Computes sparse softmax cross-entropy and the gradients with inputs
 * and index of sparse labels.
 *
 * Compared with ::cnnlSparseSoftmaxCrossEntropyWithLogits, this function allows you to choose
 * whether to perform the sparse softmax cross-entpy with logits operation with faster algorithm
 * or higher precision.
 *
 * When \b prefer is set to \p CNNL_COMPUTATION_HIGH_PRECISION, this function has the same function
 * with ::cnnlSparseSoftmaxCrossEntropyWithLogits.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlSparseSoftmaxCrossEntropyWithLogits_v2 operation.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t.
 * @param[in] mode
 *   Input. The specific algorithm used to determine which dimension the operation would be
 *   performed.
 *   The algorithms are defined in ::cnnlSoftmaxMode_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of logits tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the logits tensor.
 *   The value of \b x is per-label activation, indicating the probabilities of the
 *   corresponding output from the previous layer in a artificial intelligence.
 * @param[in] idx_desc
 *   Input. The descriptor of index tensor that determines the sparse label.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] idx
 *   Input. Pointer to the MLU memory that stores the index tensor.
 *   For the i-th batch, the probability of the idx[i]-th class is 1.0, while the probabilities of
 *   other classes are 0.0.
 * @param[in] y_desc
 *   Input. The descriptor of softmax cross-entropy tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the softmax cross-entropy tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the gradients tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_y
 *   Output. Pointer to the MLU memory that stores the gradients tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "SparseSoftmaxCrossEntropyWithLogits Operator" section in "Cambricon CNNL User Guide"
 *   for details.
 *
 * @par Data Type
 *  - This function supports the following data types for \b x, \b idx, \b y, and \b diff_y:
 *    - \b x: half, float.
 *    - \b idx: int32.
 *    - \b y: half, float.
 *    - \b diff_y: half, float.
 * - Data type of \b x, \b y, and \b diff_y should be the same.
 *
 * @par Data Layout
 * - The supported data layout of \b x, \b idx, \b y, \b diff_y tensor are as follows:
 *   - x tensor: \p CNNL_LAYOUT_ARRAY.
 *   - idx tensor: \p CNNL_LAYOUT_ARRAY.
 *   - y tensor: \p CNNL_LAYOUT_ARRAY.
 *   - diff_y tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - Only \p CNNL_SOFTMAX_MODE_LOW_DIMENSION of \b mode is supported.
 * - The shape of \b x should be [high_dim, mid_dim, low_dim] or [high_dim, low_dim].
 * - If \b mode is set to \p CNNL_SOFTMAX_MODE_LOW_DIMENSION:
 *   - If the shape of \b x is [high_dim, mid_dim, low_dim],
 *     the shape of \b idx should be [high_dim, mid_dim].
 *   - If the shape of \b x is [high_dim, low_dim], the shape of \b idx should be [high_dim].
 * - The shape of \b y and \b idx should be the same.
 * - The shape of \b diff_y and \b x should be the same.
 * - |x - x_max| of \b x is recommended to be in the range of [-15.5, 0] for higher precision,
 *   where x represents each value of \b x and x_max represents the maximum value of \b x along the
 *   specified dimension in \b mode.
 * - Half data type is not recommended as the precision cannot be ensured for large amount of data.
 * - For half data type, the value of \b x is recommended to be in the range of [-1, 1], and the size
 *   of \p low_dim of \b x is recommended to be less than 128 bytes for higher precision.
 * - The sum of \p exp(x - x_max) along the specified dimension in \b mode should be in the value
 *   range of the corresponding data type.
 * - When \p low_dim = 1, the value of \b idx should be in the range of [-2^23, 2^23] on MLU200 series.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   the \b mode is recommended to be \p CNNL_SOFTMAX_MODE_LOW_DIMENSION.
 *
 * @note
 * - When \b x contains NaN/infinity:
 *   - On MLU200 series:
 *     - NaN in \b x is considered as positive saturation value.
 *     - Positive infinity in \b x is considered as positive saturation value.
 *     - Negative infinity in \b x is considered as negative saturation value.
 *     - If the lowest dimension of \b x is 1, then \b y and \b diff_y are 0.
 *   - On MLU300 series and CE3226:
 *     - If the lowest dimension of \b x is 1, then \b y and \b diff_y are 0.
 *
 * @par Requirements
 * - The value of \b idx should be in the range of [0, low_dim), otherwise the corresponding \b y and
 *   \b diff_y will be NaN.
 *
 * @par Example
 * - The example of the ::cnnlSparseSoftmaxCrossEntropyWithLogits_v2 operation is as follows:
     @verbatim
      input two arrays by 3 * 4 and 3 -->
          x: [[1, 2, 3, 4],
              [4, 3, 2, 1],
              [1, 4, 3, 2]]

      --> idx: [0, 1, 2]

      param:
        mode: CNNL_SOFTMAX_MODE_LOW_DIMENSION

      output two arrays by 3 and 3 * 4 -->
          y: [3.44019, 1.44019, 2.44019]

      --> diff_y: [[-0.96794, 0.08714, 0.23688, 0.64391],
                   [0.64391, -0.76312, 0.08714, 0.03206],
                   [0.03206, 0.64391, -0.76312, 0.08714]]
      @endverbatim
 *
 * @par Reference
 * - http://www.tensorflow.org/api_doc/python/tf/nn/sparse_softmax_cross_entropy_with_logits
 */

cnnlStatus_t CNNL_WIN_API
cnnlSparseSoftmaxCrossEntropyWithLogits_v2(cnnlHandle_t handle,
                                           const cnnlComputationPreference_t prefer,
                                           const cnnlSoftmaxMode_t mode,
                                           const cnnlTensorDescriptor_t x_desc,
                                           const void *x,
                                           const cnnlTensorDescriptor_t idx_desc,
                                           const void *idx,
                                           const cnnlTensorDescriptor_t y_desc,
                                           void *y,
                                           const cnnlTensorDescriptor_t diff_y_desc,
                                           void *diff_y);

// Group:Diag
/*!
 * @brief Returns in \b output the square matrix where the input tensor \b input is the diagonal
 *        \b k, or the k-th diagonal of a given matrix.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the diag
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] k
 *   Input. The diagonal used in this operation.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Diag Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - output tensor: If the input tensor is one-dimension, the element number of output tensor
 *     should be positive and within int range (0, 2147483647].
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the diag operation is as follows:
     @verbatim
     input array by 3 * 3 --> input: [[1,2,3],[4,5,6],[7,8,9]]

     param:
       k: 1

     output array by 1 * 2 --> output: [[2,6]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.diag.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDiag(cnnlHandle_t handle,
                                   const int k,
                                   const cnnlTensorDescriptor_t input_desc,
                                   const void* input,
                                   const cnnlTensorDescriptor_t output_desc,
                                   void *output);

// Group:Median
/*!
 * @brief Computes the median of the input tensor. If \b is_dim_None is true, \b dim is disabled,
 * and this function returns the median value of the input tensor flattened to one dimension.
 * Otherwise, \b dim is enabled, and this function returns the median values and the corresponding
 * indices of the input tensor in dimension \b dim.
 *
 * Generally the median is not unique for input tensor with an even number of elements. But in this
 * function, only the smaller of the two medians is returned when the input tensor has an even
 * number of elements.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the median
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor.
 * @param[in] dim
 *   Input. An int value which determines the dimension to get median.
 * @param[in] is_dim_None
 *   Input. A boolean value which determines whether \b dim is enabled.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Median Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, output tensor
 *   \b output_values, and index tensor \b output_indices.
 *   <b>Data type of input tensor and output tensor should be the same.</b>
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *   - index tensor: int32.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - When \b is_dim_None is false, the shape of \b output_values and \b output_indices should be
 *   the same.
 *   - When \b is_dim_None is false, the total size of \b input in dimension \b dim should be less
 *   than 167936 bytes.
 *   - When \b is_dim_None is true, the total size of \b input should be less than 671744 bytes.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.median.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlMedian(cnnlHandle_t handle,
           const cnnlTensorDescriptor_t input_desc,
           const void *input,
           const cnnlTensorDescriptor_t output_values_desc,
           void *output_values,
           const cnnlTensorDescriptor_t output_indices_desc,
           void *output_indices,
           const int dim,
           const bool is_dim_None);

// Group:AsStrided
/*!
 * @brief Creates a view of the input tensor \b input with specified \b strides and
 *        the offset of the first element \b storage_offset.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   as_strided operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] stride
 *   Input. The stride for every dimension of output.
 * @param[in] storage_offset
 *   Input. The offset of the first element of the output tensor \b output in the input tensor
 *   \b input.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "AsStrided Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of input tensor and output tensor should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 *  - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the as_strided operation is as follows:
     @verbatim
     input array by 3 * 3 --> input: [[1,2,3],[4,5,6],[7,8,9]]

     param:
       stride: [1,2], offset: 1

     output array by 2 * 2 --> output: [[2,4],[3,5]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.as_strided.html
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlAsStrided(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const uint32_t stride[],
                                        const uint32_t storage_offset,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void *output);

// Group:Softplus
/*!
 * @brief Computes the gradient of an operation of softplus backward.
 *
 * The softplus operation is used in activation layer to obtain the nonlinearity for artificial intelligence.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the softplus backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Iuput. Pointer to the MLU memory that stores the input tensor which is a gradient.
 * @param[out] diff_x_desc
 *   Output. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] beta
 *   Input. Scale value compatible with Pytorch and Tensorflow interfaces, default 1.
 * @param[in] threshold
 *   Input. Scale value compatible with Pytorch and Tensorflow interfaces, default 20.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "SoftplusBackward operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the softplus backward operation is as follows:
     @verbatim
      - \b x: [4, 1, 3]
      - \b diff_y: [4, 1, 3]
      - \b diff_x: [4, 1, 3]
     @endverbatim
 *
 * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - input tensors: float, half.
 *   - output tensors: float, half.
 * - Softplus backward operation is an element-wise operation. The dimensions of \b x, \b diff_y
 *   and \b diff_x must be the same.
 * - The number of dimensions is no more than \p CNNL_DIM_MAX.
 *
 * @note
 * - The input tensor \b x should be in the following range to guarantee the accuracy of output:
 *   - [-7.75, 7.75].
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/generated/torch.nn.Softplus.html#torch.nn.Softplus.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlSoftplusBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t x_desc,
                                               const void *x,
                                               const cnnlTensorDescriptor_t diff_y_desc,
                                               const void *diff_y,
                                               const cnnlTensorDescriptor_t diff_x_desc,
                                               void *diff_x,
                                               int beta,
                                               int threshold);

// Group:BceWithLogits
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlBceWithLogitsBackward operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlBceWithLogitsBackward,
 * including the \b target tensor descriptor \b target_desc, \b filter tensor descriptor \b filter_desc,
 * and \b pos_filter tensor descriptor \b pos_filter_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceWithLogitsBackwardWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] target_desc
 *   Input. The descriptor of \b target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of \b filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter_desc
 *   Input. The descriptor of \b pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlBceWithLogitsBackward operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBceWithLogitsBackwardWorkspaceSize(
                                                    cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t target_desc,
                                                    const cnnlTensorDescriptor_t filter_desc,
                                                    const cnnlTensorDescriptor_t pos_filter_desc,
                                                    size_t *size);

// Group:BceWithLogits
/*!
 *  @brief Computes gradients of ::cnnlBceWithLogits with \b grad tensor, \b input tensor,
 *  \b target tensor, \b filter tensor and \b pos_filter tensor, and returns the results
 *  in the \b diff_input tensor.
 *
 *  If \b filter tensor and \b pos_filter tensor need to be broadcasted, this function needs
 *  extra host memory as the workspace to broadcast \b filter tensor and \b pos_filter tensor.
 *  You can get the size of the workspace \b workspace_size with the ::cnnlGetBceWithLogitsBackwardWorkspaceSize
 *  function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceWithLogitsBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *    Input. Pointer to the MLU memory that stores the grad tensor. The \b grad
 *    tensor is gradient.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor. The \b target tensor
 *   has the same shape as \b input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \b filter tensor is
 *   a filter of the result of binary cross entropy, and must match \b target tensor shape.
 * @param[in] pos_filter_desc
 *   Input. The descriptor of pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \b pos_filter tensor
 *   is a filter for dealing with data imbalance. It must be a vector and match \b target tensor shape.
 * @param[in] reduction
 *   Input. An enum value that describing the reduction dimension, including:
 *   CNNL_BCE_WITH_LOGITS_REDUCTION_NONE, CNNL_BCE_WITH_LOGITS_REDUCTION_MEAN,
 *   CNNL_BCE_WITH_LOGITS_REDUCTION_SUM. See the ::cnnlBceWithLogitsReduction_t
 *   enum definition.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceWithLogitsBackward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceWithLogitsBackward. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsBackwardWorkspaceSize function.
 * @param[out] diff_input_desc
 *   Output. The descriptor of diff_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] diff_input
 *   Output. Pointer to the MLU memory that stores the diff_input tensor. The \b diff_input
 *   tensor is the gradients of ::cnnlBceWithLogits.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b grad tensor, \b input tensor, \b target tensor, \b filter tensor,
 *   \b pos_filter tensor and \b diff_input tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - grad tensor: half, float
 *   - input tensor: half, float
 *   - target tensor: half, float
 *   - filter tensor: half, float
 *   - pos_filter tensor: half, float
 *   - diff_input tensor: half, float
 *
 * @par Scale Limitation
 * - Half type is not recommended as the precision cannot be ensured for large amount of data.
 *   Because the max number of half is 65504.
 * - The range of \b grad tensor, \b input tensor,\b target tensor, \b filter tensor and
 *   \b pos_filter tensor should be small enough to prevent the result from data overflow in half data type.
 *
 * @par API Dependency
 * - You need get the extra space size by ::cnnlGetBceWithLogitsBackwardWorkspaceSize which need to get
 *   with ::cnnlBceWithLogitsBackward operation.
 *
 * @note
 * - The dimension of \b input tensor, \b target tensor, \b diff_input tensor should be same.
 * - If reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   the \b grad tensor should be a number. Otherwise, the dimension of \b grad tensor and
 *   \b input tensor should be the same.
 * - The dimension values of the \b filter tensor and the \b target tensor need to meet the requirements of broadcast.
 * - The dimension values of the \b pos_filter tensor and the \b target tensor need to meet the requirements of broadcast.
 * - \b filter tensor and \b pos_filter tensor are optional. when you do not need them, please set them NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogitsBackward function is as follows:
     @verbatim
       grad:  [2.],

       input:  [1.,2.,3.,4.]

       target: [1.,2.,3.,4.]

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the diff_input.

       diff_input  -->: [-0.13424, -0.5594, -1.0236, -1.5090].
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html?highlight=binary_cross_entropy_with_logits#
 *   torch.nn.functional.binary_cross_entropy_with_logits
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogitsBackward(cnnlHandle_t handle,
                          const cnnlTensorDescriptor_t grad_desc,
                          const void *grad,
                          const cnnlTensorDescriptor_t input_desc,
                          const void *input,
                          const cnnlTensorDescriptor_t target_desc,
                          const void *target,
                          const cnnlTensorDescriptor_t filter_desc,
                          const void *filter,
                          const cnnlTensorDescriptor_t pos_filter_desc,
                          const void *pos_filter,
                          cnnlBceWithLogitsReduction_t reduction,
                          void *workspace,
                          size_t workspace_size,
                          const cnnlTensorDescriptor_t diff_input_desc,
                          void *diff_input);

// Group:AsStrided
/*!
 * @brief Calculates the gradient of input \b grad_x based on the gradient of response \b grad_y to
 * perform the backpropagation of ::cnnlAsStrided.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in]  grad_y_desc
 *   Input. The descriptor of the input gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  grad_y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  grad_x_desc
 *   Input. The descriptor of the output gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  grad_x
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in]  strides_y
 *   Input. Pointer to the host memory that stores the stride of every dimension.
 *   The length of \b strides_y should be equal to the length of \b grad_y.
 * @param[in]  storage_offset_y
 *   Input. The offset of the first element.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the AsStrided Backward
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the AsStrided Backward
 *   operation. You can get the size of the workspace with the ::cnnlGetAsStridedBackwardWorkspaceSize
 *   function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function only supports float data type for both \b grad_y and \b grad_x.
 *
 * @par Scale Limitation
 * - The value of \b strides_y cannot be less than 0 for any dimension.
 * - The number of input dimensions is no more than 8.
 *
 * @par API Dependency
 * - Before calling this function, you need to call the ::cnnlGetAsStridedBackwardWorkspaceSize
 *   function to get the \b workspace_size.
 *
 * @note
 * - Data type of input should be the same with output.
 * - The \b grad_x_desc of ::cnnlAsStridedBackward should be the same with the \b input_desc of ::cnnlAsStrided.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the AsStrided Backward is as follows:
 @verbatim
 grad_y: input an array by 4*4 --> [[11, 12, 13, 14],
                                    [21, 22, 23, 24],
                                    [31, 32, 33, 34],
                                    [41, 42, 43, 44]]

 shape of grad_x: [4,8]

 strides_y: an array by 2  --> [3,2]

 storage_offset_y: 3

 Then we will get the output:

 output: an array by 4*8 --> [[11,  0, 12, 21, 13, 22, 45, 23],
                              [32, 65, 33, 42, 34, 43,  0, 44],
                              [0,   0,  0,  0,  0,  0,  0,  0],
                              [0,   0,  0,  0,  0,  0,  0,  0]]
 @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/master/tensors.html#torch.Tensor.expand
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlAsStridedBackward(cnnlHandle_t handle,
                      const cnnlTensorDescriptor_t grad_y_desc,
                      const void *grad_y,
                      const cnnlTensorDescriptor_t grad_x_desc,
                      void *grad_x,
                      uint32_t strides_y[],
                      uint32_t storage_offset_y,
                      void *workspace,
                      uint32_t workspace_size);

// Group:AsStrided
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlAsStridedBackward operation.
 *
 * The size of extra workspace is based on the given information of the AsStrided Backward
 * operation, including the input tensor descriptors \b grad_y_desc. For more information about
 * the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the AsStrided
 *   Backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  grad_y_desc
 *   Input. The descriptor of the output gradient tensor in the backpropagation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *   AsStrided Backward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetAsStridedBackwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t grad_y_desc,
                                      uint32_t *workspace_size);

/*!
 * @brief Enumeration variables describing the interpolation algorithms used to implement the
 *        InterpBackward operation.
 *
 */
typedef enum {
  CNNL_INTERP_BACKWARD_NEAREST = 0,
  /*!< The interp_backward mode is Nearest-Neighbor,
   * which means using the nearest pixel for interpolation.*/
  CNNL_INTERP_BACKWARD_BILINEAR = 1,
  /*!< The interp_backward mode is Bilinear,
   * which means using the four corner pixels with bilinear algorithm for interpolation.*/
  CNNL_INTERP_BACKWARD_LINEAR = 2,
  /*!< Linear interpolation for 3-D tensor only.*/
  CNNL_INTERP_BACKWARD_TRILINEAR = 3,
  /*!< Trilinear interpolation for 5-D tensor only.*/
  CNNL_INTERP_BACKWARD_BICUBIC = 4,
  /*!< Bicubic convolution interpolation for 4-D tensor only.*/
} cnnlInterpBackwardMode_t;

// Group:Interp
/*!
 * @brief Computes the gradients of the original images \b output based on the gradients of
 *        images after interpolation \b input for the backpropagation of ::cnnlInterp operation.
 *
 * @deprecated
 *   ::cnnlInterpBackward is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlInterpBackward_v2 instead, which supports parameters of \b scale_factors and
 *   \b recompute_scale_factor that determine whether to recompute the scaling factors in the
 *   interpolation.
 *
 * This function supports algorithms defined in ::cnnlInterpBackwardMode_t.
 *
 * Additional parameters are \b align_corners and \b align_center, which affect the calculation of
 * the scaling factor of H dimension (scale_w) and the scaling factor of W dimension (scale_h).
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInterpBackward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable that determines the method to align 4 corner pixels between
 *          the original images and the images after interpolation.
 *          Generally, pixels of images are considered to be squares.
 *          If \b align_corners is set to true, images after interpolation and the original images
 *          are all aligned by the center points of 4 corner pixels.
 *          Otherwise, images after interpolation and the original images are aligned by the
 *          upper-left corner points of the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable that determines whether to shift the coordinates of each pixel from
 *          the upper-left corner coordinates by an offset of 0.5.
 *          If \b align_center is set to true, the center coordinates are used to represent each
 *          pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when \b align_center is set to true, the coordinates of the first pixel
 *          are (0, 0).
 *          When \b align_center is set to false, the coordinates of the first pixel are (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpBackwardMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the gradients tensor of images after interpolation.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the gradients tensor of images
 *          after interpolation.
 * @param[in] output_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "InterpBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \b align_corners, \b align_center,
 *   \b input, and \b output:
 *   - \b align_corners: bool.
 *   - \b align_center: bool.
 *   - \b input: half, float.
 *   - \b output: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b input and \b output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - \b align_corners and \b align_center cannot be set to true at the same time.
 * - If \b mode is set to CNNL_INTERP_BACKWARD_NEAREST and the layout of \b input and \b output is
 *   \p CNNL_LAYOUT_NLC, \b align_corners and \b align_center can only be set to false.
 * - If the layout of \b input and \b output is \p CNNL_LAYOUT_NLC, the shape of \b input and
 *   \b output should be [n, li, c] and [n, lo, c] respectively.
 * - If the layout of \b input and \b output is \p CNNL_LAYOUT_NHWC, the shape of \b input and
 *   \b output should be [n, hi, wi, c] and [n, ho, wo, c] respectively.
 * - \p n and \p c represent the batches and channels of both \b input and \b output tensor
 *   respectively.
 *   \p hi and \p ho represent the height of \b input and \b output tensor respectively.
 *   \p wi and \p wo represent the width of \b input and \b output tensor respectively.
 *   \p li and \p lo represent the length of \b input and \b output tensor respectively.
 * - Data type of \b input and \b output should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   channels \p c of both \b input and \b output are recommended to be aligned to 128 Bytes.
 *
 * @note
 * - Only the size of \b input and \b output can be set in this operation.
 *   To use scaling factors rather than the size of \b input and \b output,
 *   the formula which converts scaling factors to the size of \b input and \b output is:
 *   - If align_corners is set to true, scale_h = (hi - 1) / (ho - 1),
 *      scale_w = (wi - 1) / (wo - 1).
 *   - If align_corners is set to false, scale_h = hi / ho, scale_w = wi / wo.
 *   The scale_h and scale_w represent the scaling factors along the corresponding dimensions.
 *   You can use this formula to obtain the requisite size of \b input and \b output with
 *   scaling factors.
 * - When \b input contains infinity:
 *   - On MLU200 series:
 *     - NaN in \b input is considered as positive saturation value.
 *     - Positive infinity in \b input is considered as positive saturation value.
 *     - Negative infinity in \b input is considered as negative saturation value.
 *   - On MLU300 series and CE3226:
 *     - If \b mode is ::CNNL_INTERP_BACKWARD_BILINEAR, \b output will contain infinity.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=interp#torch.nn.functional.inter
 *   polate
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeBilinearGrad
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeNearestNeighborGrad
 */
cnnlStatus_t CNNL_WIN_API cnnlInterpBackward(cnnlHandle_t handle,
                                             bool align_corners,
                                             bool align_center,
                                             cnnlInterpBackwardMode_t mode,
                                             const cnnlTensorDescriptor_t input_desc,
                                             const void *input,
                                             const cnnlTensorDescriptor_t output_desc,
                                             void *output);

// Group:Interp
/*!
 * @brief Computes the gradients of the original images \b output based on the gradients of
 *        images after interpolation \b input for the backpropagation of ::cnnlInterp or
 *        ::cnnlInterp_v2 operation.
 *
 * Parameters of \b align_corners and \b align_center affect the calculation of
 * the scaling factor of H dimension (scale_w) and the scaling factor of W dimension (scale_h).
 *
 * Compared with ::cnnlInterpBackward, this function provides the flexibility of whether to
 * recompute the scaling factors or to use the scaling factors with additional parameters
 * \b recompute_scale_factor and \b scale_factors. This function also supports modes of linear
 * and trilinear interpolation defined in ::cnnlInterpBackwardMode_t.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in ::cnnlInterpBackward_v2 operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] align_corners
 *   Input. Boolean variable that determines the method to align corner pixels between
 *          the original images and the images after interpolation.
 *          Generally, pixels of images are considered to be squares.
 *          If \b align_corners is set to true, images after interpolation and the original images
 *          are all aligned by the center points of corner pixels.
 *          Otherwise, images after interpolation and the original images are aligned by the
 *          upper-left corner points of the corresponding corner pixels.
 * @param[in] align_center
 *   Input. Boolean variable that determines whether to shift the coordinates of each pixel from
 *          the upper-left corner coordinates by an offset of 0.5.
 *          If \b align_center is set to true, the center coordinates are used to represent each
 *          pixel.
 *          Otherwise, the upper-left corner coordinates of each pixel are used.
 *          For example, when \b align_center is set to true, the coordinates of the first pixel
 *          are (0, 0).
 *          When \b align_center is set to false, the coordinates of the first pixel are (0.5, 0.5).
 * @param[in] mode
 *   Input. The specific algorithm used to interpolate from original images to get output images.
 *          The algorithms are defined in ::cnnlInterpBackwardMode_t enum.
 * @param[in] scale_factors
 *   Input. Pointer to the host memory that stores the scale factors, which are the same as the ones
 *          passed to the ::cnnlInterp_v2.
 * @param[in] recompute_scale_factor
 *   Input. Boolean variable that determines whether or not to recompute the scale factors. When \b
 *   align_corners is set to false and \b recompute_scale_factor is set to false or not specified,
 *   the valid values set in \b scale_factors will be used in the interpolation.
 *   Otherwise, \b scale_factors will be recomputed based on the sizes of \b input and \b output.
 * @param[in] input_desc
 *   Input. The descriptor of the gradients tensor of images after interpolation.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the gradients tensor of images
 *          after interpolation.
 * @param[in] output_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 *          For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "InterpBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for \b align_corners, \b align_center,
 *   \b scale_factors, \b recompute_scale_factor, \b input, and \b output:
 *   - \b align_corners: bool.
 *   - \b align_center: bool.
 *   - \b scale_factors: float.
 *   - \b recompute_scale_factor: bool.
 *   - \b input: half, float.
 *   - \b output: half, float.
 *   - Data type of \b input and \b output should be the same.
 *
 * @par Data Layout
 * - The supported data layout of \b input and \b output are as follows:
 *   - \b input: \p CNNL_LAYOUT_NLC, \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - \b output: \p CNNL_LAYOUT_NLC, CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - Layout of \b input and \b output should be the same.
 *
 * @par Scale Limitation
 * - For all modes defined in ::cnnlInterpBackwardMode_t, \b align_corners and \b align_center
 *   cannot be set to true at the same time.
 * - If \b mode is set to CNNL_INTERP_BACKWARD_NEAREST and the layout of \b input and \b output is
 *   \p CNNL_LAYOUT_NLC, \b align_corners and \b align_center can only be set to false.
 * - When mode is \p CNNL_INTERP_BACKWARD_LINEAR or \p CNNL_INTERP_BACKWARD_TRILINEAR,
 *   \b align_corners and \b align_center cannot be set to false at the same time.
 * - When mode is \p CNNL_INTERP_BACKWARD_NEAREST or \p CNNL_INTERP_BACKWARD_BILINEAR,
 *   \b recompute_scale_factor should be set to true and \b scale_factors should be set to NULL.
 * - The half data type is not recommended due to low precision.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance,
 *   channels \p c of both \b input and \b output are recommended to be aligned to 128 Bytes.
 *
 * @note
 * - \b scale_factors should be greater than 0.
 * - If \b recompute_scale_factor is true, then \b scale_factors must be set to NULL.
 * - When mode is \p CNNL_INTERP_BACKWARD_LINEAR, the shape and layout of \b input should be
 *   [n, li, c] and \p CNNL_LAYOUT_NLC respectively. The shape of \b output should be [n, lo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_NEAREST, the layout of \b input and \b output should be
 *   \p CNNL_LAYOUT_NLC or \CNNL_LAYOUT_NHWC. If the layout of \b input and \b output is
 *   \p CNNL_LAYOUT_NLC, the shape of \input and \b output should be [n, li, c] and [n, lo, c]
 *   respectively. If the layout of \b input and \b output is \p CNNL_LAYOUT_NHWC, the shape of
 *   \b input and \b output should be [n, hi, wi, c] and [n, ho, wo, c] respectively.
 * - When mode is \p CNNL_INTERP_BACKWARD_BILINEAR, the shape and layout of \b input should be
 *   [n, hi, wi, c] and \p CNNL_LAYOUT_NHWC respectively. The shape of \b output should
 *   be [n, ho, wo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_TRILINEAR, the shape and layout of \b input should be
 *   [n, di, hi, wi, c] and \p CNNL_LAYOUT_NDHWC respectively. The shape of \b output should be
 *   [n, do, ho, wo, c].
 * - When mode is \p CNNL_INTERP_BACKWARD_BICUBIC, the shape and layout of \b input should be
 *   [n, hi, wi, c] and \p CNNL_LAYOUT_NHWC respectively. The shape of \b output should be
 *   [n, ho, wo, c].
 * - \p n and \p c represent the batches and channels of both \b input and \b output tensor
 *   respectively.
 *   \p di and \p do represent the depth of \b input and \b output tensor respectively.
 *   \p hi and \p ho represent the height of \b input and \b output tensor respectively.
 *   \p wi and \p wo represent the width of \b input and \b output tensor respectively.
 *   \p li and \p lo represent the length of \b input and \b output tensor respectively.
 * - Given valid \b scale_factors, \p di and \p do should satisfy the following equation (C-style):
 *   \p di = (int)((double)\p scale_factor_d * \p do), where \p scale_factor_d is the scale factor
 *   for d-dimension. The same relationship applies to other dimensions except \p n and \p c.
 * - When \b input contains infinity:
 *   - On MLU300 series and CE3226:
 *     - If \b mode is ::CNNL_INTERP_BACKWARD_BILINEAR, \b output will contain infinity.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingBilinear2d
 * - https://www.pytorch.org/docs/stable/generated/torch.nn.UpsamplingNearest2d
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=interp#torch.nn.functional.inter
 *   polate
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeBilinearGrad
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/ResizeNearestNeighborGrad
 */
cnnlStatus_t CNNL_WIN_API cnnlInterpBackward_v2(cnnlHandle_t handle,
                                                bool align_corners,
                                                bool align_center,
                                                cnnlInterpBackwardMode_t mode,
                                                const float scale_factors[],
                                                bool recompute_scale_factor,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output);

// Group:ApplyAdaGrad
/*!
 * @brief Updates \b var tensor by using adagrad method. If the \b update_slots is
 * true, this operation also updates the \b accum tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyAdaGrad. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor. It is the gradient of \b var.
 *   With the \b grad value, ::cnnlApplyAdaGrad function can be used to calculate \b accum and \b var.
 * @param[in] accum_desc
 *   Input. The descriptor of the \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \b accum tensor.
 *   The \b accum is the accumulation of gradient. It will be updated when \b update_slots is true.
 * @param[in] var_desc
 *   Input. The descriptor of the \b var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \b var tensor.
 *   The \b var value is the optimization goal of whole algorithm.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] update_slots
 *   Input. A boolean value that specifies whether to update \b accum. If the value of
 *   this parameter is true, \b accum is updated. If the value of this parameter is false,
 *   \b accum will not be updated.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b grad tensor, \b accum tensor, \b var tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - grad tensor: half, float
 *   - accum tensor: half, float
 *   - var tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \b grad, \b accum and \b var should be same.
 * - The number of dimensions is no more than 8.
 *
 * @note
 * - The \b accum should be greater than zero.
 * - The precision may lose if the data type of \b accum is half, especially when \b accum is close to
 *   zero or (var - lr * grad / sqrt(accum)) is close to zero. If the data type of \b accum is half,
 *   the \b accum value should be greater than 5e-5 to avoid precision loss.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad:  [0, 1, 2, 3]
     --> accum: [0, 1, 2, 3]
     --> var:   [0, 1, 2, 3]

     param:
        lr: 0.01, update_slots:true,

     output array by 4, 4
      --> accum: [0,2,6,12]
      --> var: [0, 0.992929,1.99181,2.99134]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlApplyAdaGrad(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t grad_desc,
                 const void *grad,
                 const cnnlTensorDescriptor_t accum_desc,
                 void *accum,
                 const cnnlTensorDescriptor_t var_desc,
                 void *var,
                 const void *lr,
                 const bool update_slots);
// Group:ApplyAdaGrad
/*!
 * @brief Updates \b var tensor by using adagrad method. If the \b update_slots is
 * true, this operation also updates the \b accum tensor. Compared with ::cnnlApplyAdaGrad,
 * this function uses \b epsilon to avoid division by 0.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyAdaGradV2. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor. It is the gradient of \b var.
 *   With the \b grad value, ::cnnlApplyAdaGradV2 function can be used to calculate \b accum and \b var.
 * @param[in] accum_desc
 *   Input. The descriptor of the \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \b accum tensor.
 *   The \b accum is the accumulation of gradient. It will be updated when \b update_slots is true.
 * @param[in] var_desc
 *   Input. The descriptor of the \b var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \b var tensor.
 *   The \b var value is the optimization goal of whole algorithm.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \b epsilon parameter.
 *   It is a small positive number just as 10^-8, to avoid division by 0.
 * @param[in] update_slots
 *   Input. A boolean value that specifies whether to update \b accum. If the value of
 *   this parameter is true, \b accum is updated. If the value of this parameter is false,
 *   \b accum will not be updated.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - Data type of \b grad tensor, \b accum tensor and \b var tensor must be the same.
 * - And also, the data type of \b lr parameter and \b epsilon parameter must be the same as
 * \b var tensor.
 * - The supported data types are as follows:
 *   - grad tensors: half, float.
 *   - accum tensors: half, float.
 *   - var tensors: half, float.
 *   - lr parameter: half, float
 *   - epsilon parameter: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \b grad, \b accum and \b var should be same.
 *
 * @note
 * - It is recommended to use data type of float for \b grad, \b accum and \b var for higher
 * precision.
 * - On MLU200 series, if the data type is half and the data value is not in the range of
 * [-65504, 65504], the value will be treated as a saturation value.
 * - On MLU200 series, if the value of (\b accum + \b grad * \b grad) is less than 0, the output
 * \b var will be unpredictable.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad:  [0, 1, 2, 3]
     --> accum: [0, 1, 2, 3]
     --> var:   [0, 1, 2, 3]

     param:
        lr: 0.01, epsilon: 0.0001, update_slots: true,

     output array by 4, 4
      --> accum: [0, 2, 6, 12]
      --> var: [0, 0.992929, 1.991835, 2.99134]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */

cnnlStatus_t CNNL_WIN_API
cnnlApplyAdaGradV2(cnnlHandle_t handle,
                   const cnnlTensorDescriptor_t grad_desc,
                   const void *grad,
                   const cnnlTensorDescriptor_t accum_desc,
                   void *accum,
                   const cnnlTensorDescriptor_t var_desc,
                   void *var,
                   const void *lr,
                   const void *epsilon,
                   const bool update_slots);

// Group:BceWithLogits
/*!
 * @brief Computes binary cross entropy with \b input tensor, \b target tensor,
 * \b filter tensor and \b pos_filter tensor, and returns the results
 * in the \b output tensor.
 *
 * If \b filter tensor and \b pos_filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \b filter tensor and \b pos_filter tensor.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetBceWithLogitsWorkspaceSize
 * function.
 *
 * To perform the binary cross entropy operation with faster algorithm, use the
 * ::cnnlBceWithLogits_v2 function.
 *
 * @deprecated
 *   ::cnnlBceWithLogits is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlBceWithLogits_v2 instead.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceWithLogits. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \b filter tensor is
 *   a filter of the binary cross entropy. Set it to NULL if not required.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \b pos_filter
 *   tensor is a filter for dealing with imbalance. Set it to NULL if not required.
 * @param[in] reduction
 *   Input. An enum value that describes the reduction dimension. For detailed information,
 *   see ::cnnlBceWithLogitsReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceWithLogits. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceWithLogits. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsWorkspaceSize function.
 * @param[in] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \b output
 *   tensor is the result of ::cnnlBceWithLogits.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BceWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b target tensor, \b filter tensor,\b pos_filter tensor
 *   and \b output tensor.
 *   <b>Note that the combinations of these tensors must be half-half or float-float.</b>
 *   - input: half, float.
 *   - target: half, float.
 *   - filter: half, float.
 *   - pos_filter: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetBceWithLogitsWorkspaceSize function to
 *   get extra workspace size for ::cnnlBceWithLogits_v2 function.
 *
 * @note
 * - The number of dimensions of \b input tensor and that of \b target tensor, should be the same.
 * - If \b reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   then the \b output tensor should be a number. Otherwise, the dimension of \b output tensor and
 *   \b input tensor should be the same.
 * - For each dimension, the dimension length of the \b filter tensor and the \b target tensor need to meet the requirements of broadcasting.
 * - For each dimension, the dimension length of the \b pos_filter tensor and the \b target tensor need to meet the requirements of broadcasting.
 * - The precision cannot be ensured for large amounts of data if the data type of input, target, filter, pos_filter is half, because the maximum number of half is 65504.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogits function is as follows:
     @verbatim
       input:  shape=(4) -> [1.5, 1.5, 1.5, 1.5],

       target:  shape=(4) -> [1, 1, 1, 1],

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [0.2014].

     @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogits(cnnlHandle_t handle,
                  const cnnlTensorDescriptor_t desc_input,
                  const void *input,
                  const cnnlTensorDescriptor_t desc_target,
                  const void *target,
                  const cnnlTensorDescriptor_t desc_filter,
                  const void *filter,
                  const cnnlTensorDescriptor_t desc_pos_filter,
                  const void *pos_filter,
                  const cnnlBceWithLogitsReduction_t reduction,
                  void *workspace,
                  size_t workspace_size,
                  const cnnlTensorDescriptor_t desc_output,
                  void *output);

// Group:BceWithLogits
/*!
 * @brief Computes binary cross entropy with \b input tensor, \b target tensor,
 * \b filter tensor and \b pos_filter tensor, and returns the results
 * in the \b output tensor.
 *
 * Compared with ::cnnlBceWithLogits, this function allows you to choose whether to perform the
 * binary cross entropy operation with faster algorithm or higher precision.
 *
 * If \b filter tensor and \b pos_filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \b filter tensor and \b pos_filter tensor.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetBceWithLogitsWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceWithLogits_v2. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t. Only supports \p CNNL_COMPUTATION_HIGH_PRECISION and
 *   \p CNNL_COMPUTATION_FAST currently.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \b filter tensor is
 *   a filter of the binary cross entropy. Set it to NULL if not required.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of pos_filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] pos_filter
 *   Input. Pointer to the MLU memory that stores the pos_filter tensor. The \b pos_filter
 *   tensor is a filter for dealing with imbalance. Set it to NULL if not required.
 * @param[in] reduction
 *   Input. An enum value that describes the reduction dimension. For detailed information,
 *   see ::cnnlBceWithLogitsReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceWithLogits_v2. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceWithLogits_v2. You can get the size of the workspace with
 *   the ::cnnlGetBceWithLogitsWorkspaceSize function.
 * @param[in] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \b output
 *   tensor is the result of ::cnnlBceWithLogits_v2.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "BceWithLogits Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b target tensor, \b filter tensor,\b pos_filter tensor
 *   and \b output tensor.
 *   <b>Note that the combinations of these tensors must be half-half or float-float.</b>
 *   - input: half, float.
 *   - target: half, float.
 *   - filter: half, float.
 *   - pos_filter: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlGetBceWithLogitsWorkspaceSize function to
 *   get extra workspace size for ::cnnlBceWithLogits_v2 function.
 *
 * @note
 * - The number of dimensions of \b input tensor and that of\b target tensor, should be the same.
 * - If \b reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   then the \b output tensor should be a number. Otherwise, the dimension of \b output tensor and
 *   \b input tensor should be the same.
 * - For each dimension, the dimension length of the \b filter tensor and the \b target tensor need
 *   to meet the requirements of broadcasting.
 * - For each dimension, the dimension length of the \b pos_filter tensor and the \b target tensor
 *   need to meet the requirements of broadcasting.
 * - The precision cannot be ensured for large amount of data if the data type of input, target,
 *   filter, pos_filter is half, because the maximum number of half is 65504.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceWithLogits_v2 function is as follows:
     @verbatim
       input:  shape=(4) -> [1.5, 1.5, 1.5, 1.5],

       target:  shape=(4) -> [1, 1, 1, 1],

       filter: NULL,

       pos_filter:  NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [0.2014].

     @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceWithLogits_v2(cnnlHandle_t handle,
                     const cnnlComputationPreference_t prefer,
                     const cnnlTensorDescriptor_t desc_input,
                     const void *input,
                     const cnnlTensorDescriptor_t desc_target,
                     const void *target,
                     const cnnlTensorDescriptor_t desc_filter,
                     const void *filter,
                     const cnnlTensorDescriptor_t desc_pos_filter,
                     const void *pos_filter,
                     const cnnlBceWithLogitsReduction_t reduction,
                     void *workspace,
                     const size_t workspace_size,
                     const cnnlTensorDescriptor_t desc_output,
                     void *output);

// Group:BceWithLogits
/*!
 * @brief Returns in \b size the size of the host memory that is used as an extra workspace
 * in ::cnnlBceWithLogits or ::cnnlBceWithLogits_v2 operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlBceWithLogits or
 * ::cnnlBceWithLogits_v2, including the \b target tensor descriptor \b target_desc, \b filter
 * tensor descriptor \b filter_desc, and \b pos_filter tensor descriptor \b pos_filter_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceWithLogitsWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_filter
 *   Input. The descriptor of \b filter tensor. The \b filter tensor is a filter
 *   of the binary cross entropy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] desc_pos_filter
 *   Input. The descriptor of \b pos_filter tensor. The \b pos_filter tensor is
 *   a filter for dealing with imbalance.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra MLU workspace in bytes that is used in
 *   ::cnnlBceWithLogits or ::cnnlBceWithLogits_v2 function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBceWithLogitsWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t desc_input,
                                  const cnnlTensorDescriptor_t desc_filter,
                                  const cnnlTensorDescriptor_t desc_pos_filter,
                                  size_t *size);

// Group:Flip
/*!
 * @brief Reverses the order of an N-dimensional tensor along the axis in dims \b dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   flip operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] dimension
 *   Input. An array that stores the axis to flip on. For each dimension.
 * @param[in] dimenson_len
 *   Input. The length of \b dimension which is an array that stores the axis to flip on.
 * @param[in] desc_input
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores input tensor.
 * @param[in] desc_ouput
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to MLU memory that stores output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output
 *   tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor should be same.</b>
 *   - input: uint8, int8, uint16, int16, uint32, int31, int32, bool, half, float.
 *   - output: uint8, int8, uint16, int16, uint32, int31, int32, bool, half, float.
 *
 * @par Data Layout
 * - The dimension of input tensor should be less than or equal to 8-dimension.
 *
 * @par Scale Limitation
 * - The \b dimension_len and \b dimension must meet the following requirements:
 *   - \p 0 < dimension_len <= n, where n is the rank of \b input.
 *   - \p -n <= dimension[i] < n, where n is the rank of \b input.
 *   - \p dimension[i] is not equal to dimension[j], where i is not equal to j.
 *   If dimension[i] < 0, which corresponds to dimension[i] + n.
 * - The \b input and \b output have same shape, \b input or \b output dimension i should be
 *   greater than 0.
 * - The size of \b input should be less than or equal to 2^31-1.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the flip operation is as follows:
     @verbatim
     input array by 2 * 2 * 2-->
         input: [[[0, 1], [2, 3]],
                 [[4, 5], [6, 7]]]
     param:
       dimension_len: 3, dimension: (0, 1, 2),

     output array by 2 * 2 * 2 -->
         output: [[[7, 6], [5, 4]],
                  [[3, 2], [1, 0]]]
   @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlFlip(cnnlHandle_t handle,
          const int dimension[],
          const int dimension_len,
          const cnnlTensorDescriptor_t desc_input,
          const void * input,
          const cnnlTensorDescriptor_t desc_output,
          void * output);

// Group:PoolingWithIndex
/*!
 * @brief Computes the maximum pooling forward and pooling index with the ::cnnlPoolingDescriptor_t
 * pooling_desc, returns the maximum pooling in the \b output tensor, and the index in the \b index tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPoolingForwardWithIndex. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of pooling operation. For detailed information,
 *   see ::cnnlPoolingMode_t.
 * @param[in] alpha, beta
 *   Input. Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor. The
 *   \b y tensor is the result of maximum pooling.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the index of the \b y tensor.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlPoolingForwardWithIndex. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlPoolingForwardWithIndex. You can get the size of the workspace with
 *   the ::cnnlGetPoolingWithIndexWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Pooling Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b output tensor, \b index tensor.
 *   <b>Note that the combinations of these tensors must be half-half-int16 or float-float-int32.</b>
 *   - input tensor: half, float
 *   - output tensor: half, float
 *   - index tensor: int16, int32
 *
 * @par Data Layout
 * - The supported data layout of the \b x tensor, \b y tensor, \b index tensor are as follows:
 *   <b>Note that the layout of these tensor must be same.</b>
 *   - x tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - y tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW.
 *
 * @par Performance Optimization
 * - For best practices, to have better performance, set the layout of the \b x
 *   tensor, \b y tensor, \b index tensor to NHWC.
 *
 * @note
 * - Currently, 2-D and 3-D pooling have been supported.
 * - In the 2-D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN, and the \b index value is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN and
 *      the \b index is the index of the last value. Otherwise, the \b output value is the maximum
 *      value after the last NaN and the \b index value is the index of the output value.
 * - In the 3-D maximum pooling, the \b output is unpredictable since a NaN value is noncomparable.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters in the \b pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1.
 * - The parameters in the \b x_desc should satisfy the following conditions: in_batch > 0, in_height > 0, in_width > 0, in_channel > 0.
 *   The in_batch is the batch of the input tensor. The in_height is the height of the input tensor.
 *   The in_width is the width of the input tensor. The in_channel is the channel of the input tensor.
 * - The parameters in the \b y_desc should satisfy the following conditions: out_batch > 0, out_height > 0, out_width > 0, out_channel > 0.
 *   The out_batch is the batch of the output tensor. The out_height is the height of the output tensor.
 *   The out_width is the width of the output tensor. The out_channel is the channel of the output tensor.
 * - The parameters kernel in the \b pooling_desc should satisfy the following conditions:
 *   kw * kh < 1535. The kw and kh represent the width and the height of the pooling kernel size respectively.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 *  - The example of this operation is as follows:
     @verbatim
      input layout is NHWC, and input shape is (1,4,4,1).
        input: [[[1,2,3,4],
                 [5,6,7,8],
                 [9,10,11,12],
                 [13,14,15,16]]]
      param:
        pad:(0,0,0,0), stride:(2,2), kernel:(2,2), mode: CNNL_POOLING_MAX

      output: [[[6],[8],[14],[16]]]

      index:  [[[3],[3],[3],[3]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d
 * - https://pytorch.org/docs/stable/nn.functional.html#max-pool2d
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPoolingForwardWithIndex(cnnlHandle_t handle,
                                                      const cnnlPoolingDescriptor_t pooling_desc,
                                                      const void *alpha,
                                                      const cnnlTensorDescriptor_t x_desc,
                                                      const void *x,
                                                      const void *beta,
                                                      const cnnlTensorDescriptor_t y_desc,
                                                      void *y,
                                                      const cnnlTensorDescriptor_t index_desc,
                                                      void *index,
                                                      void *workspace,
                                                      size_t workspace_size);

// Group:PoolingWithIndex
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlPoolingForwardWithIndex operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlPoolingForwardWithIndex,
 * including the \b x tensor descriptor \b x_desc, \b y tensor descriptor \b y_desc. For more information
 * about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetPoolingWithIndexWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of \b x tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of \b y tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlPoolingForwardWithIndex operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlPoolingForwardWithIndex function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlGetPoolingWithIndexWorkspaceSize(cnnlHandle_t handle,
                                                               const cnnlTensorDescriptor_t x_desc,
                                                               const cnnlTensorDescriptor_t y_desc,
                                                               size_t *workspace_size);
// Group:NanInf
/*!
 * @brief Checks if the exception values NaN (not a number) or infinity is contained in the
 *        input tensor \b input.
 *
 * You can call this function to check if the input tensor includes NaN or infinity. Returns true when
 * the following conditions are met:
 *
 * - The data type of the input tensor is half and the value of input tensor includes NaN, infinity,
 *   0x7bff, or 0xfbff.
 * - The data type of the input tensor is float and the value of input tensor includes NaN, infinity,
 *   0x7f7fffff, or 0xff7fffff.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the nan_inf
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output value.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "NanInf Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b input
 *   and output \b output:
 *   - input tensor: half, float.
 *   - output: bool.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlNanInf(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void * input,
                                     bool * output);

// Group: Normalize
/*!
 * @brief Creates a descriptor pointed by \b desc for normalization operation, and allocates
 * memory for holding the information about the normalization operation. The information is
 * defined in ::cnnlNormalizeDescriptor_t. For more information about descriptor,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] desc
 *   Input. Pointer to the normalization descriptor that holds information about the ::cnnlNormalize.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetNormalizeDescriptor function to initialize
 *   and set the information to the normalization descriptor.
 * - You need to call the ::cnnlDestroyNormalizeDescriptor function to destroy the descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateNormalizeDescriptor(cnnlNormalizeDescriptor_t *desc);

// Group: Normalize
/*!
 * @brief Destroys a normalization descriptor \b desc that is previously created with the
 *        ::cnnlCreateNormalizeDescriptor function.
 *
 * The normalization descriptor is defined in ::cnnlNormalizeDescriptor_t and holds
 * the information about the ::cnnlNormalize operation.
 *
 * @param[in] desc
 *   Input. The normalization descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlNormalize operation.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyNormalizeDescriptor(cnnlNormalizeDescriptor_t desc);

// Group: Normalize
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * to optimize the normalize operation.
 *
 * The size of extra workspace is based on the given information of the normalize operation,
 * including the normalize descriptor \b normalize_desc, input tensor descriptor \b input_desc,
 * output tensor descriptor \b output_desc and norm tensor descriptor \b norm_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the normalize operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  normalize_desc
 *   Input. The descriptor of the normalize operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] norm_desc
 *   Input. The descriptor of the norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the normalize operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called to calculate the workspace size \b workspace_size.
 * - The allocated extra workspace should be passed to the ::cnnlNormalize_v3 function
 *   to perform the normalize operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetNormalizeWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlNormalizeDescriptor_t
                                                        normalize_desc,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const cnnlTensorDescriptor_t output_desc,
                                                        const cnnlTensorDescriptor_t norm_desc,
                                                        size_t *workspace_size);
// Group: Normalize
/*!
 * @brief Initializes the normalization descriptor \b normalize_desc that is previously created with the
 * ::cnnlCreateNormalizeDescriptor function, and sets the information about the ::cnnlNormalize or ::cnnlNormalize_v2
 * to the normalization descriptor \b normalize_desc. Different from ::cnnlSetNormalizeDescriptor, :cnnlSetNormalizeDescriptor_v2
 * supports arbitrary p-norm and scaling the \b output tensor by setting the parameters \b pnorm, \b channel_shared and \b across_spatial,
 * and then launchs the function ::cnnlNormalize_v2 or ::cnnlNormalize. Note that if the function ::cnnlNormalize is launched, the
 * \b channel_shared and \b across_spatial will be regarded as 0 in this function. The information includes the normalization \b axis,
 * the number of axises \b axis_num, the NaN propagation mode \b nan_propagation, the lower bound of normalization \b eps,
 * the exponent value \b pnorm, control parameters \b channel_shared and \b across_spatial.
 *
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] axis[]
 *   Input. The dimension vector of the ::cnnlNormalize operation. The size of axis vector cannot be greater than the dimension
 *   size of the input tensor \b input that is used in the ::cnnlNormalize.
 * @param[in] axis_num
 *   Input. The number of axises. The \b axis_num should equal to the size of \b axis.
 * @param[in] nan_propagation
 *   Input. The NaN propagation mode defined in ::cnnlNormalizeMode_t.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 * @param[in] eps
 *   Input. A lower bound value for \b norm that is used in ::cnnlNormalize or ::cnnlNormalize_v2. If the value of \b norm is less than
 *   the value of this parameter, this parameter is used as the divisor.
 * @param[in] pnorm
 *   Input. The exponent value in the norm formulation.
 * @param[in] channel_shared
 *   Input. If it is set to 0, the \b output tensor will not be scaled; If it is set to 1, the \b output tensor will be scaled by the only one
 *   element in \b scale tensor; If it is set to 2, the \b output tensor will be scaled by the different element in \b scale tensor.
 * @param[in] across_spatial
 *   Input. If it is set to 0, the input \b axis[] will be used; If it is set to 1, the input \b axis[] will be discarded and replaced by [n-1];
 *   If it is set to 2, the input \b axis[] will be discarded and replaced by [1,...,n-1], where n is equal to the dimension of input \b input tensor.
 *
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follow:
 * - ::cnnlCreateNormalizeDescriptor needs to be called before this function.
 * - ::cnnlDestroyNormalizeDescriptor needs to be called after this function.
 *
 * @note
 * - None.
 *
 * @par requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetNormalizeDescriptor_v2(cnnlNormalizeDescriptor_t normalize_desc,
                           int axis[],
                           int axis_num,
                           cnnlNanPropagation_t nan_propagation,
                           float eps,
                           float pnorm,
                           int channel_shared,
                           int across_spatial);

// Group: Normalize
/*!
 * @brief Initializes the normalization descriptor \b normalize_desc that is previously created with the
 * ::cnnlCreateNormalizeDescriptor function, and sets the information about the ::cnnlNormalize or ::cnnlNormalize_v2
 * to the normalization descriptor \b normalize_desc. cnnlSetNormalizeDescriptor only supports 2-norm,
 * if there is a need to use arbitrary p-norm or scale the \b output tensor, please refer to ::cnnlSetNormalizeDescriptor_v2.
 * The information includes the normalization \b axis, the number of axises \b axis_num,
 * the NaN propagation mode \b nan_propagation, and the lower bound of normalization \b eps.
 *
 * @deprecated
 *   ::cnnlSetNormalizeDescriptor is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlSetNormalizeDescriptor_v2 instead, which supports the parameters \b pnorm, \b channel_shared
 *   and \b across_spatial.
 *
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] mode
 *   Input. The \b mode describes the algorithm that is used in the implementation of the ::cnnlNormalize operation.
 *   This parameter only supports CNNL_NORMALIZE_EUCLIDEAN now.
 * @param[in] axis[]
 *   Input. The dimension vector of the ::cnnlNormalize operation. The size of axis vector cannot be greater than the dimension
 *   size of the input tensor \b input that is used in the ::cnnlNormalize.
 * @param[in] axis_num
 *   Input. The number of axises. The \b axis_num should equal to the size of \b axis.
 * @param[in] nan_propagation
 *   Input. The NaN propagation mode defined in ::cnnlNormalizeMode_t.
 *   Only supports CNNL_NOT_PROPAGATE_NAN now.
 * @param[in] eps
 *   Input. A lower bound value for \b norm that is used in ::cnnlNormalize. If the value of \b norm is less than the value of this parameter,
 *   this parameter is used as the divisor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS
 *
 * @par API Dependency
 * - Some functions need to be called before and after this function. The dependencies are as follow:
 * - ::cnnlCreateNormalizeDescriptor needs to be called before this function.
 * - ::cnnlDestroyNormalizeDescriptor needs to be called after this function.
 *
 * @note
 * - None.
 *
 * @par requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetNormalizeDescriptor(cnnlNormalizeDescriptor_t normalize_desc,
                           int axis[],
                           int axis_num,
                           cnnlNormalizeMode_t mode,
                           cnnlNanPropagation_t nan_propagation,
                           float eps);

// Group:Normalize
/*!
 * @brief Normalizes the input tensor \b input with the normalization descriptor \b normalize_desc,
 * and returns the p-norm in \b norm tensor, and the normalization results in the \b output tensor.
 * If \b channel_shared or \b across_spatial has been set in cnnlSetNormalizeDescriptor_v2, \b channel_shared and
 * \b across_spatial will be regarded as 0 in this function.
 *
 * @deprecated
 *   ::cnnlNormalize is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlNormalize_v2 instead, which supports the \b scale tensor that determines
 *   whether to scale the \b output tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlNormalize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor. The
 *   \b output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \b norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \b norm tensor. The \b
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b output tensor, \b norm tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - input tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize function only supports normalization if the dimension of the input param \b axis[] is continuous when the \b across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \b input tesnsor is less than 2e-5 in half type, the accuracy problem may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlNormalize(cnnlHandle_t handle,
                                        const cnnlNormalizeDescriptor_t normalize_desc,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void * input,
                                        const cnnlTensorDescriptor_t output_desc,
                                        void * output,
                                        const cnnlTensorDescriptor_t norm_desc,
                                        void * norm);

// Group:Normalize
/*!
 * @brief Normalizes the input tensor \b input with the normalization descriptor \b normalize_desc,
 * returns the p-norm in \b norm tensor, and the normalization results in the \b output tensor.
 * If \b channel_shared is set to 1 or 2 in ::cnnlSetNormalizeDescriptor_v2, the \b scale tensor will scale
 * the \b output tensor. And if \b channel_shared is set to 0, the \b scale tensor will not work.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlNormalize_v2. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the \b scale tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \b scale tensor. The \b
 *   scale is used to scale the \b output tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor. The
 *   \b output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \b norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \b norm tensor. The \b
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b scale tensor, \b output tensor and \b norm tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - input tensor: half, float
 *   - scale tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize function only supports normalization if the dimension of the input param \b axis[] is continuous when the across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \b input tesnsor is less than 2e-5 in half type, the accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 */

cnnlStatus_t CNNL_WIN_API cnnlNormalize_v2(cnnlHandle_t handle,
                                          const cnnlNormalizeDescriptor_t normalize_desc,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void * input,
                                          const cnnlTensorDescriptor_t scale_desc,
                                          const void * scale,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          const cnnlTensorDescriptor_t norm_desc,
                                          void * norm);

// Group: Normalize
/*!
 * @brief Normalizes the input tensor \b input with the normalization descriptor \b normalize_desc,
 * returns the p-norm in \b norm tensor, and returns the normalization results in the \b output tensor.
 * If \b channel_shared is set to 1 or 2 in ::cnnlSetNormalizeDescriptor_v2, the \b scale tensor will scale
 * the \b output tensor. And if \b channel_shared is set to 0, the \b scale tensor will not work. Compared with
 * ::cnnlNormalize_v2, ::cnnlNormalize_v3 supports p-norm and normalization for discontinuous axes with extra
 * input space.
 *
 * * @deprecated
 *   ::cnnlNormalize and ::cnnlNormalize_v2 are deprecated and will be removed in the future release. It is
 *   recommended to use ::cnnlNormalize_v3 instead, which supports the parameters \b pnorm, \b channel_shared
 *   and \b across_spatial.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlNormalize_v3. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] normalize_desc
 *   Input. The descriptor of normalization operation. For detailed information,
 *   see ::cnnlNormalizeDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] scale_desc
 *   Input. The descriptor of the \b scale tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] scale
 *   Input. Pointer to the MLU memory that stores the \b scale tensor. The \b
 *   scale is used to scale the \b output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   normalize operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the normalize operation. You can get the size of the workspace with
 *   the ::cnnlGetNormalizeWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor. The
 *   \b output tensor is the results of the normalization operation.
 * @param[in] norm_desc
 *   Input. The descriptor of the \b norm tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] norm
 *   Output. Pointer to the MLU memory that stores the \b norm tensor. The \b
 *   norm tensor is the results of p-norm.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b scale tensor, \b output tensor and \b norm tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - input tensor: half, float
 *   - scale tensor: half, float
 *   - output tensor: half, float
 *   - norm tensor: half, float
 *
 * @note
 * - The ::cnnlNormalize_v3 function supports normalization if the dimension of the input parameter \b axis[] is discontinuous when the across_spatial is set to 0.
 * - The half type only can represent up to 65504, thus when the results of p-norm exceeds 65504 in half type, there will be an accuracy problem.
 * - This function involves using the activation table for high precision in MLU270, such as exphp (exp in high precision mode), loghp (log in high precision
 *   mode) and sqrthp (sqrt in high precision mode). When the absolute value of \b input tesnsor is less than 2e-5 in half type, the accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/master/nn.functional.html?highlight=normalize#toch.nn.functional.normalize
 */

cnnlStatus_t CNNL_WIN_API cnnlNormalize_v3(cnnlHandle_t handle,
                                          const cnnlNormalizeDescriptor_t normalize_desc,
                                          const cnnlTensorDescriptor_t input_desc,
                                          const void * input,
                                          const cnnlTensorDescriptor_t scale_desc,
                                          const void * scale,
                                          void *workspace,
                                          const size_t workspace_size,
                                          const cnnlTensorDescriptor_t output_desc,
                                          void * output,
                                          const cnnlTensorDescriptor_t norm_desc,
                                          void * norm);
/*!
 * @brief Computes gradients of 2D or 3D adaptive average and maximum pooling. For detailed information,
 *        see ::cnnlAdaptivePoolingForward.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the adaptive
 *   pooling backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the index tensor used in maximum mode. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the tensor used to store index of each element of
 *   \b dx in corresponding pooling kernel.
 * @param[in] mode
 *   Input. The pooling mode, which is defined in the ::cnnlPoolingMode_t enum.
 * @param[in] dx_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "AdaptivePoolingBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   \b y - \b index - \b output.
 *
 *   The supported data type combinations are:
 *
 *   - half-int16-half.
 *   - float-int32-float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor, index tensor, and output tensor are
 *   as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - \b mode: \b mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING or \b mode ==
 *   \p CNNL_POOLING_MAX
 * - In the 2-D adaptive pooling backward, pooling acts on the HW dimension. The input tensor, output tensor, index tensor, and mode must meet the following requirements:
 *   - The input tensor and output tensor must have four dimensions.
 *   - Size of the first and fourth dimension of input tensor and output tensor should be the same.
 *   - input tensor: \p batch > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: each dimension should fall within the range of int32. When \b mode == \p
 *     CNNL_POOLING_MAX, the size of each kernel should meet the following requirement, where the maximum
 *     kernel size \p max_kernel_size is computed as (\p output_h / \p input_h + 2) * (\p output_w / \p input_w + 2):
 *      - if the data type of \b output is float32, (\p max_kernel_size - 1) should be no greater than (2^31 - 1),
 *        which is the upper limit of int32;
 *      - if the data type of \b output is float16, (\p max_kernel_size - 1) should be no greater than (2^15 - 1), which is
 *        the upper limit of int16.
 *   - If \b mode == \p CNNL_POOLING_MAX, the index tensor should have four dimensions and the same
 *     shape as the input tensor.
 * - In the 3-D adaptive pooling backward, pooling acts on the DHW dimension. The input tensor, output tensor, index tensor, and mode must meet the following requirements:
 *   - The input tensor and output tensor must have five dimensions.
 *   - The first and fifth dimension of input tensor and output tensor should be the same.
 *   - input tensor: \p batch > 0, \p depth > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: kernel size cannot be too big, and CNNL_POOLING_MAX mode should satisfy
 *     (\p output_d / \p input_d + 2) * (\p output_h / \p input_h + 2) * (\p output_w / \p input_w + 2) <= 3582
 *     CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING mode should satisfy
 *     (\p output_h / \p input_h + 2) * (\p output_w / \p input_w + 2) <= 3582
 *   - If \b mode == \p CNNL_POOLING_MAX, the index tensor should have five dimensions and the same
 *     shape as the input tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool3d.html
 * - https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlAdaptivePoolingBackward(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t y_desc,
                                                      const void * y,
                                                      const cnnlTensorDescriptor_t index_desc,
                                                      const void * index,
                                                      cnnlPoolingMode_t mode,
                                                      const cnnlTensorDescriptor_t dx_desc,
                                                      void * dx);

/******************************************************************************
 * Cambricon CNNL FusedOp
 ******************************************************************************/

/*! A pointer to cnnlFusedOpsPlan struct that holds the descriptors of the fusion operation
 *  including the constant parameters and variant parameters.
 *
 *  You need to call the ::cnnlCreateFusedOpsPlan function to create a plan, and set the fusion
 *  information to this plan. Also, you need to destroy the plan at the end with the
 *  ::cnnlDestroyFusedOpsPlan function.
 */
typedef struct cnnlFusedOpsPlan* cnnlFusedOpsPlan_t;

/*! A pointer to cnnlFusedOpsConstParamPack struct that holds the information of the constant
 *  parameters in the fusion operation such as the operator descriptors and the tensor descriptors.
 *  You need to call the ::cnnlCreateFusedOpsConstParamPack function to
 *  create a pack and call the ::cnnlSetFusedOpsConstParamPackAttribute
 *  function to add a new constant parameter to the pack.
 *  You can call the ::cnnlGetFusedOpsConstParamPackAttribute function
 *  to get a specific parameter from the pack.
 *  Also, you need to destroy the pack at the end with the
 *  ::cnnlDestroyFusedOpsConstParamPack function.
 */
typedef struct cnnlFusedOpsConstParamPack* cnnlFusedOpsConstParamPack_t;

/*! A pointer to cnnlFusedOpsVariantParamPack struct that holds the information of the variant
 *  parameters in the fusion operation such as the input data addresses and the workspace address.
 *
 *  You need to call the ::cnnlCreateFusedOpsVariantParamPack function to
 *  create a pack and call the ::cnnlSetFusedOpsVariantParamPackAttribute function
 *  to add a new variant parameter to the pack. You can call
 *  the ::cnnlGetFusedOpsVariantParamPackAttribute function to get a specific
 *  parameter from the pack.
 *  Also, you need to destroy the pack at the end with the
 *  ::cnnlDestroyFusedOpsVariantParamPack function.
 */
typedef struct cnnlFusedOpsVariantParamPack* cnnlFusedOpsVariantParamPack_t;

/*!
 * @brief Enumeration variables describing the supported fusion mode in CNNL. The fusion mode
 * specifies the order of the operations to be implemented.
 */
typedef enum {
  CNNL_CONV_SCALE_BN_ACTIVATION,
  /*!< Per-channel basis, performs these operations in following order: ::cnnlConvolutionForward,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlActivationForward. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. */
  CNNL_CONV_SCALE_BN_PRELU,
  /*!< Per-channel basis, performs these operations in following order: ::cnnlConvolutionForward,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlPrelu. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. Not support now.*/
  CNNL_DECONV_SCALE,
  /*!< Per-channel basis, performs these operations in following order: ::cnnlDeconvolution,
   * ::cnnlScale. Apart from ::cnnlDeconvolution, the other operation is optional. */
  CNNL_ACTIVATION_CONV_BN_ACTIVATION_MASKZERO,
  /*!< Performs these operations in following order: ::cnnlActivationForward, ::cnnlConvolutionForward,
   * ::cnnlBatchNormForwardInference, ::cnnlActivationForward and ::cnnlMaskZero. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. */
  CNNL_ACTIVATION_CONV_BN_MASKZERO_ACTIVATION,
  /*!< Performs these operations in following order: ::cnnlActivationForward, ::cnnlConvolutionForward,
   * ::cnnlBatchNormForwardInference, ::cnnlMaskZero and ::cnnlActivationForward. Apart from
   * ::cnnlConvolutionForward, the other operations are optional. */
  CNNL_DECONV_SCALE_BN_ACTIVATION,
  /*!< Per-channel basis, performs these operations in following order: ::cnnlDeconvolution,
   * ::cnnlScale, ::cnnlBatchNormForwardInference and ::cnnlActivationForward. Apart from
   * ::cnnlDeconvolution, the other operations are optional. */
} cnnlFusedOps_t;

/*!
 * @brief Enumeration variables describing the supported constant parameter type used in the fusion
 * operation. The constant parameter can be a tensor descriptor, an operation descriptor or a
 * scalar variable used in operations.
 *
 * - Setting:
 *
 *   You need to call the ::cnnlSetFusedOpsConstParamPackAttribute to set specific constant
 *   parameter to a previously created ::cnnlFusedOpsConstParamPack_t pack that storing all
 *   constant parameters used in the fusion operation.
 * - Getting:
 *
 *   You need to call the ::cnnlGetFusedOpsConstParamPackAttribute to get specific constant
 *   parameter from the previously created and initialized ::cnnlFusedOpsConstParamPack_t pack.
 */
typedef enum {
  CNNL_XDESC = 0,
  /*!< The descriptor of the input tensor \b x. In the setting, the \b cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_WDESC = 1,
  /*!< The descriptor of the filter tensor \b w. In the setting, the \b cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_YDESC = 2,
  /*!< The descriptor of the output tensor \b y. In the setting, the \b cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_BIAS_DESC = 3,
  /*!< The descriptor of the bias tensor. In the setting, the \b cparam should be a pointer to a
   * previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_CONV_DESC = 4,
  /*!< The descriptor of the convolution operation. In the setting, the \b cparam should be a
   * pointer to a previously initialized ::cnnlConvolutionDescriptor_t. */
  CNNL_BN_WEIGHT_BIAS_MEAN_VAR_DESC = 5,
  /*!< The descriptor of the filter, bias, mean and variance tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t.*/
  CNNL_SCALE_ALPHA_DESC = 6,
  /*!< The descriptor of the alpha tensor used in ::cnnlScale operation. In the setting,
   * the \b cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_SCALE_BETA_DESC = 7,
  /*!< The descriptor of the beta tensor used in ::cnnlScale operation. In the setting,
   * the \b cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_ACTIVATION_DESC = 8,
  /*!< The descriptor of the ::cnnlActivationForward operation. In the setting, the \b cparam
   * should be a pointer to a previously initialized ::cnnlActivationDescriptor_t. */
  CNNL_PRELU_ALPHA_DESC = 9,
  /*!< The descriptor of the alpha tensor used in ::cnnlPrelu operation. In the setting,
   * the \b cparam should be a pointer to a previously initialized ::cnnlTensorDescriptor_t. */
  CNNL_SCALAR_CONV_FWD_ALGO = 10,
  /*!< The scalar describing the convolution algorithms that used to implement the
   * ::cnnlConvolutionForwardInference operation. In the setting, the \b cparam should be
   * a pointer to a previously initialized ::cnnlConvolutionForwardAlgo_t. */
  CNNL_SCALAR_CONV_FWD_CAST_MODE = 11,
  /*!< The scalar describing the cast mode used to convert the data type of
   * input and output tensors for ::cnnlConvolutionForwardInference operation.
   * In the setting, the \b cparam should be a pointer to a previously initialized
   * ::cnnlConvolutionCastMode_t. */
  CNNL_DECONV_DESC = 12,
  /*!< The descriptor of the deconvolution operation. In the setting, the \b cparam should be a
   * pointer to a previously initialized ::cnnlDeconvolutionDescriptor_t. */
  CNNL_SCALAR_DECONV_ALGO = 13,
  /*!< The scalar describing the deconvolution algorithms that is used to implement the
   * ::cnnlDeconvolution operation. In the setting, the \b cparam should be
   * a pointer to a previously initialized ::cnnlDeconvolutionAlgo_t. */
  CNNL_MASKZERO_LABEL_DESC = 14,
  /*!< The descriptor of the label used in
   * ::cnnlMaskZero operation. In the setting, the \b cparam should be a pointer
   * to a previously initialized ::cnnlTensorDescriptor_t.*/
  CNNL_PRE_ACTIVATION_DESC = 15,
  /*!< The descriptor of the ::cnnlActivationForward operation that is implemented before
   * the ::cnnlConvolutionForward operation. In the setting, the \b cparam
   * should be a pointer to a previously initialized ::cnnlActivationDescriptor_t. */
  CNNL_SCALAR_DECONV_CAST_MODE = 16,
  /*!< The scalar describing the cast mode used to convert the data type of
   * input and output tensors for ::cnnlDeconvolutionInference function.
   * In the setting, the \b cparam should be a pointer to a previously initialized
   * ::cnnlDeconvolutionCastMode_t. */
  CNNL_ADAPTIVE_QUANTIZE_PLACEHOLDER = 17,
  /*!< Adaptive quantize will be used.
   * In the setting, the \b cparam should be a pointer to a previously
   * initialized ::cnnlFusedOpsPointerPlaceHolder_t.
   * If adaptive quantize is needed, set this enum to ::CNNL_PTR_VALID.
   * Otherwise, set it to ::CNNL_PTR_NULL or leave it unset.*/
} cnnlFusedOpsConstParamLabel_t;

/*!
 * @brief Enumeration variables describing the supported variant parameter type used
 * in the fusion operation. The variant parameter is the pointer to the host or device
 * memory, which can be changed in each iteration.
 *
 * - Setting:
 *
 *   You need to call the ::cnnlSetFusedOpsVariantParamPackAttribute to set specific
 *   variant parameter to a previously created ::cnnlFusedOpsVariantParamPack_t pack that storing
 *   all variant parameters used in the fusion operation.
 * - Getting:
 *
 *   You need to call the ::cnnlGetFusedOpsVariantParamPackAttribute to get specific
 *   variant parameter from the previously created and initialized ::cnnlFusedOpsVariantParamPack_t pack.
 */
typedef enum {
  CNNL_PTR_X = 0,
  /*!< Pointer to the device memory that stores the input tensor \b x.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_W = 1,
  /*!< Pointer to the device memory that stores the filter tensor \b w.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_Y = 2,
  /*!< Pointer to the device memory that stores the output tensor \b y.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_BIAS = 3,
  /*!< Pointer to the device memory that stores the bias tensor. In the setting, the \b vparam
   * with the void * type is expected to pass in. */
  CNNL_PTR_BN_MEAN = 4,
  /*!< Pointer to the device memory that stores the mean tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_PTR_BN_VAR = 5,
  /*!< Pointer to the device memory that stores the variance tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_PTR_BN_WEIGHT = 6,
  /*!< Pointer to the device memory that stores the filter tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_PTR_BN_BIAS = 7,
  /*!< Pointer to the device memory that stores the bias tensor used in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_PTR_SCALE_ALPHA = 8,
  /*!< Pointer to the device memory that stores the alpha tensor used in ::cnnlScale operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_SCALE_BETA = 9,
  /*!< Pointer to the device memory that stores the beta tensor used in ::cnnlScale operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_PRELU_ALPHA = 10,
  /*!< Pointer to the device memory that stores the alpha tensor used in ::cnnlPrelu operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_PTR_WORKSPACE = 11,
  /*!< Pointer to the workspace memory on the device allocated by user. In the setting,
   * the \b vparam with the void * type is expected to pass in. */
  CNNL_SCALAR_WORKSPACE_SIZE = 12,
  /*!< Scalar in the host memory describing workspace size allocated by user in bytes.
   * The amount need to be equal to or larger than that requested in the ::cnnlMakeFusedOpsPlan.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_SCALAR_BN_EPSILON = 13,
  /*!< Scalar value added to the denominator for numerical stability in
   * ::cnnlBatchNormForwardInference operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_PTR_MASKZERO_LABEL = 14,
  /*!< Pointer to the device memory that stores the label tensor used in
   * ::cnnlMaskZero operation. In the setting, the \b vparam with the void * type
   * is expected to pass in. */
  CNNL_SCALAR_MASKZERO_PAD_LABEL = 15,
  /*!< Scalar value used to compare with label in ::cnnlMaskZero operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_X_POSITION = 16,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_X_SCALE = 17,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_X_OFFSET = 18,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_W_POSITION = 19,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_W_SCALE = 20,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
  CNNL_W_OFFSET = 21,
  /* Scalar value used to compare with label in ::cnnlQuantizeConvolutionForward operation.
   * In the setting, the \b vparam with the void * type is expected to pass in. */
} cnnlFusedOpsVariantParamLabel_t;

// Group:FusedOp
/*!
 * @brief Creates a plan descriptor pointed by \b fusion_plan for the fusion operation,
 * and allocates memory for holding the information about the fusion operation.
 * The information is defined in ::cnnlFusedOpsPlan_t.
 *
 * @param[in] fusion_plan
 *  Input. Pointer to the instance of the fusion plan created by this function.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations for which this fusion plan
 *  should be created. For more information, see ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetFusedOpsConstParamPackAttribute
 *   and ::cnnlSetFusedOpsVariantParamPackAttribute function to initialize and set the
 *   information to the fusion plan.
 * - You need to call the ::cnnlDestroyFusedOpsPlan function to destroy the plan.
 *   Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsPlan(cnnlFusedOpsPlan_t *fusion_plan,
                                                 const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Creates a constant parameter pack pointed by \b cparam_pack for the fusion operation,
 * and allocates memory for holding the information about the constant parameter.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations to perform as defined in the
 *  enumeration type ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetFusedOpsConstParamPackAttribute
 *   to add specific parameter to the constant parameter pack one by one.
 * - You need to call the ::cnnlDestroyFusedOpsConstParamPack function to
 *   destroy the pack. Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsConstParamPack(
    cnnlFusedOpsConstParamPack_t *cparam_pack,
    const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Adds the constant parameter pointed by the \b cparam to \b cparam_pack with
 * constant parameter type \b cparam_label. The \b cparam should be a pointer
 * pointed to a previously initialized constant parameter. The supported constant parameter
 * type is defined in ::cnnlFusedOpsConstParamLabel_t.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] cparam_label
 *  Input. The \b cparam_label indicates the constant parameter type pointed to by the \b cparam.
 *  For detailed information, see ::cnnlFusedOpsConstParamLabel_t.
 * @param[in] cparam
 *  Input. Pointer to the specific constant parameter on the host memory. The type of
 *  the constant parameter depends on the value of \b cparam_label, and the values or
 *  opaque structure pointed by this pointer will be copied into the \b cparam_pack.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsConstParamPack to initialize the constant parameter pack.
 * - After calling this function, you can call the ::cnnlGetFusedOpsConstParamPackAttribute
 *   to get the specific constant parameter from the constant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFusedOpsConstParamPackAttribute(
    cnnlFusedOpsConstParamPack_t cparam_pack,
    const cnnlFusedOpsConstParamLabel_t cparam_label,
    const void *cparam);

// Group:FusedOp
/*!
 * @brief Retrieves the constant parameter with the type of \b cparam_label from
 * \b cparam_pack. The parameter type is defined in ::cnnlFusedOpsConstParamLabel_t and the
 * retrieved value or opaque structure will be copied to the host memory buffer pointed by
 * the \b cparam.
 *
 * @param[in] cparam_pack
 *  Input. Pointer to the constant parameter pack that holds information about
 *  fusion operation, such as the descriptors of tensor and operation.
 * @param[in] cparam_label
 *  Input. The \b cparam_label indicates the constant parameter type pointed to by the \b cparam.
 *  For detailed information, see ::cnnlFusedOpsConstParamLabel_t.
 * @param[in] cparam
 *  Input. Pointer to the specific constant parameter that should be retrieved on the host memory.
 *  The type of the constant parameter depends on the value of \b cparam_label. The value or the opaque
 *  structure in the \b cparam_pack will be copied to the host memory buffer pointed by the
 *  \b cparam. If no specific constant parameter type \b cparam_label can be found, the pointer \b cparam
 *  will be set to NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlSetFusedOpsConstParamPackAttribute to add specific parameter to constant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFusedOpsConstParamPackAttribute(
    const cnnlFusedOpsConstParamPack_t cparam_pack,
    const cnnlFusedOpsConstParamLabel_t cparam_label,
    void *cparam);

// Group:FusedOp
/*!
 * @brief Destroys a constant parameter pack \b cparam_pack in fusion operation that is
 * previously created with the ::cnnlCreateFusedOpsConstParamPack function.
 *
 * @param[in] cparam_pack
 *   Input. The constant parameter pack to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsConstParamPack and ::cnnlFusedOpsExecute function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the constant parameter pack.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsConstParamPack(
     cnnlFusedOpsConstParamPack_t cparam_pack);

// Group:FusedOp
/*!
 * @brief Creates a variant parameter pack pointed by \b vparam_pack for the fusion operation,
 * and allocates memory for holding the information about the variant parameter.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about
 *  the variant parameters of fusion operation. For detailed information,
 *  see ::cnnlFusedOpsVariantParamPack_t.
 * @param[in] fusion_type
 *  Input. The specific sequence of fusion operations to perform as defined in the
 *  enumeration type ::cnnlFusedOps_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you need to call the ::cnnlSetFusedOpsVariantParamPackAttribute
 *   to add specific parameter to the variant parameter pack one by one.
 * - You need to call the ::cnnlDestroyFusedOpsVariantParamPack function to
 *   destroy the pack. Otherwise, the memory leak may occur.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateFusedOpsVariantParamPack(
    cnnlFusedOpsVariantParamPack_t *vparam_pack,
    const cnnlFusedOps_t fusion_type);

// Group:FusedOp
/*!
 * @brief Adds a specific variant parameter \b vparam to \b vparam_pack with variant parameter type
 * \b vparam_label. The supported variant parameter type is defined in ::cnnlFusedOpsVariantParamLabel_t.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about the
 *  variant parameters of fusion operation.
 * @param[in] vparam_label
 *  Input. Enumeration variable describing the variant parameter type to be set in this function.
 *  For detailed information, see ::cnnlFusedOpsVariantParamLabel_t.
 * @param[in] vparam
 *  Input. Pointer to the host or device memory that stores the variant parameter.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsVariantParamPack to initialize the variant parameter pack.
 * - After calling this function, you can call the
 *   ::cnnlGetFusedOpsVariantParamPackAttribute to get the specific variant parameter
 *   from the variant parameter pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetFusedOpsVariantParamPackAttribute(
    cnnlFusedOpsVariantParamPack_t vparam_pack,
    const cnnlFusedOpsVariantParamLabel_t vparam_label,
    const void *vparam);

// Group:FusedOp
/*!
 * @brief Retrieves a specific parameter \b vparam from \b vparam_pack by variant parameter type
 * \b vparam_label. The supported variant parameter type is defined in ::cnnlFusedOpsVariantParamLabel_t.
 *
 * @param[in] vparam_pack
 *  Input. Pointer to the variant parameter pack that holds information about
 *  the variant parameters of fusion operation.
 * @param[in] vparam_label
 *  Input. Enumeration variable describing the variant parameter type in the \b vparam_pack.
 *  The retrieved variant parameter values vary according to the value of this parameter.
 *  For detailed information, see ::cnnlFusedOpsVariantParamLabel_t.
 * @param[out] vparam
 *  Output. Pointer to the host or device memory where the retrieved value is written into. If no
 *  specific \b vparam_label can be found in \b vparam_pack, this pointer \b vparam will be set to NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - Before calling this function, you should call the
 *   ::cnnlSetFusedOpsVariantParamPackAttribute to add specific parameter to variant parameter
 *   pack.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetFusedOpsVariantParamPackAttribute(
    const cnnlFusedOpsVariantParamPack_t vparam_pack,
    const cnnlFusedOpsVariantParamLabel_t vparam_label,
    void *vparam);

// Group:FusedOp
/*!
 * @brief Destroys a variant parameter pack \b vparam_pack in fusion operation that is
 *        previously created with the ::cnnlCreateFusedOpsVariantParamPack function.
 *
 * @param[in] vparam_pack
 *   Input. The variant parameter pack to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the
 *   ::cnnlCreateFusedOpsVariantParamPack and ::cnnlFusedOpsExecute function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the variant parameter pack.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsVariantParamPack(
    cnnlFusedOpsVariantParamPack_t vparam_pack);

// Group:FusedOp
/*!
 * @brief Returns in \b size the byte size of the MLU memory that is used as an extra
 *        workspace to optimize the fusion operation. The size of extra workspace is
 *        based on the constant parameters of the fusion operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the fusion operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] cparam_pack
 *   Input. Pointer to the constant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsConstParamPack_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that
 *   is used in the fusion operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlCreateFusedOpsPlan,
 *   the ::cnnlCreateFusedOpsConstParamPack and the ::cnnlSetFusedOpsConstParamPackAttribute
 *   functions to create and set the constant parameter pack \b cparam_pack.
 * - The allocated extra workspace should be added to the variant parameter by
 *   ::cnnlSetFusedOpsVariantParamPackAttribute function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlMakeFusedOpsPlan(cnnlHandle_t handle,
    const cnnlFusedOpsPlan_t fusion_plan,
    const cnnlFusedOpsConstParamPack_t cparam_pack,
    size_t *size);

// Group:FusedOp
/*!
 * @brief Computes a series operations which initialized in \b fusion_plan.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan.
 *   For detailed information, see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - Before calling this function to implement fusion operation, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * - To perform fusion operations, you need to call the related APIs with the following order:
 *   1. Create a fusedop plan with ::cnnlCreateFusedOpsPlan.
 *   2. Create a pack that stores the constant parameters with ::cnnlCreateFusedOpsConstParamPack.
 *   3. Set a constant parameter to the pack with ::cnnlSetFusedOpsConstParamPackAttribute.
 *      This function can only set a parameter for each call. You need to call this function
 *      multiple times to set all constant parameters.
 *   4. Retrieve the constant parameter you set in the pack with ::cnnlGetFusedOpsConstParamPackAttribute.
 *      This function can only retrieve a parameter for each call. You need to call this function
 *      multiple times to retrieve all constant parameters.
 *   5. Create a pack to store variant parameters with ::cnnlCreateFusedOpsVariantParamPack.
 *   6. Set a variant parameter to the pack with ::cnnlSetFusedOpsVariantParamPackAttribute.
 *      This function can only set a parameter for each call. You need to call this function
 *      multiple times to set all variant parameters.
 *   7. Retrieve the variant parameter you set in the pack with ::cnnlGetFusedOpsVariantParamPackAttribute.
 *      This function can only retrieve a parameter for each call. You need to call this function
 *      multiple times to retrieve all variant parameters.
 *   8. Retrieve the extra workspace size to be used in the fusion operation with ::cnnlMakeFusedOpsPlan.
 *   9. Release all the resoureces with ::cnnlFusedOpsExecute, ::cnnlDestroyFusedOpsConstParamPack,
 *      ::cnnlDestroyFusedOpsVariantParamPack, ::cnnlDestroyFusedOpsPlan accrodingly.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFusedOpsExecute(cnnlHandle_t handle,
                                              const cnnlFusedOpsPlan_t fusion_plan,
                                              const cnnlFusedOpsVariantParamPack_t vparam_pack);

// Group:FusedOp
/*!
 * @brief Destroys a fusion plan \b fusion_plan that is previously created with the
 *        ::cnnlCreateFusedOpsPlan function.
 *
 * @param[in] fusion_plan
 *   Input. The fusion plan to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - Before calling this function, you should call the ::cnnlCreateFusedOpsPlan
 *   and ::cnnlFusedOpsExecute function. Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the fusion plan. Otherwise,
 *   the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyFusedOpsPlan(cnnlFusedOpsPlan_t fusion_plan);

// Group:AdaptivePooling
/*!
 * @brief Computes average and maximum pooling of \b input tensor. Different from ::cnnlPoolingForward,
 * ::cnnlAdaptivePoolingForward calculates pooling based on the dimensions of \b input tensor and
 * \b output tensor. This operation can calculate the kernel and stride parameters of
 * ::cnnlPoolingForward automatically.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   ::cnnlAdaptivePoolingForward. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] mode
 *   Input. The \b mode parameter is defined in the ::cnnlPoolingMode_t.
 *   This operation only supports the CNNL_POOLING_MAX and the
 *   CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING mode.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor, which is the results of
 *   this operation.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] index
 *   Output. Pointer to the MLU memory that stores the pooling index in the CNNL_POOLING_MAX mode.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b output tensor, \b index tensor.
 *   <b>Note that the combinations of these tensor must be half-half-int16 or float-float-int32.</b>
 *   - input tensor: half, float
 *   - output tensor: half, float
 *   - index tensor: int16, int32
 *
 * @par Data Layout
 * - The data layout of the \b input tensor, \b output tensor, and \b index tensor should be same,
 *   supported data layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - output tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *   - index tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NDHWC.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and mode must meet the following requirements:
 *   - input tensor: \p batch > 0, \p depth > 0, \p height > 0, \p width > 0, \p channel > 0
 *   - output tensor: In the maxpool with index, kernel size cannot be too big, and should satisfy
 *     (\p input_h / \p output_h + 2) * (\p input_w / \p output_w + 2) <= 3582
 *   - \b mode: \b mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING or \b mode ==
 *     \p CNNL_POOLING_MAX for 2-D pooling, and \b mode == \p CNNL_POOLING_AVERAGE_COUNT_INCLUDE_PADDING
 *     for 3-D pooling.
 *
 * @note
 * - This operation only supports 2-D and 3-D pooling. 3-D pooling only supports average mode.
 * - In the average pooling, set the \b index_desc and \b index to NULL.
 * - In the maximum pooling, if you do not need pooling index, set the \b index_desc
 *   and \b index tensor to NULL.
 * - In the 2-D maximum pooling, when the kernel of the pooling contains NaN:
 *   - On MLU200 series:
 *    - The \b output value is the NaN, and the \b index value is the index of the last NaN.
 *   - On MLU300 series and CE3226:
 *    - If the last value in the kernel of the pooling is NaN, the \b output value is NaN and
 *      the \b index is the index of the last value. Otherwise, the \b output value is the maximum
 *      value after the last NaN and the \b index value is the index of the output value.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor by 1 *1 * 14 * 1
     --> input:[[[0,1,2,3,4,5,6,7,8,9,10,11,12,13]]]
     --> index: NULL
     param:
       mode: CNNL_POOLING_MAX
     output array by 1 * 1 * 4 * 1 -->output [[[3,6,10,13]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/generated/torch.nn.AdaptiveAvgPool3d.html
 */
cnnlStatus_t CNNL_WIN_API cnnlAdaptivePoolingForward(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t input_desc,
                                                     const void * input,
                                                     const cnnlPoolingMode_t mode,
                                                     const cnnlTensorDescriptor_t output_desc,
                                                     void * output,
                                                     const cnnlTensorDescriptor_t index_desc,
                                                     void * index);

// Group:IndexAdd
/*!
 * @brief Adds vectors or scalars from \b input_b into \b input_a along \b dim according to the entries in \b index and
 * stores the results into \b output, then leaves other values in \b input_a as they are in \b output. Supports inplace: \b output = \b input_a.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index add operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \b input_a and \b input_b to be indexed.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \b input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_a
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_a which to be indexed and added into.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensors. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the input tensor \b index which maps vectors or scalars from \b input_a to \b input_b.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \b input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_b which to be added.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Index Add Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - input tensor \b input_a - input tensor \b input_b - output tensor \b output
 *   The supported data type combinations are:
 *   - float - float - float
 *   - half - half - half
 *   - int32 - int32 - int32
 *   - int16 - int16 - int16
 * - \b dim: int64
 * - \b index: int32
 *
 * @par Scale Limitation
 * - The \b input_a tensor, \b index tensor, \b input_b tensor and \b dim must meet the following
 *   requirements:
 *   - The dim-th dimension of \b input_b must have the same size as the length of
 *        \b index.
 *   - \b index must be a vector.
 *   - All dimensions apart from dim-th dimension of \b input_b must match
 *        all other dimensions of \b input_a.
 *
 * @note
 * - The accuracy of computing result might decrease when the data type of \b input_a and \b input_b is half.
 * - The maximum of \b index must be smaller than the length of dim-th dimension of \b input_a and every element
 *   of \b index must be greater than or equal to zero.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index add operation is as follows:
     @verbatim
     dim = 0
     input_a array
       input_a = [[1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1]]
     index array
       index = [0, 4, 2]
     input_b array
       input_b = [[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 8]]
     output array
       output = [[2, 3, 4],
                 [1, 1, 1],
                 [8, 9, 10],
                 [1, 1, 1],
                 [5, 6, 7]]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API cnnlIndexAdd(cnnlHandle_t handle,
                                       const int64_t dim,
                                       const cnnlTensorDescriptor_t input_a_desc,
                                       const void *input_a,
                                       const cnnlTensorDescriptor_t index_desc,
                                       const void *index,
                                       const cnnlTensorDescriptor_t input_b_desc,
                                       const void *input_b,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);

// Group:InTopK
/*!
 * @brief Computes whether the \b targets are in top k \b predictions. If both \b k_desc and \b k
 *        are not NULL, \b k is used in this operation, and \b k_int is not used. Otherwise,
 *        \b k_int is used.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the intopk
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] predictions_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] predictions
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] targets_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] targets
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] k_desc
 *   Input. The descriptor of the k tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] k
 *   Input. Pointer to the MLU memory that stores the k tensor, which has at most one element and
 *   determines the number of top elements in \b predictions to compare with.
 * @param[in] k_int
 *   Input. An int value which determines the number of top elements in \b predictions to compare
 *   with.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "InTopK Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports only the combination of the following data types for \b predictions,
 *   \b targets, \b k and \b output:
 *   - predictions tensor: float.
 *   - targets tensor: int32.
 *   - k tensor: int32.
 *   - output tensor: bool.
 *
 * @par Scale Limitation
 * - The predictions tensor, targets tensor, and k tensor must meet the following requirement:
 *   - The first dimension size of \b predictions and \b targets should be equal.
 *   - k tensor: If valid, the element number should be 1.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/raw_ops/InTopKV2
 */
cnnlStatus_t CNNL_WIN_API cnnlInTopK(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t predictions_desc,
                                     const void *predictions,
                                     const cnnlTensorDescriptor_t targets_desc,
                                     const void *targets,
                                     const cnnlTensorDescriptor_t k_desc,
                                     const void *k,
                                     const int k_int,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

// Group:Convolution
/*!
 * @brief Gets the memory size that stores the output of reordering convolution filter data or
 *        convolution bias data on host.
 *
 * @param[in] dev
 *   Input. The MLU device information. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] w_tensor_desc
 *   Input. The descriptor of the filter tensor in the convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_tensor_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the convolution quantization. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[in] x_dtype
 *   Input. The offchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] x_onchip_dtype
 *   Input. The onchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_dtype
 *   Input. The offchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_onchip_dtype
 *   Input. The onchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @param[out] w_reorder_bytesize
 *   Output. The size of storing the output of reordering convolution filter.
 * @param[out] bias_reorder_bytesize
 *   Output. The size of the memory int bytes that stores the output of
 *   reordering convolution bias data.
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlSetConvolutionDescriptorReorderType
 *   to set filter and bias reorder type, and when running fusedOp, you should call
 *   the ::cnnlMakeFusedOpsPlan and ::cnnlSetFusedOpsVariantParamPackAttribute to get fusion_plan
 *   and vparam_pack.
 * - After calling this function, you can call the ::cnnlHostReorderConvData function to reorder
 *   filter data or bias data on the host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - Due to the limitation of cnnlHostReorderConvData, only MLU Edge devices can use
 *   cnnlGetReorderConvDataSize when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetReorderConvDataSize(
                                                cnnlDeviceType_t dev,
                                                const cnnlConvolutionDescriptor_t conv_desc,
                                                const cnnlTensorDescriptor_t w_tensor_desc,
                                                const cnnlTensorDescriptor_t bias_tensor_desc,
                                                const cnnlConvolutionCastMode_t cast_mode,
                                                const cnnlConvolutionForwardAlgo_t algo,
                                                const cnnlDataType_t x_dtype,
                                                const cnnlDataType_t x_onchip_dtype,
                                                const cnnlDataType_t y_dtype,
                                                const cnnlDataType_t y_onchip_dtype,
                                                const cnnlFusedOpsPlan_t fusion_plan,
                                                const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                                size_t *w_reorder_bytesize,
                                                size_t *bias_reorder_bytesize);

// Group:Convolution
/*!
 * @brief Reorders filter data and bias data for convolution forward on the host.
 *
 * @param[in] dev
 *   Input. The MLU device information. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor in the convolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the convolution quantization. For detailed information,
 *   see ::cnnlConvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to compute the convolution. The algorithms are
 *   defined in the ::cnnlConvolutionForwardAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetConvolutionForwardAlgorithm function.
 * @param[in] x_dtype
 *   Input. The offchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] x_onchip_dtype
 *   Input. The onchip data type of convolution forward input. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_dtype
 *   Input. The offchip data type of convolution forward output. For detailed information,
 *   See ::cnnlDataType_t.
 * @param[in] y_onchip_dtype
 *   Input. The onchip data type of convolution forward output. For detailed information,
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan. For detailed information,
 *   see ::cnnlFusedOpsPlan_t.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 * @param[in] w
 *   Input. A pointer to the address of filter data before reordering filter.
 * @param[out] reordered_w
 *   Output. A pointer to the address of filter data after reordering filter.
 * @param[in] w_reorder_bytesize
 *   Input. The size of storing the output of reordering convolution filter.
 * @param[in] bias
 *   Input. A pointer to the address of bias data before reordering bias data.
 * @param[out] reorder_bias
 *   Output. A pointer to the address of bias data after reordering bias data.
 * @param[in] bias_reorder_bytesize
 *   Input. The size of the memory in bytes that stores the output of
 *   reordering convolution bias data.
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlGetReorderConvDataSize
 *   to get filter_reorder_bytesize or get bias_reorder_bytesize.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - The parameter cast_mode must be set to CNNL_NO_QUANTIZE, when input onchip type is float or half.
 * - Due to the limitation of cnnlHostReorderConvData now, only MLU Edge devices can use
 *   the API when you use the cluster num optional function.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlHostReorderConvData(cnnlDeviceType_t dev,
                                     const cnnlConvolutionDescriptor_t conv_desc,
                                     const cnnlTensorDescriptor_t w_tensor_desc,
                                     const cnnlTensorDescriptor_t bias_tensor_desc,
                                     const cnnlConvolutionCastMode_t cast_mode,
                                     const cnnlConvolutionForwardAlgo_t algo,
                                     const cnnlDataType_t x_dtype,
                                     const cnnlDataType_t x_onchip_dtype,
                                     const cnnlDataType_t y_dtype,
                                     const cnnlDataType_t y_onchip_dtype,
                                     const cnnlFusedOpsPlan_t fusion_plan,
                                     const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                     const void *w,
                                     void *reordered_w,
                                     const size_t w_reorder_bytesize,
                                     const void *bias,
                                     void *reordered_bias,
                                     const size_t bias_reorder_bytesize);
// Group:Deconvolution
/*!
 * @brief Returns the size in bytes of memory which stores the reordered deconvolution filter and
 *        bias on host.
 *
 * @param[in] dev
 *   Input. The enum indicating the MLU device. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_dtype
 *   Input. The offchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] input_onchip_dtype
 *   Input. The onchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_dtype
 *   Input. The offchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_onchip_dtype
 *   Input. The onchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are
 *   defined in the ::cnnlDeconvolutionAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetDeconvolutionAlgorithm_v2 function.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan used in the fusion
 *   operation. For detailed information, see ::cnnlFusedOpsPlan_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in the fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[out] filter_reorder_size
 *   Output. The size in bytes of memory storing the reordered filter on host.
 * @param[out] bias_reorder_size
 *   Output. The size in bytes of memory storing the reordered bias on host.
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlSetDeconvolutionDescriptorReorderType
 *   function to set the reorder type of filter and bias. In addition, if the deconvolution
 *   operation is fused, you should call the ::cnnlMakeFusedOpsPlan and
 *   ::cnnlSetFusedOpsVariantParamPackAttribute functions to get fusion_plan and vparam_pack
 *   in advance.
 * - After calling this function, you can call the ::cnnlHostReorderDeconvolutionData function to
 *   perform the reordering operation for filter and bias on host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the filter tensor
 *   and bias tensor:
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the filter tensor and bias tensor are as follows:
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetReorderDeconvolutionDataSize(cnnlDeviceType_t dev,
                                    const cnnlTensorDescriptor_t filter_desc,
                                    const cnnlTensorDescriptor_t bias_desc,
                                    const cnnlDataType_t input_dtype,
                                    const cnnlDataType_t input_onchip_dtype,
                                    const cnnlDataType_t output_dtype,
                                    const cnnlDataType_t output_onchip_dtype,
                                    const cnnlDeconvolutionDescriptor_t deconv_desc,
                                    const cnnlDeconvolutionCastMode_t cast_mode,
                                    const cnnlDeconvolutionAlgo_t algo,
                                    const cnnlFusedOpsPlan_t fusion_plan,
                                    const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                    size_t *filter_reorder_size,
                                    size_t *bias_reorder_size);
// Group:Deconvolution
/*!
 * @brief Performs the reordering operation for filter and bias of the deconvolution operation
 *        on host.
 *
 * @param[in] dev
 *   Input. The enum indicating the MLU device. For detailed information,
 *   see ::cnnlDeviceType_t.
 * @param[in] filter_desc
 *   Input. The descriptor of the filter tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] bias_desc
 *   Input. The descriptor of the bias tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_dtype
 *   Input. The offchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] input_onchip_dtype
 *   Input. The onchip data type of input tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_dtype
 *   Input. The offchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] output_onchip_dtype
 *   Input. The onchip data type of output tensor in the deconvolution operation.
 *   For detailed information, see ::cnnlDataType_t.
 * @param[in] deconv_desc
 *   Input. The descriptor of the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionDescriptor_t.
 * @param[in] cast_mode
 *   Input. The quantization mode used for the deconvolution operation. For detailed information,
 *   see ::cnnlDeconvolutionCastMode_t.
 * @param[in] algo
 *   Input. The algorithm used to perform the deconvolution operation. The algorithms are
 *   defined in the ::cnnlDeconvolutionAlgo_t enumeration. You can get the best
 *   suited algorithm with the ::cnnlGetDeconvolutionAlgorithm_v2 function.
 * @param[in] fusion_plan
 *   Input. Pointer to a previously created and initialized fusion plan used in the fusion
 *   operation. For detailed information, see ::cnnlFusedOpsPlan_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] vparam_pack
 *   Input. Pointer to the variant parameter pack in the fusion operation.
 *   For detailed information, see ::cnnlFusedOpsVariantParamPack_t.
 *   This pointer should be empty if the deconvolution operation is not fused.
 * @param[in] filter
 *   Input. Pointer to the CPU memory that stores the filter data before the reordering operation.
 * @param[in] filter_reorder_size
 *   Input. The size in bytes of memory storing the reordered filter on host.
 * @param[out] reordered_filter
 *   Output. Pointer to the CPU memory that stores the reordered filter data after the reordering
 *   operation.
 * @param[in] bias
 *   Input. Pointer to the CPU memory that stores the bias data before the reordering operation.
 * @param[in] bias_reorder_size
 *   Input. The size in bytes of memory storing the reordered bias on host.
 * @param[out] reordered_bias
 *   Output. Pointer to the CPU memory that stores the reordered bias data after the reordering
 *   operation.
 *
 * @par API Dependency
 * - Before calling this function, you should call the ::cnnlGetReorderDeconvolutionDataSize function
 *   to get the size in bytes of memory which stores the reordered filter and bias on host.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the filter tensor
 *   and bias tensor:
 *   - filter tensor: int8, int16, half, float.
 *   - bias tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of the filter tensor and bias tensor are as follows:
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *   - bias tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_HWCN, \p CNNL_LAYOUT_NCHW, and \p CNNL_LAYOUT_NDHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlHostReorderDeconvolutionData(cnnlDeviceType_t dev,
                                 const cnnlTensorDescriptor_t filter_desc,
                                 const cnnlTensorDescriptor_t bias_desc,
                                 const cnnlDataType_t input_dtype,
                                 const cnnlDataType_t input_onchip_dtype,
                                 const cnnlDataType_t output_dtype,
                                 const cnnlDataType_t output_onchip_dtype,
                                 const cnnlDeconvolutionDescriptor_t deconv_desc,
                                 const cnnlDeconvolutionCastMode_t cast_mode,
                                 const cnnlDeconvolutionAlgo_t algo,
                                 const cnnlFusedOpsPlan_t fusion_plan,
                                 const cnnlFusedOpsVariantParamPack_t vparam_pack,
                                 const void *filter,
                                 const size_t filter_reorder_size,
                                 void *reordered_filter,
                                 const void *bias,
                                 const size_t bias_reorder_size,
                                 void *reordered_bias);
// Group:Svd
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra
 * workspace to optimize the SVD operation.
 *
 * The size of extra workspace is based on the given information of the SVD operation,
 * including the input tensor descriptor \b input_desc, the parameter \b some and
 * the parameter \b compute_uv. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the SVD operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] some
 *   Input. Boolean value. When \b some is true, ::cnnlSvd will
 *   compute only the leading min(m, n) singular vectors (min(m, n) means the minimum
 *   of m and n), i.e., the returned \b u and \b v matrices will contain only min(m,n)
 *   orthonormal columns. When \b some is false, ::cnnlSvd will compute full-sized
 *   \b u and \b v matrices.
 * @param[in] compute_uv
 *   Input. Boolean value. When \b compute_uv is true, left and
 *   right singular matrices will be computed and returned in \b u and \b v,
 *   respectively. When \b compute_uv is false, the returned \b u and \b v matrices
 *   will be zero matrices of shape [...,m, m] and [..., n, n] respectively, and
 *   \b some will be ignored here.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that
 *   is used in the SVD operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par API Dependency
 * - You need to call the ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor
 *   functions to create and set the tensor descriptors \b input_desc before calling
 *   this function.
 * - The allocated extra workspace should be passed to the ::cnnlSvd function to
 *   perform the SVD operation.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetSvdWorkspaceSize(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t input_desc,
                                                  const bool some,
                                                  const bool comput_uv,
                                                  size_t *workspace_size);
// Group:Svd
/*!
 * @brief Computes the singular value decomposition (SVD) of batches of real matrices
 * \b input such that \b input[..., :, :] = \b u[..., :, :] * diag(\b s[..., :]) *
 * \b transpose(v[..., :, :]).
 *
 * This function needs extra MLU memory as the workspace to improve the SVD
 * performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetSvdWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the SVD operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor with the
 *   shape of [..., m, n].
 * @param[in] some
 *   Input. Boolean value. When \b some is true, the returned \b u and \b v matrices
 *   will contain only the min(m, n) orthonormal columns (min(m, n) means the minimum
 *   of m and n). When \b some is false, ::cnnlSvd will compute full-sized \b u and
 *   \b v matrices.
 * @param[in] compute_uv
 *   Input. Boolean value. When \b compute_uv is true, left and
 *   right singular matrices will be computed and returned in \b u and \b v,
 *   respectively. When \b compute_uv is false, the returned \b u and \b v matrices
 *   will be zero matrices of shape [...,m, m] and [..., n, n] respectively, and
 *   \b some will be ignored here.
 * @param[in] is_trans_u
 *   Input. Boolean value. When \b is_trans_u is true, the returned \b u matrices
 *   will be transposed. When \b is_trans_u is false, the returned \b u matrices
 *   will not be transposed.
 * @param[in] is_trans_v
 *   Input. Boolean value. When \b is_trans_v is true, the returned \b v matrices
 *   will be transposed. When \b is_trans_v is false, the returned \b v matrices
 *   will not be transposed.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *   SVD backward operation. You can get the size of the workspace with the
 *   ::cnnlGetSvdWorkspaceSize function.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   SVD operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] u_desc
 *   Input. The descriptor of the output left singular matrices \b u tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] u
 *   Output. Pointer to the MLU memory that stores the output left singular
 *   matrices \b u tensor.
 * @param[in] s_desc
 *   Input. The descriptor of the output singular values \b s tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] s
 *   Output. Pointer to the MLU memory that stores the output singular values \b s tensor.
 * @param[in] v_desc
 *   Input. The descriptor of the output right singular matrices \b u tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] v
 *   Output. Pointer to the MLU memory that stores the output right singular
 *   matrices \b v tensor.
 * @param[in] infos_desc
 *   Input. The descriptor of the output infos tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] infos
 *   Output. Pointer to the MLU memory that stores the output infos tensor, which
 *   indicates whether SVD computation converges or not. If infos[i] == 0, then the computation
 *   of the i-th input matrix SVD is success. If infos[i] > 0, then the i-th input
 *   matrix SVD computation do not converge, the precision of the results might be low.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "SVD Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor
 *   \b input and output tensor \b u, \b s, \b v and \b infos.
 *   <b>Note that \b input, \b u, \b s and \b v should have the same data type.</b>
 *   - input tensor: float.
 *   - u tensor: float.
 *   - s tensor: float.
 *   - v tensor: float.
 *   - infos tensor: int32.
 *
 * @par Scale Limitation
 * - The input tensor with shape [...,m, n] must meet the following requirements:
 *   - The maximum of m and n should be less than 150.
 *
 * @note
 *   - When m or n is over 20, the precision of the results of ::cnnlSvd might be low.
 *
 * @par Example
 * - The example of the SVD operation is as follows:
     @verbatim
       input tensor:
         input array by 1 * 4 * 3 -->
           [[0.22589493 0.20218787 0.69084793]
            [0.52529126 0.8826841  0.6898656 ]
            [0.11243416 0.86975896 0.9611005 ]
            [0.8365484  0.5822014  0.8437774 ]]

       parameters:
         some: true
         compute_uv: true
         is_trans_u: false
         is_trans_v: false

       output tensor:
         u array by 1 * 4 * 3 -->
           [[-0.3076, -0.0010, -0.7794],
            [-0.5386, -0.0253,  0.6153],
            [-0.5479,  0.7277, -0.0696],
            [-0.5614, -0.6854, -0.0954]]
         s array by 1 * 3 -->
           [2.2559, 0.5571, 0.3801]
         v array by 1 * 3 * 3 -->
           [[-0.3917, -0.9067,  0.1566],
            [-0.5944,  0.3793,  0.7091],
            [-0.7023,  0.1846, -0.6875]]
         infos array by 1 * 1 -->
           [0]
   @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.svd.html
 * - https://tensorflow.org/api_docs/python/tf/raw_ops/Svd
 */
cnnlStatus_t CNNL_WIN_API cnnlSvd(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const bool some,
                                  const bool compute_uv,
                                  const bool is_trans_u,
                                  const bool is_trans_v,
                                  const size_t workspace_size,
                                  void *workspace,
                                  const cnnlTensorDescriptor_t u_desc,
                                  void *u,
                                  const cnnlTensorDescriptor_t s_desc,
                                  void *s,
                                  const cnnlTensorDescriptor_t v_desc,
                                  void *v,
                                  const cnnlTensorDescriptor_t infos_desc,
                                  void *infos);
// Group:Threshold
/*!
 * @brief Computes threshold operation on input tensor \b x by the threshold value \b threshold and
 *        replacement value \b value, and returns the result in the output tensor \b y.
 *        If \b x is greater than \b threshold, \b y is equal to \b x, otherwise \b y is equal to \b value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the threshold
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] threshold
 *   Input. Pointer to the host memory that stores the threshold which is used to compare with input tensor \b x
 *   and to decide the return value \b y.
 * @param[in] value
 *   Input. Pointer to the host memory that stores the value to be returned in \b y if \b x is less than
 *   or equal to the threshold.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Threshold Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b x, output tensor \b y, threshold \b threshold and
 *   \b value must be the same.
 *   The supported data types of input tensor \b x, output tensor \b y, threshold \b threshold
 *   and \b value are as follows:
 *   - input tensor: half, float, uint8, int8, int16, int32.
 *   - output tensor: half, float, uint8, int8, int16, int32.
 *   - threshold: half, float, uint8, int8, int16, int32.
 *   - value: half, float, uint8, int8, int16, int32.
 *
 * @note
 * - Before calling this function, if data type of threshold \b threshold or value \b value is not
 *   the same as that of input tensor \b x, users need to cast data types of threshold \b threshold
 *   and value \b value to be consistent with input tensor \b x. Among of the data type casting, when
 *   casting float to int(int32, int16, int8, uint8), users need to use the \b to_zero rounding mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor x : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
     param:
       threshold: 4
       value: 666
     output tensor y : [666, 666, 666, 666, 666, 5, 6, 7, 8, 9]
     @endverbatim
 *
 * @par Reference
 * - http://pytroch.org/docs/1.6.0/generated/torch.nn.Threshold.html?highlight=threshold#torch.nn.Threshold
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlThreshold(cnnlHandle_t handle,
                                        const cnnlTensorDescriptor_t x_desc,
                                        const void *x,
                                        const void *threshold,
                                        const void *value,
                                        const cnnlTensorDescriptor_t y_desc,
                                        void *y);
// Group:Threshold
/*!
 * @brief Computes threshold_backward operation on input tensor \b x and input tensor \b diff_y
 *        by the threshold value \b threshold, and returns the result in the output tensor \b diff_x.
 *        If \b x is greater than \b threshold, \b diff_x is equal to \b diff_y, otherwise \b diff_x
 *        is equal to 0.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the threshold
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] diff_y_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] diff_y
 *   Input. Pointer to the MLU memory that stores the input tensor which is the gradient.
 *   The value of this parameter is returned in \b diff_x if \b x is greater than \b threshold.
 * @param[in] threshold
 *   Input. Pointer to the host memory that stores the threshold which is used to compare with input tensor \b x
 *   and to decide the return value \b diff_x.
 * @param[in] diff_x_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] diff_x
 *   Output. Pointer to the MLU memory that stores the output tensor which is the gradient.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Threshold Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b x, input tensor \b diff_y, threshold \b threshold and
 *   \b diff_x must be the same.
 *   The supported data types of input tensor \b x, input tensor \b diff_y, threshold \b threshold
 *   and output tensor \b diff_x are as follows:
 *   - input tensor: half, float, uint8, int8, int16, int32.
 *   - output tensor: half, float, uint8, int8, int16, int32.
 *   - threshold: half, float, uint8, int8, int16, int32.
 *
 * @note
 * - Before calling this function, if data type of threshold \b threshold is not same as that of
 *   input tensor \b x and input tensor \b diff_y, users need to cast the data type of threshold
 *   \b threshold to be consistent with input tensor \b x and input tensor \b diff_y. Among of
 *   the data type casting, when casting float to int(int32, int16, int8, uint8), users need to
 *   use the \b to_zero rounding mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input tensor x : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
     input tensor diff_y : [0, 11, 22, 33, 44, 55, 66, 77, 88, 99]
     param:
       threshold: 4
     output tensor diff_x : [0, 0, 0, 0, 0, 55, 66, 77, 88, 99]
     @endverbatim
 *
 * @par Reference
 * - http://pytroch.org/docs/1.6.0/generated/torch.nn.Threshold.html?highlight=threshold#torch.nn.Threshold
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlThresholdBackward(cnnlHandle_t handle,
                                                const cnnlTensorDescriptor_t x_desc,
                                                const void *x,
                                                const cnnlTensorDescriptor_t diff_y_desc,
                                                const void *diff_y,
                                                const void *threshold,
                                                const cnnlTensorDescriptor_t diff_x_desc,
                                                void *diff_x);

// Group:Det
/*!
 * @brief Computes determinants or log(determinants) of square matrices on input tensor
 * \b input depending on \b mode, and returns the results in the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   determinant computing operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] mode
 *   Input. The mode used to compute the determinant. The modes are defined in the
 *   ::cnnlDetMode_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Det" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor \b input and output tensor \b output must be the same.
 * - The supported data types are as follows:
 *   - input: float, half.
 *   - output: float, half.
 *
 * @par Scale Limitation
 * - The input square matrices should be no bigger than 285 * 285 on MLU300.
 * - On MLU200 series, for the half type, the input square matrices should be less than 254 * 254;
 *   for other data types, the input square matrices should be less than 313 * 313.
 * - The dimension of input tensor should be no less than 2. When the dimension of input
 *   tensor is 2, the dimension of output tensor should be 1. When the dimension of input
 *   tensor is greater than 2, the dimension number of output tensor should be 2 less than
 *   that of input tensor and the dimension length of output tensor and input tensor must
 *   be same except the last two dimensions.
 * - The length of the last 2 dimensions of input tensor should be same.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the determinant computing is as follows:
     @verbatim
      input one array by 1 * 2 * 2
      --> [[[1, 2],
            [3, 4]]]

      param:
        mode: 0

      output one array by 1
      --> output: [-2]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.det.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDet(cnnlHandle_t handle,
                                  cnnlDetMode_t mode,
                                  const cnnlTensorDescriptor_t input_desc,
                                  const void *input,
                                  const cnnlTensorDescriptor_t output_desc,
                                  void *output);

/*****************************************************************************
 * Cambricon CNNL OP: RNN
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing RNN cell implementation, but now only supports
 *        CNNL_LSTM and CNNL_LSTM_TANH.
 */
typedef enum {
  CNNL_RNN_RELU  = 0,  /*!< Basic RNN cell with ReLu activation. This is not supported now.*/
  CNNL_RNN_TANH  = 1,  /*!< Basic RNN cell with Tanh activation. This is not supported now.*/
  CNNL_LSTM      = 2,  /*!< LSTM cell. */
  CNNL_LSTM_TANH = 3, /*!< LSTM cell with Tanh activation before peephole. */
} cnnlRNNMode_t;

/*!
 * @brief Enumeration variables describing RNN forward mode.
 */
typedef enum {
  CNNL_FWD_MODE_INFERENCE = 0,  /* The RNN forward operation for the inference scenario. */
  CNNL_FWD_MODE_TRAINING,       /* The RNN forward operation for the training scenario. */
} cnnlForwardMode_t;

/*!
 * @brief Enumeration variables describing the number of bias vectors in the RNN cell.
 */
typedef enum {
  CNNL_RNN_NO_BIAS          = 0,
  /*!< No bias is used in the RNN cell formulas. */
  CNNL_RNN_SINGLE_INP_BIAS  = 1,
  /*!< One bias is used after input GEMM in the RNN cell formulas.*/
  CNNL_RNN_DOUBLE_BIAS      = 2,
  /*!< Two biases are used in the RNN cell formulas. This value is used by default.*/
  CNNL_RNN_SINGLE_REC_BIAS  = 3,
  /*!< One bias is used after recurrent GEMM in the RNN cell formulas. */
} cnnlRNNBiasMode_t;

/*!
 * @brief Enumeration variables describing how the RNN cell processes the input sequence data.
 */
typedef enum {
  CNNL_RNN_UNIDIRECTIONAL = 0,
  /*!< The RNN network iteratively processes the input sequence data from front to back.*/
  CNNL_RNN_BIDIRECTIONAL  = 1,
  /*!< Two RNN network iteratively processes the input sequence data from two directions, one from
       front to back, and the other from back to front. Finally, the results of the two RNN
       network are concatenated at each iteration as the output.
       Note: two RNN networks have different model parameters.*/
} cnnlDirectionMode_t;

/*!
 * @brief Enumeration variables describing how to process the RNN's input, whether to omit the gemm
 *        of the first RNN layer.
 *
 */
typedef enum {
  CNNL_RNN_LINEAR_INPUT = 0,
  /*!< Enable input's gemm in the first recurrent layer.*/
  CNNL_RNN_SKIP_INPUT   = 1,
  /*!< Omit input's gemm in first recurrent layer.*/
} cnnlRNNInputMode_t;

/*!
 * @brief Enumeration variables describing whether to pad the input and output sequence data.
 *        It is used in the ::cnnlSetRNNPaddingMode(), ::cnnlGetRNNPaddingMode().
 *        Default is ::CNNL_RNN_PADDED_IO_DISABLED.
 *
 * @note
 * - Currently, this enum is not supported for RNN inference.
 *
 */
typedef enum {
  CNNL_RNN_PADDED_IO_DISABLED = 0, /*!< Disable padded RNN's input and output. */
  CNNL_RNN_PADDED_IO_ENABLED  = 1, /*!< Enable padded RNN's input and output. */
} cnnlRNNPaddingMode_t;

/*!
 * @brief Enumeration variables are used to choose whether to make a clipping in RNN cell. It is
 *        used in the ::cnnlSetRNNClip(), ::cnnlGetRNNClip(). Default is ::CNNL_RNN_CLIP_NONE.
 *
 * @note
 * - Currently not support. It will be used in the future.
 *
 */
typedef enum {
  CNNL_RNN_CLIP_NONE   = 0,  /*!< Disables clipping operation in the RNN cell. */
  CNNL_RNN_CLIP_MINMAX = 1,  /*!< Enables clipping operation in the RNN cell. */
} cnnlRNNClipMode_t;

/*!
 * @brief Enumeration variables used to choose filter order of recurrent unit in LSTM cell. It is
 *        used in ::cnnlSetRNNWeightOrder() and::cnnlGetRNNWeightOrder(). Default is
 *        ::CNNL_LSTM_IFGO.
 */
typedef enum {
  CNNL_LSTM_IFGO = 0,  /*!< In the order of input gate, forget gate, update gate and output gate. */
  CNNL_LSTM_IFOG = 1,  /*!< In the order of input gate, forget gate, output gate and update gate. */
  CNNL_LSTM_IOFG = 2,  /*!< In the order of input gate, output gate, forget gate and update gate. */
} cnnlRNNWeightOrder_t;

/*!
 * @brief Enumeration variables used to choose whether to make a peephole connections in LSTM
 *        cell. Peephole connections allow the gates to use the previous internal state as well
 *        as the previous hidden state.
 *        It is used in the ::cnnlSetRNNPeepholeMode(), ::cnnlGetRNNPeepholeMode().
 *        Default is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 */
typedef enum {
  CNNL_LSTM_PEEPHOLE_DISABLED = 0, /*!< Disables LSTM peephole. */
  CNNL_LSTM_PEEPHOLE_ENABLED  = 1, /*!< Enables LSTM peephole. */
} cnnlRNNPeepholeMode_t;

/*!
 * @brief Enumeration variables describing whether to enable the mask operation in LSTM/RNN cell.
 *        It is used in the ::cnnlSetRNNMaskMode(), ::cnnlGetRNNMaskMode().
 *        Default is ::CNNL_LSTM_MASK_DISABLED.
 */
typedef enum {
  CNNL_LSTM_MASK_DISABLED  = 0,
  /*!< Disables LSTM/RNN mask operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_ENABLED   = 1,
  /*!< Enables LSTM/RNN mask operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_CONST     = 2,
  /*!< Enables LSTM/RNN mask const operation in the LSTM/RNN cell. */
  CNNL_LSTM_MASK_SEQUENCES = 3,
  /*!< Enables LSTM/RNN mask sequence length operation in the LSTM/RNN cell. */
} cnnlRNNMaskMode_t;

/*!
 * @brief Enumeration variables describing whether RNN cell has output layer.
 *        It is used in the ::cnnlSetRNNOutputMode() and: ::cnnlGetRNNOutputMode().
 *        It only works for rnn mode with CNNL_RNN_TANH/CNNL_RNN_RELU.
 *        Default is ::CNNL_RNN_NO_OUT_LAYER.
 */
typedef enum {
  CNNL_RNN_NO_OUT_LAYER = 0, /*!< There is no output layer in the RNN cell. */
  CNNL_RNN_HAS_OUT_LAYER  = 1, /*!< There is output layer in the RNN cell. */
} cnnlRNNOutputMode_t;

/*!
 * @brief Enumeration variables describing whether the calculation of each gate that corresponds to the
 *        input and hidden state is combined.
 *        It is used in the ::cnnlSetLSTMGateMode(), ::cnnlGetLSTMGateMode().
 *        Default is ::CNNL_LSTM_GATE_SEPARATION.
 *
 * @note
 * - Currently not support. It will be used in the future.
 *
 */
typedef enum {
  CNNL_LSTM_GATE_SEPARATION = 0, /*!< Performs GEMM once per gate. */
  CNNL_LSTM_GATE_MERGING    = 1, /*!< Performs a GEMM after merge the filter of the four gate.*/
} cnnlLSTMGateMode_t;

#define CNNL_LSTM_MAX_ACTIVATION_NUM 6
/*!
 * @brief Enumeration variables describing activation function for each activation position
 *        in the LSTM.
 *        It is used in the ::cnnlSetLSTMActivationMode(), ::cnnlGetLSTMActivationMode().
 *        The default activation function is the implementation in the standard LSTM cell.
 *
 * @note
 * - Currently not support. It will be used in the future.
 *
 */
typedef enum {
  CNNL_LSTM_GATE_FORGET = 0, /*!< The activation function of the forget gate. */
  CNNL_LSTM_GATE_INPUT  = 1, /*!< The activation function of the input gate. */
  CNNL_LSTM_GATE_UPDATE = 2, /*!< The activation function of the cell gate. */
  CNNL_LSTM_GATE_OUTPUT = 3, /*!< The activation function of the output gate. */
  CNNL_LSTM_CY  = 4, /*!< The activation function of the output cell state. */
  CNNL_LSTM_PROJECTION  = 5, /*!< The activation function of after projection. */
} cnnlLSTMActivationLocation_t;

// Group:RNN
/*!
 * @brief Computes the forward process of RNN network in the inference scenario. The specific
 *        network structure is determined by the description \b rnn_desc set by the user.
 *        Using the input data \b x, \b hx, \b cx, \b weight, \b bias, \b mask, according to
 *        the specific network structure, writes the calculation result into the output memory \b y,
 *        \b hy, \b cy.
 *
 * This function requires two additional MLU memory as the extra input and the workspace to improve
 * the RNN network performance. You can get the size of the workspace \b workspace_size
 * with the ::cnnlGetRNNWorkspaceSize function, and the size of the extra input \b extra_input_size
 * with the ::cnnlGetRNNExtraInputSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] cx_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[in]  w_desc
 *   Input. The descriptor of the filter tensor set.
 *   For detailed information, see ::cnnlTensorSetDescriptor_t.
 * @param[in]  filter
 *   Input. Pointer to the MLU memory that stores filter.
 * @param[in]  b_desc
 *   Input. The descriptor of the bias tensor set.
 *   For detailed information, see ::cnnlTensorSetDescriptor_t.
 * @param[in] bias
 *   Input. Pointer to the MLU memory that stores bias data and peephole data.
 * @param[in] mask_desc
 *   Input. The descriptor of mask sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] mask
 *   Input. Pointer to the MLU memory that stores mask data.
 * @param[in] extra_input
 *   Input. Pointer to the MLU memory that stores the extra input data.
 *   You need to copy the extra input data to MLU from the host that is
 *   initialized with ::cnnlInitRNNExtraInput. For more information
 *   about extra input data, see Cambricon CNNL User Guide
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores output sequence data.
 * @param[in] hy_desc
 *   Input. The descriptor of output hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] cy_desc
 *   Input. The descriptor of output cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] k_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] keys
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] c_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] i_attn
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] q_desc
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in] queries
 *   Reserved for future use. Set the value of these parameters to NULL.
 * @param[in]  workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in]  workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation. You can get the size of the workspace with
 *   the ::cnnlGetRNNWorkspaceSize function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the input tensor
 *   \b x_desc, and \b hx_desc, filter tensor \b w_desc, bias tensor \b cx_desc, \b cy_desc,
 *   \b b_desc and \b mask_desc, output_tensor \b y_desc, and \b hy_desc.
 *    - input tensor: int8, int16, half, float.
 *    - filter tensor: int8, int16, int31, half, float.
 *    - bias tensor: half, float.
 *    - output tensor: int8, int16, half, float.
 *
 * - If \b rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - Onchip and offchip data type of filter tensor must be same, and can be int8, int16, or int31.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - Onchip and offchip data type of filter tensor must be the same, which can be int8, int16,
 *     or int31 on MLU200/MLU270/MLU290/CE3226, and can be int8, int16, int31, half or float on
 *     MLU370.
 *   - If onchip data type of filter tensor is float or half, then the input tensor, bias tensor and
 *     output tensor should have the same onchip and offchip data type.
 *   - If the offchip data type of input tensor is float or half, then bias tensor should have the
 *     same offchip data type with input tensor.
 *   - If offchip data type of input tensor is int8 or int16, then filter tensor should have the
 *     same onchip data type with input tensor.
 *   - Input tensor and filter tensor should have the same onchip data type, besides input tensor
 *     and output tensor should have the same offchip data type.
 *   - If \b rec_proj_size is greater than zero, then onchip data type of filter tensor must be
 *     int8 or int16 on CE3226.
 *
 * - If \b mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - Mask data type must be int32.
 *
 * @par Data Layout
 * - The supported data layout descriptor are as follows:
 *   - \b x_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \b hx_desc must be set to \p CNNL_LAYOUT_ARRAY and \b dimNb in \b hx_desc must be 3.
 *   - \b cx_desc must be set to \p CNNL_LAYOUT_ARRAY and \b dimNb in \b cx_desc must be 3.
 *   - \b w_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \b b_desc must be set to \p CNNL_LAYOUT_NHWC.
 *   - \b mask_desc must be set to \p CNNL_SEQDATA_TN.
 *   - \b y_desc must be set to \p CNNL_SEQDATA_TNC.
 *   - \b hy_desc must be set to \p CNNL_LAYOUT_ARRAY and \b dimNb in \b hy_desc must be 3.
 *   - \b cy_desc must be set to \p CNNL_LAYOUT_ARRAY and \b dimNb in \b cy_desc must be 3.
 *
 * @par Scale Limitation
 * - \b rnn_mode supports ::CNNL_RNN_RELU, ::CNNL_RNN_TANH, ::CNNL_LSTM and ::CNNL_LSTM_TANH.
 * - Layout of \b x_desc and \b y_desc only supports ::CNNL_SEQDATA_TNC.
 * - If \b rnn_mode is not ::CNNL_LSTM or ::CNNL_LSTM_TANH, then \b cx_desc, \b cx, \b cy_desc and
 *   \b cy are NULL.
 * - The descriptor of filter tensor set \b w_desc, contains all filter tensor releated information
 *   of RNN. It's dimensions is (\b num_layers, direction_num, cell_filter_num).
 * - When \b direction is ::CNNL_RNN_BIDIRECTIONAL, \b direction_num must be 2, else
 *   \b direction_num must be 1.
 * - The descriptor of filter tensor sets \b w_desc contains all filter tensor releated information
 *   of RNN. The dimensions is (\b num_layers, direction_num, cell_filter_num).
 * - The descriptor of bias tensor set \b b_desc, containing all bias and peephole data.
 *   Its dimensions is (layer_num, direction_num, cell_data_num).
 *   The principle is the same as above w_desc, the difference is that addition to bias, peephole
 *   tensor is included.
 *
 * - If rnn_mode is ::CNNL_RNN_RELU or ::CNNL_RNN_TANH:
 *   - If \b input_mode is ::CNNL_RNN_LINEAR_INPUT and \b output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \b cell_filter_num should be 2.
 *   - If \b input_mode is ::CNNL_RNN_LINEAR_INPUT and \b output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \b cell_filter_num should be 3.
 *   - If \b input_mode is ::CNNL_RNN_SKIP_INPUT and \b output_mode is ::CNNL_RNN_NO_OUT_LAYER, the
 *     first layer \b cell_filter_num should be 1.
 *     Other layers \b cell_filter_num should be 2.
 *   - If \b input_mode is ::CNNL_RNN_SKIP_INPUT and \b output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     the first layer \b cell_filter_num should be 2.
 *     Other layers \b cell_filter_num should be 3.
 *   - The overall filter order of \b w_desc is the matrix multiplication with input,
 *     and the matrix multiplication with the hidden state. The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some filter may disappear according to RNN configurations.
 *
 *   - If \b bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \b output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \b cell_data_num is 2.
 *   - If \b bias_mode is ::CNNL_RNN_SINELG_INP_BIAS or ::CNNL_RNN_SINELG_REC_BIAS and
 *     \b output_mode is
 *     ::CNNL_RNN_NO_OUT_LAYER, \b cell_data_num is 1.
 *   - If \b bias_mode is ::CNNL_RNN_NO_BIAS and output_mode is ::CNNL_RNN_NO_OUT_LAYER,
 *     \b cell_data_num is 0.
 *   - If \b bias_mode is ::CNNL_RNN_DOUBLE_BIAS and \b output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \b cell_data_num is 3.
 *   - If \b bias_mode is ::CNNL_RNN_SINELG_INP_BIAS or ::CNNL_RNN_SINELG_REC_BIAS and
 *     \b output_mode is ::CNNL_RNN_HAS_OUT_LAYER, \b cell_data_num is 2.
 *   - If \b bias_mode is ::CNNL_RNN_NO_BIAS and \b output_mode is ::CNNL_RNN_HAS_OUT_LAYER,
 *     \b cell_data_num is 1.
 *   - The overall bias order of \b b_desc is the input related and hidden state related.
 *     The specific location is as follows:
 *     - Value 0 corresponds to the input gate.
 *     - Value 1 corresponds to the recursive gate.
 *     - Value 2 corresponds to the output gate.
 *     some bias may disappear according to RNN configurations.
 *   - For \b y_desc, its \b layout and \b dtype need to match that of x_desc.
 *   - For \b hy_desc, its \b layout and \b dtype and \b shape must be set the same way as
 *     \b hx_desc.
 *   - If the shape of the input sequence data \b x_desc is (\b T, \b N, \b C), \b layer_num must
 *     be greater than or equal to 1,
 *     \b T must be greater than or equal to 1, and \b C must be greater than or equal to 1.
 *     If \b input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must equal to hidden_size.
 *
 * - If rnn_mode is ::CNNL_LSTM or ::CNNL_LSTM_TANH:
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is enabled, \b cell_filter_num
 *     should be 9.
 *   - If input_mode is ::CNNL_RNN_LINEAR_INPUT and LSTM projection is disabled, \b cell_filter_num
 *     should be 8.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is enabled, the first layer
 *     \b cell_filter_num should be 5.
 *     Other layers \b cell_filter_num should be 9.
 *   - If input_mode is ::CNNL_RNN_SKIP_INPUT and LSTM projection is disabled, the first layer
 *     \b cell_filter_num should be 4.
 *     Other layers \b cell_filter_num should be 8.
 *   - If \b filter_order is CNNL_LSTM_IFGO, then the overall filter order of \b w_desc is the
 *     matrix multiplication
 *     with input, and the matrix multiplication with the hidden state.
 *     The specific location is as follows:
 *     - Value 0 and value 4 correspond to the input gate.
 *     - Value 1 and value 5 correspond to the forget gate.
 *     - Value 2 and value 6 correspond to the update gate.
 *     - Value 3 and value 7 correspond to the output gate.
 *     - Value 8 correspond to the projection filter.
 *     some filter may disappear and filter orders may be different according to RNN configurations.
 *
 *   - If \b bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \b peephole is enabled, then \b cell_data_num
 *     is 11.
 *   - If \b bias_mode is ::CNNL_RNN_SINELG_INP_BIAS or ::CNNL_RNN_SINELG_REC_BIAS, and \b peephole
 *     is enabled,
 *     then \b cell_data_num is 7.
 *   - If \b bias_mode is ::CNNL_RNN_NO_BIAS, and \b peephole is enabled, then \b cell_data_num
 *     is 3.
 *   - If \b bias_mode is ::CNNL_RNN_DOUBLE_BIAS, and \b peephole is disabled, then
 *     \b cell_data_num is 8.
 *   - If \b bias_mode is ::CNNL_RNN_SINELG_INP_BIAS or ::CNNL_RNN_SINELG_REC_BIAS, and
 *     \b peephole is disabled,
 *     then \b cell_data_num is 4.
 *   - If \b bias_mode is ::CNNL_RNN_NO_BIAS, and \b peephole is disabled, then
 *     \b cell_data_num is 0.
 *   - If \b filter_order is CNNL_LSTM_IFGO, then the overall bias order of \b b_desc
 *     is the input related,
 *     hidden state related and peephole connection related. The specific location is as follows:
 *     - Value 0 and value 4 correspond to bias of input gate.
 *     - Value 1 and value 5 correspond to bias of forget gate.
 *     - Value 2 and value 6 correspond to bias of update gate.
 *     - Value 3 and value 7 correspond to bias of output gate.
 *     - Value 8 coreespond to peephole of input gate.
 *     - Value 9 coreespond to peephole of forget gate.
 *     - Value 10 coreespond to peephole of output gate.
 *     some bias may disappear and bias orders may be different according to RNN configurations.
 *   - For \b y_desc, its \b layout and \b dtype need to match that of x_desc.
 *   - For \b hy_desc, its \b layout and \b dtype and \b shape must be set the same way as
 *     \b hx_desc.
 *   - If the shape of the input sequence data \b x_desc is (\b T, \b N, \b C),
 *     then the input sequence \b x_desc and the RNN descriptor must meet the following
 *     requirements:
 *     - \b state_size and \b rec_proj_size must be less than or equal to \b hidden_size.
 *     - Size of \b rec_filter 4 *  hidden_size *  state_size must be less than or equal to 2^31-1.
 *     - If \b input_mode is ::CNNL_RNN_SKIP_INPUT, then input_size must meet the requirements:
 *       - input_size = 4 * hidden_size.
 *     - On CE3226:
 *       - Pad(\b hidden_size, 64) * 4 * 64 * sizeof(\b filter_type) <= \b wram_size / 2.
 *       - layer num must be less than or equal to 1.
 *       - \b N must be less than or equal to 64.
 *       - \b C must be less than or equal to 1024.
 *
 * - If mask_mode is ::CNNL_LSTM_MASK_SEQUENCES:
 *   - The mask tensor stores the valid sequence length for each token, and the shape should be
 *     (1, \b N).
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \b x_desc and output sequence data \b y_desc
 *   to ::CNNL_SEQDATA_TNC, set the layout of the filter tensor and bias tensor to NHWC.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - Long Short-Term Memory, Hochrereiter, 1997.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlRNNForwardInference(cnnlHandle_t handle,
                        const cnnlRNNDescriptor_t rnn_desc,
                        const cnnlSeqDataDescriptor_t x_desc,
                        const void *x,
                        const cnnlTensorDescriptor_t hx_desc,
                        const void *hx,
                        const cnnlTensorDescriptor_t cx_desc,
                        const void *cx,
                        const cnnlTensorSetDescriptor_t w_desc,
                        const void *filter,
                        const cnnlTensorSetDescriptor_t b_desc,
                        const void *bias,
                        const cnnlSeqDataDescriptor_t mask_desc,
                        const void *mask,
                        const void *extra_input,
                        const cnnlSeqDataDescriptor_t y_desc,
                        void *y,
                        const cnnlTensorDescriptor_t hy_desc,
                        void *hy,
                        const cnnlTensorDescriptor_t cy_desc,
                        void *cy,
                        const cnnlSeqDataDescriptor_t k_desc, /* reserved, should pass NULL */
                        const void *keys,                     /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t c_desc, /* reserved, should pass NULL */
                        void *cAttn,                          /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t i_desc, /* reserved, should pass NULL */
                        void *iAttn,                          /* reserved, should pass NULL */
                        const cnnlSeqDataDescriptor_t q_desc, /* reserved, should pass NULL */
                        void *queries,                        /* reserved, should pass NULL */
                        void *workspace,
                        size_t workspace_size);
// Group:RNN
/*!
 * @brief Computes the forward process of RNN network in the training scenario. The specific
 *        network structure is determined by the description \b rnn_desc set by the user.
 *        Using the input data \b x, \b hx, \b cx, \b filterspace, according to the specific
 *        network structure, writes the calculation result into the output memory \b y,
 *        \b hy, \b cy.
 *
 * This function requires two additional MLU memory as the \b reservespace and the \b workspace to
 * improve the RNN network performance. You can get the size of the \b workspace \b workspace_size and
 * \b reservespace \b reservespace_size with the ::cnnlGetRNNTempSizes function, and the size of the
 * \b filterspace \b filterspace_size with the ::cnnlGetRNNWeightSpaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \b seqLengthArray set in \b x_desc or \b y_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores output sequence data.
 * @param[in] h_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[out] hy
 *   Output. Pointer to the MLU memory that stores output hidden state tensor.
 * @param[in] c_desc
 *   Input. The descriptor of cell state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] cx
 *   Input. Pointer to the MLU memory that stores cell state tensor.
 * @param[out] cy
 *   Output. Pointer to the MLU memory that stores output cell state tensor.
 * @param[in] filterspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra \b workspace for the RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the RNN operation.
 *   You can get the size of the workspace with the ::cnnlGetRNNTempSizes function.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *   intermediate results of RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in the RNN operation.
 *   You can get the size of the reservespace with the ::cnnlGetRNNTempSizes function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \b x_desc must be set to \p ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \b y_desc must be set to the same layout as \b x_desc.
 *   - \b h_desc must be set to \p ::CNNL_LAYOUT_ARRAY and \b dimNb in \b h_desc must be 3.
 *   - \b c_desc must be set to \p ::CNNL_LAYOUT_ARRAY and \b dimNb in \b c_desc must be 3.
 *
 * @par Scale Limitation
 * - The \b rnn_mode of rnn_desc \b only supports ::CNNL_LSTM with projection layer.
 * - The \b input_mode of rnn_desc \b only supports ::CNNL_RNN_LINEAR_INPUT.
 * - The \b padding_mode of rnn_desc \b only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 * - If \b direction is ::CNNL_RNN_UNIDIRECTIONAL, then the first dimension of hidden state
 *   tensor or cell state tensor should match its layer_num argument passed to ::cnnlSetRNNDescriptor_v2.
 * - If dir_mode is ::CNNL_RNN_BIDIRECTIONAL, then the first dimension should be double the
 *   \b layer_num argument passed to ::cnnlSetRNNDescriptor_v2.
 * - The second dimension must match the batch's size and the third dimension must match the hidden's size.
 * - \b layer_num must be set to 1 currently.
 * - The \b layout and \b dtype of \b y_desc need to match that of \b x_desc.
 * - The \b math_prec must be int16 on CE3226 and MLU200 series and it must be the same as
 *   \b data_type or int16 on MLU300 series.
 * - The \b dev_seq_lengths must be batch's sequence and descending order and the length of
 *   the \b dev_seq_lengths must be equal to x_desc->dims[0] when the \b cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED.
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance,
 *   set the layout of the input sequence data \b x_desc and output sequence data \b y_desc
 *   to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - The first two dimensions of \b x_desc must be equal to those of \b y_desc.
 */
cnnlStatus_t CNNL_WIN_API
cnnlRNNForwardTraining(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       const int dev_seq_lengths[],
                       const cnnlSeqDataDescriptor_t x_desc,
                       const void *x,
                       const cnnlSeqDataDescriptor_t y_desc,
                       void *y,
                       const cnnlTensorDescriptor_t h_desc,
                       const void *hx,
                       void *hy,
                       const cnnlTensorDescriptor_t c_desc,
                       const void *cx,
                       void *cy,
                       const void *filterspace,
                       size_t filterspace_size,
                       void *workspace,
                       size_t workspace_size,
                       void *reservespace,
                       size_t reservespace_size);

// Group:LSTMGates
/*!
* @brief Computes the forward process of LSTM cell network in the training scenario.
*        It use the input data \b x_gates, \b x_bias, \b h_gates, \b h_bias and \b cx
*        according to the specific network structure and writes the calculation result
*        into the output memory \b cy and \b hy.
*
* This function requires an additional MLU memory as the \b reservespace to improve the
* performance of LSTM cell network. You can get the size of the \b reservespace
* \b reservespace_size with the ::cnnlGetLSTMGatesTempSize function.
*
* @param[in] handle
*   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
*   queues in the LSTM cell operation.
*   For detailed information, see ::cnnlHandle_t.
* @param[in] gates_desc
*   Input. The descriptor of gates tensor, \b x_gates, \b x_bias, \b h_gates and \b h_bias.
*   For detailed information, see ::cnnlTensorDescriptor_t.
* @param[in] x_gates
*   Input. Pointer to the MLU memory that stores input gates data.
* @param[in] x_bias
*   Input. Pointer to the MLU memory that stores bias of input gates data.
* @param[in] h_gates
*   Input. Pointer to the MLU memory that stores hidden gates data.
* @param[in] h_bias
*   Input. Pointer to the MLU memory that stores bias of hidden gates data.
* @param[in] c_desc
*   Input. The descriptor of cell state tensor.
*   For detailed information, see ::cnnlTensorDescriptor_t.
* @param[in] cx
*   Input. Pointer to the MLU memory that stores cell state tensor.
* @param[out] cy
*   Output. Pointer to the MLU memory that stores output cell state tensor.
* @param[in] h_desc
*   Input. The descriptor of hidden state tensor.
*   For detailed information, see ::cnnlTensorDescriptor_t.
* @param[out] hy
*   Output. Pointer to the MLU memory that stores output hidden state tensor.
* @param[in] reservespace
*   Input. Pointer to the MLU memory that is used as an extra memory space for saving
*   intermediate results of LSTM cell operation.
* @param[in] reservespace_size
*   Input. The size of the extra reservespace in bytes that needs to be used in the LSTM cell operation.
*   You can get the size of the reservespace with the ::cnnlGetLSTMGatesTempSize function.
* @par Return
* - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
*
* @par Formula
*  i = sigmoid(xi + hi + xi_bias + hi_bias)
*  f = sigmoid(xf + hf + xf_bias + hf_bias)
*  g = tanh(xg + hg + xg_bias + hg_bias)
*  o = sigmoid(xo + ho + xo_bias + ho_bias)
*  cy = f * cx + i * g
*  hy = o * tanh(cy)

* @par Data Type
* - Date types of input tensor and output tensor should be the same.
* - The supported data types of input and output tensors are as follows:
*   - input tensor: half, float.
*   - output tensor: half, float.
*
* @par Data Layout
* - The supported data layouts set in descriptors are as follows:
*   - \b gates_desc must be set to \p ::CNNL_LAYOUT_NC.
*   - \b c_desc must be be set to \p ::CNNL_LAYOUT_NC.
*   - \b h_desc must be be set to \p ::CNNL_LAYOUT_NC.
*   - The gate layout of gates data and bias must be ifgo.
*
*@par Note
* - The \b x_bias and \b h_bias can be set to NULL,
*   which means no bias participate in operations.
* - The size of the \b x_bias and \b h_bias is 4 * C, 4 means four gates and
*   C means hidden.
* - The first dimemsion of the \x_gates and \h_gates is 4 * N, 4 means four gates
*   and N means batch.
*/

cnnlStatus_t CNNL_WIN_API
cnnlLSTMGatesForward(cnnlHandle_t handle,
                    const cnnlTensorDescriptor_t gates_desc,
                    const void *x_gates,
                    const void *x_bias,
                    const void *h_gates,
                    const void *h_bias,
                    const cnnlTensorDescriptor_t c_desc,
                    const void *cx,
                    void *cy,
                    const cnnlTensorDescriptor_t h_desc,
                    void *hy,
                    void *reservespace,
                    size_t reservespace_size);
// Group:RNN
/*!
 * @brief Returns in \b size_in_bytes the size of the MLU memory that is used as an extra workspace
 * to optimize the RNN forward operation.
 *
 * The size of extra workspace is based on the given information of the RNN
 * forward operation, including RNN descriptor \b conv_desc,
 * the input sequence data descriptor \b x_desc tht output sequence data descriptor \b y_desc.
 * For more information about the extra workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] y_desc
 *   Input. The descriptor of the output sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] size_in_bytes
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the RNN forward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor functions to create and
 *   set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \b x_desc and \b y_desc.
 * - The allocated extra workspace memory should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN forward operation.
 *
 * @note
 * - For the RNN descriptor, in addition to the basic information set by calling the function
 *   ::cnnlSetRNNDescriptor, if you want to run the optional variant mode, you need to call the
 *   corresponding function to set it. Such as:
 *   ::cnnlSetRNNProjectionLayers, ::cnnlSetRNNPeepholeMode, ::cnnlSetRNNBiasMode,
 *   ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip, ::cnnlSetRNNPaddingMode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWorkspaceSize(cnnlHandle_t handle,
                        const cnnlRNNDescriptor_t rnn_desc,
                        const cnnlSeqDataDescriptor_t x_desc,
                        const cnnlSeqDataDescriptor_t y_desc,
                        size_t *size_in_bytes);

// Group:RNN
/*!
 * @brief Returns in \b extra_input_size the size of the MLU memory and host memory that is used as an extra
 * input data to optimize the RNN forward operation. You need to allocate memory both on host and MLU based on
 * the size returned in \b extra_input_size.
 *
 * The size of extra input data is based on the given information of the RNN
 * forward operation, including RNN descriptor \b rnn_desc,
 * the input sequence data descriptor \b x_desc.
 * For more information about the extra input, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the convolution operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] extar_input_size
 *   Output. A host pointer to the returned size of the extra input data in bytes
 *   that is used in the RNN operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_size is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor functions to create and
 *   set up a complete the RNN operation descriptors,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \b x_desc.
 * - After calling this function, you need to call ::cnnlInitRNNExtraInput to initialize the memory on host.
 * - The allocated extra input should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetRNNExtraInputSize(cnnlHandle_t handle,
                                                   cnnlRNNDescriptor_t desc,
                                                   cnnlSeqDataDescriptor_t x_desc,
                                                   size_t *extra_input_size);
// Group:RNN
/*!
 * @brief Initializes the extra input data space \b extra_input_host_ptr on host.
 *
 * @param[out]  extra_input_host_ptr
 *   Output. Pointer to the host memory that is used as an extra input space
 *   for the RNN forward operation.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] hx_desc
 *   Input. The descriptor of the hidden state tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor set. For detailed information,
 *   see ::cnnlTensorSetDescriptor_t.
 * @param[in] b_desc
 *   Input. The descriptor of the bias tensor set. For detailed information,
 *   see ::cnnlTensorSetDescriptor_t.
 * @param[in] w_ptr
 *   Input. The pointer to all filter data address on the MLU device memory.
 * @param[in] b_ptr
 *   Input. Pointer to all bias data address on the MLU device memory.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameter \b extra_input_host_ptr is a pointer, should not be NULL.
 *
 * @par API Dependency
 * - You need to get the size of the extra input data with ::cnnlGetRNNExtraInputSize.
 *   The memory of the extra input data should be allocated before calling this function.
 * - This function must be called after ::cnnlCreateSeqDataDescriptor, ::cnnlSetSeqDataDescriptor,
 *   ::cnnlCreateTensorDescriptor, ::cnnlSetTensorDescriptor,
 *   ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor and
 *   ::cnnlInitTensorSetMemberDescriptorPositionAndScale.
 *   The ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions are used to create
 *   and set the sequence data descriptors \b x_desc.
 *   The ::cnnlCreateTensorDescriptor and ::cnnlSetTensorDescriptor functions are used to create and set
 *   the tensor descriptors \b hx_desc.
 *   The ::cnnlCreateTensorSetDescriptor, ::cnnlInitTensorSetMemberDescriptor,
 *   and ::cnnlInitTensorSetMemberDescriptorPositionAndScale functions are used to create and set
 *   the tensor set descriptors \b w_desc, \b b_desc.
 * - This function must be called after allocate MLU memory of \b w_ptr, \b b_ptr.
 * - The allocated extra input should be passed to the ::cnnlRNNForwardInference function
 *   to perform the RNN operation.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlInitRNNExtraInput(void *extra_input_host_ptr,
                                                cnnlRNNDescriptor_t rnn_desc,
                                                cnnlSeqDataDescriptor_t x_desc,
                                                cnnlTensorDescriptor_t hx_desc,
                                                cnnlTensorSetDescriptor_t w_desc,
                                                cnnlTensorSetDescriptor_t b_desc,
                                                void *w_ptr,
                                                void *b_ptr);

// Group:RNN
/*!
 * @brief Creates a descriptor pointed by \b rnn_desc for a RNN inference forward
 *        operation, and allocates memory for holding the information about the RNN
 *        operation. The information is defined in ::cnnlRNNDescriptor_t. For more
 *        information about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[out] rnn_desc
 *   Input. A host pointer to the RNN descriptor that holds information about the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetRNNDescriptor function to initialize
 *   and set the basic information to the RNN descriptor. In addition to the basic information,
 *   there are some optional modes that can be set through other function.
 *   E.g ::cnnlSetRNNProjectionLayers, ::cnnlSetRNNPeepholeMode.
 * - At last, you need to call the ::cnnlDestroyRNNDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateRNNDescriptor(cnnlRNNDescriptor_t *rnn_desc);

// Group:RNN
/*!
 * @brief Destroys a RNN descriptor \b rnn_desc that is previously created with the
 *        ::cnnlCreateRNNDescriptor function.
 *
 * The RNN descriptor is defined in ::cnnlRNNDescriptor_t
 * and holds the information about the RNN inference forward operation.
 *
 * @param[in] rnn_desc
 *   Input. The convolution descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlRNNForwardInference function.
 * - This function should be called to destroy the RNN descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyRNNDescriptor(cnnlRNNDescriptor_t rnn_desc);

// Group:RNN
/*!
 * @brief Initializes the RNN descriptor \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function.
 * The information includes the number of features in the hidden state \b hidden_size,
 * the number of recurrent layers \b num_layers, how the first recurrent layer processes the input
 * \b input_mode, the recurrence pattern \b direction, RNN cell implementation \b mode.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] hidden_size
 *   Input. The number of features in the hidden state input vector.
 * @param[in] num_layers
 *   Input. The Number of stacked layers in the deep RNN mode.
 * @param[in] input_mode
 *   Input. Specifies how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \b input_mode is  ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 *   For detailed information, see ::cnnlRNNInputMode_t.
 * @param[in] direction
 *   Input. Determines recurrence pattern. In ::CNNL_RNN_BIDIRECTIONAL mode, the result of the
 *   current layer are concatenations of forward and backward hidden states in the feature dimension.
 *   For detailed information, see ::cnnlDirectionMode_t.
 * @param[in] mode
 *   Input. Determines how to implement the RNN cell. For detailed information, see ::cnnlRNNMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetRNNDescriptor(cnnlRNNDescriptor_t rnn_desc,
                                               const int hidden_size,
                                               const int num_layers,
                                               const cnnlRNNInputMode_t input_mode,
                                               const cnnlDirectionMode_t direction,
                                               const cnnlRNNMode_t rnn_mode);
// Group:RNN
/*!
 * @brief Retrieves the RNN operation information from \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function and set by ::cnnlSetRNNDescriptor function.
 * The information includes the number of features in the hidden state \b hidden_size,
 * number of recurrent layers \b num_layers, how the first recurrent layer processes the input
 * \b input_mode, the recurrence pattern \b direction, RNN cell implementation \b mode.
 *
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[out] hidden_size
 *   Output. The number of features in the hidden state input vector.
 * @param[out] num_layers
 *   Output. Number of stacked layers in the deep RNN mode.
 * @param[out] input_mode
 *   Output. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \b input_mode is  ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[out] direction
 *   Output. Determine recurrence pattern. In ::CNNL_RNN_BIDIRECTIONAL mode, the result of the
 *   current layer are concatenations of forward and backward hidden states in the feature dimension.
 * @param[out] mode
 *   Output. Determine the RNN cell implementation in mode(CNNL_RNN_RELU, CNNL_RNN_TANH, CNNL_LSTM, CNNL_LSTM_TANH).
 *   For detailed information, see ::cnnlRNNMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Output pointer \b hidden_size, \b numLayers, \b inputMode, \b direction, \b mode should not be NULL.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNDescriptor(cnnlRNNDescriptor_t rnn_desc,
                     int *hidden_size,
                     int *numLayers,
                     cnnlRNNInputMode_t *inputMode,
                     cnnlDirectionMode_t *direction,
                     cnnlRNNMode_t *mode);
// Group:RNN
/*!
 * @brief Sets bias number for the RNN descriptor \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation. For detailed information,
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] bias_mode
 *   Input. Specify bias number of the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If this function is not called, the default bias mode is ::CNNL_RNN_DOUBLE_BIAS.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNBiasMode(cnnlRNNDescriptor_t rnn_desc,
                   const cnnlRNNBiasMode_t bias_mode);

// Group:RNN
/*!
 * @brief Gets bias number from the RNN descriptor \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] bias_mode
 *   Output. Pointer to the host memory where the RNN bias mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNBiasMode function is not called, default bias mode is ::CNNL_RNN_DOUBLE_BIAS.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNBiasMode(cnnlRNNDescriptor_t rnn_desc,
                   cnnlRNNBiasMode_t *bias_mode);

// Group:RNN
/*!
 * @brief Sets computing precison type for \b rnn_desc which is already created by ::cnnlCreateRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] math_prec
 *   Input. The data type of computing precision used in RNN operation. Supported values are ::CNNL_DTYPE_HALF
 *   and ::CNNL_DTYPE_FLOAT.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \b rnn_mode is CNNL_RNN_RELU, CNNL_RNN_TANH,
 *   CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNMathPrec(cnnlRNNDescriptor_t rnn_desc,
                   const cnnlDataType_t math_prec);

// Group:RNN
/*!
 * @brief Gets computing precison type of \b rnn_desc set with ::cnnlSetRNNMathPrec.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] math_prec
 *   Output. Pointer to the host memory of \b math_prec.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor and ::cnnlSetRNNMathPrec
 *   functions.
 *
 * @note
 * - If ::cnnlSetRNNMathPrec function is not called, the default computing precison type is uncertain.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNMathPrec(cnnlRNNDescriptor_t rnn_desc,
                   cnnlDataType_t* math_prec);

// Group:RNN
/*!
 * @brief Sets LSTM filter order for \b rnn_desc which is already created by ::cnnlCreateRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] filter_order
 *   Input. The LSTM filter order used in RNN operation defined in ::cnnlRNNWeightOrder_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If this function is not called, the default filter order is ::CNNL_LSTM_IFGO.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \b rnn_mode is CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNWeightOrder(cnnlRNNDescriptor_t rnn_desc,
                      const cnnlRNNWeightOrder_t filter_order);

// Group:RNN
/*!
 * @brief Gets LSTM weight order of \b rnn_desc set with ::cnnlSetRNNWeightOrder.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] filter_order
 *   Output. The LSTM weight order used in RNN operation defined in ::cnnlRNNWeightOrder_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor and ::cnnlSetRNNWeightOrder
 *   functions.
 *
 * @note
 * - If ::cnnlSetRNNMathPrec function is not called, the default computing precison type is ::CNNL_LSTM_IFGO.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightOrder(cnnlRNNDescriptor_t rnn_desc,
                      cnnlRNNWeightOrder_t* weight_order);

// Group:RNN
/*!
 * @brief Sets LSTM/RNN computation preference mode for \b rnn_desc.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] comp_pref
 *   Input. The RNN computation preference mode defined in ::cnnlComputationPreference_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If this function is not called, the default computation preference mode is ::CNNL_COMPUTATION_FAST.
 * - If computation preference mode is set to ::CNNL_COMPUTATION_HIGH_PRECISION, math_pre must be
 *   CNNL_DTYPE_FLOAT and MLU device must not be MLU220/MLU270/MLU290.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works when \b rnn_mode is CNNL_RNN_RELU, CNNL_RNN_TANH,
 *   CNNL_RNN_LSTM or CNNL_RNN_LSTM_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNComputationPreference(cnnlRNNDescriptor_t rnn_desc,
                                const cnnlComputationPreference_t comp_pref);

// Group:RNN
/*!
 * @brief Gets LSTM/RNN computation preference mode of \b rnn_desc set with ::cnnlSetRNNComputationPreference.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] comp_pref
 *   Output. Pointer to the memory that stores the LSTM/RNN computation preference mode defined in
 *   ::cnnlComputationPreference_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If ::cnnlSetRNNComputationPreference function is not called, the default computation preference mode
 *   is CNNL_COMPUTATION_FAST.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirments
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNComputationPreference(cnnlRNNDescriptor_t rnn_desc,
                                cnnlComputationPreference_t* comp_pref);

// Group:RNN
/*!
 * @brief Sets RNN output mode for \b rnn_desc.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] output_mode
 *   Input. The RNN output mode defined in ::cnnlRNNOutputMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If this function is not called, the default output mode is
 *   ::CNNL_RNN_NO_OUT_LAYER.
 * - This function is only used for ::cnnlRNNForwardInference.
 * - This function only works only if \b rnn_mode is CNNL_RNN_RELU or CNNL_RNN_TANH.
 *
 * @par Requirements
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNOutputMode(cnnlRNNDescriptor_t rnn_desc,
                     const cnnlRNNOutputMode_t output_mode);

// Group:RNN
/*!
 * @brief Gets RNN output mode of \b rnn_desc set with ::cnnlSetRNNOutputMode.
 *
 * @param[in] rnn_desc
 *   Input. The RNN operation descriptor created by ::cnnlCreateRNNDescriptor.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] output_mode
 *   Output. Pointer to the memory that stores the RNN output mode defined in
 *   ::cnnlRNNOutputMode_t.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function, you need to call ::cnnlCreateRNNDescriptor to create the RNN operation
 *   descriptor.
 *
 * @note
 * - If ::cnnlSetRNNOutputMode function is not called, the default output mode
 *   is CNNL_RNN_NO_OUT_LAYER.
 * - This function is only used for ::cnnlRNNForwardInference.
 *
 * @par Requirments
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNOutputMode(cnnlRNNDescriptor_t rnn_desc,
                     cnnlRNNOutputMode_t* output_mode);

// Group:RNN
/*!
 * @brief Sets the size of the LSTM cell output after the projection layer
 *  for the RNN descriptor \b rnn_desc that is previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] rec_proj_size
 *   Input. Specify recurrent projection layer size on the RNN operation.
 * @param[in] out_proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - Currently, output projection is not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNProjectionLayers(cnnlRNNDescriptor_t rnn_desc,
                           const int rec_proj_size,
                           const int out_proj_size);

// Group:RNN
/*!
 * @brief Gets projeciton layer parameters from the RNN descriptor \b rnn_desc
 * that is previously set by ::cnnlSetRNNProjectionLayers function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] rec_proj_size
 *   Output. Pointer to host memory where recurrent projection layer parameter should be saved.
 * @param[out] out_proj_size
 *   Output. Pointer to host memory where output projection layer parameter should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNProjectionLayers function is not call,
 *   \b *rec_proj_size and \b *out_proj_size is 0.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNProjectionLayers(cnnlRNNDescriptor_t rnn_desc,
                           int *rec_proj_size,
                           int *out_proj_size);

// Group:RNN
/*!
 * @brief Sets peephole connections with cell state for the RNN operation that is previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] peephole_mode
 *   Input. Specify peephole connections for the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function not call,
 *   \b peephole_mode default is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 * - Currently, ::CNNL_LSTM_PEEPHOLE_ENABLED must be used with projection layer.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNPeepholeMode(cnnlRNNDescriptor_t rnn_desc,
                       const cnnlRNNPeepholeMode_t peephole_mode);

// Group:RNN
/*!
 * @brief Gets peephole connection mode from the RNN descriptor \b rnn_desc
 * that is previously set by ::cnnlSetRNNPeepholeMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] peephole_mode
 *   Output. Pointer to host memory where peephole mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function is not called,
 *   \b *peephole_mode is ::CNNL_LSTM_PEEPHOLE_DISABLED.
 * - Currently, ::CNNL_LSTM_PEEPHOLE_ENABLED must be used with projection layer.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNPeepholeMode(cnnlRNNDescriptor_t rnn_desc,
                       cnnlRNNPeepholeMode_t *peephole_mode);

// Group:RNN
/*!
 * @brief Sets mask mode for the RNN operation that is previously created with the
 *        ::cnnlCreateRNNDescriptor function.
 *
 * This function is used to enable mask operation.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] mask_mode
 *   Input. Specify mask operaiton for the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNPeepholeMode function not call,
 *   \b mask_mode default is ::CNNL_LSTM_MASK_DISABLED.
 *
 * @par Requirements
 * - None.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNMaskMode(cnnlRNNDescriptor_t desc,
                   const cnnlRNNMaskMode_t mask_mode);

// Group:RNN
/*!
 * @brief Gets mask mode from the RNN descriptor \b rnn_desc that is previously set by
 *        ::cnnlSetRNNMaskMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] mask_mode
 *   Output. Pointer to host memory where mask mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - If ::cnnlSetRNNMaskMode function is not called,
 *   \b *mask_mode is ::CNNL_LSTM_MASK_DISABLED.
 * - Currently, ::CNNL_LSTM_MASK_ENABLED must be used with projection layer.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNMaskMode(cnnlRNNDescriptor_t desc,
                   cnnlRNNMaskMode_t *mask_mode);

// Group:RNN
/*!
 * @brief Sets clip operation for the RNN operation that is previously
 *        created with the ::cnnlCreateRNNDescriptor function. When you want to constrain the cell
 *        state value range within a certain range, you can use this function to set related parameters.
 *
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] clip_mode
 *   Input. Specify clip operation for the RNN operation.
 *   For detailed information, see ::cnnlRNNClipMode_t.
 * @param[in] clip_nan_opt
 *   Input. Specify the output data as NaN, the upper bounds, or lower bounds, when the input data is
 *   not in the [left_clip, right_clip].
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[in] left_clip
 *   Input. Specify clip operation lower boundary.
 * @param[in] right_clip
 *   Input. Specify clip operation upper boundary.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The clip operation is not supported currently. So this function is not supported.
 *
 * @par Reference
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNClip(cnnlRNNDescriptor_t rnn_desc,
               const cnnlRNNClipMode_t clip_mode,
               const cnnlNanPropagation_t clip_nan_opt,
               const double left_clip,
               const double right_clip);

// Group:RNN
/*!
 * @brief Gets clip operation from the RNN descriptor \b rnn_desc
 * that is previously set by ::cnnlSetRNNClip function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] clip_mode
 *   Output. Pointer to host memory where clip mode should be saved.
 * @param[out] clip_nan_opt
 *   Output. Pointer to host memory where clip Nan propagation mode should be saved.
 *   For detailed information, see ::cnnlNanPropagation_t.
 * @param[out] left_clip
 *   Output. Pointer to host memory where clip lower boundary should be saved.
 * @param[out] right_clip
 *   Output. Pointer to host memory where clip upper boundary should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The clip operation is not supported currently. So this function is not supported.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNClip(cnnlRNNDescriptor_t rnn_desc,
               cnnlRNNClipMode_t *clip_mode,
               cnnlNanPropagation_t *clip_nan_opt,
               double *left_clip,
               double *right_clip);

// Group:RNN
/*!
 * @brief Sets padding mode of input and output for the RNN operation that is previously
 *  created with the ::cnnlCreateRNNDescriptor function.
 *
 * @param[out] rnn_desc
 *   Output. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] padding_mode
 *   Input. Specify padding mode for input and output.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - The padding mode is only supported for ::cnnlRNNForwardTraining.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRNNPaddingMode(cnnlRNNDescriptor_t rnn_desc,
                      const cnnlRNNPaddingMode_t padding_mode);

// Group:RNN
/*!
 * @brief Retrieves padding mode from the RNN descriptor \b rnn_desc
 * that is previously set by ::cnnlSetRNNPaddingMode function.
 *
 * @param[in] rnn_desc
 *   Input. The previously initialized descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] padding_mode
 *   Output. Pointer to host memory where padding mode should be saved.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * The padding mode is only supported for ::cnnlRNNForwardTraining.
 *
 * @par Example
 * - See the Cambricon CNNL sample code in ``samples/rnn_sample/rnn_sample.cc``
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNPaddingMode(cnnlRNNDescriptor_t rnn_desc,
                      cnnlRNNPaddingMode_t *padding_mode);

// Group:BceLoss
/*!
 * @brief Computes binary cross entropy with \b input tensor, \b target tensor,
 * \b filter tensor, and returns the results in the \b output tensor.
 *
 * If \b filter tensor need to be broadcasted, this function needs
 * extra host memory as the workspace to broadcast \b filter tensor.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetBceLossWorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceLoss. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] desc_target
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target value of input.
 * @param[in] desc_filter
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. The \b filter tensor is
 *   a filter of the binary cross entropy.
 * @param[in] reduction
 *   Input. An enum value that describing the reduction dimension, for detailed information,
 *   see ::cnnlBceLossReduction_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceLoss. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceLoss. You can get the size of the workspace with
 *   the ::cnnlGetBceLossWorkspaceSize function.
 * @param[out] desc_output
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \b output
 *   tensor is the result of ::cnnlBceLoss.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "BceLoss Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b target tensor, \b filter tensor and \b output tensor.
 *   <b>Note that the combinations of these tensor must be half-half or float-float.</b>
 *   - input: half, float.
 *   - target: half, float.
 *   - filter: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - The precision cannot be ensured for large amount of data if the data types of input, target and filter are half.
 *   Because the maximum value of half is 65504.
 * - The range of \b input, \b target, \b filter should be small enough to prevent
 *   the result from data overflow.
 * - The value of \b input should be between [0, 1].
 * - For better accuracy, the numerical ranges for \b target and \b filter are recommended within [0, 1] and [-100, 100], respectively.
 *
 * @par API Dependency
 * - You need to get the extra space size by ::cnnlGetBceLossWorkspaceSize.
 *
 * @note
 * - The total number of dimensions of \b input tensor, \b target tensor, should be the same.
 * - If \b reduction is CNNL_BCE_LOSS_MEAN or CNNL_BCE_LOSS_SUM,
 *   then the \b output tensor should be a number. Otherwise, the dimension of \b output tensor and
 *   \b input tensor should be the same.
 * - For each dimension, the dimension length of the \b filter tensor need to meet the requirements of broadcasting.
 * - \b filter tensor are optional. When you do not need them, set them NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceLoss function is as follows:
     @verbatim
       input:  shape=(4) -> [0.5, 0.5, 0.5, 0.5],

       target:  shape=(4) -> [1.0, 1.0, 1.0, 1.0],

       filter: NULL,

       reduction: CNNL_BCE_LOSS_MEAN.

       Then we will get the output.

       output  -->: [0.6931].

   @endverbatim
 *
 *  @par Reference
 *  - https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bceloss#torch.nn.BCELoss.
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceLoss(cnnlHandle_t handle,
            const cnnlTensorDescriptor_t desc_input,
            const void *input,
            const cnnlTensorDescriptor_t desc_target,
            const void *target,
            const cnnlTensorDescriptor_t desc_filter,
            const void *filter,
            const cnnlBceLossReduction_t reduction,
            void *workspace,
            size_t workspace_size,
            const cnnlTensorDescriptor_t desc_output,
            void *output);

// Group:BceLoss
/*!
 * @brief Returns in \b size the size of the host memory that is used as an extra workspace
 * in the ::cnnlBceLoss operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlBceLoss,
 * including the \b target tensor descriptor \b target_desc, \b filter tensor descriptor \b filter_desc.
 * For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceLossWorkspaceSize. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_input
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] desc_filter
 *   Input. The descriptor of \b filter tensor. The \b filter tensor is a filter
 *   of the binary cross entropy.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. A host pointer to the returned size of the extra MLU workspace in bytes that is used in
 *   the ::cnnlBceLoss operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetBceLossWorkspaceSize(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t desc_input,
                            const cnnlTensorDescriptor_t desc_filter,
                            size_t *size);
// Group:BceLoss
/*!
 * @brief Returns in \b size the size of the MLU memory that is used as an extra workspace
 * in the ::cnnlBceLossBackward operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlBceLossBackward,
 * including the \b target tensor descriptor \b target_desc, \b filter tensor descriptor \b filter_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the ::cnnlGetBceLossBackwardWorkspaceSize. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] target_desc
 *   Input. The descriptor of \b target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter_desc
 *   Input. The descriptor of \b filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in
 *   the ::cnnlBceLossBackward operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlBceLossBackward function
 *   to broadcast the \b filter tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetBceLossBackwardWorkspaceSize(
                                                    cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t target_desc,
                                                    const cnnlTensorDescriptor_t filter_desc,
                                                    size_t *size);
// Group:BceLoss
/*!
 *  @brief Computes the gradients of ::cnnlBceLoss with \b grad tensor, \b input tensor,
 *  \b target tensor, \b filter tensor, and returns the results in the \b output tensor.
 *
 *  If \b filter tensor needs to be broadcasted, this function needs extra MLU memory
 *  as the workspace to broadcast \b filter tensor. You can get the size of the workspace \b workspace_size
 *  with the ::cnnlGetBceLossBackwardWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlBceLossBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] grad_desc
 *   Input. The descriptor of grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *    Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor. The \b target tensor
 *   has the same shape as \b input tensor.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] weight
 *   Input. Pointer to the MLU memory that stores the weight tensor. The \b weight tensor is
 *   a weight of the result of binary cross entropy, and must match \b target tensor shape.
 * @param[in] reduction
 *   Input. An enum value describing the reduction dimension.
 *   See the ::cnnlBceLossReduction_t enum definition.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlBceLossBackward. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlBceLossBackward. You can get the size of the workspace with
 *   the ::cnnlGetBceLossBackwardWorkspaceSize function.
 * @param[out] output_desc
 *   Output. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The \b output
 *   tensor is the gradients of ::cnnlBceLoss.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b grad tensor, \b input tensor, \b target tensor, \b filter tensor
 *   and \b output tensor.
 *   <b>Note that the data type of these tensors must be the same.</b>
 *   - grad tensor: half, float.
 *   - input tensor: half, float.
 *   - target tensor: half, float.
 *   - filter tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Scale Limitation
 * - If \b reduction is CNNL_BCE_WITH_LOGITS_MEAN and the data type of input is half,
 *   the number of input elements should be smaller than 65504.
 * - If the data type of the \b grad tensor, the \b input tensor, the \b target tensor
 *   and the \b filter tensor parameters is in half, the value of these parameters should be
 *   small enough to prevent the data overflow.
 *
 * @par API Dependency
 * - You need to get the extra space size by ::cnnlGetBceLossBackwardWorkspaceSize.
 *
 * @note
 * - The number of dimensions of \b input tensor, \b target tensor, \b output tensor should be the same.
 * - If reduction is CNNL_BCE_WITH_LOGITS_MEAN or CNNL_BCE_WITH_LOGITS_SUM,
 *   the \b grad tensor should have one element. Otherwise, the number of dimensions of \b grad tensor and
 *   \b input tensor should be the same.
 * - The dimension values of the \b filter tensor and the \b target tensor need to meet the requirements of broadcasting.
 * - The \b filter tensor tensor is optional. When you do not need them, please set them NULL.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the ::cnnlBceLossBackward function is as follows:
     @verbatim
       grad:  [2.],

       input:  [0.1,0.2,0.3,0.4.]

       target: [1.,1.,1.,1.]

       filter: NULL,

       reduction: CNNL_BCE_WITH_LOGITS_MEAN.

       Then we will get the output.

       output  -->: [-4.999.,-2.500.,-1.666,-1.250].
     @endverbatim
 *
 * @par Reference
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlBceLossBackward(cnnlHandle_t handle,
                    const cnnlTensorDescriptor_t grad_desc,
                    const void *grad,
                    const cnnlTensorDescriptor_t input_desc,
                    const void *input,
                    const cnnlTensorDescriptor_t target_desc,
                    const void *target,
                    const cnnlTensorDescriptor_t filter_desc,
                    const void *filter,
                    const cnnlBceLossReduction_t reduction,
                    void *workspace,
                    const size_t workspace_size,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output);

// Group:KthValue
/*!
 * @brief Computes the k-th smallest element of each row of the input tensor in the given dimension \b dim.
 *
 * If there are more than one valid indices for the k-th smallest values, this function returns any one of the
 * indices that the corresponding value is k-th samallest value.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the k-th smallest
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] output_values_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_values
 *   Output. Pointer to the MLU memory that stores tensor for the k-th smallest elements in the given dim.
 * @param[in] output_indices_desc
 *   Input. The descriptor of the index tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output_indices
 *   Output. Pointer to the MLU memory that stores the index tensor for the k-th smallest elements.
 * @param[in] k
 *   Input. An int value which determines the k-th smallest element to get.
 * @param[in] dim
 *   Input. An int value which determines the dimension of input tensor to get k-th smallest element.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH,
 *   ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "k-th_value Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, output tensor
 *   \b output_values, and index tensor \b output_indices.
 *   <b>Data type of input tensor and output tensor should be the same.</b>
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *   - index tensor: int32.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and index tensor must meet the following requirement:
 *   - The shape of \b output_values and \b output_indices should be the same.
 *   - The total size of \b input in dimension \b dim should be less than 167936 bytes.
 *   - There is no dimension limit for the input tensor, output tensor and index tensor.
 *   - The output shape should be [1] if the \b input is a one-dimensional tensor.
 *
 * @par Example
 * The example of kth_value is as follows:
   @verbatim
    input one array by 1*5 -->
        input: [1.0, 2.0, 3.0, 4.0, 5.0]

    param:

    k: [4], dim: [0],

    output array by 1*1 and 1*1:
        --> values: [4.0]

        --> indices: [3]
   @endverbatim
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.kthvalue.html
 */
cnnlStatus_t CNNL_WIN_API
cnnlKthValue(cnnlHandle_t handle,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const uint32_t k,
             const int dim,
             const cnnlTensorDescriptor_t output_values_desc,
             void *output_values,
             const cnnlTensorDescriptor_t output_indices_desc,
             void *output_indices);

// Group:AdvancedIndex
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 *        workspace to optimize the advanced index operation.
 *
 * The size of extra workspace is based on the given information of the advanced
 * index operation, including the input descriptor \b input_desc and indices descriptor
 * array \b indices_desc.
 * For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc[]
 *   Input. An array of descriptor for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is
 *   used in ::cnnlAdvancedIndex.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The array size of \b indices_desc should be equal to 8, in which unused elements
 *   should be empty.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlAdvancedIndex function
 *   to perform the advanced index operation.
 *
 * @note
 * - If the data types and scale of input tensor and indices tensor described with \b input_desc and
 *   \b indices_desc do not match the requirements, ::CNNL_STATUS_BAD_PARAM would be returned when
 *   you call this function. For detailed information on data type and scale limitation,
 *   see ::cnnlAdvancedIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAdvancedIndexWorkspaceSize(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t indices_desc[],
    size_t *workspace_size);

// Group:AdvancedIndex
/*!
 * @brief Returns the dimension number \b output_desc_dim and shape \b output_desc_dims of the
 * output descriptor of the advanced index operation with the given input descriptor \b input_desc
 * and an array of indices tensors \b indices_desc.
 *
 * The shape of the output tensor can be set in the output tensor descriptor based on
 * the return value of this function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices_desc[]
 *   Input. An array of descriptor for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output_desc_dim
 *    Output. An integer number that stores the number of dimensions of the output tensor of the
 *    advanced index. The number is within the range of [0, 8].
 * @param[out] output_desc_dims[]
 *    Output. An array that stores the shape of the output tensor of the advanced index
 *    operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The array size of \b indices_desc should be equal to 8, in which unused elements
 *   should be empty.
 * - The array size of \b output_desc_dims should be equal to 8.
 *
 * @par API Dependency
 * - The output descriptor that is created according to \b output_desc_dim and \b output_desc_dims
 *   should be passed to the ::cnnlAdvancedIndex function to perform the advanced index operation.
 *
 * @note
 * - If the data types and scale of input tensor and indices tensor described with \b input_desc and
 *   \b indices_desc do not match the requirements, ::CNNL_STATUS_BAD_PARAM would be returned when
 *   you call this function. For detailed information on data type and scale limitation,
 *   see ::cnnlAdvancedIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAdvancedIndexOutputDim(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const cnnlTensorDescriptor_t indices_desc[],
    int *output_desc_dim,
    int output_desc_dims[]);

// Group:AdvancedIndex
/*!
 * @brief Selects data from \b input according to index information in \b indices and stores the
 * result into \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input which to be indexed.
 * @param[in] indices_desc[]
 *   Input. An array of descriptors for the all indices tensors. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] indices[]
 *   Input. An array of pointers to the MLU memory that stores all indices tensors. Based on indices
 *   tensors, output data is selected from the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   advanced index operation. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the advanced index operation. The workspace size can be get by calling
 *   the ::cnnlGetAdvancedIndexWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. The output tensor dimension information can be got
 *   by calling the ::cnnlGetAdvancedIndexOutputDim function. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] output_dim
 *   Output. Pointer to the MLU memory that stores the total number of output dimensions.
 * @param[out] output_dims
 *   Output. Pointer to the MLU memory that stores the tensor of output dimension shape.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \b input and indices tensors \b indices.
 *   - input tensor: uint8, int8, bool, int16, int32, half, float.
 *   - indices tensors: int32, bool, uint8.
 *   - output tensor: data type is set the same as input in each case.
 * - When \b indices array contains multiple valid tensors, the data types of all the tensors should
 *   be the same. The valid tensor means the tensor element is not a nullptr.
 *
 * @par Scale Limitation
 * - The array size of \b indices_desc and \b indices should be equal to 8, in which unused elements
 *   should be empty.
 * - The dimension sizes of \b input tensor and \b indices tensors cannot be larger than 8.
 * - The \b indices tensors should be zero elements if the corresponding dimension size of the
 *   \b input tensor is zero.
 * - The output dimension number that is calculated by \b input_desc and \b indices_desc should be
 *   no larger than 8, and it should be the same as the output dimension (dim) from \b output_desc.
 * - The output shape that is calculated by \b input_desc and \b indices_desc should be the same as
 *   the output shape (dims) from \b output_desc.
 * - The \b input tensor and \b indices tensors support stride feature, but the \b output tensor
 *   does not support stride feature.
 *
 * @par API Dependency
 * - Before calling this function to implement advanced index operation, all the parameters should
 *   be prepared to be passed to this function. See each parameter description for details.
 *
 * @note
 * - When the data type of \b indices is int32, this function supports tensor broadcasting as long as
 *   all valid tensors in \b indices satisfy the broadcast conditions. The valid tensor means the
 *   tensor element is not a nullptr.
 * - When the data type of \b indices is int32, \b Indices data can be minus numbers. The data
 *   range of n-th \b indices tensor is [- input_dim_n, input_dim_n - 1], where input_dim_n is
 *   the size of the n-th \b input dimension.
 * - When the data type of \b indices is uint8, \b Indices data value can only be either 0 or 1.
 * - When the data type of \b indices is uint8 or bool, the number of valid tensors in \b indices can
 *   only be 1, and the dimension number of the valid \b indices tensor should be no larger than the
 *   dimension number of the \b input.
 * - When the data type of \b indices is uint8 or bool, the shape of valid tensors in \b indices
 *   should be the same as the corresponding shape of the \b input.
 * - The valid index descriptors in \b indices_desc should be set to the first input_dim
 *   elements of \b indices_desc, where input_dim is the dimension number of the \b input.
 * - If the element in \b indices_desc is nullptr, the corresponding element in \b indices should be
 *   nullptr. If the element in \b indices_desc is a valid pointer and the tensor shape is not zero,
 *   the corresponding element in \b indices should be a valid pointer. If the element in
 *   \b indices_desc is a valid pointer but the tensor shape is zero, the corresponding element in
 *   \b indices should be nullptr.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlAdvancedIndex(
    cnnlHandle_t handle,
    const cnnlTensorDescriptor_t input_desc,
    const void *input,
    const cnnlTensorDescriptor_t indices_desc[],
    const void *const indices[],
    void *workspace,
    size_t workspace_size,
    const cnnlTensorDescriptor_t output_desc,
    void *output,
    void *output_dim,
    void *output_dims);
// Group:RoiAlign
/*!
 * @brief Computes the gradients of images \b grads_image based on the gradients \b grads and
 * bounding boxes \b boxes to perform the backpropagation of ::cnnlRoiAlign operation. To use
 * maximum pooling mode or average pooling mode in this operation, call ::cnnlRoiAlignBackward_v2.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the box coordinates in the origin image to
 *   the coordinates in the output.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the boxes by 0.5 pixel.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiAlignBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \b grads, boxes tensor
 *   \b boxes, and output tensor \b grads_image. Data type of all tensors should be the same.
 *   - gradient tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b grads, \b boxes, \b grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The gradient tensor and output tensor must have four dimensions.
 * - Size of the fourth dimension of gradient tensor and output tensor must be the same.
 * - The bounding boxes tensor \b boxes must have two dimensions.
 * - Size of the first dimension of gradient tensor and bounding boxes tensor must be the same.
 * - The shape of \b boxes should be [boxes_num, 5].
 * - \b boxes[i] consists of [image_id, x1, y1, x2, y2]. \p image_id specifies which image this box
 *   is in, and should be in the range of [0, batch_num - 1]. \p x1 and \p y1 specify the start
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the end coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1.
 * - \b spatial_scale should be in the range of (0, 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> grads: [[[[1.0]]]]

     --> boxes: [[0.0, 0.0, 0.0, 1.0, 1.0]]

     param:
         spatial_scale: 1.0, sampling_ratio: 2, aligned: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[0.25]], [[0.25]]], [[[0.25]], [[0.25]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align
 */
cnnlStatus_t CNNL_WIN_API cnnlRoiAlignBackward(cnnlHandle_t handle,
                                               const float spatial_scale,
                                               const int sampling_ratio,
                                               const bool aligned,
                                               const cnnlTensorDescriptor_t grads_desc,
                                               const void *grads,
                                               const cnnlTensorDescriptor_t boxes_desc,
                                               const void *boxes,
                                               const cnnlTensorDescriptor_t grads_image_desc,
                                               void *grads_image);
// Group:RoiAlign
/*!
 * @brief Computes the gradients of images \b grads_image based on the gradients \b grads,
 * bounding boxes \b boxes, the coordinate of x axis \b argmax_x and the coordinate of y axis
 * \b argmax_y to perform this operation. Compared with ::cnnlRoiAlignBackward, in addition to
 * supporting average pooling mode, ::cnnlRoiAlignBackward_v2 also supports maximum pooling mode
 * defined in \b pool_mode with two more inputs \b argmax_x and \b argmax_y.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignBackward_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the bounding boxes tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the bounding boxes tensor.
 * @param[in] argmax_x_desc
 *   Input. The descriptor of the \b argmax_x tensor that stores the coordinate of x axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax_x
 *   Input. Pointer to the MLU memory that stores the \b argmax_x tensor. \b argmax_x represents
 *   \b output coordinate of x axis returned by ::cnnlRoiAlign_v2 when \b pool_mode is maximum
 *   pooling mode. When \b pool_mode is average pooling mode, \b argmax_x is NULL.
 * @param[in] argmax_y_desc
 *   Input. The descriptor of the \b argmax_y tensor that stores the coordinate of y axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax_y
 *   Input. Pointer to the MLU memory that stores the \b argmax_y tensor. \b argmax_y represents
 *   \b output coordinate of y axis returned by ::cnnlRoiAlign_v2 when \b pool_mode is maximum
 *   pooling mode. When \b pool_mode is average pooling mode, \b argmax_y is NULL.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the box coordinates in the original image to
 *   the coordinates in the output.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the boxes by 0.5 pixel. If the value
 *   of \b aligned is set to true, the boxes are shifted by 0.5. If the value of \b aligned is set
 *   to false, the boxes are not shifted.
 * @param[in] pool_mode
 *   Input. The pooling mode which determines to use maximum pooling mode or average
 *   pooling mode. If the value of \b pool_mode is set to 1, the average pooling mode is used. If
 *   the value of \b pool_mode is set to 0, the maximum pooling mode is used.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the \b grads_image tensor .
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiAlignBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \b grads, boxes tensor
 *   \b boxes, argmax_x tensor \b argmax_x, argmax_y tensor \b argmax_y and output tensor \b
 *   grads_image. Data type of all tensors should be the same.
 *   - gradient tensor: half, float.
 *   - boxes tensor: half, float.
 *   - argmax_x tensor: half, float.
 *   - argmax_y tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of gradient tensor \b grads, boxes tensor \b boxes, argmax_x tensor
 *   \b argmax_x, argmax_y tensor \b argmax_y and output tensor \b grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - argmax_x tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_y tensor: \p CNNL_LAYOUT_NHWC.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The gradient tensor \b grads, argmax_x tensor \b argmax_x , argmax_y tensor \b argmax_y and
 *   output tensor \b grads_images must have four dimensions.
 * - Size of the fourth dimension of gradient tensor \b grads, argmax_x tensor \b argmax_x,
 *   argmax_y tensor \b argmax_y and output tensor \b grads_images must be the same.
 * - The bounding boxes tensor \b boxes must have two dimensions.
 * - Size of the first dimension of gradient tensor \b grads, argmax_x tensor \b argmax_x, argmax_y
 *   tensor \b argmax_y and bounding boxes tensor \b boxes must be the same.
 * - Size of each dimension of gradient tensor \b grads, argmax_x tensor \b argmax_x and argmax_y
 *   tensor \b argmax_y must be the same.
 * - The shape of \b boxes should be [boxes_num, 5].
 * - \b boxes[i] consists of [image_id, x1, y1, x2, y2]. \p image_id specifies which image this box
 *   is in, and should be in the range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the ending coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1.
 *
 * @par API Dependency
 * - This function should be used with ::cnnlRoiAlign_v2.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - Set the values of \b argmax_x and \b argmax_y according to the result returned by
 *   ::cnnlRoiAlign_v2.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the RoiAlignBackward_v2 operation is as follows:
     @verbatim
     input four arrays by 1 * 1 * 1 * 1, 1 * 5, 1 * 1 * 1 * 1 and 1 * 1 *1 *1--> grads: [[[[1.0]]]]

     --> boxes: [[0.0, 0.0, 0.0, 1.0, 1.0]]

     --> argmax_x:[[[[0.5]]]]

     --> argmax_y:[[[[0.5]]]]

     param:
         spatial_scale: 1.0, sampling_ratio: 0, aligned: false

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1.0]]]]
     @endverbatim
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/pytorch/roi_align_cuda.cu
 */

cnnlStatus_t CNNL_WIN_API cnnlRoiAlignBackward_v2(cnnlHandle_t handle,
                                                  const cnnlTensorDescriptor_t grads_desc,
                                                  const void *grads,
                                                  const cnnlTensorDescriptor_t boxes_desc,
                                                  const void *boxes,
                                                  const cnnlTensorDescriptor_t argmax_x_desc,
                                                  const void *argmax_x,
                                                  const cnnlTensorDescriptor_t argmax_y_desc,
                                                  const void *argmax_y,
                                                  const float spatial_scale,
                                                  const int sampling_ratio,
                                                  const bool aligned,
                                                  const int pool_mode,
                                                  const cnnlTensorDescriptor_t grads_image_desc,
                                                  void *grads_image);


/*!
 * The descriptor of roialign_function that holds the information required in the
 * roialign operation. You need to call the ::cnnlCreateRoiAlignDescriptor to create
 * a descriptor, and call the ::cnnlSetRoiAlignDescriptor or ::cnnlSetRoiAlignDescriptor_v2
 * to set the basic information of the roialign operation to the descriptor. Also, you need to
 * destroy the descriptor at the end with the ::cnnlDestroyRoiAlignDescriptor function.
 */
typedef struct cnnlRoiAlignStruct *cnnlRoiAlignDescriptor_t;

// Group:RoiAlign
/*!
 * @brief Creates a descriptor pointed by \b desc for a roialign operation, and allocates
 *        memory for holding the information about the roialign operation. The information
 *        is defined in ::cnnlRoiAlignDescriptor_t. For more information about descriptor,
 *        see "Cambricon CNNL User Guide".
 *
 * @param[out] desc
 *  Input. A host pointer to the roialign descriptor that holds information about the roialign
 *  operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetRoialignDescriptor function or
 * ::cnnlSetRoialignDescriptor_v2 function to initialize and set the information to the roialign descriptor.
 * - You need to call the ::cnnlDestroyRoiAlignDescriptor function to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlCreateRoiAlignDescriptor(cnnlRoiAlignDescriptor_t *desc);

// Group:RoiAlign
/*!
 * @brief Initializes the roialign descriptor \b desc that is previously created with
 * the ::cnnlCreateRoiAlignDescriptor function, and sets the information about the
 * roialign to the roialign descriptor \b desc. The information includes the number
 * of the roialign feature map height \b pooled_height, the roialign feature map width
 * \b pooled_width, the sampling_ratio for each boxes \b sampling_ratio, the of the
 * spatial_scale for each boxes \b spatial_scale, the spatial_scale mode \b aligned.
 *
 * @deprecated
 * ::cnnlSetRoiAlignDescriptor is deprecated and will be removed in the future
 * release. It is recommended to use ::cnnlSetRoiAlignDescriptor_v2 instead, which
 * supports both maximum and average modes.
 *
 * @param[in] desc
 *   Input. The descriptor of the roialign operation. For detailed information,
 *   see ::cnnRoiAlignlDescriptor_t.
 * @param[in] pooled_height
 *   Input. The height of output feature map.
 * @param[in] pooled_width
 *   Input. The width of output feature map.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the boxes by 0.5 pixel.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetRoiAlignDescriptor(cnnlRoiAlignDescriptor_t desc,
                          const int pooled_height,
                          const int pooled_width,
                          const int sampling_ratio,
                          const float spatial_scale,
                          const bool aligned);
// Group:RoiAlign
/*!
 * @brief Initializes the roialign descriptor \b roialign_desc that is previously created with
 * the ::cnnlCreateRoiAlignDescriptor function, and sets the information about the roialign to the
 * roialign descriptor \b roialign_desc. Compared with ::cnnlSetRoiAlignDescriptor, this function
 * supports both maximum and average modes defined in \b pool_mode. The information includes the height
 * \b pooled_height, and width \b pooled_width of output feature map, the pooling mode \b pool_mode
 * for the roialign operation, the sampling ratio \b sampling_ratio, the spatial scale
 * \b spatial_scale and the flag of pixel shift \b aligned for each box.
 *
 * @param[in] roialign_desc
 *   Input. The descriptor of the roialign operation. For detailed information,
 *   see ::cnnRoiAlignlDescriptor_t.
 * @param[in] pooled_height
 *   Input. The height of output feature map. The value of this parameter should be greater than 0.
 * @param[in] pooled_width
 *   Input. The width of output feature map. The value of this parameter should be greater than 0.
 * @param[in] sampling_ratio
 *   Input. The number of sampling points in the grid used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] pool_mode
 *   Input. If the value of \b pool_mode is set to 1, the average pooling mode is used. If the value
 * of \b pool_mode is set to 0, the maximum pooling mode is used.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the boxes by 0.5 pixel. If the value of \b aligned
 * is set to true, the boxes is shifted by 0.5. If the value of \b aligned is set to false, the boxes is not shifted.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlSetRoiAlignDescriptor_v2(cnnlRoiAlignDescriptor_t roialign_desc,
                             const int pooled_height,
                             const int pooled_width,
                             const int sampling_ratio,
                             const float spatial_scale,
                             const int pool_mode,
                             const bool aligned);

// Group:RoiAlign
/*!
 * @brief Destroys a roialign descriptor \b desc that is previously created with the
 *        ::cnnlCreateRoiAlignDescriptor function.
 *
 * The roialign descriptor is defined in ::cnnlRoiAlignDescriptor_t
 * and holds the information about the roialign operation.
 *
 * @param[in] desc
 *   Input. The roialign descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlRoiAlign,
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the roialign descriptor.
 *   Otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlDestroyRoiAlignDescriptor(cnnlRoiAlignDescriptor_t desc);

// Group:RoiAlign
/*!
 * @brief Computes the output of images \b output_image based on the input \b input_tensor and
 * bounding boxes \b boxes to perform ::cnnlRoiAlign operation. To use maximum pooling mode
 * or average pooling mode in this operation, call ::cnnlRoiAlign_v2.
 *
 * @deprecated
 * ::cnnlRoiAlign is deprecated and will be removed in the future release. It is recommended
 * to use ::cnnlRoiAlign_v2 instead, which supports both maximum and average modes.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] roialign_desc
 *   Input. The descriptor of the roialign operation. For detailed information, see
 *   ::cnnlRoiAlignDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roialign process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] boxes_desc
 *   Input. Descriptor of boxes, containing dimension and the layout of boxes.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the boxes tensor.
 * @param[out] output_desc
 *   Input.  Descriptor of output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "RoiAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, boxes tensor
 *   \b boxes, and output tensor \b output tensor. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b input, \b boxes, \b output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must have four dimensions.
 * - data type of half is not recommended due to low precision.
 * - Size of the lowest dimension of input tensor and output tensor must be the same.
 * - The boxes tensor must have two dimensions.
 * - Size of the highest dimension of output tensor and boxes tensor must be the same.
 * - The shape of \b boxes should be [boxes_num, 5].
 * - \b boxes[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in the range of
 *   [0, batch_num - 1]. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be
 *   greater than \p x1. \p y2 should be greater than \p y1.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roialign_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> input: [[[[1.0]]]]

     --> boxes: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1]]]]
     @endverbatim
 *
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_align
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlign(cnnlHandle_t handle,
             const cnnlRoiAlignDescriptor_t roialign_desc,
             const cnnlTensorDescriptor_t input_desc,
             const void *input,
             const cnnlTensorDescriptor_t boxes_desc,
             const void *boxes,
             const cnnlTensorDescriptor_t output_desc,
             void *output);
// Group:RoiAlign
/*!
 * @brief Computes the output feature map \b output based on the input feature map \b input and
 * bounding boxes \b boxes to perform this operation. Compared with ::cnnlRoiAlign, in addition to
 * supporting average pooling mode, ::cnnlRoiAlign_v2 also supports maximum pooling mode with two
 * more output \b argmax_x and \b argmax_y.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlign_v2 operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] roialign_desc
 *   Input. The descriptor of the roialign operation. For detailed information, see
 *   ::cnnlRoiAlignDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roialign process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] boxes_desc
 *   Input. The descriptor of the region proposals tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the region proposals tensor.
 * @param[in] output_desc
 *   Input.  The descriptor of the \b output tensor of the original images.  For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor.
 * @param[in] argmax_x_desc
 *   Input.  The descriptor of the \b argmax_x tensor that stores the coordinate of x axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[out] argmax_x
 *   Output. Pointer to the MLU memory that stores the \b argmax_x tensor. \b argmax_x represents
 * \b output coordinate of x axis when \b pool_mode is maximum pooling mode. When \b pool_mode is average pooling mode, \b argmax_x is NULL.
 * @param[in] argmax_y_desc
 *   Input.  The descriptor of the \b argmax_y tensor that stores the coordinate of y axis. For detailed
 * information, see ::cnnlTensorDescriptor_t.
 * @param[out] argmax_y
 *   Output. Pointer to the MLU memory that stores the \b argmax_y tensor. \b argmax_y represents \b output
 * coordinate of y axis when \b pool_mode is maximum pooling mode. When \b pool_mode is average pooling mode, \b argmax_y is NULL.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "RoiAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, boxes tensor
 *   \b boxes, output tensor \b output tensor, argmax_x tensor \b argmax_x and argmax_y tensor
 *   \b argmax_y. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - output tensor: half, float.
 *   - argmax_x tensor: half, float.
 *   - argmax_y tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b input, \b boxes, \b output, \b argmax_x and \b argmax_y are as
 *   follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_x tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_y tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \b input tensor, \b output tensor, \b argmax_x tensor and \b argmax_y tensor must have four dimensions.
 * - \b input data type of half is not recommended due to low precision.
 * - Size of the lowest dimension of \b input tensor and \b output tensor must be the same.
 * - Size of the lowest dimension of \b input tensor and \b argmax_x tensor must be the same.
 * - Size of the lowest dimension of \b input tensor and \b argmax_y tensor must be the same.
 * - The \b boxes tensor must have two dimensions.
 * - Size of the highest dimension of \b output tensor and \b boxes tensor must be the same.
 * - Size of the highest dimension of \b argmax_x tensor and \b boxes tensor must be the same.
 * - Size of the highest dimension of \b argmax_y tensor and \b boxes tensor must be the same.
 * - The shape of \b boxes should be [boxes_num, 5].
 * - \b boxes[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id specifies which image this box
 *   is in, and should be in the range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this box in origin image. \p x2 and \p y2 specify the ending coordinate of this box
 *   in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be greater
 *   than \p x1. \p y2 should be greater than \p y1.
 *
 * @par API Dependency
 * - This function should be used with ::cnnlSetRoiAlignDescriptor_v2.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - When \b input contains NaN:
 *   If \b pool_mode is maximum pooling_mode, \b output is positive saturation value on MLU200 series
 *   and \b output gets more NaN than ieee754 on MLU300 series and CE3226.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roialign_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 5 --> input: [[[[1.0]]]]

     --> boxes: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     parameters:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 0, aligned: false, pool_mode = 0

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[1]]
     argmax_x array by 1 * 1 * 1 * 1 -->
         argmax_x: [[[[0.5]]
     argmax_y array by 1 * 1 * 1 * 1 -->
         argmag_y: [[[[0.5]]

     @endverbatim
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/csrc/pytorch/roi_align_cuda.cu
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlign_v2(cnnlHandle_t handle,
                const cnnlRoiAlignDescriptor_t roialign_desc,
                const cnnlTensorDescriptor_t input_desc,
                const void *input,
                const cnnlTensorDescriptor_t boxes_desc,
                const void *boxes,
                const cnnlTensorDescriptor_t output_desc,
                void *output,
                const cnnlTensorDescriptor_t argmax_x_desc,
                void *argmax_x,
                const cnnlTensorDescriptor_t argmax_y_desc,
                void *argmax_y);

// Group:RoiPooling
/*!
 * @brief Generates fixed size feature map and input feature index
 * of argmax for each Roi(Regions of Interest) to perform ::cnnlRoiPoolingForward operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiPoolingForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] pooling_mode
 *   Input. The pooling mode of roipoolingforward defined in ::cnnlPoolingMode_t
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor in the roipoolingforward process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. The descriptor of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor. Rois means regions of interest.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the input feature map.
 * @param[in] output_desc
 *   Input. Descriptor of output tensor and argmax tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[out] argmax
 *   Output. Pointer to the MLU memory that stores the argmax tensor. Pointer may be NULL. Argmax tensor
 *   means input feature index of maximum for each Roi(Regions of Interest).
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "RoiPoolingForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, rois tensor \b rois,
 *   output tensor \b output, and argmax tensor \b argmax. The data type of the \b input \b rois \b output
 *   should be the same. The data type of the argmax is different from others.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - output tensor: half, float.
 *   - argmax tensor: int32.
 *
 * @par Data Layout
 * - The supported data layout of \b input, \b rois, \b output, \b argmax are as follows:
 *   - input tensor: \b CNNL_LAYOUT_NHWC.
 *   - rois tensor: \b CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - output tensor: \b CNNL_LAYOUT_NHWC.
 *   - argmax tensor: \b CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The value of \b pooling_mode only supports \p CNNL_POOLING_MAX.
 * - The input tensor, output tensor and argmax tensor must have four dimensions.
 * - Data type of half of \b input and \b rois is not recommended due to low precision.
 * - Size of the lowest dimension of input tensors, output tensor and argmax tensors must be the same.
 * - The total number of dimension of rois tensor must be 2.
 * - Size of the highest dimension of output tensor and rois tensor must be the same.
 * - The shape of \b rois should be [rois_num, 5].\b rois_num means the total number of \b rois.
 * - \b Rois[i] consists of [batch_id, x1, y1, x2, y2]. \b batch_id should be in the range of
 * [0, batch_num - 1]. \b x1 and \b y1 should be greater than or equal to 0. \b x2 should be
 * greater than \b x1. \b y2 should be greater than \b y1. \b batch_id means id of batch of \b rois. \b x1, \b y1,
 * \b x2 and \b y2 mean the coordinate values of rois in the input feature map. \b batch_num means the total number
 * of the batch.
 * - \b Spatial_scale should be in the range of (0, 1].
 * - \b Output consists of [rois_num, pooled_h, pooled_w, channels]. In the dimensions of the h and w of the input
 * and the output, (\b x2 - \b x1) * ( \b y2 - \b y1) * \b spatial_scale * \b spatial_scale / (\b pooled_h * \b
 * pooled_w) < (nram_limitation / 32). Nram_limitation means the limitation of the nram. When the supported MLU platform is 200,
 * the nram_limitation is (98304 - 4 * \b channels) / 2 . When the supported MLU platform is 300, the nram_limitation is
 * (163840 - 4 * \b channels) / 2. \b pooled_h means height of output.
 * \b pooled_w means width of output.
 *
 * @par API Dependency
 * - None

 * @par Performance Optimization
 * - None.
 *
 * @note
 * - It is not recommended to set the data type of grads tensor, rois tensor and grads_image tensors
 *   that may cause the low accuracy on MLU200 series.
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series:
 *   	- The \b output is the positive saturation value. The \b argmax is the index of the last NaN in the kernel of the pooling.
 *   - On MLU300 series and CE3226:
 *   	- If the last value in the kernel of the pooling is NaN, the \b argmax is the index of the last value, the \b output is the last value,
 *   	as shown in example 2 below. Otherwise, the \b argmax is the index of the maximum value after the last NaN, the \b output is the maximum value
 *   	after the last NaN, as shown in example 3 below.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example 1 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 4 * 4 * 1 and 1 * 5 -->input: [[[[1.0],[2.0],[3.0],[4.0]],
                                                             [[5.0],[6.0],[7.0],[8.0]],
                                                             [[9.0],[10.0],[11.0],[12.0]],
                                                             [[13.0],[14.0],[15.0],[16.0]]]]

     --> rois: [[1.0, 0.0, 0.0, 3.0, 3.0]]

     param:
     	 pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 2 * 2 * 1 --> output: [[[[6.0], [8.0]],
                                                 [[14.0], [16.0]]]]

     argmax array by 1 * 2 * 2 * 1 --> argmax: [[[[5], [7]],
                                                 [[13], [15]]]]
     @endverbatim

   - The example 2 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 -->input: [[[[1.0],[2.0]],
                                                             [[3.0],[NaN]]]]

     --> rois: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
     	 pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 1 * 1 * 1 --> output: [[[[NaN]]]]

     argmax array by 1 * 1 * 1 * 1 --> argmax: [[[[3]]]]
     @endverbatim

   - The example 3 of the roipoolingforward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 -->input: [[[[1.0],[NaN]],
                                                             [[3.0],[4.0]]]]

     --> rois: [[1.0, 0.0, 0.0, 1.0, 1.0]]

     param:
         pooling_mode: 0, spatial_scale: 1.0

     output array by 1 * 1 * 1 * 1 --> output: [[[[4.0]]]]

     argmax array by 1 * 1 * 1 * 1 --> argmax: [[[[3]]]]
     @endverbatim
 *
 * @par Reference
 * - https:github.com/pytorch/pytorch/caffe2/operators/roi_pool_op.cu
 */
cnnlStatus_t CNNL_WIN_API cnnlRoiPoolingForward(cnnlHandle_t handle,
                                                cnnlPoolingMode_t pooling_mode,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t rois_desc,
                                                const void *rois,
                                                float spatial_scale,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                int *argmax);

// Group:RoiPooling
/*!
 * @brief Computes the gradients of images \b grads_image based on the gradients \b grads and
 * region proposals \b rois to perform the backpropagation of ::cnnlRoiPooling operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiPoolingBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grads_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process. For detailed
 *   information, see ::cnnlTensorDescriptor_t.
 * @param[in] grads
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] rois_desc
 *   Input. The descriptor of the region proposals tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the region proposals tensor.
 * @param[in] argmax_desc
 *   Input. The descriptor of the argmax tensor that stores the index returned by
 *   ::cnnlRoiPoolingForward. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] argmax
 *   Input. Pointer to the MLU memory that stores the argmax tensor.
 * @param[in] spatial_scale
 *   Input. A scaling factor that specifies how to map the ROIs coordinates in the origin image to
 *   the coordinates in the output.
 * @param[in] mode
 *   Input. The pooling mode, which is defined in the ::cnnlPoolingMode_t enum.
 * @param[in] grads_image_desc
 *   Input. The descriptor of the gradients tensor of the original images.
 * @param[out] grads_image
 *   Output. Pointer to the MLU memory that stores the gradients tensor of the original images.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Formula
 * - See "RoiPoolingBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for gradient tensor \b grads, region proposals
 *   tensor \b rois, argmax_tensor \b argmax and output tensor \b grads_image. Data type of
 *   gradient tensor \b grads, region proposals tensor \b rois and output tensor \b grads_image
 *   should be the same.
 *   - grads tensor: half, float.
 *   - rois tensor: half, float.
 *   - argmax tensor: int32_t.
 *   - grads_image tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b grads, \b rois, \b grads_images are as follows:
 *   - grads tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY, only supports 2-D tensor.
 *   - argmax tensor: \p CNNL_LAYOUT_NHWC.
 *   - grads_image tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The value of \b mode only supports \p CNNL_POOLING_MAX.
 * - The grads tensor, argmax tensor and grads_image tensor must have four dimensions.
 * - Size of the fourth dimension of grads tensor, argmax tensor and grads_image tensor must be the
 *   same.
 * - Size of each dimension of grads tensor and argmax tensor must be the same.
 * - The region proposals tensor \b rois must have two dimensions.
 * - Size of the first dimension of grads tensor, rois tensor and argmax tensor must be the same.
 * - The shape of \b rois should be [rois_num, 5].
 * - \b rois[i] consists of [rois_num, x1, y1, x2, y2]. \p rois_num specifies which image this ROI
 *   is in, and should be in the range of [0, batch_num - 1]. \p x1 and \p y1 specify the starting
 *   coordinate of this ROI in origin image. \p x2 and \p y2 specify the ending coordinate of this
 *   ROI in origin image. \p x1 and \p y1 should be greater than or equal to 0. \p x2 should be
 *   greater than \p x1. \p y2 should be greater than \p y1.
 * - \b spatial_scale should be in the range of (0, 1].
 * - The value of argmax tensors with the data type is int32_t, should be in the range of
 *   [0, 2^31 - 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - In general, set the values of \b argmax according to the result returned by
 *   ::cnnlRoiPoolingForward. Otherwise, these values may be regarded as invalid and will not be
 *   used in this operation.
 * - It is not recommended to set the data type of grads tensor, rois tensor and grads_image tensors
 *   as half since it may get a low accuracy result on MLU200 series.
 * - When \b rois contains NaN/infinity, it may cause undefined behavior.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_pooling_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 2 * 2 * 1 and 1 * 5 --> grads: [[[[1.0], [2.0]], [[3.0], [4.0]]]]

     --> rois: [[0.0, 0.0, 0.0, 2.0, 2.0]]
     --> argmax: [[[[0], [2]], [[8], [10]]]]

     param:
         spatial_scale: 1.0

     grads_image array by 1 * 4 * 4 * 1 -->
         grads_image: [[[[1.0], [0.0], [2.0], [0.0]], [[0.0], [0.0], [0.0], [0.0]],
                        [[3.0], [0.0], [4.0], [0.0]], [[0.0]. [0.0], [0.0], [0.0]]]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/vision/stable/ops.html#torchvision.ops.roi_pool
 */
cnnlStatus_t CNNL_WIN_API cnnlRoiPoolingBackward(cnnlHandle_t handle,
                                                 cnnlPoolingMode_t mode,
                                                 const cnnlTensorDescriptor_t grads_desc,
                                                 const void *grads,
                                                 const cnnlTensorDescriptor_t rois_desc,
                                                 const void *rois,
                                                 const cnnlTensorDescriptor_t argmax_desc,
                                                 const int *argmax,
                                                 const float spatial_scale,
                                                 const cnnlTensorDescriptor_t grads_image_desc,
                                                 void *grads_image);
// Group:IsNan
/*!
 *  @brief Returns the boolean elements in \b output that represent if each element
 *  of \b input tensor is NaN or not.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *    queues in the index select operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] input_desc
 *    Input. The descriptors of the \b input tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] output_desc
 *    Input. The descriptors of the \b output tensors. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *    Returns true if the element of \b input tensor is NaN. Otherwise, returns false.
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.

 *  @par Data Type
 *  - \b input: float, half.
 *  - \b output: bool.
 *
 *  @par Scale Limitation
 *  - The dimension length of output tensor and input tensor must be same in the same dimension.
 *
 *  @par Reference
 *  https://www.tensorflow.org/api_docs/python/tf/math/is_nan
 *
 *  @par API Dependency
 *  - None.
 *
 *  @note
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
      @verbatim
        input:  [1.0, 6.2, NaN, inf, -inf]
        output: [false, false, true, false, false]
        where inf represents infinity
      @endverbatim
 */

cnnlStatus_t CNNL_WIN_API cnnlIsNan(cnnlHandle_t handle,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

/******************************************************************************
 * Cambricon CNNL Data Structure: MSELossReduction
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the dimension reduction methods that are used in the
 * implementation of the MSELoss functions.
 *
 */
typedef enum {
  CNNL_MSE_LOSS_NONE = 0,
  /*!< Compute mean squared error loss for ::cnnlMSELoss. */
  CNNL_MSE_LOSS_SUM = 1,
  /*!< Get sum of result for ::cnnlMSELoss. */
  CNNL_MSE_LOSS_MEAN = 2,
  /*!< Average the sum of the result for ::cnnlMSELoss. */
} cnnlMSELossReduction_t;

// Group:MSELoss
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the mse_loss operation.
 *
 * The size of extra workspace is based on the given information of the mse_loss operation,
 * including the input tensor descriptor \b input_desc.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the mse_loss
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the mse_loss
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - This API is only used along with ::cnnlMSELoss_v2. ::cnnlMSELoss does not require this API.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetMSELossWorkspaceSize(cnnlHandle_t handle,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      size_t *workspace_size);

// Group:MSELoss
/*!
 * @brief Computes a MSE (Mean Squared Error) Loss, which is used to train a
 * network on classification tasks, based on input tensor \b input with \b target, and
 * returns the results in the output tensor \b output.
 *
 * Compared with ::MSELoss, this function allows you to allocate some extra workspace as an input
 * parameter. If you just set \b workspace to NULL and \b workspace_size to 0, this function will
 * perform as same as ::MSELoss.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlMSELoss_v2.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the ::cnnlMSELoss_v2. You can get the size of the workspace with
 *   the ::cnnlGetMSELossWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \b input: float, half.
 *   - \b target: float, half.
 *   - \b output: float, half.
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - Before using this API, you need to get the size of the workspace with
 *   the ::cnnlGetMSELossWorkspaceSize function and pass the required extra workspace to
 *   the ::cnnlMSELoss_v2 function.
 *
 * @note
 * - The dimension of input tensor and target tensor should be same.
 * - The dimension of the input tensor and output tensor should be same, when the \b
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The number of elements in output tensor should be one, when the \b reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELoss" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input two arrays by 2 * 3, and 2 * 3
       --> input: [[1,2,3], [4,5,6]]

       --> target: [[2,3,4], [5,6,7]]

       param:
         reduction: CNNL_MSE_LOSS_NONE

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html#torch.nn.MSELoss
 */
cnnlStatus_t CNNL_WIN_API cnnlMSELoss_v2(cnnlHandle_t handle,
                                         cnnlMSELossReduction_t reduction,
                                         const cnnlTensorDescriptor_t input_desc,
                                         const void *input,
                                         const cnnlTensorDescriptor_t target_desc,
                                         const void *target,
                                         void *workspace,
                                         size_t workspace_size,
                                         const cnnlTensorDescriptor_t output_desc,
                                         void *output);

// Group:MSELoss
/*!
 * @brief Computes a MSE (Mean Squared Error) Loss, which is used to train a
 * network on classification tasks, based on input tensor \b input with \b target, and
 * returns the results in the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss forward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \b input: float, half.
 *   - \b target: float, half.
 *   - \b output: float, half.
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The dimension of input tensor and target tensor should be same.
 * - The dimension of the inputs tensor and output tensor should be same, when the \b
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The number of elements in output tensor should be one, when the \b reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELoss" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Example
 * - The example of the nllloss forward operation is as follows:
     @verbatim
       input two arrays by 2 * 3, and 2 * 3
       --> input: [[1,2,3], [4,5,6]]

       --> target: [[2,3,4], [5,6,7]]

       param:
         reduction: CNNL_MSE_LOSS_NONE

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html#torch.nn.MSELoss
 */
cnnlStatus_t CNNL_WIN_API cnnlMSELoss(cnnlHandle_t handle,
                                      cnnlMSELossReduction_t reduction,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const void *input,
                                      const cnnlTensorDescriptor_t target_desc,
                                      const void *target,
                                      const cnnlTensorDescriptor_t output_desc,
                                      void *output);

// Group:FloorDivTrunc
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the floor_div_trunc operation.
 *
 * The size of extra workspace is based on the given information of the floor_div_trunc operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and the output
 * tensor descriptor \b output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   floor_div_trunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input which is dividend tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input which is divisor tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   floor_div_trunc operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \b input1, \b input2, and \b output respectively:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetFloorDivTruncWorkspaceSize(cnnlHandle_t handle,
                                  const cnnlTensorDescriptor_t input1_desc,
                                  const cnnlTensorDescriptor_t input2_desc,
                                  const cnnlTensorDescriptor_t output_desc,
                                  size_t *workspace_size);

// Group:FloorDivTrunc
/*!
 * @brief Divides \b input1 by \b input2, rounding to the zero direction, and returns
 *        the results in the output tensor \b output.
 *
 * This function may need extra MLU memory as the workspace to improve the floor_div_trunc
 * performance.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   floor_div_trunc operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The algorithm used to compute the output. For detailed information,
 *   see ::cnnlComputationPreference_t.
 *   The default value of this parameter is \p CNNL_COMPUTATION_HIGH_PRECISION.
 * @param[in] input1_desc
 *   Input. The descriptor of the first input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the dividend tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the second input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the divisor tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the floor_div_trunc
 *   operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the floor_div_trunc
 *   operation.
 *   You can get the size of the workspace with the ::cnnlGetFloorDivTruncWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "FloorDivTrunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensors and output tensor must be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float, int32.
 *   - output tensor: half, float, int32.
 *
 * @par Data Layout
 * - The supported data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - According to the rule of tensor broadcast, the parameters should satisfy the following
 *   conditions. For example, c1_dim, c2_dim, and c3_dim represent the lowest dimension of
 *   \b input1, \b input2, and \b output respectively:
 *
 *   min(c1_dim, c2_dim) == 1 or c1_dim == c2_dim
 *
 *   max(c1_dim, c2_dim) == c3_dim
 *
 * @par API Dependency
 * - You need to call the ::cnnlGetFloorDivTruncWorkspaceSize function to allocate extra workspace
 *   which should be passed to the ::cnnlFloorDivTrunc before calling this function.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional arrays, supporting up to
 *   CNNL_DIM_MAX dimensions.
 * - This API returns -1 if the input \b input2 is fixed-point zero.
 * - In the situation when this function is in the COMPUTATION_FAST mode, the accuracy problem may occur.
 *
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlFloorDivTrunc(cnnlHandle_t handle,
                                            cnnlComputationPreference_t prefer,
                                            const cnnlTensorDescriptor_t input1_desc,
                                            const void *input1,
                                            const cnnlTensorDescriptor_t input2_desc,
                                            const void *input2,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output,
                                            void *workspace,
                                            size_t workspace_size);
// Group:MSELoss
 /*!
 * @brief Computes the gradient of a MSE (Mean Squared Error) Loss, which is used to
 * train a network on classification tasks, based on input tensor \b input with \b target,
 * and \b grad, returns the results in the output tensor \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   mse_loss_backward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] reduction
 *   Input. The reduction used to compute the mse_loss_backward. The reduction is defined in the
 *   ::cnnlMSELossReduction_t enum.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor, which represents
 *   the predicted value.
 * @param[in] target_desc
 *   Input. The descriptor of the target tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor, which represents
 *   the ground truth value.
 * @param[in] grad_desc
 *   Input. The descriptor of the grad tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the grad tensor, which represents
 *   the gradient with respect to \b output.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor, which represents
 *   the gradient with respect to \b input.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH.
  * @par Data Type
 * - Data type of input tensors and output tensor should be the same.
 *   The supported data types of input and output tensors are as follows:
 *   - \b input : float, half.
 *   - \b target: float, half.
 *   - \b grad  : float, half.
 *   - \b output: float, half.
 *
 * @par Limitations
 * - None.
 *
 * @par API Dependency
 * - None.
 *
 * @note
 * - The dimension of the inputs tensor and output tensor should be same, when the \b
 *   reduction is \p CNNL_MSE_LOSS_NONE.
 * - The dimension of the grad tensor should be one, when the \b reduction is \p
 *   CNNL_MSE_LOSS_SUM or \p CNNL_MSE_LOSS_MEAN.
 *
 * @par Requirements
 * - None.
 *
 * @par Formula
 * - See "MSELossBackward" section in "Cambricon CNNL User Guide" for details.  *
 * @par Example
 * - The example of the mseloss backward operation is as follows:
     @verbatim
       input three arrays by 2 * 3, 2 * 3 and 1
       --> input : [[4,5,6], [7,8,9]]

       --> target: [[1,2,3], [4,5,6]]

       --> grad  : [1]

       param:
         reduction: CNNL_MSE_LOSS_MEAN

       output one array by 2 * 3
       --> output: [[1,1,1], [1,1,1]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.8.0/generated/torch.nn.MSELoss.html?highlight=mse#torch.nn.MSELoss
 */
cnnlStatus_t CNNL_WIN_API cnnlMSELossBackward(cnnlHandle_t handle,
                                              cnnlMSELossReduction_t reduction,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t target_desc,
                                              const void *target,
                                              const cnnlTensorDescriptor_t grad_desc,
                                              const void *grad,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:Ceil
/*!
 * @brief Computes ceil of each element in the input tensor \b x, and returns
 * the result in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the ceil
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \b y. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Ceil Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data Layout of input tensor and output tensor should be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - The values of input tensor should be in the range [-2^23 + 1, 2^23 - 1].
 * - The shape of input tensor and output tensor should be the same.
 * - The total number of dimensions of input tensor and output tensor should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
     @verbatim
       input:  [-2.1, -1.8, 9.125, 6.9, 4.0]
       output: [-2.0, -1.0, 10.0, 7.0, 4.0]
     @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/ceil
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlCeil(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);
// Group:RNN
/*!
 * @brief Retrieves the RNN operation information from \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function and set by ::cnnlSetRNNDescriptor_v2 function.
 * The information includes the number of RNN cell implementation \b cell_mode, how to use bias
 * after recurrent GEMM in the RNN cell formulas, the recurrence pattern \b direction, features
 * in the input state \b input_size, features in the hidden state \b hidden_size, projection layer
 * size for the RNN projection operation, how the first recurrent layer processes the input
 * \b input_mode, number of recurrent layers \b num_layers. This function is used for training
 * mode only. To initialize RNN descriptor for inference mode, call ::cnnlSetRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[out] cell_mode
 *   Output. Determine the RNN cell implementation in mode basic_RNN, LSTM, GRU.
 *   For detailed information, see ::cnnlRNNMode_t.
 * @param[out] bias_mode
 *   Output. Pointer to the host memory where the RNN bias mode should be saved.
 *   For detailed information, see ::cnnlRNNBiasMode_t.
 * @param[out] direction
 *   Output. Determine recurrence pattern. For detailed information, see ::cnnlDirectionMode_t.
 * @param[out] input_mode
 *   Output. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \b input_mode is ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[out] data_type
 *   Output. The data type of \b inputs, \b output and \b filter. see ::cnnlDataType_t.
 * @param[out] math_prec
 *   Output. The data type for computing all matrix multiplications in the RNN operation.
 * @param[out] input_size
 *   Output. The number of features in the input state input vector.
 * @param[out] hidden_size
 *   Output. The number of features in the hidden state input vector.
 * @param[out] proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 *   /b proj_size must be the same as /b hidden_size currently.
 * @param[out] layer_num
 *   Output. Number of stacked layers in the deep RNN mode.
 * @param[out] dropout_desc
 *   Input. The dropout rate of the dropout layer after RNN operation output.
 *   Dropout is used only in the training mode.
 * @param[out] padding_mode
 *   Output. Pointer to host memory where padding mode should be saved.
 *   For detailed information, see ::cnnlRNNPaddingMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - Output pointer \b cell_mode, \b bias_mode, \b direction, \b input_mode,
 *   \b data_type, \b input_size, \b hidden_size, \b proj_size, \b numLayers,
 *   should not be NULL.
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetRNNDescriptor_v2(const cnnlRNNDescriptor_t rnn_desc,
                        cnnlRNNMode_t *cell_mode,
                        cnnlRNNBiasMode_t *bias_mode,
                        cnnlDirectionMode_t *direction,
                        cnnlRNNInputMode_t *input_mode,
                        cnnlDataType_t *data_type,
                        cnnlDataType_t *math_prec,
                        int32_t *input_size,
                        int32_t *hidden_size,
                        int32_t *proj_size,
                        int32_t *layer_num,
                        void **dropout_desc,
                        cnnlRNNPaddingMode_t *padding_mode);
// Group:RNN
/*!
 * @brief Initializes the RNN descriptor \b rnn_desc that is previously created
 * with the ::cnnlCreateRNNDescriptor function. The information includes the number
 * of features in the hidden state \b hidden_size, the number of recurrent layers
 * \b num_layers, how the first recurrent layer processes the input \b input_mode,
 * the recurrence pattern \b direction, RNN cell implementation \b mode. This function
 * is used for training mode only. To initialize RNN descriptor for inference mode,
 * call ::cnnlSetRNNDescriptor.
 *
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] cell_mode
 *   Input. Determine the RNN cell implementation in mode basic_RNN, LSTM and GRU.
 *   For detailed information, see ::cnnlRNNMode_t.
 * @param[in] bias_mode
 *   Input. Pointer to the host memory where the RNN bias mode should be saved.
 *   For detailed information, see ::cnnlRNNBiasMode_t.
 * @param[in] direction
 *   Input. Determine recurrence pattern. For detailed information, see ::cnnlDirectionMode_t.
 * @param[in] input_mode
 *   Input. Specify how the first stacked layer handles the input of the RNN mode.
 *   When the parameter \b input_mode is ::CNNL_RNN_SKIP_INPUT, no matrix formation operation is
 *   performed on the original input vectors.
 * @param[in] data_type
 *   Input. The data type of \b inputs, \b output and \b filter. see ::cnnlDataType_t.
 * @param[in] math_prec
 *   Input. The data type for computing all matrix multiplications in the RNN operation.
 * @param[in] input_size
 *   Input. The number of features in the input state input vector.
 * @param[in] hidden_size
 *   Input. The number of features in the hidden state input vector.
 * @param[in] proj_size
 *   Input. Specify output projection layer size on the RNN operation.
 * @param[in] layer_num
 *   Input. Number of stacked layers in the deep RNN mode.
 * @param[in] dropout_desc
 *   Input. The dropout rate of the dropout layer can be applied between
 *   RNN physical layers.Dropout is used only in the training mode.
 * @param[in] padding_mode
 *   input. Pointer to host memory where padding mode should be saved.
 *   For detailed information, see ::cnnlRNNPaddingMode_t.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlCreateRNNDescriptor function.
 *
 * @note
 * - No dropout can be applied if the RNN network only has one layer.
 * - Currently, \b cell_mode only supports ::CNNL_LSTM. \b input_mode only supports
 *   ::CNNL_RNN_LINEAR_INPUT. \b dropout_desc only supports NULL. The padding mode
 *   is not supported currently.
 */

cnnlStatus_t CNNL_WIN_API
cnnlSetRNNDescriptor_v2(cnnlRNNDescriptor_t rnn_desc,
                        cnnlRNNMode_t cell_mode,
                        cnnlRNNBiasMode_t bias_mode,
                        cnnlDirectionMode_t direction,
                        cnnlRNNInputMode_t input_mode,
                        cnnlDataType_t data_type,
                        cnnlDataType_t math_prec,
                        int input_size,
                        int hidden_size,
                        int proj_size,
                        int layer_num,
                        void *dropout_desc,
                        cnnlRNNPaddingMode_t padding_mode);
// Group:RNN
/*!
 * @brief Returns in \b workspace_size and \b reserve_size the size of the MLU memory
 * that is used as an extra workspace to optimize the RNN operation, and pass the
 * intermediate results between RNN backward operations respectively.
 *
 * For more information about the extra \b workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[out] workspace_size
 *   Output. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 * @param[out] reservespace_size
 *   Output. The size of the reservespace in bytes that needs to be used in
 *   the RNN operation. The reservespace is used to pass intermediate results.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions to create and
 *   set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \b x_desc.
 * - The allocated extra workspace memory should be passed to the ::cnnlRNNForwardTraining function,
 *   ::cnnlRNNBackwardData function and ::cnnlRNNBackwardWeights function to perform the RNN
 *   forward or backward operation.
 *
 * @note
 * - For the RNN descriptor, in addition to the basic information set by calling the function
 *   ::cnnlSetRNNDescriptor_v2, if you want to run the optional variant mode, you need to call the
 *   corresponding function to set it, such as: ::cnnlSetRNNMaskMode, ::cnnlSetRNNClip.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetRNNTempSizes(cnnlHandle_t handle,
                    const cnnlRNNDescriptor_t rnn_desc,
                    const cnnlSeqDataDescriptor_t x_desc,
                    size_t *workspace_size,
                    size_t *reservespace_size);
// Group:RNN
/*!
 * @brief Returns in \b weightpace_size, the size of MLU memory that is used to
 * store RNN filter and bias.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[out] weightspace_size
 *   output. The minimum size in bytes of MLU memory needed for RNN trainable
 *   parameters in the RNN operation.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function,
 *   you need to call ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions
 *   to create and set the RNN operation descriptor,
 *   and call the ::cnnlCreateSeqDataDescriptor and ::cnnlSetSeqDataDescriptor functions
 *   to create and set the tensor descriptors \b x_desc.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightSpaceSize(cnnlHandle_t handle,
                          const cnnlRNNDescriptor_t rnn_desc,
                          size_t *weightspace_size);
// Group:RNN
/*!
 * @brief Gets the starting address and shape information of every RNN filter and bias
 * in each \b pseudo_layer within the RNN network.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input.The descriptor of the RNN operation. For detailed information,
 *   see ::cnnlRNNDescriptor_t.
 * @param[in] pseudo_layer
 *   Input. When \b direction is ::CNNL_RNN_UNIDIRECTIONAL, pseudo_layer is
 *   the same as physical layer. When \b direction is ::CNNL_RNN_BIDIRECTIONAL,
 *   the number of pseudo_layers is twice that of physical_layers.
 * - When \b direction is ::CNNL_RNN_UNIDIRECTIONAL:
 *   - If pseudo_layer = 0, this layer is RNN input layer.
 *   - If pseudo_layer = 1, this layer is RNN first hidden layer.
 * - When \b direction is ::CNNL_RNN_BIDIRECTIONAL:
 *   - pseudo_layer = 0, this layer is a sub-layer of input forward layer.
 *   - pseudo_layer = 1, this layer is a sub-layer of input backward layer.
 *   - pseudo_layer = 2, this layer is a sub-layer of first hidden forward layer.
 *   - pseudo_layer = 3, this layer is a sub-layer of first hidden backward layer.
 *   and so on.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter. You can
 *   call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] filterspace
 *   Input. Pointer to the MLU memory that stores filter and bias.
 * @param[in] lin_layer_id
 *   Input. The filter or bias linear id index.
 * - For \b rnn_mode only supports ::CNNL_LSTM, The supported values of lin_layer_id are
 *   as follows:
 *   - value 0, 1, 2 and 3 reference filter or bias used in computing with input tensor.
 *   - value 4, 5, 6 and 7 reference filter or bias used in computing with hidden state tensor.
 *   - value 8 corresponds to the projection matrix, but there is no bias in RNN projection
 *   operation.
 *   - value 0 and 4 correspond to the input gate.
 *   - value 1 and 5 correspond to the forget gate.
 *   - value 2 and 6 correspond to the update gate.
 *   - value 3 and 7 correspond to the output gate.
 * @param[out] m_desc
 *   Input. The descriptor of filter data. The shape of this filter is returned in
 *   this descriptor in the format of {hidden_size, input_size, 1, 1}. If the filter
 *   of the specified pseudo_layer and lin_layer_id does not exist, the return dim is 0.
 * @param[out] m_addr
 *   Output. Pointer to starting address of the MLU memory that stores filter data. If
 *   the filter of the specified pseudo_layer and lin_layer_id dose not exist, the return
 *   address is NULL.
 * @param[out] b_desc
 *   Input. The descriptor of bias data. The shape of this bias is returned in
 *   this descriptor in the format of {hidden_size, 1, 1, 1}. If the bias of the specified
 *   pseudo_layer and lin_layer_id dose not exist, the return dim is 0.
 * @param[out] b_addr
 *   Output. Pointer to starting address of the MLU memory that stores bias data. If
 *   the bias of the specified pseudo_layer and lin_layer_id dose not exist, the return address
 *   is NULL.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. You need to call
 *   ::cnnlCreateRNNDescriptor, ::cnnlSetRNNDescriptor_v2 functions to create and
 *   set the RNN operation descriptor.
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetRNNWeightParams(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       int32_t pseudo_layer,
                       size_t filterspace_size,
                       const void *filter_space,
                       int32_t lin_layer_id,
                       cnnlTensorDescriptor_t m_desc,
                       void **m_addr,
                       cnnlTensorDescriptor_t b_desc,
                       void **b_addr);

// Group:FusedDropout
/*!
 *
 * @deprecated
 *   ::cnnlFusedDropout is deprecated and will be removed in the future release. It is recommended
 *   to use ::cnnlFusedDropout_v2 instead, which supports parameters of \b state that generates
 *   random number.
 *
 * @brief Randomly set the value of input elements to zero and returns the processed
 * output and the state of each element.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the fused_dropout operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] p
 *   Input. A float value used as the probability that the value of input element is
 *   set to zero, \b p must be in [0, 1].
 * @param[in] seed
 *   Input. A unsigned int pointer pointing to the random seed.
 * @param[out] mask_desc
 *   Output. The descriptor of the mask tensor. The element of mask tensor is the
 *   state of the element of input tensor after deactivation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] mask
 *   Output. Pointer to the MLU memory that stores the mask tensor.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. The value of the output element is
 *   the value of the input element is randomly zeroed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Data Type
 * - Data Type of input tensor \b input, output tensor \b output must be the same.
 * - The supported data types of \b input, \b mask and \b output are as follows:
 *   - \b input: int8, uint8, int16, int32, half, float.
 *   - \b mask: uint8.
 *   - \b output: int8, uint8, int16, int32, half, float.
 * @par Data Layout
 * - The supported data layout of \b input, \b mask and \b output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 * @par Scale Limitation
 * - The input tensor, mask tensor and output tensor must have the same shapes.
 * @par Requirements
 * - None.
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
      input: an array [0, 1, 2, 3, ...]
      p: 0.5.
      mask: an array [0, 1, 0, 1, ...]
      output: an array [0, 1, 0, 3, ...]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlFusedDropout(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t input_desc,
                 const void *input,
                 const float p,
                 uint32_t *seed,
                 const cnnlTensorDescriptor_t mask_desc,
                 const void *mask,
                 const cnnlTensorDescriptor_t output_desc,
                 void *output);

// Group:FusedDropout
/*!
 * @brief Randomly set the value of input elements to zero and returns the processed
 * output and the state of each element. On the basis of the cnnlFusedDropout,
 * cnnlFusedDropout_v2 adds generator to determine the mode of generating random
 * number and uses state instead of seed to generate random numbersequences.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *   in the fused_dropout operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] generator
 *   Input. The descriptor of random \b generator. For detailed information,
 *   see ::cnnlRandGenerator_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] p
 *   Input. A float value used as the probability that the value of input element is
 *   set to zero, \b p must be in [0, 1].
 * @param[in,out] state
 *   Input and output. Pointer to device state data, which is used to generate random sequence.
 * Set NULL if you use \b CNNL_RAND_RNG_FAST \b generator type.
 * @param[out] mask_desc
 *   Output. The descriptor of the mask tensor. The element of mask tensor is the
 *   state of the element of input tensor after deactivation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] mask
 *   Output. Pointer to the MLU memory that stores the mask tensor.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. The value of the output element is
 *   the value of the input element is randomly zeroed. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Data Type
 * - Data Type of input tensor \b input, output tensor \b output must be the same.
 * - The supported data types of \b input, \b mask and \b output are as follows:
 *   - \b input: int8, uint8, int16, int32, half, float.
 *   - \b mask: uint8.
 *   - \b output: int8, uint8, int16, int32, half, float.
 * @par Data Layout
 * - The supported data layout of \b input, \b mask and \b output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - mask tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 * @par Scale Limitation
 * - The input tensor, mask tensor and output tensor must have the same shapes.
 * @par API Dependency
 * - You need to call this function after calling the ::cnnlRandCreateGenerator function.
 *   Additionally, if using ::CNNL_RAND_RNG_MTGP32 random generator type, you also need to
 *   call the ::cnnlRandGetMTGP32StateSize function and the ::cnnlRandMakeMTGP32KernelState
 *   function before calling this function.
 *
 * @par Requirements
 * - None.
 * @par Example
 * - The example of this operation is as follows:
 *    @verbatim
 *     input: an array [0, 1, 2, 3, ...]
 *     p: 0.5.
 *     mask: an array [0, 1, 0, 1, ...]
 *     output: an array [0, 1, 0, 3, ...]
 *    @endverbatim
 * @note
 *   On MLU200 series, users should generate random numbers with the \b CNNL_RAND_RNG_FAST
 *   generator type; on MLU300 series, users should generate random numbers with
 *   the \b CNNL_RAND_RNG_MTGP32 generator type, otherwise, the performance will be very poor.
 */
cnnlStatus_t CNNL_WIN_API
cnnlFusedDropout_v2(cnnlHandle_t handle,
                    const cnnlRandGenerator_t generator,
                    const cnnlTensorDescriptor_t input_desc,
                    const void *input,
                    const float p,
                    void *state,
                    const cnnlTensorDescriptor_t mask_desc,
                    const void *mask,
                    const cnnlTensorDescriptor_t output_desc,
                    void *output);
// Group:RNN
/*!
 * @brief Computes the first-order derivatives of filter and bias of RNN network in the training
 *        process. The specific network structure is determined by the description \b rnn_desc
 *        set by the user. Using the input data \b x, \b hx, \b y, \b filterspace and \b reservespace
 *        according to the specific network structure, writes the calculation result into the \b dfilterspace.
 *
 * This function requires two additional MLU memory as the \b reservespace and the \b workspace to
 * improve the RNN network performance. You can get the size of the \b workspace \b workspace_size
 * and \b reservespace \b reservespace_size with the ::cnnlGetRNNTempSizes function, and the size
 * of the \b weightspace \b weightspace_size with the ::cnnlGetRNNWeightSpaceSize function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the RNN operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] rnn_desc
 *   Input. The descriptor of the RNN operation.
 *   For detailed information, see ::cnnlRNNDescriptor_t.
 * @param[in] add_grad
 *   Input. The filter gradient mode, see ::cnnlWgradMode_t for more details.
 * @param[in] dev_seq_lengths
 *   Input. A copy of \b seqLengthArray set in \b x_desc or \b y_desc RNN data descriptor.
 *   The dev_seq_lengths array must be stored in MLU memory.
 * @param[in] x_desc
 *   Input. The descriptor of input sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores input sequence data.
 * @param[in] hx_desc
 *   Input. The descriptor of hidden state tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] hx
 *   Input. Pointer to the MLU memory that stores hidden state tensor.
 * @param[in] y_desc
 *   Input. The descriptor of output sequence data.
 *   For detailed information, see ::cnnlSeqDataDescriptor_t.
 * @param[in] y
 *   Input. Pointer to the MLU memory that stores output sequence data.
 * @param[out] dfilterspace
 *   Output. Pointer to the MLU memory that stores gradient deltas filter and bias.
 * @param[in] filterspace_size
 *   Input. Specifies the size of the buffer in bytes that stores filter.
 *   You can call ::cnnlGetRNNWeightSpaceSize to get the size of the buffer to be used.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   RNN operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the workspace with the ::cnnlGetRNNTempSizes function.
 * @param[in] reservespace
 *   Input. Pointer to the MLU memory that is used as an extra memory space for
 *   saving intermediate results of RNN operation.
 *   RNN operation.
 * @param[in] reservespace_size
 *   Input. The size of the extra reservespace in bytes that needs to be used in
 *   the RNN operation.
 *   You can get the size of the reservespace with the ::cnnlGetRNNTempSizes function.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RNN Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Date types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts set in descriptors are as follows:
 *   - \b x_desc must be set to \p ::CNNL_SEQDATA_TNC, ::CNNL_SEQDATA_NTC or ::CNNL_SEQDATA_TNC_PACKED.
 *   - \b hx_desc must be set to \p ::CNNL_LAYOUT_ARRAY and \b dimNb in \b hx_desc must be 3.
 *   - \b y_desc must be set to the same layout as \b x_desc.
 *
 * @par Scale Limitation
 * - The \b rnn_mode of \b rnn_desc only supports ::CNNL_LSTM with projection layer.
 * - The \b input_mode of \b rnn_desc only supports ::CNNL_RNN_LINEAR_INPUT.
 * - The \b padding_mode of \b rnn_desc only supports ::CNNL_RNN_PADDED_IO_DISABLED.
 * - The \b math_prec must be int16 on CE3226 and MLU200 series and it must be the same as
 *   \b data_type or int16 on MLU300 series.
 * - The \b dev_seq_lengths must be batch's sequence and descending order and the length of
 *   the \b dev_seq_lengths must be equal to x_desc->dims[0] when the \b cnnlSeqDataLayout_t
 *   is CNNL_SEQDATA_TNC_PACKED.
 *
 * @par API Dependency
 * - Before calling this function to implement RNN, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.::cnnlRNNBackwardWeights function should be invoked after
 *   ::cnnlRNNBackwardData.
 *
 * @par Performance Optimization
 * - For best practices, to have a better performance, set the layout of the input
 *   sequence data \b x_desc and output sequence data \b y_desc to ::CNNL_SEQDATA_TNC.
 *
 * @note
 * - The first two dimensions of \b x_desc must be equal to those of \b y_desc.
 * - \b add_grad only supports ::CNNL_WGRAD_MODE_SET.
 */

cnnlStatus_t CNNL_WIN_API
cnnlRNNBackwardWeights(cnnlHandle_t handle,
                       const cnnlRNNDescriptor_t rnn_desc,
                       cnnlWgradMode_t add_grad,
                       const int32_t dev_seq_lengths[],
                       const cnnlSeqDataDescriptor_t x_desc,
                       const void *x,
                       const cnnlTensorDescriptor_t hx_desc,
                       const void *hx,
                       const cnnlSeqDataDescriptor_t y_desc,
                       const void *y,
                       void *dfilterspace,
                       size_t filterspace_size,
                       void *workspace,
                       size_t workspace_size,
                       void *reservespace,
                       size_t reservespace_size);

// Group:FocalLossSigmoid
/*!
 * @brief Computes cross entropy loss with weighting factor and focusing factor to reduce the
 *        filter of samples which are easy to classify.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlFocalLossSigmoidForward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. An enum to indicate the algorithm used to compute the output.
 *   For detailed information, see ::cnnlComputationPreference_t. Only supports
 *   \p CNNL_COMPUTATION_HIGH_PRECISION currently.
 * @param[in] reduction
 *   Input. An enum to indicate the reduction mode used to compute the operation.
 *   For detailed information, see ::cnnlLossReduction_t. Only supports
 *   \p CNNL_LOSS_REDUCTION_NONE currently.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target
 *   of input.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor. which is the filter
 *   value of input.
 * @param[in] alpha
 *   Input. A float value which is the filtering factor of the focal loss sigmoid forward.
 * @param[in] gamma
 *   Input. A float value which is the focusing factor of the focal loss sigmoid forward.
 * @param[in] output_desc
 *   in. The descriptor of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Focal Loss Sigmoid Forward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b input tensor, \b target tensor, \b filter tensor and \b output tensor:
 *   - input: half, float.
 *   - target: int32.
 *   - filter: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - The shape of \b input must be [N, C].
 * - The shape of \b input and \b output must be equal.
 * - The shape of \b target is [N] when the shape of \b input is [N, C].
 * - The shape of \b filter is [C] when the shape of \b input is [N, C].
 * - \b input value should be in the range of [-20, 20] when the data type of \b input is float.
 * - \b input value should be in the range of [-5, 5] when the data type of \b input is half.
 * - \b target value should be in the range of [0, C] when \b filter is NULL and the shape of
 *   \b input is [N, C].
 * - \b target value should be in the range of [0, C-1] when \b filter is not NULL and the
 *   shape of \b input is [N, C].
 * - \b gamma should be greater than or equal to 0.
 *
 * @note
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series:
 *     - If \b input, \b filter, \b alpha or \b gamma is NaN, then \b output is finite value.
 *     - If \b input, \b filter or \b alpha is infinity, but \b gamma is finite value,
 *       then \b output is finite value.
 *     - If \b gamma is positive infinity, but \b input, \b filter and \b alpha are finite value,
 *       then \b output is finite value.
 *   - On MLU300 series and CE3226:
 *     - If \b input is infinity, but \b filter, \b alpha and \b gamma are finite value,
 *       then \b output is NaN or finite value.
 *     - If \b filter is positive infinity, but \b input, \b alpha and \b gamma are finite value,
 *       then \b output is NAN or positive infinity.
 *     - If \b filter is negative infinity, but \b input, \b alpha and \b gamma are finite value,
 *       then \b output is NAN or negative infinity.
 *     - If \b alpha is infinity and data type of \b input is float,
 *       but \b input, \b filter and \b gamma are finite value,
 *       then \b output is NAN or infinity.
 *     - If \b alpha is infinity and data type of \b input is half,
 *       but \b input, \b filter and \b gamma are finite value,
 *       then \b output is NAN or finite value.
 *     - If \b gamma is positive infinity, but \b input, \b filter and \b alpha are finite value,
 *       then \b output is NAN or 0.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar. Proceedings of the IEEE
 *   International Conference on Computer Vision(ICCV), 2017, oo.2980-2988.
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/focal_loss.py
 *
*/
cnnlStatus_t CNNL_WIN_API cnnlFocalLossSigmoidForward(cnnlHandle_t handle,
                                                      const cnnlComputationPreference_t prefer,
                                                      const cnnlLossReduction_t reduction,
                                                      const cnnlTensorDescriptor_t input_desc,
                                                      const void *input,
                                                      const cnnlTensorDescriptor_t target_desc,
                                                      const void *target,
                                                      const cnnlTensorDescriptor_t filter_desc,
                                                      const void *filter,
                                                      const float alpha,
                                                      const float gamma,
                                                      const cnnlTensorDescriptor_t output_desc,
                                                      void *output);

// Group:FocalLossSigmoid
/*!
 *  @brief Computes the gradients of ::cnnlFocalLossSigmoidBackward with \b input tensor,
 *  \b target tensor, \b filter tensor, \b grad_output tensor, and returns the results in the \b grad_input tensor.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlFocalLossSigmoidBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. An enum to indicate the algorithm used to compute the output.
 *   For detailed information, see ::cnnlComputationPreference_t. Only supports
 *   \p CNNL_COMPUTATION_FAST currently.
 * @param[in] reduction
 *   Input. An enum to indicate the reduction mode used to compute the operation.
 *   For detailed information, see ::cnnlLossReduction_t. Only supports
 *   \p CNNL_LOSS_REDUCTION_NONE currently.
 * @param[in] input_desc
 *   Input. The descriptor of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] target_desc
 *   Input. The descriptor of target tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] target
 *   Input. Pointer to the MLU memory that stores the target tensor which is the target
 *   of input.
 * @param[in] filter_desc
 *   Input. The descriptor of filter tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] filter
 *   Input. Pointer to the MLU memory that stores the filter tensor, which is the filter
 *   value of input.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \b grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *    Input. Pointer to the MLU memory that stores the gradient tensor.
 * @param[in] alpha
 *   Input. A float value which is the filtering factor of the focal loss sigmoid backward.
 * @param[in] gamma
 *   Input. A float value which is the focusing factor of the focal loss sigmoid backward.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \b grad_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \b grad_input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Focal Loss Sigmoid Backward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \b input tensor, \b target tensor, \b filter tensor and \b output tensor:
 *   - input: float.
 *   - target: int32.
 *   - filter: float.
 *   - grad_input: float.
 *   - grad_output: float.
 *
 * @par Scale Limitation
 * - The shape of \b input must be [N, C].
 * - The shape of \b input and \b grad_output must be consistent.
 * - The shape of \b input and \b grad_input must be consistent.
 * - The shape of \b target is [N] when the shape of \b input is [N, C].
 * - The shape of \b filter is [C] when the shape of \b input is [N, C].
 * - \b target value should be in the range of [0, C] when \b filter is NULL and the shape of
 *   \b input is [N, C].
 * - \b target value should be in the range of [0, C-1] when \b filter is not NULL and the
 *   shape of \b input is [N, C].
 * - prefer only supports CNNL_COMPUTATION_FAST currently.
 * - reduction only supports CNNL_LOSS_REDUCTION_NONE currently.
 * - The layout of \b input, \b target, \b filter, \b grad_output and \b grad_input must be ARRAY.
 *
 * @note
 * - If the shape of \b input is set to [N, C]. The length of C should be in the range of [0, 13615] when
 *   \b filter is NULL on MLU300 series. The length of C should be in the range of [0, 12544] when
 *   \b filter is not NULL on MLU300 series.
 * - If the shape of \b input is set to [N, C]. The length of C should be in the range of [0, 8154] when
 *   \b filter is NULL on MLU200 series. The length of C should be in the range of [0, 7520] when
 *   \b filter is not NULL on MLU200 series.
 * - \b filter does not support positive infinity and negative infinity currently.
 * - \b grad_output does not support positive infinity and negative infinity currently.
 * - \b gamma should be in the range of [0, 8] on MLU200 series, and should be in the range of [0, 10000]
 *   on MLU300 series.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/focal_loss.py
 *
*/
cnnlStatus_t CNNL_WIN_API
cnnlFocalLossSigmoidBackward(cnnlHandle_t handle,
                             const cnnlComputationPreference_t prefer,
                             const cnnlLossReduction_t reduction,
                             const cnnlTensorDescriptor_t input_desc,
                             const void *input,
                             const cnnlTensorDescriptor_t target_desc,
                             const void *target,
                             const cnnlTensorDescriptor_t filter_desc,
                             const void *filter,
                             const cnnlTensorDescriptor_t grad_output_desc,
                             const void *grad_output,
                             const float alpha,
                             const float gamma,
                             const cnnlTensorDescriptor_t grad_input_desc,
                             void *grad_input);

// Group:Log
/*!
 * @brief Computes the natural logarithm of the \b input plus one, element-wise.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the log1p
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer mode defined in ::cnnlComputationPreference_t enum.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor \b x.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor \b y.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Log1p Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor should have the same shape, and the input data must meet
 * - the following input data range:
 *   - If log1p works on (m)tp_2xx:
 *     - float: [-0.999, 0]∪[1e-2, 6e4].
 *     - half:
 *        - COMPUTATION_HIGH_PRECISION: [-0.999, 0]∪[1e-2, 6e4].
 *        - COMPUTATION_FAST: [1, 6e4].
 *   - If log1p works on (m)tp_3xx:
 *     - float: [-0.99999, 0]∪[1e-20, 6e10].
 *     - half:
 *        - COMPUTATION_HIGH_PRECISION: [-0.999, 0]∪[1e-20, 6e4].
 *        - COMPUTATION_FAST: [-0.999, 0]∪[1e-16, 6e4].
 *
 * @note
 * - COMPUTATION_HIGH_PRECISION mode or COMPUTATION_FAST mode is only for half.
 * - When input data or parameter contains NaN/infinity:
 *   - On MLU200 series:
 *     - float:
 *        - If \b x is NaN, then \b y is 1.67508e+32.
 *        - If \b x is positive infinity, then \b y is 1.67508e+32.
 *        - If \b x is negative infinity, then \b y is negative saturation value.
 *     - half:
 *        - If \b x is NaN, then \b y is -17.875.
 *        - If \b x is positive infinity, then \b y is -17.875.
 *        - If \b x is negative infinity, then \b y is 12.4533.
 *   - On MLU300 series and CE3226:
 *     - If \b x is positive infinity, then \b y is NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://tensorflow.google.cn/api_docs/python/tf/math/log1p
*/
cnnlStatus_t CNNL_WIN_API cnnlLog1p(cnnlHandle_t handle,
                                    const cnnlComputationPreference_t prefer,
                                    const cnnlTensorDescriptor_t x_desc,
                                    const void *x,
                                    const cnnlTensorDescriptor_t y_desc,
                                    void *y);

// Group:GatherTree
/*!
 * @brief Calculates the full beams from the \b step_ids tensor and \b parent_ids tensor.
 *   If the value of parent id is out of range [0, beam_width), a \p -1 is stored
 *   in the corresponding output value and the execution for that beam returns early.
 *   For a given beam, past the time step containing the first decoded \p end_token
 *   all values are filled in with \p end_token.
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlGatherTree. For detailed information, see ::cnnlHandle_t.
 * @param[in] step_ids_desc
 *   Input. The descriptor \b of step_ids tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] step_ids
 *   Input. Pointer to the MLU memory that stores the \b step_ids tensor which is
 *   the predicted token IDs.
 * @param[in] parent_ids_desc
 *   Input. The descriptor of \b parent_ids tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[in] parent_ids
 *   Input. Pointer to the MLU memory that stores the \b parent_ids tensor which is
 *   the parent beam indices.
 * @param[in] max_seq_len_desc
 *   Input. The descriptor of \b max_sequence_lengths tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] max_sequence_lengths
 *   Input. Pointer to the MLU memory that stores the \b max_sequence_lengths tensor
 *   which is the maximum sequence length of each batch.
 * @param[in] end_token
 *   Input. An int32 value which is end token ID.
 * @param[in] beams_desc
 *   Input. The descriptor of \b beams tensor. For detailed information,
 *   see ::cnnlSeqDataDescriptor_t.
 * @param[out] beams
 *   Output. Pointer to the MLU memory that stores the \b beams tensor which is
 *   the reorderd token IDs based on \b parent_ids.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Gather Tree Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \b step_ids tensor, \b parent_ids tensor, \b max_sequence_lengths tensor and \b beams tensor:
 *   - step_ids: int32.
 *   - parent_ids: int32.
 *   - max_sequence_lengths: int32.
 *   - beams: int32.
 *
 * @par Data Layout
 *   The supported data layouts are as follows:
 *   - \b step_ids_desc must be set to \p ::CNNL_SEQDATA_TNC.
 *   - \b parent_ids_desc and \b beams_desc must be set to the same layout as \b step_ids.
 *   - \b max_seq_len_desc must be set to \p ::CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The shape of \b step_ids must be [max_time, batch_size, beam_width].
 * - The shape of \b parent_ids must be [max_time, batch_size, beam_width].
 * - The shape of \b beams must be [max_time, batch_size, beam_width].
 * - The shape of \b max_sequence_lengths must be [batch_size]
 * - The value of max_time * beam_width should be in the range of [0, 32746] on MLU200 series.
 *   The value of max_time * beam_width should be in the range of [0, 54591] on MLU300 series.
 *
 * @note
 * - The shape of \b step_ids and \b parent_ids must be consistent.
 * - The shape of \b step_ids and \b beams must be consistent.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/seq2seq/
 *   kernels/beam_search_ops.cc
 * - github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/contrib/seq2seq/
 *   kernels/beam_search_ops_gpu.cu.cc
*/
cnnlStatus_t CNNL_WIN_API cnnlGatherTree(cnnlHandle_t handle,
                                         const cnnlSeqDataDescriptor_t step_ids_desc,
                                         const void *step_ids,
                                         const cnnlSeqDataDescriptor_t parent_ids_desc,
                                         const void *parent_ids,
                                         const cnnlTensorDescriptor_t max_seq_len_desc,
                                         const void *max_sequence_lengths,
                                         const int32_t end_token,
                                         const cnnlSeqDataDescriptor_t beams_desc,
                                         void *beams);
// Group:TinShift
/*!
 * @brief Shifts data from \b input according to shift information in \b shifts and stores the
 * result into \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor for the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input to be shifted.
 * @param[in] shifts_desc
 *   Input. The descriptor for the shifts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] shifts
 *   Input. Pointer to the MLU memory that stores all shifts tensor. Based on shifts
 *   tensor, output data is from the input tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "TinShift Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \b input and shifts tensor \b shifts.
 *   - input tensor: half, float.
 *   - shifts tensor: int32.
 *   - output tensor: data type is set the same as input in each case.
 *
 * @par Scale Limitation
 * - The input batch size and shifts batch size should be the same.
 * - The input channel size should be multiple of shifts group size. For example,
 *   if the input shape is [N, T, C, HW], the shifts shape is [N, G], C should be multiple of G, C and G should not be zero.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/tree/master/mmcv/ops/tin_shift.py
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlTinShiftForward(cnnlHandle_t handle,
                                              const cnnlTensorDescriptor_t input_desc,
                                              const void *input,
                                              const cnnlTensorDescriptor_t shifts_desc,
                                              const void *shifts,
                                              const cnnlTensorDescriptor_t output_desc,
                                              void *output);

// Group:TinShift
/*!
 * @brief Shifts gradients from \b grad_output according to shift information in \b shifts and stores the
 * result into \b grad_input.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in this
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor for the grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the input tensor \b grad_output to be shifted.
 * @param[in] shifts_desc
 *   Input. The descriptor the shifts tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] shifts
 *   Input. Pointer to the MLU memory that stores all shifts tensor. Based on shifts
 *   tensor, grad_input data is from the grad_output tensor.
 * @param[in] grad_input_desc
 *   Input. The descriptor of the grad_input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the grad_input tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "TinShift Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for the
 *   input tensor \b grad_output and shifts tensor \b shifts.
 *   - input tensor: half, float.
 *   - shifts tensor: int32.
 *   - output tensor: data type is set the same as input in each case.
 *
 * @par Scale Limitation
 * - The grad_output batch size and shifts batch size should be the same.
 * - The grad_output channel size should be multiple of shifts group size. For example,
 *   if the grad_output shape is [N, T, C, HW], the shifts shape is [N, G], C should be multiple of G, C and G should not be zero.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/tree/master/mmcv/ops/tin_shift.py
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlTinShiftBackward(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t grad_output_desc,
                                               const void *grad_output,
                                               const cnnlTensorDescriptor_t shifts_desc,
                                               const void *shifts,
                                               const cnnlTensorDescriptor_t grad_input_desc,
                                               void *grad_input);
// Group:Psamask
/*!
 * @brief Moves the \b x tensor to \b y tensor according to \b h_mask,
 *        \b w_mask and \b psa_type.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPsamaskForward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] psa_type
 *   Input. Type of the psamask computation, including COLLECT and DISTRIBUTE.
 * @param[in] x_desc
 *   Input. The descriptor of data of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the data of input tensor.
 * @param[in] h_mask
 *   Input. An integer value which is the h_mask factor of the psamask.
 * @param[in] w_mask
 *   Input. An integer value which is the w_mask factor of the psamask.
 * @param[in] y_desc
 *   Input. The descriptor of data of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the data of output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Psamask Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data type for
 *   data of input tensor \b x and data of output tensor \b y:
 *   - x: float.
 *   - y: float.
 *
 * @par Data Layout
 * - This function supports the following data layout for
 *   data of input tensor \b x and data of output tensor \b y:
 *   - x: NHWC.
 *   - y: NHWC.
 *
 * @par Scale Limitation
 * - The shape of \b x must be [N, H, W, C].
 * - The shape of \b y must be [N, H, W, C].
 * - All dimensions size of \b x and \b y must be the same, except the C dimension.
 * - If the shape of \b x is set to [N, H, W, C], the size of C dimension should be \b h_mask * \b w_mask .
 * - If the shape of \b y is set to [N, H, W, C], the size of C dimension should be H * W.
 * - On MLU200 series:
 *   - When psa_type is COLLECT, the size of \b x channels ci and \b y channels co should be satisfied:
 *     ci + co <= 6144.
 *   - When psa_type is DISTRIBUTE, the size of \b x channels ci and \b y channels co should be satisfied:
 *     ci + 2 * co <= 6144.
 * - On MLU300 series:
 *   - When psa_type is COLLECT, the size of \b x channels ci and \b y channels co should be satisfied:
 *     ci + co <= 10240.
 *   - When psa_type is DISTRIBUTE, the size of \b x channels ci and \b y channels co should be satisfied:
 *     ci + 2 * co <= 10240.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/psa_mask.py
 *
*/
cnnlStatus_t CNNL_WIN_API
cnnlPsamaskForward(cnnlHandle_t handle,
                   const int psa_type,
                   const cnnlTensorDescriptor_t x_desc,
                   const void* x,
                   const int h_mask,
                   const int w_mask,
                   const cnnlTensorDescriptor_t y_desc,
                   void* y);

// Group:Psamask
/*!
 * @brief Computes the gradients of input tensor \b dx with the gradients of output tensor \b dy
 *        according to \b h_mask , \b w_mask and \b psa_type.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlPsamaskBackward. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] psa_type
 *   Input. Type of the psamask computation, including COLLECT and DISTRIBUTE.
 * @param[in] dy_desc
 *   Input. The descriptor of gradient of output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] dy
 *   Input. Pointer to the MLU memory that stores the gradient of output tensor.
 * @param[in] h_mask
 *   Input. An integer value which is the h_mask factor of the psamask.
 * @param[in] w_mask
 *   Input. An integer value which is the w_mask factor of the psamask.
 * @param[in] dx_desc
 *   Input. The descriptor of gradient of input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] dx
 *   Output. Pointer to the MLU memory that stores the gradient of input tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "Psamask Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data type for
 *   gradient of output tensor \b dy and gradient of input tensor \b dx:
 *   - dy: float.
 *   - dx: float.
 *
 * @par Data Layout
 * - This function supports the following data layout for
 *   gradient of output tensor \b dy and gradient of input tensor \b dx:
 *   - dy: NHWC.
 *   - dx: NHWC.
 *
 * @par Scale Limitation
 * - The shape of \b dy must be [N, H, W, C].
 * - The shape of \b dx must be [N, H, W, C].
 * - All dimensions size of \b dy and \b dx must be the same, except the C dimension.
 * - If the shape of \b dx is set to [N, H, W, C], the size of C dimension should be \b h_mask * \b w_mask .
 * - If the shape of \b dy is set to [N, H, W, C], the size of C dimension should be H * W.
 * - On MLU200 series:
 *   - When psa_type is COLLECT, the size of \b dx channels ci and \b dy channels co should be satisfied:
 *     ci + co <= 6144.
 *   - When psa_type is DISTRIBUTE, the size of \b dx channels ci and \b dy channels co should be satisfied:
 *     ci + 2 * co <= 6144.
 * - On MLU300 series:
 *   - When psa_type is COLLECT, the size of \b dx channels ci and \b dy channels co should be satisfied:
 *     ci + co <= 10240.
 *   - When psa_type is DISTRIBUTE, the size of \b dx channels ci and \b dy channels co should be satisfied:
 *     ci + 2 * co <= 10240.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/psa_mask.py
 *
*/
cnnlStatus_t CNNL_WIN_API
cnnlPsamaskBackward(cnnlHandle_t handle,
                    const int psa_type,
                    const cnnlTensorDescriptor_t dy_desc,
                    const void *dy,
                    const int h_mask,
                    const int w_mask,
                    const cnnlTensorDescriptor_t dx_desc,
                    void *dx);

// Group:ApplyAddSign
/*!
 * @brief Updates filter using AddSign method.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyAddSign. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor. It is the gradient of \b var.
 *   With the \b grad value, ::cnnlApplyAddSign function can be used to calculate \b m and \b var.
 * @param[in] var_desc
 *   Input. The descriptor of the \b var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \b var tensor.
 *   The \b var value is the optimization goal of whole algorithm.
 * @param[in] m_desc
 *   Input. The descriptor of the \b m tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] m
 *   Input/Output. Pointer to the MLU memory that stores the \b m tensor.
 *   The \b m is the accumulation of gradient.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] alpha
 *   Input. Pointer to the MLU memory that stores the \b alpha parameter.
 *   It is base bias.
 * @param[in] sign_decay
 *   Input. Pointer to the MLU memory that stores the \b sign_decay parameter.
 *   It is the decay rate for sign.
 * @param[in] beta
 *   Input. Pointer to the MLU memory that stores the \b beta parameter.
 *   It is the exponential decay rate .
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b grad tensor, \b var tensor and \b m tensor.
 *   <b>Note that the combinations of these tensors must be half-half-half or float-float-float.</b>
 *   - grad tensor: half, float
 *   - var tensor: half, float
 *   - m tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \b grad, \b var and \b m should be same.
 * - The number of dimensions is no more than CNNL_DIM_MAX.

 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> grad: [0.3, 0.7, 0.1, 0.8]
     --> var:  [0.6, 0.4, 0.1, 0.5]
     --> m:    [0.6, 0.5, 0.2, 0.6]

     param:
        lr: 0.01
        alpha: 1.0
        sign_decay: 0.99
        beta: 0.9

     output array by 4, 4
      --> var: [0.599403, 0.398607,0.0999808,0.498048]
      --> m: [0.57,0.52,0.19,0.62]
     @endverbatim
 *
 * @par Reference
 * - https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/core/kernels/training_ops.cc
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlApplyAddSign(cnnlHandle_t handle,
                 const cnnlTensorDescriptor_t grad_desc,
                 const void *grad,
                 const cnnlTensorDescriptor_t var_desc,
                 void *var,
                 const cnnlTensorDescriptor_t m_desc,
                 void *m,
                 const void *lr,
                 const void *alpha,
                 const void *sign_decay,
                 const void *beta);
// Group:UnpoolForward
/*!
 * @brief Computes a partial inverse of pooling_forward operation. This operation
 *        supports the CNNL_POOLING_FIXED and CNNL_POOLING_MAX mode now. In the
 *        CNNL_POOLING_FIXED mode, each input pixel will be put to the center of the
 *        pooling kernel regardless of the index. In the CNNL_POOLING_MAX mode, each
 *        input piexl will be put to the corresponding positon of index.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unpool_forward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the unpool_forward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensor. Reserved for future use,
 *   set the \b index_desc to NULL now. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of the maxpool.
 *   Reserved for future use, set the \b index to NULL now.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "UnpoolForward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported combinations of data types are shown below with the following order:
 *   - input  ：float, half
 *   - output ：float, half
 *
 * @par Data Layout
 * - <b>Note that the layout of input and output must be same.</b>
 * - In the unpool_forward operation, the layout of input and output must be CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - According to the definition of pooling, the parameters in the \b pooling_desc should satisfy the following conditions:
 *   padding >= 0, stride >= 1, kernel >= 1.
 * - kw * kh < 1500. The kw and kh represent the width and the height of the pooling kernel size respectively.
 * - The \b mode in the \b pooling_desc supports the CNNL_POOLING_FIXED and CNNL_POOLING_MAX.
 *
 * @note
 * - In the CNNL_POOLING_MAX mode, the \b output is unpredictable when the input contains NaN since a NaN value is
 *   noncomparable.
 * - In the CNNL_POOLING_MAX mode, ensure that the output size of the ::cnnlUnpoolForward is equal to the input size of
 *   ::cnnlPoolingForwardWithIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
     @verbatim
     input : [1, 2, 2, 1]
     output : [1, 4, 4, 1]
     padding : [0, 0, 0, 0]
     kernel : [2, 2]
     stride : [2, 2]
     mode : CNNL_POOLING_FIXED
     Unpoolforward:
           input                               output
     ------------------                -------------------------
     |       |        |                |  1  |  0  | 0.8 |  0  |
     |   1   |  0.8   |                |-----+-----+-----+-----|
     |       |        |                |  0  |  0  |  0  |  0  |
     |-------+--------|  --unpool-->   |-----+-----+-----+-----|
     |       |        |                | 0.4 |  0  | 0.6 |  0  |
     |  0.4  |  0.6   |                |-----+-----+-----+-----|
     |       |        |                |  0  |  0  |  0  |  0  |
     ------------------                -------------------------
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnpoolForward(cnnlHandle_t handle,
                                            const cnnlPoolingDescriptor_t pooling_desc,
                                            const cnnlTensorDescriptor_t input_desc,
                                            const void *input,
                                            const cnnlTensorDescriptor_t index_desc,
                                            const void *index,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:UnpoolBackward
/*!
 * @brief Computes the gradient of ::cnnlUnpoolForward according the \b index.
 * This operation only supports the CNNL_POOLING_MAX mode now.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unpool_backward operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] pooling_desc
 *   Input. The descriptor of the unpool_backward operation. For detailed information,
 *   see ::cnnlPoolingDescriptor_t.
 * @param[in] grad_desc
 *   Input. The descriptor of \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the index of the maxpool.
 *   It is the output of ::cnnlPoolingForwardWithIndex.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the \b output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "UnpoolBackward operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \b grad tensor and \b output tensor must be the same.
 * - The supported data types of grad, index and output tensors are as follows:
 *   - grad - index - output.
 *   - half - int16 - half.
 *   - float - int32 - float.
 *
 * @par Data Layout
 * - The supported layouts of \b grad, \b index and \b output are CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - padding >= 0, stride >= 1, kernel >= 1.
 * - kw * kh <= 1500. The kw and kh represent the width and the height of the pooling kernel size respectively.
 * - The \b mode in the \b pooling_desc only supports the CNNL_POOLING_MAX.
 *
 * @note
 * - The operation does not support NaN/infinity in input data.
 * - In the CNNL_POOLING_MAX mode, ensure that the grad(input) size of the ::cnnlUnpoolBackward is equal to
 *   the input size of the ::cnnlPoolingForwardWithIndex.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
     @verbatim
     grad : [1, 4, 4, 1]
     index : [1, 2, 2, 1]
     output : [1, 2, 2, 1]
     padding : [0, 0, 0, 0]
     kernel : [2, 2]
     stride : [2, 2]
     mode : CNNL_POOLING_MAX
     Unpoolbackward:

              grad                    index                output
      -------------------------  ------------------  ------------------
      |  1  |  2  |  3  |  4  |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |   1   |   2    |  |   2   |   7    |
      |  5  |  6  |  7  |  8  |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |-------+--------|  |-------+--------|
      |  9  |  10 |  11 |  12 |  |       |        |  |       |        |
      |-----+-----+-----+-----|  |   3   |   1    |  |   14  |   12   |
      |  13 |  14 |  15 |  16 |  |       |        |  |       |        |
      -------------------------  ------------------  ------------------
     @endverbatim
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnpoolBackward(cnnlHandle_t handle,
                                            const cnnlPoolingDescriptor_t pooling_desc,
                                            const cnnlTensorDescriptor_t grad_desc,
                                            const void *grad,
                                            const cnnlTensorDescriptor_t index_desc,
                                            const void *index,
                                            const cnnlTensorDescriptor_t output_desc,
                                            void *output);

// Group:ClipGradNorm
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 *        to optimize the ::cnnlClipGradNorm operation.
 *
 * The size of extra workspace is based on the given information of the ::cnnlClipGradNorm
 * operation, including the number of input tensors \b input_num and the list of descriptors of
 * input tensors \b input_desc. For more information about the workspace, see "Cambricon CNNL
 * User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in the
 *           ::cnnlClipGradNorm operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - The parameters must meet the following requirements:
 *   - The parameter \b input_num should be greater than 0.
 *
 * @par API Dependency
 * - This function must be called before the ::cnnlClipGradNorm function.
 *
 * @note
 * - The length of the list of input tensors should be equal to \b input_num.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetClipGradNormWorkspaceSize(cnnlHandle_t handle,
                                 const int input_num,
                                 const cnnlTensorDescriptor_t input_desc[],
                                 size_t *workspace_size);

// Group:ClipGradNorm
/*!
 * @brief Clips gradient norm of input tensors. The norm is computed over all gradients together.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *          ::cnnlClipGradNorm operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_num
 *   Input. The number of input tensors.
 * @param[in] input_desc
 *   Input. The list of descriptors of input tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. A host pointer to a list of MLU pointers, which points to the MLU memory that store the
 *          input tensors.
 * @param[in] max_norm
 *   Input. A float value which is the max norm of the clip grad norm.
 * @param[in] norm_type
 *   Input. A float value which indicates the type of used p-norm.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the ::cnnlClipGradNorm
 *          operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the
 *          ::cnnlClipGradNorm operation. You can get the size of the workspace with the
 *          ::cnnlGetClipGradNormWorkspaceSize function.
 * @param[in] total_norm_desc
 *   Input. The descriptor of total norm. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] total_norm
 *   Output. Pointer to the MLU memory that stores the total norm.
 * @param[in] output_desc
 *   Input. The list of descriptors of output tensors. For detailed information, see
 *          ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. A host pointer to a list of MLU pointers, which points to the MLU memory that store
 *           the output tensors.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM,:: CNNL_STATUS_NOT_SUPPORTED,
 *   ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par Formula
 * - See "Clip Grad Norm Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor, total_norm, and output tensor should be the same.
 * - The supported data types of input, total_norm, and output tensors are as follows:
 *   - input: half, float.
 *   - total_norm: half, float.
 *   - output: half, float.
 *
 * @par Scale Limitation
 * - The parameters must meet the following conditions:
 *   - \b input_num should be greater than 0.
 *   - \b max_norm should be in the range of [-65504, 65504] on MLU200 series when
 *     the data type of \b input is half.
 *   - \b norm_type should be equal to 2.0.
 *
 * @par API Dependency
 * - Before calling this function to implement clip grad norm, you need to call
 *   ::cnnlGetClipGradNormWorkspaceSize to get the extra space size needed in ::cnnlClipGradNorm
 *   operation.
 *
 * @note
 * - Only supports Euclidean-Norm currently.
 * - This function supports in-place operation, which means the input tensor list
 *   \b input and the output tensor list \b output can be the same one.
 * - The length of the list of \b input tensors should be equal to \b input_num.
 * - The length of the list of \b output tensors should be equal to \b input_num.
 * - The shape of the i-th input tensor and the i-th output tensor must be matched.
 * - The stride of the i-th input tensor and the i-th output tensor must be matched.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/pytorch/pytorch/blob/master/torch/nn/utils/clip_grad.py
*/
cnnlStatus_t CNNL_WIN_API cnnlClipGradNorm(cnnlHandle_t handle,
                                           const int input_num,
                                           const cnnlTensorDescriptor_t input_desc[],
                                           const void *input[],
                                           const float max_norm,
                                           const float norm_type,
                                           void *workspace,
                                           size_t workspace_size,
                                           const cnnlTensorDescriptor_t total_norm_desc,
                                           void *total_norm,
                                           const cnnlTensorDescriptor_t output_desc[],
                                           void *output[]);

// Group:IndexCopy
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the index_copy operation.
 *
 * The size of extra workspace is based on the given information of the index_copy operation,
 * including the input tensor descriptors \b index_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] index_desc
 *   Input. The descriptor of the input index tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   index_copy operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetIndexCopyWorkspaceSize(cnnlHandle_t handle,
                              const cnnlTensorDescriptor_t index_desc,
                              size_t *workspace_size);

// Group:IndexCopy
/*!
 * @brief Copies vectors or scalars from \b input_b into \b input_a along \b dim according to the
 * entries in \b index and stores the result into \b output. Supports inplace operation: \b output = \b input_a.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the index_copy operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] dim
 *   Input. The dimension of the \b input_a and \b input_b to be indexed.
 * @param[in] deterministic_mode
 *   Input. The mode that decides if index_copy operation uses deterministic algorithms. That is whether to
 *   always produce the same output based on the same input when running on the same software and hardware.
 *   When \b index contains duplicate entries, multiple elements from \b input_b will be copied to the same index,
 *   and the result is nondeterministic since it depends on which copy occurs last.
 *   True mode will cost some time to make sure the output is deterministic according to the order of \b index.
 *   False mode is not supported yet.
 * @param[in] input_a_desc
 *   Input. The descriptor of the \b input_a. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_a
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_a to be indexed and copied into.
 * @param[in] input_b_desc
 *   Input. The descriptor of the \b input_b. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_b
 *   Input. Pointer to the MLU memory that stores the input tensor \b input_b to be copied from.
 * @param[in] index_desc
 *   Input. The descriptor of the \b index tensors. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] index
 *   Input. Pointer to the MLU memory that stores the input tensor \b index which maps vectors or
 *   scalars from \b input_b to \b input_a.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the index
 *   copy operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the index
 *   copy operation. You can get the size of the workspace with the
 *   ::cnnlGetIndexCopyWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output tensor. For more detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Index Copy Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input_a , \b input_b and
 *   output tensor \b output.
 *   <b>Note that the data type of input tensor and output tensor should be same.</b>
 *   - input tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int31, int32, uint64, int64, bool, half, float.
 *
 * - \b dim: int32
 * - \b index: int64
 *
 * @par Scale Limitation
 * - The \b input_a tensor, \b index tensor, \b input_b tensor and \b dim must meet the following
 *   requirements:
 *   - The dim-th dimension of \b input_b must have the same size as the length of
 *        \b index .
 *   - \b index must be a vector.
 *   - The length of all dimensions of \b input_b apart from the dim-th dimension must match
 *        the length of all dimensions of \b input_a apart from the dim-th dimension.
 *   - The value of \b index must be less than the length of dim-th dimension of \b input_a and not less than 0.
 *   - On MLU200 series, when the sum size of \b index and \b input_b is greater than 1920KB,
 *        the sum of the length of dim-th dimension of \b input_a and twice the length of \b index must not be greater than 49152.
 *   - On MLU300 series, when the sum size of \b index and \b input_b is greater than 3968KB,
 *        the sum of the length of dim-th dimension of \b input_a and twice the length of \b index must not be greater than 81920.
 *   - The maximum of \b index must be less than the length of dim-th dimension of \b input_a and
 *        every element of \b index must be greater than or equal to zero.
 *   - The value of \b dim must be less than the dimension of \b input_a and not less than 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the index add operation is as follows:
     @verbatim
     dim = 0
     input_a array
       input_a = [[1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1],
                  [1, 1, 1]]
     index array
       index = [0, 4, 2]
     input_b array
       input_b = [[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 8]]
     output array
       output = [[1, 2, 3],
                 [1, 1, 1],
                 [7, 8, 8],
                 [1, 1, 1],
                 [4, 5, 6]]
     @endverbatim
 */

cnnlStatus_t CNNL_WIN_API
cnnlIndexCopy(cnnlHandle_t handle,
              int32_t dim,
              bool deterministic_mode,
              const cnnlTensorDescriptor_t input_a_desc,
              const void *input_a,
              const cnnlTensorDescriptor_t input_b_desc,
              const void *input_b,
              const cnnlTensorDescriptor_t index_desc,
              const void *index,
              void *workspace,
              size_t workspace_size,
              const cnnlTensorDescriptor_t output_desc,
              void *output);

// Group:Diagonal
/*！
 * @brief Obtains the designated diagonal on designated 2-dimensional plane of multidimensional tensor \b input.
 * \b offset determines which diagonal on the plane. \b dim1 and \b dim2 determine the plane to be considered.
 * Diagonal obtained will be appended as the last dimension of \b output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the diagonal operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input. It contains information including dimension number N and tensor shape SHAPE_IN.
 *   For more detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor \b input.
 * @param[in] offset
 *   Input. The index of the diagonal to be obtained. If \b offset is positive, diagonal above main diagonal will be considered.
 *   If \b offset is negative, diagonal below main diagonal will be considered. If offset equals 0, main diagonal will be considered.
 * @param[in] dim1
 *   Input. The first dimension of the designated 2-dimensional plane to take diagonal.
 * @param[in] dim2
 *   Input. The second dimension of the designated 2-dimensional plane to take diagonal.
 * @param[in] output_desc
 *   Input. The descriptor of the \b output. It contains information including dimension number M and tensor shape SHAPE_OUT.
 *   M should be equal to N-1. To get SHAPE_OUT, two dimensions represented by \b dim1 and \b dim2 should be removed from SHAPE_IN.
 *   Then, between the removed dimensions, the one has minor length should be appended at the end of SHAPE_OUT.
 *   For more detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Diagonal Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The \b input data types this operator supports are as follows. Data types such as int31, all complex are not supported.
 *   Data types of \b input and \b output must be the same.
 *   - input tensor: bool, int8, int16, int32, int64, half, float32, double
 *   - output tensor: bool, int8, int16, int32, int64, half, float32, double
 * - Other parameters only support int32.
 *   - \b offset: int32
 *   - \b dim1: int32
 *   - \b dim2: int32
 *
 * @par Scale Limitation
 * - \b dim1 and \b dim2 should be greater than -N-1 and less than N, where N is the dimension number of \b input.
 * - \b dim1 and \b dim2 should not be the same.
 *
 * @note
 * - Users should define output tensor correctly in \b output_desc, otherwise,  CNNL_STATUS_BAD_PARAM may be returned.
 * - \b offset has no scale limitation. But when absolute value of \b offset is greater than the minor one between the
 *   the lengths of \b dim1 and \b dim2 dimensions, \b output_desc SHAPE_OUT elements should be all 0 and the function
 *   returns 0 element tensor.
 * - Currently this function only supports \b offset = 0 and \b input tensor with the same length in all dimensions for
 *   trace computations.
 *
 * @par Requirements
 * - None
 *
 * @par Example
    @verbatim
    input shape: [2, 3, 4, 5]
    offset: 0
    dim1: 1
    dim2: 2
    output shape: [2, 5, 3]
    input: tensor([[[[  0,   1,   2,   3,   4],
                     [  5,   6,   7,   8,   9],
                     [ 10,  11,  12,  13,  14],
                     [ 15,  16,  17,  18,  19]],

                    [[ 20,  21,  22,  23,  24],
                     [ 25,  26,  27,  28,  29],
                     [ 30,  31,  32,  33,  34],
                     [ 35,  36,  37,  38,  39]],

                    [[ 40,  41,  42,  43,  44],
                     [ 45,  46,  47,  48,  49],
                     [ 50,  51,  52,  53,  54],
                     [ 55,  56,  57,  58,  59]]],


                   [[[ 60,  61,  62,  63,  64],
                     [ 65,  66,  67,  68,  69],
                     [ 70,  71,  72,  73,  74],
                     [ 75,  76,  77,  78,  79]],

                    [[ 80,  81,  82,  83,  84],
                     [ 85,  86,  87,  88,  89],
                     [ 90,  91,  92,  93,  94],
                     [ 95,  96,  97,  98,  99]],

                    [[100, 101, 102, 103, 104],
                     [105, 106, 107, 108, 109],
                     [110, 111, 112, 113, 114],
                     [115, 116, 117, 118, 119]]]])
    output: tensor([[[  0,  25,  50],
                    [  1,  26,  51],
                    [  2,  27,  52],
                    [  3,  28,  53],
                    [  4,  29,  54]],

                   [[ 60,  85, 110],
                    [ 61,  86, 111],
                    [ 62,  87, 112],
                    [ 63,  88, 113],
                    [ 64,  89, 114]]])
   @endverbatim
 *
 * @par Reference
 * - pytorch.org/docs/stable/generated/torch.diagonal.html
 */
cnnlStatus_t CNNL_WIN_API cnnlDiagonal(const cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void* input,
                                       const int32_t offset,
                                       const int32_t dim1,
                                       const int32_t dim2,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void* output);

/******************************************************************************
 * CNNL Data Structure: DiagPart
 ******************************************************************************/
/*!
 * @brief Enumeration variables describing the \b align modes that can be used to define
 * how superdiagonals and subdiagonals are aligned in the ::cnnlDiagPart function.
 *
 */
typedef enum {
  CNNL_DIAG_PART_RIGHT_LEFT = 0,
  /*!< aligns superdiagonals to the right and subdiagonals to the left for ::cnnlDiagPart. */
  CNNL_DIAG_PART_LEFT_RIGHT = 1,
  /*!< aligns superdiagonals to the left and subdiagonals to the right for ::cnnlDiagPart. */
  CNNL_DIAG_PART_LEFT_LEFT = 2,
  /*!< aligns superdiagonals to the left and subdiagonals to the left for ::cnnlDiagPart. */
  CNNL_DIAG_PART_RIGHT_RIGHT = 3
  /*!< aligns superdiagonals to the right and subdiagonals to the right for ::cnnlDiagPart. */
} cnnlDiagPartMode_t;

// Group:DiagPart
/*!
 * @brief Returns a batched diagonal tensor with the \b lower_diag_index to \b upper_diag_index
 * diagonals of \b input. This operation may need padding with \b padding_value and is usually
 * used in Conformer network for Tensorflow.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the diag_part
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  lower_diag_index
 *   Input. A host pointer to the \b lower_diag_index that represents the lower bound of the diagonal
 *   index of \b output.
 * @param[in]  upper_diag_index
 *   Input. A host pointer to the \b upper_diag_index that represents the upper bound of the diagonal
 *   index of \b output.
 * @param[in]  padding_value
 *   Input. A host pointer to the \b padding_value that holds the constant value to be added for the
 *   diagonal of \b output with padding operation.
 * @param[in]  align
 *   Input. The padding alignment, see ::cnnlDiagPartMode_t for more details.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 *  - Data types of input tensor \b input, \b padding_value and output tensor \b output must be the same.
 *   The supported data types are as follows:
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - lower_diag_index: int32.
 *   - upper_diag_index: int32.
 *   - padding_value: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \b input should be no less than 2 and no more than CNNL_DIM_MAX.
 *   - \b lower_diag_index > \p -rows && \b lower_diag_index < cols. \p rows represents the length of the
 *   second to last dimension of the input tensor. \p cols represents the length of the last dimension of
 *   the input tensor.
 *   - \b upper_diag_index > \p -rows && \b upper_diag_index < cols. \p rows represents the length of the
 *   second to last dimension of the input tensor. \p cols represents the length of the last dimension of
 *   the input tensor.
 *   - \b lower_diag_index <= \b upper_diag_index.
 *   - If \b lower_diag_index == \b upper_diag_index, \p rank_output == \p rank_input - 1.
 *   \p rank_output represents the number of dimensions of the output tensor. \p rank_input represents the
 *   number of dimensions of the input tensor.
 *   - If \b lower_diag_index < \b upper_diag_index, \p rank_output == \p rank_input. \p rank_output represents
 *   the number of dimensions of the output tensor. \p rank_input represents the number of dimensions of the
 *   input tensor.
 *   - \p max_diag_len = min(\p rows + min(\b upper_diag_index, 0), \p cols - max(\b lower_diag_index, 0)).
 *   \p max_diag_len is the maximum length of the batched diagonals of the output tensor. \rows represents
 *   the length of the second to last dimension of the input tensor. \p cols represents the length of the
 *   last dimension of the input tensor.
 *   - \p output[rank_output - 1] = \p max_diag_len. \p rank_output represents the number of dimensions of
 *   the output tensor. \p max_diag_len is the maximum length of the batched diagonals of the output tensor.
 *   - \p num_diags = \b upper_diag_index - \b lower_diag_index + 1. \p num_diags represents the number of
 *   diagonals of each batch of the output tensor.
 *   - If \p rank_input > 2 and \p rank_output == \p rank_input, output[\p rank_output - 2] == \p num_diags.
 *   \p rank_input represents the number of dimensions of the input tensor. \p rank_output represents the
 *   number of dimensions of the output tensor. \p num_diags represents the number of diagonals of each batch
 *   of the output tensor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of diag_part is as follows:
   @verbatim
   input: tensor([[[[0., 1., 2.],
                    [3., 4., 5.],
                    [6., 7., 8.],
                    [9., 0., 1.]]]])
   lower_diag_index: -2
   upper_diag_index: 2
   padding_value: 0
   align: CNNL_DIAG_PART_RIGHT_LEFT
   output: tensor([[[[0., 0., 2.]
                     [0., 1., 5.]
                     [0., 4., 8.]
                     [3., 7., 1.]
                     [6., 0., 0.]]]])
   @endverbatim
 *
 * @par Reference
 * - https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part
 * - https://www.tensorflow.org/versions/r2.5/api_docs/python/tf/linalg/diag_part
 * - https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/ops/array_ops.py
 */

cnnlStatus_t CNNL_WIN_API cnnlDiagPart(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const int lower_diag_index,
                                       const int upper_diag_index,
                                       const void *padding_value,
                                       const cnnlDiagPartMode_t align,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
// Group: FFT
/*!
 *  @brief Creates a descriptor pointed by \b fft_plan for the FFT operation, and allocates memory
 *  for holding the information about the FFT operation. The information is defined in ::cnnlFFTPlan_t.
 *
 *  @param[in] fft_plan
 *    Input. Pointer to the FFT descriptor that holds information about the FFT operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 *  @par API Dependency
 *  - After calling this function, you can call the ::cnnlMakeFFTPlanMany function to initialize and set the
 *    information to the created descriptor.
 *  - You need to call the ::cnnlDestroyFFTPlan to destroy the descriptor.
 *    Otherwise, the memory leak may occur.
 *
 *  @note
 *  - This function only supports 1-D FFT currently. 2-D FFT and 3-D FFT
 *    will be supported in the future.
 *  - When the data type of input is float or complex_float, the 1-D FFT length should be equal to:
      length = \f$base * 2^{m}\f$ , and the base should be less than or equal to 4096.
 *  - When the data type of input is half or complex_half, the 1-D FFT length should be equal to:
      length = \f$2^{m}\f$.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example
 *  - None.
 */
cnnlStatus_t cnnlCreateFFTPlan(cnnlFFTPlan_t *fft_plan);

// Group: FFT
/*!
 *  @brief Initializes the FFT descriptor pointed by \b fft_plan that is previously created
 *  with the ::cnnlCreateFFTPlan function, and sets the information about the
 *  tensor descriptors of input tensor and output tensor, the rank of FFT, and the FFT size on each
 *  dimension.
 *
 *  This function also gets the size of MLU memory buffers for FFT execution, including \b reservespace_size and
 *  \b workspace_size. The size of extra workspace is based on the given information of the
 *  \b fft_plan.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *    in the FFT operation. For detailed information, see ::cnnlHandle_t.
 *  @param[in] fft_plan
 *    Input. The descriptor of FFT. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] input_desc
 *    Input. The descriptor of input signals. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] output_desc
 *    Input. The descriptor of output signals. For detailed information,
 *    see ::cnnlTensorDescriptor_t.
 *  @param[in] rank
 *    Input. The dimensionality of the FFT operation. It can be 1-D, 2-D or 3-D.
 *  @param[in] n
 *    Input. An array of size \b rank describing the FFT size of each dimension. n[0]
 *    is the size of the outermost dimension and n[rank - 1] is the innermost dimension
 *    of FFT operation. If n[i] is greater than the size of input on dimension i, the input
 *    signal will be zero-padded on that dimension. Otherwise, input signal is trimmed
 *    on the dimension i.
 *  @param[out] reservespace_size
 *    Output. The size of the extra reserved space in bytes that needs to be used in FFT operation.
 *  @param[out] workspace_size
 *    Output. The size of the extra workspace in bytes that needs to be used in FFT operation.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_NOT_INITIALIZED
 *
 *  @par Data Type
 *  - The supported data types of \b input and \b output tensors are as follows:
 *    - real-to-complex fft:
 *      - half(input offchip)-complex_half(output offchip)-int16(input onchip)
 *      - float(input offchip)-complex_float(output offchip)-int31(input onchip)
 *    - complex-to-real fft:
 *      - complex_half(input offchip)-half(output offchip)-int16(input onchip)
 *      - complex_float(input offchip)-float(output offchip)-int31(input onchip)
 *    - complex-to-complex fft:
 *      - complex_half(input offchip)-complex_half(output offchip)-int16(input onchip)
 *      - complex_float(input offchip)-complex_float(output offchip)-int31(input onchip)
 *  - On MLU300 series or above, this function also supports the combinations of
 *    data types as follows:
 *    - real-to-complex fft:
 *      - half(input offchip)-complex_half(output offchip)-half(input onchip)
 *      - float(input offchip)-complex_float(output offchip)-float(input onchip)
 *    - complex-to-real fft:
 *      - complex_half(input offchip)-half(output offchip)-half(input onchip)
 *      - complex_float(input offchip)-float(output offchip)-float(input onchip)
 *    - complex-to-complex fft:
 *      - complex_half(input offchip)-complex_half(output offchip)-half(input onchip)
 *      - complex_float(input offchip)-complex_float(output offchip)-float(input onchip)
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan function to
 *  create an FFT descriptor firstly, call the ::cnnlSetTensorDescriptor or
 *  ::cnnlSetTensorDescriptorEx function to set the input and output tensor descriptor,
 *  and then call the ::cnnlSetTensorDescriptorOnchipDataType to set the onchip data type
 *  of input tensor descriptor.
 *
 *  @note
 *  - The advanced data layout parameters, (i/o)nembed, (i/o)istride, (i/o)idist, are set through
 *    ::cnnlSetTensorDescriptorEx. If stride information is not needed, you can set the simple data layout
 *    through ::cnnlSetTensorDescriptor.
 *  - The dimension size of input / output should be equal to \b rank or \b rank + 1. In the former case,
 *    the batch size is considered as 1. Otherwise, the outermost dimension is the batch size.
 *  - For real-to-complex FFTs, the innermost dimension of FFT length and output arrays are not the same.
 *    For a x-length 1-D real-to-complex FFT, the output is x/2 + 1 complex numbers (the non-redundant outputs).
 *    For a N-D real-to-complex FFT with n=[z, y, x], the output shape will be [z, y, x/2+1].
 *  - For complex-to-real FFTs, the input tensor only holds the non-redundant part of the Fourier coefficients.
 *    And the output tensor stores the real output values.
 *  - When n[0] is greater than 4096, the data type of input only supports float or complex_float.
 *
 *  @Scale Limitation
 *  - None.
 *
 *  @par Requirements
 *  - None.
 *
 */
cnnlStatus_t cnnlMakeFFTPlanMany(cnnlHandle_t handle,
                                 cnnlFFTPlan_t fft_plan,
                                 const cnnlTensorDescriptor_t input_desc,
                                 const cnnlTensorDescriptor_t output_desc,
                                 const int rank,
                                 const int n[],
                                 size_t *reservespace_size,
                                 size_t *workspace_size);

// Group: FFT
/*!
 *  @brief Bond the \b reservespace to the \b fft_plan. The size of reserved space can be derived
 *  through ::cnnlMakeFFTPlanMany.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *    ::cnnlExecFFT. For detailed information, see ::cnnlHandle_t.
 *  @param[in, out] fft_plan
 *    Input/Output. The descriptor of FFT. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] reservespace
 *    Input. Pointer to the MLU memory that is used as an extra memory space for saving
 *    intermediate results of FFT operation.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan function
 *  to create an FFT descriptor firstly, call the ::cnnlMakeFFTPlanMany function to set the
 *  FFT descriptor and get the size of reserved space, and then call the
 *  ::cnrtMalloc function to create MLU memory according to the rservespace_size given.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR
 */
cnnlStatus_t cnnlSetFFTReserveArea(cnnlHandle_t handle,
                                   cnnlFFTPlan_t fft_plan,
                                   void *reservespace);

// Group: FFT
/*!
 *  @brief Executes any FFT. In case of complex-to-real and real-to-complex
 *  transforms \b direction parameter is ignored. This function stores the Fourier coefficients
 *  in the output array. If the address of input and output are the same, an in-place FFT
 *  is adopted.
 *
 *  @param[in] handle
 *    Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues
 *    in the FFT execution. For detailed information, see ::cnnlHandle_t.
 *  @param[in] fft_plan
 *    Input. The plan for FFT execution. For detailed information, see ::cnnlFFTPlan_t.
 *  @param[in] input
 *    Input. Pointer to the MLU memory that stores the input tensor.
 *  @param[in] scale_factor
 *    Input. A float-point scalar used to multiply the FFT output.
 *  @param[in, out] workspace
 *    Input/Output. Pointer to the MLU memory that is used as an extra workspace for the
 *    ::cnnlExecFFT.
 *  @param[out] output
 *    Output. Pointer to the MLU memory that stores the output tensor.
 *  @param[in] direction
 *    Input. The transform direction: 0 means FFT forward and 1 means FFT inverse.
 *    Direction is ignored for real-to-complex and complex-to-real transforms.
 *
 *  @note
 *  - For in-place 1-D real-to-complex FFTs, the input is a batch of n real numbers, and the
 *    output is n/2 + 1 non-redundant complex numbers. This requires a padding of input array.
 *  - For in-place N-D real-to-complex FFTs, extra padding of the real-data array on the innermost
 *    dimension is necessary to accommodate the size of the complex-data output.
 *  - When \b input contains NaN/infinity and the input onchip data type of FFT is not quantized
 *    data type, the output is computed through the fft formula with computation rules of NaN or
 *    infinity based on IEEE754.
 *  - When \b input contains NaN/infinity and the input onchip data type of FFT is quantized
 *    data type, i.e. int31 or int16, the output will be unpredictable.
 *  - The range of \b input is recommended to be in the range of [-10, 10] with uniform
 *    distribution for higher precision.
 *  - The range of \b scale_factor is recommended to be in the range of [-1, 1] to avoid exceeding
 *    the data representation range.
 *  - Half data type of \b input is not recommended due to low precision. The first element of the
 *    FFT result is the sum of all input elements, and it is likely to overflow.
 *
 *  @par API Dependency
 *  - Before calling this function, you need to call the ::cnnlCreateFFTPlan
 *  function to create an FFT descriptor firstly, call the ::cnnlMakeFFTPlanMany
 *  function to set the FFT descriptor and the size of reserved space and work space,
 *  and then call the ::cnnlSetFFTReserveArea to bond the reservespace area to the descriptor.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_INTERNAL_ERROR
 */
cnnlStatus_t cnnlExecFFT(cnnlHandle_t handle,
                         const cnnlFFTPlan_t fft_plan,
                         const void *input,
                         const float scale_factor,
                         void *workspace,
                         void *output,
                         int direction);

// Group: FFT
/*!
 *  @brief Destroys an FFT plan \b fft_plan that is created with the
 *  ::cnnlCreateFFTPlan function.
 *
 *  The fft plan is defined in ::cnnlFFTPlan_t and holds the information about the
 *  FFT operation.
 *
 *  @param[in] fft_plan
 *    Input. The fft plan to be destroyed.
 *
 *  @par Return
 *  - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 *  @note
 *  - You need to call this function after calling the ::cnnlExecFFT.
 *    Otherwise, memory leak may occur.
 *
 *  @par Requirements
 *  - None.
 *
 *  @par Example.
 *  - None.
 */
cnnlStatus_t cnnlDestroyFFTPlan(cnnlFFTPlan_t fft_plan);

// Group:ReflectionPad
/*!
 * @brief Computes the gradients of reflection_pad.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the reflection_pad_backward
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_grad_desc
 *   Input. The descriptor of the \b x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x_grad
 *   Input. Pointer to the MLU memory that stores the \b x_grad tensor. \b x_grad is the output gradient of ReflectionPad.
 * @param[in]  paddings
 *   Input. Pointer to the host memory that stores the padding parameter. In 2D mode, the pointer holds the padding size to
 *   be added for certain dimensions of \b x_grad in the order of left, right, top and bottom. Positive and zero padding values
 *   represent the padding size and negative padding values are not supported now. In 1D mode, the pointer holds the padding
 *   size to be added for certain dimensions of \b x_grad in the order of left, right, padding values can be positive, zero
 *   and negative.
 * @param[out]  y_grad_desc
 *   Output. The descriptor of the \b y_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y_grad
 *   Output. Pointer to the MLU memory that stores the \b y_grad tensor. \b y_grad is the input gradient of ReflectionPad.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x_grad and output tensor
 *   \b y_grad. Data types of both tensors should be the same.
 *   - input tensor: half, float.
 *   - paddings: int32.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \b x_grad and output tensor \b y_grad must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NCHW(2D mode), \p CNNL_LAYOUT_NLC(1D mode).
 *   - output tensor: \p CNNL_LAYOUT_NCHW(2D mode), \P CNNL_LAYOUT_NLC(1D mode).
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - The number of dimensions of \b input equals to 3, and element number of \b paddings equals to 2 in 1D mode.
 *   - The number of dimensions of \b input equals to 4, and element number of \b paddings equals to 4 in 2D mode.
 *   - \p pad_top < \p y_h && \p pad_bottom < \p y_h. \p y_h represents the length of the H dimension in
 *   the output tensor. \p pad_top and \p pad_bottom represent the length of corresponding dimension in \b paddings
 *   respectively(2D mode).
 *   - \p pad_left < \p y_w and \p pad_right < \p y_w. \p y_w represents the length of the W dimension in
 *   the output tensor. \p pad_left and \p pad_right represent the length of corresponding dimension in \b paddings
 *   respectively(2D mode).
 *   - \p x_h = \p y_h + \p pad_top + \p pad_bottom && \p x_w = \p y_w + \p pad_left + \p pad_right. \p x_h represents
 *   the length of the H dimension in the input tensor. \p x_w represents the length of the W dimension in
 *   the input tensor(2D mode).
 *   - \p pad_left < \p y_l and \p pad_right < \p y_l. \p y_l represents the length of the L dimension in
 *   the output tensor. \p pad_left and \p pad_right represent the length of corresponding dimension in \b paddings
 *   respectively(1D mode).
 *   - \p x_l = \p y_l + \p pad_left + \p pad_right. \p x_l represents the length of the L dimension in the input tensor.
 *   (1D mode).
 *   - Negative padding values are not supported in 2D mode now and padding values can be positive, zero
 *   and negative in 1D mode.
 *
 * @note
 * - ReflectionPadBackward has 3 modes: 1D, 2D and 3D. When the dimension of input is equal to 3, the mode
 *   of ReflectionPadBackward is 1D; When the dimension of input is equal to 4, the mode of ReflectionPadBackward
 *   is 2D; When the dimension of input is equal to 5, the mode of ReflectionPadBackward is 3D.
 *   It only supports 1D and 2D now.
 * - The example of ReflectionPadBackward is as follows:
   @verbatim
   The following is an example of 2D mode:
     input: tensor([[[[1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]
                      [1., 1., 1., 1., 1., 1.]]]])
     output: tensor([[[[2., 1., 1., 2.],
                       [4., 2., 2., 4.],
                       [4., 2., 2., 4.],
                       [2., 1., 1., 2.]]]])
     paddings: array([1., 1., 1., 1.])
   The following is an example of 1D mode:
     input: tensor([[[1., 1., 1., 1., 1., 1.]]])
     output: tensor([[[1., 2., 2., 1.]]])
     paddings: array([1., 1., 1., 1.])
   @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlReflectionPadBackward(cnnlHandle_t handle,
                          const cnnlTensorDescriptor_t x_grad_desc,
                          const void *x_grad,
                          const int *paddings,
                          const cnnlTensorDescriptor_t y_grad_desc,
                          void *y_grad);

// Group:ReplicationPad
/*!
 * @brief Computes the gradients of replication_pad.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the replication_pad_backward operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in]  x_grad_desc
 *   Input. The descriptor of the \b x_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in]  x_grad
 *   Input. Pointer to the MLU memory that stores the \b x_grad tensor. \b x_grad is the output gradient of ReplicationPad.
 * @param[in]  paddings
 *   Input. Pointer to the host memory that stores the padding parameter which holds the padding size to be added for certain
 *   dimensions of \b x_grad in the order of left, right, top and bottom. Padding
 *   values represent the padding size.
 * @param[in]  y_grad_desc
 *   Input. The descriptor of the \b y_grad tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out]  y_grad
 *   Output. Pointer to the MLU memory that stores the \b y_grad tensor. \b y_grad is the input gradient of ReplicationPad.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x_grad and output tensor
 *   \b y_grad. Data types of both tensors should be the same.
 *   - input tensor: half, float.
 *   - paddings: int32.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensor \b x_grad and output tensor \b y_grad must be the same. The supported data
 *  layouts are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NLC or CNNL_LAYOUT_NHWC.
 *   - output tensor: \p CNNL_LAYOUT_NLC or CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must meet the following requirements:
 *   - For 1D, the number of dimensions of \b input equals to 3, and element number of \b paddings
 *   equals to 2.
 *   - \p x_w = y_w + pad_left + pad_right
 *   - For 2D, the number of dimensions of \b input equals to 4, and element number
 *   of \b paddings equals to 4.
 *   - \p x_h = y_h + pad_top + pad_bottom &&  x_w = y_w + pad_left + pad_right.
 *   \p x_h represents the length of the H dimension in the input tensor.
 *   \p x_w represents the length of the W dimension in the input tensor.
 *   \p y_h represents the length of the H dimension in the output tensor.
 *   \p y_w represents the length of the W dimension in the output tensor.
 *   \p pad_top and \p pad_bottom represent the length of corresponding dimension in \b paddings respectively.
 *   \p pad_left and \p pad_right represent the length of corresponding dimension in \b paddings respectively.
 *
 * @note
 * - ReplicationPadBackward has 3 modes: 1D, 2D and 3D. When the dimension of input is equal to 3, the mode
 *   of ReplicationPadBackward is 1D; When the dimension of input is equal to 4, the mode of ReplicationPadBackward
 *   is 2D; When the dimension of input is equal to 5, the mode of ReplicationPadBackward is 3D.
 *   It only supports 1D and 2D now.
 * @par Example
 *  - The example of ReplicationPadBackward is as follows:
   @verbatim
     1D:
     input: tensor([[[1., 1., 1., 1., 1., 1.]]])
     paddings:(1, 1)
     output: tensor([[[2., 1., 1., 2.]]])
     2D:
     input: tensor([[[[1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.],
                      [1., 1., 1., 1., 1., 1.]]]])
     paddings: (0, 1, 2, 3)
     output: tensor([[[[3., 3., 3., 3., 6.],
                       [1., 1., 1., 1., 2.],
                       [1., 1., 1., 1., 2.],
                       [1., 1., 1., 1., 2.],
                       [4., 4., 4., 4., 8.]]]])
   @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlReplicationPadBackward(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t x_grad_desc,
                           const void* x_grad,
                           const int padding[],
                           const cnnlTensorDescriptor_t y_grad_desc,
                           void* y_grad);
// Group:VarForward
/*!
* @brief Calculates the variance for each row of the input tensor in a given
* dimension.
*
* @param[in] handle
*   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
*   queues in the var forward operation. For detailed information,
*   see ::cnnlHandle_t.
* @param[in] dim
*   Input. The dimension of \b input to reduce.
* @param[in] unbiased
*   Input. Whether to use the unbiased estimation or not. If unbiased is false, then the
*   variance will be calculated via the biased estimator.
*   Otherwise, Bessel's correction will be used.
* @param[in] input_desc
*   Input. The descriptor of the \b input tensor. For detailed information,
*   see ::cnnlTensorDescriptor_t.
* @param[in] input
*   Input. Pointer to the MLU memory that stores the input tensor.
* @param[in] output_desc
*   Input. The descriptor of the \b output tensor. For detailed information,
*   see ::cnnlTensorDescriptor_t.
* @param[out] output
*   Output. Pointer to the MLU memory that stores the output tensor.
*
* @par Return
* - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ARCH_MISMATCH
*
* @par Data Type
* - This function supports the following data types for \b unbiased, \b dim,
*   input tensor \b input and output tensor \b output. Data types of both tensors should be the
*   same.
* - \b unbiased: bool
* - \b dim: int32
* - \b input: float, half
* - \b output: float, half
*
* @par Reference
* - https://pytorch.org/docs/1.6.0/generated/torch.var.html#torch.var
*
* @par API Dependency
* - None.
*
* @par Example
*  The example of the var forward operation is as follows:
  @verbatim
  input: a tensor with shape by 4 * 4  --> [[-0.3567, 1.7385, -1.3042, 0.7423,
                                             [ 1.3436, -0.1015, -0.9834, -0.8438],
                                             [ 0.6056, 0.1089, -0.3112, -1.4085],
                                             [-0.7700, 0.6074, -0.1469, 0.7777]]
   param: dim = 1, unbiased = True

   Then get the output:

   output: a tensor by 4 * 1             --> [ 1.7444, 1.1363, 0.7356, 0.5112]
  @endverbatim
*
*/
cnnlStatus_t CNNL_WIN_API cnnlVarForward(cnnlHandle_t handle,
                                        int dim,
                                        bool unbiased,
                                        const cnnlTensorDescriptor_t input_desc,
                                        const void *input,
                                        const cnnlTensorDescriptor_t output_desc,
                                        const void *output);

// Group:RepeatInterleave
/*!
 * @brief Returns a new tensor with the same shape as \b input by repeating
 * elements of the \b input tensor on the specied dimension.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlRepeatInterleave. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] repeats_desc
 *   Input. The descriptor of the repeats tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] repeats
 *   Input. Pointer to the MLU memory that stores the repeats tensor. It means
 *   the number of repetitions for each element.
 * @param[in] dim
 *   Input. An integer value which is the dimension of the input tensor along
 *   which the elements to be repeated.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "RepeatInterleave operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of \b input tensor and \b output tensor must be the same.
 * - The supported data types of input, repeats, and output tensors are as follows:
 *   - input: uint8, int8, int16, int32, int64, half, float, double.
 *   - repeats: int32.
 *   - output: uint8, int8, int16, int32, int64, half, float, double.
 *
 * @par Scale Limitation
 * - The parameters must meet the following conditions:
 *   - \b dim must be smaller than the number of dimension of \b input and not less than
 *     negative value of the number of dimension of \b input.
 *   - \b repeats must be 1-D tensor.
 *   - The 0-th dimension of \b repeats must be equal to the dim-th dimension of \b input
 *     or 1.
 *   - The number of dimensions of \b output and \b input should be the same.
 *   - The shape of \b input must be the same as the shape of \b output, except the
 *     dimension \b dim.
 *   - The dim-th dimension of \b output must be equal to the product of the first value
 *     of \b repeats and the dim-th dimension of \b input when the 0-th dimension of
 *     \b repeat is equal to 1.
 *   - The dim-th dimension of \b output and the sum of \b repeats must be equal when
 *     the 0-th dimension of \b repeat is greater than 1.
 *   - The value of \b repeats should be greater than or equal to 0.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html
 * - https://github.com/pytorch/pytorch/blob/1.6/aten/src/ATen/native/Repeat.cpp
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/Repeat.cpp
*/
cnnlStatus_t CNNL_WIN_API cnnlRepeatInterleave(cnnlHandle_t handle,
                                               const cnnlTensorDescriptor_t input_desc,
                                               const void *input,
                                               const cnnlTensorDescriptor_t repeats_desc,
                                               const void *repeats,
                                               const int32_t dim,
                                               const cnnlTensorDescriptor_t output_desc,
                                               void *output);

/*****************************************************************************
 * Cambricon CNNL OP: grid_sample
 * ***************************************************************************/
/*!
 * @brief Enumeration variables describing which padding mode to be used when the grid value
 * is out of the coordinate range. It is used in the ::cnnlSetGridSampleDescriptor.
 * The coordinate value in the grid tensor is normalized by the input dimensions,
 * therefore, the grid value should be in the range of [-1, 1]. If the grid tensor has values
 * outside the range of [-1, 1], the corresponding outputs are handled as defined
 * by padding mode.
 */
typedef enum {
  CNNL_GRIDSAMPLE_PADDING_ZEROS = 0,
  /*!< A type of padding mode, which uses zero value for out-of-bound grid locations. */
  CNNL_GRIDSAMPLE_PADDING_BORDER  = 1,
  /*!< A type of padding mode, which uses border values for out-of-bound grid locations. */
  CNNL_GRIDSAMPLE_PADDING_REFLECTION  = 2
  /*!< A type of padding mode, which uses values at locations reflected by the border
   * for out-of-bound grid locations.
   */
} cnnlGridSamplePaddingMode_t;

// Group:GridSample
/*!
 * @brief Creates a descriptor pointed by \b grid_sample_desc for ::cnnlGridSampleForward
 * operation, and allocates memory for holding the information about the operation.
 * The information is defined in ::cnnlGridSampleDescriptor_t.
 *
 * @param[out] grid_sample_desc
 *   Output. A host pointer to the ::cnnlGridSampleForward descriptor
 *   that holds information about the operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_NOT_INITIALIZED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetGridSampleDescriptor function
 *   to initialize and set the information to the descriptor.
 * - You need to call the ::cnnlDestroyGridSampleDescriptor function at the end
 *   to destroy the descriptor.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlCreateGridSampleDescriptor(cnnlGridSampleDescriptor_t *grid_sample_desc);

// Group:GridSample
/*!
 * @brief Initializes the ::cnnlGridSampleForward operation descriptor \b grid_sample_desc
 * that is previously created with the ::cnnlCreateGridSampleDescriptor function,
 * and sets the information about the ::cnnlGridSampleForward operation to the descriptor
 * \b grid_sample_desc. The information includes
 * the interpolation mode, padding mode and align_corners mode.
 *
 * @param[in] grid_sample_desc
 *   Input. The descriptor of the ::cnnlGridSampleForward operation.
 *   For detailed information, see ::cnnlGridSampleDescriptor_t.
 * @param[in] interp_mode
 *   Input. The interpolation mode, for detailed information, see ::cnnlInterpMode_t.
 * @param[in] padding_mode
 *   Input. The padding mode, for detailed information, see ::cnnlGridSamplePaddingMode_t.
 * @param[in] align_corners
 *   Input. Boolean variable determines the method to align 4 corner pixels between output
 *   and original input images. Generally, pixels of input and output images are considered to be
 *   squares. If \b align_corners is set to true, the input and output images are aligned by the
 *   center points of 4 corner pixels. Otherwise, the input and output images are aligned
 *   by the upper-left corner points of the corresponding corner pixels.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlSetGridSampleDescriptor(cnnlGridSampleDescriptor_t grid_sample_desc,
                                         const cnnlInterpMode_t interp_mode,
                                         const cnnlGridSamplePaddingMode_t padding_mode,
                                         const bool align_corners);

// Group:GridSample
/*!
 * @brief Destroys a ::cnnlGridSampleForward descriptor \b grid_sample_desc that is
 * previously created with the ::cnnlCreateGridSampleDescriptor function.
 *
 * The descriptor is defined in ::cnnlGridSampleDescriptor_t and holds the
 * information about the ::cnnlGridSampleForward operation.
 *
 * @param[in] grid_sample_desc
 *   Input. The ::cnnlGridSampleForward descriptor to be destroyed.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - After a grid_sample operation descriptor is created using ::cnnlCreateGridSampleDescriptor,
 *   if it is not used any more, this function should be called to destroy the descriptor,
 *   otherwise, the memory leak may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t
CNNL_WIN_API cnnlDestroyGridSampleDescriptor(cnnlGridSampleDescriptor_t grid_sample_desc);

// Group:GridSample
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlGridSampleForward operation.
 *
 * The size of extra workspace is based on the given information of the operation,
 * including the input tensor descriptor \b input_desc, the grid tensor descriptor \b grid_desc,
 * and the output tensor descriptor \b output_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   index_copy operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grid_desc
 *   Input. The descriptor of the grid tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGridSampleForward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGridSampleForwardWorkspaceSize(cnnlHandle_t handle,
                                      const cnnlTensorDescriptor_t input_desc,
                                      const cnnlTensorDescriptor_t grid_desc,
                                      const cnnlTensorDescriptor_t output_desc,
                                      size_t *workspace_size);

// Group:GridSample
/*!
 * @brief Using the coordinate value in the grid, and the pixel value of the coordinate in the
 * input tensor, computes the corresponding pixel value of output tensor. An interpolation
 * algorithm is selected in the computing.
 *
   @verbatim
       ___________________
      |input              |
      |                   |
      |         _         |
      |        |_|--------+-----------------
      |      (x0, y0)     |                |
      |                   |                |
      |                   |                |
      |___________________|                |
                                           |
      ---------------------                |
      |grid  _           y|                |
      |     |_|y0         |                |
    --------------------- |         -------V-------------
    |grid  _           x| |         |      _      output|
    |     |_|x0         | |         |     |_|           |
    |      |            | |         |      ^            |
    |      |            | |         |      |            |
    |      |            |_|         |      |            |
    |      |            |           |      |            |
    |      |            |           |      |            |
    |______|____________|           |______|____________|
           |_______________________________|
   @endverbatim
 *
 * As shown in the schematic diagram, given an input tensor with shape [n, h_in, w_in, c],
 * and a grid tensor with shape [n, h_out, w_out, 2], the output tensor will have shape
 * [n, h_out, w_out, c]. For each output pixel `c_nhw` , get the location information from
 * grid tensor pixel `xy_nhw`. The corresponding pixel of grid has two numbers, which indicates
 * the coordinate [x, y] in the input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grid_sample_desc
 *   Input. The grid_sample descriptor. For detailed information,
 *   see ::cnnlGridSampleDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] grid_desc
 *   Input. The descriptor of the grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grid
 *   Input. Pointer to the MLU memory that stores the grid tensor.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGridSampleForward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetGridSampleForwardWorkspaceSize function.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "GridSampleForward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors and out tensor. <b>Note
 *   that The two input(input, grid) tensors and output tensor should have the same data type.</b>
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensors \b input and \b grid, and output tensor \b output
 *   must be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The tensor shape must meet the following requirements:
 *   - The N-dimension of input tensor must be equal to N-dimension of grid and output;
 *   - The channel dimension of grid tensor must be 2;
 *   - The channel dimension of input tensor must be equal to the channel dimension of output;
 *   - The height and width dimensions of grid must be equal to those of output tensor.
 *
 * @note
 * - The operation does not support NaN/infinity in input data.
 * - Currently, only 4-dimensions input is supported.
 * - The input tensor with stride feature is supported while the output tensor
 *   with stride feature is not supported now.
 * - Only supports \p CNNL_INTERP_BILINEAR interpolation mode.
 * - Only supports \p CNNL_GRIDSAMPLE_PADDING_ZEROS padding mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/GridSample.h
 */
cnnlStatus_t CNNL_WIN_API cnnlGridSampleForward(cnnlHandle_t handle,
                                                const cnnlGridSampleDescriptor_t grid_sample_desc,
                                                const cnnlTensorDescriptor_t input_desc,
                                                const void *input,
                                                const cnnlTensorDescriptor_t grid_desc,
                                                const void *grid,
                                                const cnnlTensorDescriptor_t output_desc,
                                                void *output,
                                                void *workspace,
                                                size_t workspace_size);

// Group:GridSampleBackward
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlGridSampleBackward operation.
 *
 * The size of extra workspace is based on the given information of the operation,
 * including the \b grad_output tensor descriptor \b grad_output_desc, the \b input tensor
 * descriptor \b input_desc, and the \b grid tensor descriptor \b grid_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   GridSampleBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the \b grad_output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the \b input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] grid_desc
 *   Input. The descriptor of the \b grid tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlGridSampleBackward operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API
cnnlGetGridSampleBackwardWorkspaceSize(const cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t grad_output_desc,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const cnnlTensorDescriptor_t grid_desc,
                                       size_t *workspace_size);

// Group:GridSampleBackward
/*!
 * @brief Implements the backward propagation for the grid_sample function.
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grid_sample_desc
 *   Input. The descriptor of grid_sample operation. For detailed information,
 *   see ::cnnlGridSampleDescriptor_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of \b grad_output tensor. It should not be NULL.
 *   For detailed information, see::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the gradient tensor.
 *   \b grad_output is the input gradient of cnnlGridSampleBackward.
 * @param[in] input_desc
 *   Input. The descriptor of \b input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the \b input tensor.
 * @param[in] grid_desc
 *   Input. The descriptor of \b grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grid
 *   Input. Pointer to the MLU memory that stores the \b grid tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlGridSampleBackward operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetGridSampleBackwardWorkspaceSize function.
 * @param[in] grad_input_desc
 *   Input. The descriptor of \b grad_input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the \b grad_input tensor.
 *   \b grad_input is the calculated gradient result of \b input.
 * @param[in] grad_grid_desc
 *   Input. The descriptor of \b grad_grid tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] grad_grid
 *   Output. Pointer to the MLU memory that stores the \b grad_grid tensor.
 *   \b grad_grid is the calculated gradient result of \b grid.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "GridSampleBackward Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input and output tensor. <b>Note that the
 *   three input(grad_output, input, grid) tensors and two output(grad_input, grad_grid)
 *   tensors should have the same data type.</b>
 *   - input tensors: half, float.
 *   - output tensors: half, float.
 *
 * @par Data Layout
 * - Data layouts of input tensors \b grad_output, \b input and \b grid, and output tensors
 *   \b grad_input and \b grad_grid must be \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 *   - The N-dimension of input tensor must be equal to N-dimension of grad_output, grid,
 *     grad_input and grad_grid.
 *   - The channel dimension of grid and grad_grid tensor must be 2.
 *   - The channel dimension of input tensor must be equal to the channel dimension of grad_output
 *     and grad_input tensor.
 *   - The height and width dimensions of grid tensor must be equal to those of grad_grid and
 *     grad_output tensor.
 *   - The height and width dimensions of input tensor must be equal to those of grad_input tensor.
 *
 * @note
 * - The operation does not support NaN/infinity in input data.
 * - Currently, only 4-dimensions input is supported.
 * - The input tensor with stride feature is supported while the output tensor
 *   with stride feature is not supported now.
 * - Only supports \p CNNL_INTERP_BILINEAR interpolation mode.
 * - Only supports \p CNNL_GRIDSAMPLE_PADDING_ZEROS padding mode.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/GridSampler.cu
 */
cnnlStatus_t CNNL_WIN_API
cnnlGridSampleBackward(const cnnlHandle_t handle,
                       const cnnlGridSampleDescriptor_t grid_sample_desc,
                       const cnnlTensorDescriptor_t grad_output_desc,
                       const void *grad_output,
                       const cnnlTensorDescriptor_t input_desc,
                       const void *input,
                       const cnnlTensorDescriptor_t grid_desc,
                       const void *grid,
                       void *workspace,
                       size_t workspace_size,
                       const cnnlTensorDescriptor_t grad_input_desc,
                       void *grad_input,
                       const cnnlTensorDescriptor_t grad_grid_desc,
                       void *grad_grid);

// Group:RoiAlignRotated
/*!
 * @brief According to the \b rois with rotation, extracts the corresponding \b features information to \b output
 * by bilinear interpolation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignRotatedForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] features_desc
 *   Input. The descriptor of the features tensor.
 * @param[in] features
 *   Input. Pointer to the MLU memory that stores the features tensor. The shape of \b features is [batch_num, H, W, C].
 * @param[in] rois_desc
 *   Input. Descriptor of rois tensor, containing dimension and the layout of rois.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores rois tensors. \b rois[i] consists of [batch_id, x, y, w, h, theta], where \p batch_id
 *   is the ID of the batch, \p x and \p y are the coordinate of center point, \p w and \p h are the width and height of rois, and \p theta
 *   is the rotated angle.
 * @param[in] pooled_height
 *   Input. The height of output.
 * @param[in] pooled_width
 *   Input. The width of output.
 * @param[in] sample_ratio
 *   Input. The number of sampling points in the bin used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the roi by 0.5 pixel. If the value of \b aligned
 *   is set to true, the roi is shifted by 0.5. If the value of \b aligned is set to false, the roi is not shifted.
 * @param[in] clockwise
 *   Input. A boolean value which determines whether the rotation of roi is clockwise.
 * @param[out] output_desc
 *   Input.  Descriptor of output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "RoiAlignRotatedForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b features, \b rois,
 *   and output tensor \b output. Data type of all tensors should be the same.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b features, \b rois, and \b output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \b features tensor and \b output tensor should be 4D.
 * - The half data type is not recommended due to low precision.
 * - Size of the lowest dimension of \b features tensor and \b output tensor should be the same.
 * - The \b rois tensor should be 2-D array.
 * - Size of the highest dimension of \b output tensor and \b rois tensor should be the same.
 * - The shape of \b rois should be [rois_num, 6].
 * - \p batch_id should be in the range of [0, \p batch_num - 1], \p x and \p y should be greater than or
 *   equal to 0 and less than \p H and \p W respectively. Both of \p h and \p w should be greater than zero
 *   and less than \p H and \p W respectively.
 * - \p spatial_scale and \p sample_ratio should not be less than zero.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - NaN and infinity are not supported for all parameters in \b boxes, except for the \p x and \p y parameters
 *   that support infinity.
 * - The values of the parameters \p x , \p y, \p w and \p h in \b rois multiplied by \p spatial_scale cannot exceed
 *   the range that can be represented by the parameter type.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_rotated_forward operation is as follows:
     @verbatim
     input two arrays by 1 * 3 * 3 * 1 and 1 * 6 --> input: [[[[1.0],[1.0],[1.0]],[[1.0],[1.0],[1.0]],[[1.0],[1.0],[1.0]]]]

     --> rois: [[0.0, 1.0, 1.0, 1.0, 1.0, 0.0]]

     param:
            pooled_height: 2, pooled_width: 2, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false, clockwise: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[1],[1]],[[1],[1]]]]
     @endverbatim
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/roi_align_rotated.py
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlignRotatedForward(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t features_desc,
                           const void* features,
                           const cnnlTensorDescriptor_t rois_desc,
                           const void* rois,
                           const int pooled_height,
                           const int pooled_width,
                           const int sample_ratio,
                           const float spatial_scale,
                           const bool aligned,
                           const bool clockwise,
                           const cnnlTensorDescriptor_t output_desc,
                           void* output);

// Group:RoiAlignRotated
/*!
 * @brief Computes the gradients of feature map \b bottom_grad based on the input \b top_grad and
 * \b rois to perform the backpropagation of the ::cnnlRoiAlignRotatedForward operator.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlRoiAlignRotatedBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] top_grad_desc
 *   Input. The descriptor of the gradient tensor in the backpropagation process.
 * @param[in] top_grad
 *   Input. Pointer to the MLU memory that stores the top_grad tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of rois tensor, containing dimension and the layout of rois.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores rois tensors. \b rois[i] consists of [batch_id, x, y, w, h, theta], where \p batch_id
 *   is the ID of the batch, \p x and \p y are the coordinate of center point, \p w and \p h are the width and height of rois, and \p theta
 *   is the rotated angle.
 * @param[in] pooled_height
 *   Input. The height of output.
 * @param[in] pooled_width
 *   Input. The width of output.
 * @param[in] sample_ratio
 *   Input. The number of sampling points in the bin used to compute the output.
 * @param[in] spatial_scale
 *   Input. The spatial scale of each regions of interest in the output.
 * @param[in] aligned
 *   Input. A boolean value which determines whether to shift the roi by 0.5 pixel. If the value of \b aligned
 *   is set to true, the roi is shifted by 0.5. If the value of \b aligned is set to false, the roi is not shifted.
 * @param[in] clockwise
 *   Input. A boolean value which determines whether the rotation of roi is clockwise.
 * @param[in] bottom_grad_desc
 *   Input. Descriptor of the gradient tensor of the origin feature map.
 * @param[out] bottom_grad
 *   Output. Pointer to the MLU memory that stores the bottom_grad tensor. The shape of bottom_grad is [batch_num, H, W, C].
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "RoiAlignRotatedBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b top_grad, \b rois,
 *   and output tensor \b bottom_grad. Data type of all tensors should be the same.
 *   - top_grad tensor: half, float.
 *   - rois tensor: half, float.
 *   - bottom_grad tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b top_grad, \b rois, and \b bottom_grad are as follows:
 *   - top_grad tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - bottom_grad tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \b bottom_grad tensor and \b top_grad tensor should be 4D.
 * - The half data type is not recommended due to low precision.
 * - Size of the lowest dimension of \b bottom_grad tensor and \b top_grad tensor should be the same.
 * - The \b rois tensor should be 2-D array.
 * - Size of the highest dimension of \b top_grad tensor and \b rois tensor should be the same.
 * - \p batch_id should be in the range of [0, \p batch_num - 1], \p x and \p y should be greater than or
 *   equal to 0 and less than \p H and \p W respectively. Both of \p h and \p w should be greater than zero
 *   and less than \p H and \p W respectively.
 * - \p spatial_scale and \p sample_ratio should not be less than zero.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - NaN and infinity are not supported for all parameters in \b boxes, except for the \p x and \p y parameters
 *   that support infinity.
 * - The values of the parameters \p x , \p y, \p w and \p h in \b rois multiplied by \p spatial_scale cannot exceed
 *   the range that can be represented by the parameter type.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the roi_align_rotated_backward operation is as follows:
     @verbatim
     input two arrays by 1 * 1 * 1 * 1 and 1 * 6 --> input: [[[[1.0]]]]

     --> rois: [[0.0, 0.0, 0.0, 1.0, 1.0, 0.0]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 2, aligned: false, clockwise: false

     output array by 1 * 2 * 2 * 1 -->
         output: [[[[0.25], [0.25]], [[0.25], [0.25]]]]
     @endverbatim
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/roi_align_rotated.py
 */
cnnlStatus_t CNNL_WIN_API
cnnlRoiAlignRotatedBackward(cnnlHandle_t handle,
                            const cnnlTensorDescriptor_t top_grad_desc,
                            const void* top_grad,
                            const cnnlTensorDescriptor_t rois_desc,
                            const void* rois,
                            const int pooled_height,
                            const int pooled_width,
                            const int sample_ratio,
                            const float spatial_scale,
                            const bool aligned,
                            const bool clockwise,
                            const cnnlTensorDescriptor_t bottom_grad_desc,
                            void* bottom_grad);

// Group:ApplyProximalAdagrad
/*
 * @brief Updates \b var and \b accum tensor according to FOBOS with Adagrad learning rate.
 * This function only supports in-place operation when the pointer to the input and output
 * tensor are the same tensor.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyProximalAdagrad. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of the \b var tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input and output. Pointer to the MLU memory that stores the \b var tensor.
 *   The \b var value is the optimization goal of whole algorithm.
 * @param[in] accum_desc
 *   Input. The descriptor of the \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input and output. Pointer to the MLU memory that stores the \b accum tensor.
 *   The \b accum is the accumulation of gradient.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor. It is the gradient of \b var.
 *   With the \b grad value, ::cnnlApplyProximalAdagrad function can be used to calculate \b accum and \b var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter.
 *   It is the learning rate of this optimizer.
 * @param[in] l1
 *   Input. Pointer to the MLU memory that stores the \b l1 parameter.
 *   It is the L1 regularization of this optimizer.
 * @param[in] l2
 *   Input. Pointer to the MLU memory that stores the \b l2 parameter.
 *   It is the L2 regularization of this optimizer.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Apply Proximal Adagrad" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for
 *   \b grad tensor, \b accum tensor, and \b var tensor.
 *   <b>Note that the combinations of these tensors must be half-half or float-float.</b>
 *   - grad tensor: half, float
 *   - accum tensor: half, float
 *   - var tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \b grad, \b accum and \b var should be same.
 *
 * @note
 * - It is recommended to use data type of float for \b var, \b accum and \b grad for higher
 * precision.
 * - On MLU200 series, if data type is half, the value of (\b accum + \b grad * \b grad)
 *   should be greater than 0.000367 and less than 65504; if data type is float, the value of
 *   (\b accum + \b grad * \grad) should be greater than 0.000367 and less than 1e6.
 *
 * @par Requirements
 * - None
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 2, 2, 2
     --> var:   [1.0, 1.0]
     --> accum: [4.0, 2.0]
     --> grad:  [1.0, 2.0]

     param:
        lr: 1.0
        l1: 1.0
        l2: 1.0

     output array by 2, 2
      --> var: [1.5147185, 0.45491502]
      --> accum: [2.0, 5.0]
     @endverbatim
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlApplyProximalAdagrad(cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t var_desc,
                                                   void *var,
                                                   const cnnlTensorDescriptor_t accum_desc,
                                                   void *accum,
                                                   const cnnlTensorDescriptor_t grad_desc,
                                                   const void *grad,
                                                   const void *lr,
                                                   const void *l1,
                                                   const void *l2);


// Group:Atan2
/*!
 * @brief Computes atan2 on input tensor \b input1 and \b input2, and returns the results
 *        in the output tensor \b output.
 *
 * This function may need extra MLU memory as the workspace to improve the operation performance.
 * You can get the size of the workspace \b workspace_size with the ::cnnlGetAtan2WorkspaceSize
 * function.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the atan2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] prefer
 *   Input. The \b prefer modes defined in ::cnnlComputationPreference_t enum.
 * @param[in] input1_desc
 *   Input. The descriptor of the input1 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input1
 *   Input. Pointer to the MLU memory that stores the atan2 first input tensor.
 * @param[in] input2_desc
 *   Input. The descriptor of the input2 tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2
 *   Input. Pointer to the MLU memory that stores the atan2 second input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the atan2 operation.
 *   For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the atan2 operation.
 *   You can get the size of the workspace with the ::cnnlGetAtan2WorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Atan2 Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data type of input tensors and output tensor must be the same.
 * - The data types of input and output tensors are as follows:
 *   - input1 tensor: half, float.
 *   - input2 tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The data layout of the input tensors and output tensor must be \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - The allocated extra workspace should be passed to the ::cnnlAtan2 function to perform the atan2 operation.
 *
 * @note
 * - The inputs \b input1 and \b input2 are multi-dimensional array, supporting up to CNNL_DIM_MAX dimensions.
 * - This operation supports tensor broacasting. \b input1, \b input2 and \b output must be broadcastable.
 * - You can specify the stride of all dimensions for input1_desc, input2_desc and output_desc with ::cnnlSetTensorDescriptorEx.
 * - No matter what \b prefer is , the calculation method for half and float is same Currently.
 * - On MLU200 series, if data type is half and the data value is not in the range of [-65504, 65504],
 *   the value will be treated as a saturation value.
 * - On MLU 200 series:
 *   - The absolute value of \b input2 is recommended to be in [2e-16, 2e6] for higher precision.
 *   - The absolute value of ( \b input1 / \b input2 ) is recommended to be greater than 1e-6 for higher precision.
 *   - The inputs \b input1 and \b input2 with NaN or infinity are not supported currently.
 * - On MLU300 series and CE3226:
 *   - The absolute value of ( \b input1 / \b input2 ) is recommended to be greater than 1e-6 for higher precision.
 *   - The inputs \b input1 and \b input2 with NaN or infinity are supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://www.tensorflow.google.cn/api_docs/python/tf/math/atan2
 */
cnnlStatus_t CNNL_WIN_API cnnlAtan2(cnnlHandle_t handle,
                                    cnnlComputationPreference_t prefer,
                                    const cnnlTensorDescriptor_t input1_desc,
                                    const void *input1,
                                    const cnnlTensorDescriptor_t input2_desc,
                                    const void *input2,
                                    void *workspace,
                                    size_t workspace_size,
                                    const cnnlTensorDescriptor_t output_desc,
                                    void *output);

// Group:Atan2
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the atan2 operation.
 *
 * The size of extra workspace is based on the given information of the atan2 operation,
 * including the input tensor descriptors \b input1_desc and \b input2_desc, and the output
 * tensor descriptor \b output_desc. For more information about the workspace, see "Cambricon
 * CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the atan2
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input1_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input2_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the atan2
 *   operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetAtan2WorkspaceSize(cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t input1_desc,
                                                    const cnnlTensorDescriptor_t input2_desc,
                                                    const cnnlTensorDescriptor_t output_desc,
                                                    size_t *workspace_size);

// Group:DeformRoiPool
/*!
 * @brief Computes deformable roi pooling over \b input tensor. This function firstly divides the obtained
 * candidate region  into regions with the same size according to the specified pooling width and pooling height,
 * then add offsets to rois, and finally calculates the mean value of the sampling points in each bin as output.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlDeformRoiPoolForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of rois, containing the dimension and layout of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor.
 * @param[in] offset_desc
 *   Input. Descriptor of offset, containing the dimension and layout of offset tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] pooled_height
 *   Input. An integer value which is the height of the output after pooling.
 * @param[in] pooled_width
 *   Input. An integer value which is the width of the output after pooling.
 * @param[in] spatial_scale
 *   Input. A float value which is the scale factor of coordinates of rois.
 * @param[in] sampling_ratio
 *   Input. An integer value which is the number of sample in one bin. This parameter
 *   only works when it is greater than zero.
 * @param[in] gamma
 *   Input. A float value which is the scale factor of offset.
 * @param[in] output_desc
 *   Input.  Descriptor of output tensor, containing the dimension and layout of output tensor.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "DeformRoiPoolForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, rois tensor
 *   \b rois, offset tensor \b offset and output tensor \b output. Data types of all tensors should be the same.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - offset tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \b input, \b rois, \b offset and \b output are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - offset tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The input tensor and output tensor must be 4-D.
 * - Size of the lowest dimension of input tensor and output tensor must be the same.
 * - The rois tensor must be 2-D.
 * - The offset tensor must be 4-D.
 * - Size of the highest dimension of output tensor, rois tensor and offset tensor must be the same.
 * - Size of the middle two dimensions of output tensor and the lower two dimensions of offset tensor must be the same.
 * - The shape of \b input should be [batch_num, height, width, channels].
 * - The shape of \b rois should be [rois_num, 5].
 * - The shape of \b offset should be [rois_num, 2, pooled_height, pooled_width].
 * - The shape of \b output should be [rois_num, pooled_height, pooled_width, channels].
 * - \b rois[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in the range of [0, batch_num - 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The inputs \b rois and \b offset with NaN or infinity are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the deform_roi_pool_forward operation is as follows:
     @verbatim
     input three arrays by 1 * 2 * 2 * 1, 1 * 5 and 1 * 2 * 1 * 1
     --> input: [[[[1.0], [2.0]], [[2.0], [4.0]]]]
     --> rois: [[0.0, 0.0, 0.0, 1.0, 1.0]]
     --> offset: [[[[0.5]], [[0.5]]]]

     param:
            pooled_height: 1.0, pooled_width: 1.0, spatial_scale: 1.0,
            sampling_ratio: 1, gamma: 1

     output array by 1 * 1 * 1 * 1 -->
         output: [[[[2.25]]]]
     @endverbatim
 *
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py
 */
cnnlStatus_t CNNL_WIN_API cnnlDeformRoiPoolForward(const cnnlHandle_t handle,
                                                   const cnnlTensorDescriptor_t input_desc,
                                                   const void *input,
                                                   const cnnlTensorDescriptor_t rois_desc,
                                                   const void *rois,
                                                   const cnnlTensorDescriptor_t offset_desc,
                                                   const void *offset,
                                                   const int pooled_height,
                                                   const int pooled_width,
                                                   const float spatial_scale,
                                                   const int sampling_ratio,
                                                   const float gamma,
                                                   const cnnlTensorDescriptor_t output_desc,
                                                   void *output);

// Group:DeformRoiPool
/*!
 * @brief Computes the gradient of input \b grad_input and the gradient of offset \b grad_offset
 * based on the gradient of ouput \b grad_output, input \b input, regions of interest \b rois
 * and offset \b offset.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlDeformRoiPoolBackward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] grad_output_desc
 *   Input. The descriptor of the grad_output tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] grad_output
 *   Input. Pointer to the MLU memory that stores the grad_output tensor.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] rois_desc
 *   Input. Descriptor of the rois tensor, containing the dimension and layout of rois tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] rois
 *   Input. Pointer to the MLU memory that stores the rois tensor.
 * @param[in] offset_desc
 *   Input. Descriptor of the offset tensor, containing the dimension and layout of offset tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] offset
 *   Input. Pointer to the MLU memory that stores the offset tensor.
 * @param[in] pooled_height
 *   Input. An integer value which is the height of the output after pooling.
 * @param[in] pooled_width
 *   Input. An integer value which is the width of the output after pooling.
 * @param[in] spatial_scale
 *   Input. A float value which is the scale factor of coordinates of rois.
 * @param[in] sampling_ratio
 *   Input. An integer value which is the number of sample in one bin. This parameter
 *   only works when it is greater than zero.
 * @param[in] gamma
 *   Input. A float value which is the scale factor of offset.
 * @param[in] grad_input_desc
 *   Input.  Descriptor of grad_input tensor, containing the dimension and layout of grad_input tensor.
 * @param[out] grad_input
 *   Output. Pointer to the MLU memory that stores the gradient of the input tensor.
 * @param[in] grad_offset_desc
 *   Input.  Descriptor of grad_offset tensor, containing the dimension and layout of grad_offset tensor.
 * @param[out] grad_offset
 *   Output. Pointer to the MLU memory that stores the gradient of the offset tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED.
 *
 * @par Formula
 * - See "DeformRoiPoolBackward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for grad_output tensor \b grad_output, input tensor \b input,
 *   rois tensor \b rois, offset tensor \b offset, grad_input tensor \b grad_input and grad_offset tensor \b grad_offset.
 *   Data types of all tensors should be the same.
 *   - grad_output tensor: half, float.
 *   - input tensor: half, float.
 *   - rois tensor: half, float.
 *   - offset tensor: half, float.
 *   - grad_input tensor: half, float.
 *   - grad_offset tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of grad_output tensor \b grad_output, input tensor \b input, rois tensor \b rois,
 *   offset tensor \b offset, grad_input tensor \b grad_input and grad_offset tensor \b grad_offset are as follows:
 *   - grad_output tensor: \p CNNL_LAYOUT_NHWC.
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - rois tensor: \p CNNL_LAYOUT_ARRAY.
 *   - offset tensor: \p CNNL_LAYOUT_ARRAY.
 *   - grad_input tensor: \p CNNL_LAYOUT_NHWC.
 *   - grad_offset tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - The grad_output tensor, input tensor and grad_input tensor must be 4-D.
 * - Size of the lowest dimension of grad_output tensor, input tensor and grad_input tensor must be the same.
 * - The rois tensor must be 2-D.
 * - The offset tensor and grad_offset tensor must be 4-D.
 * - Size of the highest dimension of output tensor, rois tensor, offset tensor and grad_offset tensor must be the same.
 * - Size of the middle two dimensions of grad_output tensor, the lower two dimensions of offset tensor
 *   and the lower two dimensions of grad_offset tensor must be the same.
 * - The shape of \b grad_output should be [rois_num, pooled_height, pooled_width, channels].
 * - The shape of \b input should be [batch_num, height, width, channels].
 * - The shape of \b rois should be [rois_num, 5].
 * - The shape of \b offset should be [rois_num, 2, pooled_height, pooled_width].
 * - The shape of \b grad_input should be [batch_num, height, width, channels].
 * - The shape of \b grad_offset should be [rois_num, 2, pooled_height, pooled_width].
 * - \b rois[i] consists of [batch_id, x1, y1, x2, y2]. \p batch_id should be in the range of [0, batch_num - 1].
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The inputs \b rois and \b offset with NaN or infinity are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/tree/master/mmcv/ops/deform_roi_pool.py
 */
cnnlStatus_t CNNL_WIN_API cnnlDeformRoiPoolBackward(const cnnlHandle_t handle,
                                                    const cnnlTensorDescriptor_t grad_output_desc,
                                                    const void *grad_output,
                                                    const cnnlTensorDescriptor_t input_desc,
                                                    const void *input,
                                                    const cnnlTensorDescriptor_t rois_desc,
                                                    const void *rois,
                                                    const cnnlTensorDescriptor_t offset_desc,
                                                    const void *offset,
                                                    const int pooled_height,
                                                    const int pooled_width,
                                                    const float spatial_scale,
                                                    const int sampling_ratio,
                                                    const float gamma,
                                                    const cnnlTensorDescriptor_t grad_input_desc,
                                                    void *grad_input,
                                                    const cnnlTensorDescriptor_t grad_offset_desc,
                                                    void *grad_offset);

// Group:CosineSimilarity
/*!
 * @brief Returns cosine similarity between \b x1 and \b x2, computed along \b cos_dim .
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   cosine similarity operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] desc_x1
 *   Input. The descriptor of the \b x1 tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x1
 *   Input. Pointer to the MLU memory that stores the \b x1 tensor.
 * @param[in] desc_x2
 *   Input. The descriptor of the \b x2 tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x2
 *   Input. Pointer to the MLU memory that stores the \b x2 tensor.
 * @param[in] desc_y
 *   Input. The descriptor of the output tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   cosine similarity operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the cosine similarity operation. You can get the size of the workspace with
 *   the ::cnnlGetCosineSimilarityWorkspaceSize function.
 * @param[in] cos_dim
 *   Input. Dimension where cosine similarity is computed.
 * @param[in] epsilon
 *   Input. Pointer to the MLU memory that stores the \b epsilon parameter.
 *   It is a small positive number just as 10^-8, to avoid division by 0.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Cosine Similarity" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input and output tensor
 *   \b output. Data type of both tensors should be the same.
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - None.
 *
 * @par Scale Limitation
 * - None
 *
 * @note
 * - When \b cos_dim is set to one of the  middle dimensions,
 *   broadcasting is not recommended due to performace limitation.
 *
 * - The element number of two input tensors should satisfy the regular broadcasting rules.
 *
 * - The example of the expand operation is as follows:
     @verbatim
     if \b cos_dim is zero
     input array by 3 * 2 * 2 --> input: [[[4, 4], [4, 4]],
                                            [[4, 4], [4, 4]],
                                            [[4, 4], [4, 4]]]

     input array by 3 * 2 * 2 --> input: [[[5, 5], [5, 5]],
                                            [[5, 5], [5, 5]],
                                            [[5, 5], [5, 5]]]

     output one array by 2 * 2 --> output: [[1, 1], [1, 1]]

     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/nn.functional.html?highlight=cosine_similarity#torch.nn.functional.cosine_similarity
 */
cnnlStatus_t CNNL_WIN_API cnnlCosineSimilarity(cnnlHandle_t handle,
                                               int32_t cos_dim,
                                               const float epsilon,
                                               const cnnlTensorDescriptor_t desc_x1,
                                               const void *x1,
                                               const cnnlTensorDescriptor_t desc_x2,
                                               const void *x2,
                                               void *workspace,
                                               size_t workspace_size,
                                               const cnnlTensorDescriptor_t desc_y,
                                               void *y);

// Group:CosineSimilarity
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to optimize the cosine similarity operation.
 *
 * The size of extra workspace is based on the given information of the cosine similarity operation,
 * including the tensor descriptors \b desc_x1, \b desc_x2 and \b desc_y .
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the cosine similarity operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  desc_x1
 *   Input. The descriptor of the input tensor \b x1. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  desc_x2
 *   Input. The descriptor of the input tensor \b x2. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  desc_y
 *   Output. The descriptor of the output tensor \b y. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] cos_dim
 *   Input. Dimension where cosine similarity is computed.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the cosine similarity operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors \b x_desc.
 * - The allocated extra workspace should be passed to the ::cnnlCosineSimilarity function
 *   to perform the cosine similarity operation.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCosineSimilarityWorkspaceSize(cnnlHandle_t handle,
                                                               int32_t cos_dim,
                                                               const cnnlTensorDescriptor_t desc_x1,
                                                               const cnnlTensorDescriptor_t desc_x2,
                                                               const cnnlTensorDescriptor_t desc_y,
                                                               size_t *workspace_size);

// Group:Unfold
/*!
 * @brief Unfolds the input tensor \b input along the axis \b dimension into slices of
 * size \b size based on the given stride \b step.
 *
 * @param[in]  handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the unfold operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in]  input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in]  dimension
 *   Input.  An int32 scalar, the dimension to unfold.
 * @param[in]  size
 *   Input.  An int32 scalar, the size of each slice.
 * @param[in]  step
 *   Input.  An int32 scalar, the step between two slices.
 * @param[in]  output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out]  output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED, ::CNNL_STATUS_INTERNAL_ERROR.
 *
 * - See "Unfold Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports any combinations of the following data types for input tensor \b input and output tensor \b output.
 *   <b>Note that the data types of input tensor and output tensor must be the same.</b>
 *   - input tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64, half, float, double, complex_half, complex_float.
 *   - output tensor: bool, uint8, int8, uint16, int16, uint32, int32, uint64, int64, half, float, double, complex_half, complex_float.
 *
 * @par Scale Limitation
 * - (-input_dimNb) <= \b dimension < input_dimNb
 * - \b step > 0
 * - if \b dimension < 0: \b dimension = \b dimension + input_dimNb
 * - 0 <= \b size <= input_dims[\b dimension]
 * - output_dimNb = input_dimNb + 1
 * - output_dims should meet the following requirements:
 *   - when 0 <= i < \b dimension: output_dims[i] = input_dims[i]
 *   - when \b dimension < i < input_dimNb: output_dims[i] = input_dims[i]
 *   - output_dims[\b dimension] = floor_div(input_dims[\b dimension] - \b size, \b step) + 1
 *   - output_dims[output_dimNb - 1] = \b size
 *
 * @par Example
 * - The example of unfold operation is as follows:
    @verbatim
    input: an array by 1*7 -->  [0,1,2,3,4,5,6],

    dimension:  an int32 scalar: 0

    size:       an int32 scalar: 3

    step:       an int32 scalar: 2

    Then we will get the output:

    output: an array by 3*3 --> [[0,1,2],
                                 [2,3,4],
                                 [4,5,6]]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html
 * - https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorShape.cpp
 *
 */

cnnlStatus_t CNNL_WIN_API cnnlUnfold(cnnlHandle_t handle,
                                     const int32_t dimension,
                                     const int32_t size,
                                     const int32_t step,
                                     const cnnlTensorDescriptor_t input_desc,
                                     const void *input,
                                     const cnnlTensorDescriptor_t output_desc,
                                     void *output);

/******************************************************************************
 * Cambricon CNNL OP: Conj
 ******************************************************************************/
// Group:Conj
/*!
 * @brief Computes the conjugate of the input tensor \b x element-wise and returns
 * the results in the output tensor \b y. If an element in the input tensor is a non-complex number,
 * the conjugate of the element is itself.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   Conj operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the x tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the x tensor of complex numbers.
 * @param[in] y_desc
 *   Input. The descriptor of the y tensor. For detailed information, see
 *   ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the y tensor.
 *
 * - See "Conj Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - The supported data types of x and y tensors are as follows:
 *   - x tensor: bool, int8, uint8, int16, int32, int64, half, float and complex_float.
 *   - y tensor: bool, int8, uint8, int16, int32, int64, half, float and complex_float.
 *
 * @par Scale Limitation
 * - None
 *
 * @par Example
 * - The example of the conj operation is as follows:
    @verbatim
    input one array by 1 * 3 --> input: [(-1.,1.), (-2.,2.), (3.,-3.)]

    output array by 1 * 3 --> output: [(-1.,-1.), (-2.,-2.), (3.,3.)]
    @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.conj.html?hightlight=conj#torch.conj
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cpu/UnaryOpsKernel.cpp#L196
 * - https://github.com/pytorch/pytorch/blob/release/1.9/aten/src/ATen/native/cuda/UnaryComplexKernels.cu#L83
 */
cnnlStatus_t CNNL_WIN_API cnnlConj(cnnlHandle_t handle,
                                   const cnnlTensorDescriptor_t x_desc,
                                   const void *x,
                                   const cnnlTensorDescriptor_t y_desc,
                                   void *y);


// Group:Trunc
/*!
 * @brief Computes the truncation of each element in the input tensor \b x, and returns
 * the result in the output tensor \b y.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the trunc
 *   operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] y_desc
 *   Input. The descriptor of the output tensor \b y. For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] y
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "Trunc Operation" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - Data types of input tensor and output tensor should be the same.
 * - The supported data types of input and output tensors are as follows:
 *   - input tensor: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - Data Layout of input tensor and output tensor should be the same.
 *
 * @par Requirements
 * - None.
 *
 * @par Scale Limitation
 * - The shape of input tensor and output tensor should be the same.
 * - The total number of dimensions of input tensor and output tensor should be the same.
 *
 * @par API Dependency
 * - None.
 *
 * @par Example
     @verbatim
       input:  [-2.1, -1.8, 9.125, 6.9, 4.0]
       output: [-2., -1., 9., 6.0, 4.0]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.6.0/generated/torch.trunc.html#torch.trunc
 * - https://pytorch.org/docs/1.9.0/generated/torch.trunc.html#torch.trunc
 *
 */
cnnlStatus_t CNNL_WIN_API
cnnlTrunc(cnnlHandle_t handle,
          const cnnlTensorDescriptor_t x_desc,
          const void *x,
          const cnnlTensorDescriptor_t y_desc,
          void *y);

// Group:BorderAlign
/*!
 * @brief Extracts the border features of \b input based on the bounding boxes to compute the
 *   maximum border features of \b input with the maximum pooling.
 *   The computing process of this operation is as follows:
 *     1. For each border line of each box (commonly four lines: top, left, bottom and and right lines), uniformly samples
 *        pool_size + 1 positions on this line, involving the start and end points.
 *     2. Compute the corresponding features on these points by bilinear interpolation.
 *     3. Perform the max pooling over all pool_size + 1 positions to output the pooled features.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in
 *   ::cnnlBorderAlignForward operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor. The shape of \b input is [N, H, W, 4C].
 *   Channels ranged in [0,C), [C,2C), [2C,3C), [3C,4C) represent the top, left, bottom and right features
 *   respectively.
 * @param[in] boxes_desc
 *   Input. Descriptor of bounding box tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores boxes tensors. The shape of \b boxes is [N, H * W, 4].
 * @param[in] pool_size
 *   Input. Number of positions sampled over the boxes' borders.
 * @param[in] output_desc
 *   Input.  Descriptor of \b output, containing dimension and the layout of output.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor. The shape of
 *   argmax_idx is [N, H * W, 4, C].
 * @param[in] argmax_idx_desc
 *   Input.  Descriptor of \b argmax_idx, containing dimension and the layout of \b argmax_idx.
 * @param[out] argmax_idx
 *   Output. Pointer to the MLU memory that stores the \b argmax_idx tensor, which is the indices of
 *   maximum values after the max pooling. The shape of argmax_idx is [N, H * W, 4, C].
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Formula
 * - See "BorderAlignForward Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input, \b boxes, \b pool_size,
 *    \b output and \b argmax_idx. Data type of the \b input, \b boxes and \b output tensors should be the same.
 *   - input tensor: half, float.
 *   - boxes tensor: half, float.
 *   - pool_size: int32_t.
 *   - output tensor: half, float.
 *   - argmax_idx: int32_t.
 *
 * @par Data Layout
 * - The supported data layout of \b input, \b boxes, \b output and \b argmax_idx are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NHWC.
 *   - boxes tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_NHWC.
 *   - argmax_idx tensor: \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The \b input, \b output and \b argmax_idx should be 4D.
 * - The \b boxes tensor should be 3-D array, and the highest dimension of \b boxes must be 4.
 *
 * @par API Dependency
 * - None.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 * - The example of the border_align_forward operation is as follows:
     @verbatim
     input: a tensor with shape by 1 * 4 * 3 *4  --> [[[[ 1.,  2.,  3.,  4.],
                                                        [ 5.,  6.,  7.,  8.],
                                                        [ 9., 10., 11., 12.]],

                                                       [[ 6.,  7.,  5.,  8.],
                                                        [ 2.,  1.,  3.,  4.],
                                                        [12.,  9., 11., 10.]],

                                                       [[-2., -3.,  2.,  0.],
                                                        [-4., -5.,  1., -1.],
                                                        [-1., -1., -1., -1.]],

                                                       [[ 0., -1.,  2.,  1.],
                                                        [-4., -3., -2., -1.],
                                                        [-1., -2., -3., -4.]]]]

     boxes: a tensor with shape by 1 * 12 * 4  --> [[[0., 0., 2., 1.],
                                                     [1., 0., 3., 1.],
                                                     [1., 0., 2., 1.],
                                                     [0., 0., 3., 1.],
                                                     [0., 0., 1., 2.],
                                                     [0., 0., 2., 2.],
                                                     [1., 0., 2., 1.],
                                                     [1., 0., 3., 1.],
                                                     [0., 1., 1., 2.],
                                                     [0., 0., 3., 2.],
                                                     [1., 0., 3., 2.],
                                                     [2., 0., 3., 2.]]]

     param: pool_size = 1.
     output: a tensor with shape by 1 * 1 * 12 * 4 --> [[[[ 3.,  6.,  1.,  2.],
                                                          [ 4.,  7., -1.,  1.],
                                                          [ 3.,  7.,  1.,  2.],
                                                          [ 4.,  6., -1.,  1.],
                                                          [ 2., 12., -1., -1.],
                                                          [ 3., 12., -1.,  2.],
                                                          [ 3.,  7.,  1.,  2.],
                                                          [ 4.,  7., -1.,  1.],
                                                          [ 6., 12., -1., -2.],
                                                          [ 4., 12., -1.,  1.],
                                                          [ 4.,  9., -1.,  1.],
                                                          [ 4., 11., -1.,  1.]]]]

     argmax_idx：a tensor with shape by 1 * 1 * 12 * 4 -->[[[[1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 0, 0, 1],
                                                             [1, 1, 0, 0],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1],
                                                             [1, 1, 0, 1]]]]
   @endverbatim
 *
 * @par Reference
 * - github.com/open-mmlab/mmcv/blob/master/mmcv/ops/border_align.py
 */
cnnlStatus_t CNNL_WIN_API cnnlBorderAlignForward(cnnlHandle_t handle,
                                                 const cnnlTensorDescriptor_t input_desc,
                                                 const void *input,
                                                 const cnnlTensorDescriptor_t boxes_desc,
                                                 const void *boxes,
                                                 const int32_t pool_size,
                                                 const cnnlTensorDescriptor_t output_desc,
                                                 void *output,
                                                 const cnnlTensorDescriptor_t argmax_idx_desc,
                                                 void *argmax_idx);
// Group:CopySign
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the ::cnnlCopySign operation.
 *
 * The size of extra workspace is based on the given information of the operation,
 * including the input tensor descriptor \b input_desc, the other tensor descriptor \b other_desc,
 * and the output tensor descriptor \b output_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the
 *   copysign operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the
 *   ::cnnlCopySign operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetCopySignWorkspaceSize(cnnlHandle_t handle,
                                                       const cnnlTensorDescriptor_t input_desc,
                                                       const cnnlTensorDescriptor_t other_desc,
                                                       const cnnlTensorDescriptor_t output_desc,
                                                       size_t *workspace_size);

// Group:CopySign
/*!
 * @brief Creates a new floating-point tensor with the magnitude of input tensor
 * and the sign of other tensor, elementwise.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] other_desc
 *   Input. The descriptor of the other tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] other
 *   Input. Pointer to the MLU memory that stores the other tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the
 *   ::cnnlCopySign operation.
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the operation.
 *   You can get the size of the workspace with
 *   the ::cnnlGetCopySignWorkspaceSize function.
 * @param[in] output_desc
 *   Input. The descriptor of the output tensor. It should not be NULL.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - See "Copysign Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensors and output tensor. <b>Note
 *   that The two input(input, other) tensors and output tensor should have the same data type.</b>
 *   - input tensors: half, float.
 *   - output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layout of \b input, \b other and \b output tensor are as follows:
 *   - input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - other tensor: \p CNNL_LAYOUT_ARRAY.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - This operation supports tensor broadcasting. For each dimension,
 *   the dimension length of the \b input tensor and the \b other tensor
 *   need to meet the requirements of broadcasting.
 * - The tensor \b input, \b other and \b output are multi-dimensional array, supporting
 *   up to \p CNNL_DIM_MAX dimensions.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of the copysign operation is as follows:
     @verbatim
     example 1:
     here is an example without tensor broadcasting -->
     input array by 1 * 5 --> input: [0.35, 0.20, 0.45, 0.40, 0.40]

     other array by 1 * 5 --> other: [-1.0, 2.0, 3.0, -4.0, 5.0]

     out array by 1 * 5 --> other: [-0.35, 0.20, 0.45, -0.40, 0.40]

     example 2:
     here is an example with tensor broadcasting -->
     input array by 3 * 5 --> input: [[0.25, 0.45, 0.20, 0.10, 0.01],
                                      [0.40, 0.35, 0.35, 0.50, 0.50],
                                      [0.35, 0.20, 0.45, 0.40, 0.40]]

     other array by 1 * 1 --> other: [-5.0]

     out array by 3 * 5 --> out: [[-0.25, -0.45, -0.20, -0.10, -0.01],
                                  [-0.40, -0.35, -0.35, -0.50, -0.50],
                                  [-0.35, -0.20, -0.45, -0.40, -0.40]]

     example 3:
     here is an example with tensor broadcasting -->
     input array by 3 * 5 --> input: [[0.25, 0.45, 0.20, 0.10, 0.01],
                                      [0.40, 0.35, 0.35, 0.50, 0.50],
                                      [0.35, 0.20, 0.45, 0.40, 0.40]]

     other array by 1 * 5 --> other: [-1.0, 2.0, 3.0, -4.0, 5.0]

     out array by 3 * 5 --> out: [[-0.25, 0.45, 0.20, -0.10, 0.01],
                                  [-0.40, 0.35, 0.35, -0.50, 0.50],
                                  [-0.35, 0.20, 0.45, -0.40, 0.40]]
     @endverbatim
 *
 * @par Reference
 * - https://pytorch.org/docs/1.9.0/generated/torch.copysign.html?highlight=copysign#torch.copysign
 */
cnnlStatus_t CNNL_WIN_API cnnlCopySign(cnnlHandle_t handle,
                                       const cnnlTensorDescriptor_t input_desc,
                                       const void *input,
                                       const cnnlTensorDescriptor_t other_desc,
                                       const void *other,
                                       void *workspace,
                                       size_t workspace_size,
                                       const cnnlTensorDescriptor_t output_desc,
                                       void *output);
/*!
 * @brief Detects the first 3D box that each point belongs to in  given points cloud data.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the box iou rotated operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] points_desc
 *   Input. The descriptor of the input tensor \b points.
 *   The shape of \b points is [B, npoints, 3], where '3' means
 *   the 3D coordinate position (x, y, z) for each point,
 *   'B' means batch size, and 'npoints' means points number.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] points
 *   Input. Pointer to the MLU memory that stores the input tensor \b points.
 * @param[in] boxes_desc
 *   Input. The descriptor of the input tensor \b boxes.
 *   The shape of \b boxes is [B, N, 7], where '7' means
 *   (x, y, z, dx, dy, dz, heading) with
 *   'x','y','z' indicating 3D center coordinate for each box, and
 *   'dx','dy','dz' indicating max-ranges in x,y,z direction for each box.
 *   Note that 'dx', 'dy', 'dz' is non-negative.
 *   'B' means batch size, 'N' means boxes number.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] boxes
 *   Input. Pointer to the MLU memory that stores the input tensor \b boxes.
 * @param[out] points_indices_desc
 *   Output. The descriptor of the output tensor \b points_indices.
 *   The shape of \b points_indices is [B, N].
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] points_indices
 *   Output. Pointer to the MLU memory that stores the output tensor \b points_indices.
 *   \b points_indices are indexs of the first 3D box that each point belongs to in given points cloud data.
 *   If no corresponding box exists, the output is -1.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 * @par Formula
 * - See "Points In Boxes Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - By the order of \b points - \b boxes - \b points_indices, the supported data types of
 *    \b points, \b boxes and \b points_indices are as follows:
 *   - float - float - int32_t.
 *
 * @par Scale Limitation
 * - None.
 *
 * @note
 * - Differences between MLU and CPU/GPU may occur when the point is on the edge of the box.
 *
 * @par API Dependency
 * - None.
 *
 * @par Requirements
 * - None.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlPointsInBoxes(cnnlHandle_t handle,
                                            const cnnlTensorDescriptor_t points_desc,
                                            const void *points,
                                            const cnnlTensorDescriptor_t boxes_desc,
                                            const void *boxes,
                                            const cnnlTensorDescriptor_t points_indices_desc,
                                            void *points_indices);

// Group:ApplyFtrl
/*!
 * @brief Updates \b var tensor and \b accum tensor by using FTRL-Proximal method.
 * This function contains two modes that are determined by the the parameter \b l2_shrinkage.
 * - When \b l2_shrinkage is NULL, this function adopts ApplyFtrl mode.
 * - When \b l2_shrinkage is not NULL, this function adopts ApplyFtrlV2 mode.
 * In ApplyFtrl and ApplyFtrlV2 mode, when the \b multiply_linear_by_lr is false,
 * \b l1 and \b l2 will be multiplied by the learning rate \b lr in the computation formula.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the ::cnnlApplyFtrlV2 operation. For more detailed information, see ::cnnlHandle_t.
 * @param[in] var_desc
 *   Input. The descriptor of the \b var tensor. For detailed information, see
 * ::cnnlTensorDescriptor_t.
 * @param[in,out] var
 *   Input/Output. Pointer to the MLU memory that stores the \b var tensor.
 *   The \b var value is the optimization goal of whole algorithm.
 * @param[in] accum_desc
 *   Input. The descriptor of the \b accum tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] accum
 *   Input/Output. Pointer to the MLU memory that stores the \b accum tensor.
 *   The \b accum is the accumulation of gradient.
 * @param[in]  linear_desc
 *   Input. The descriptor of the \b linear tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in,out] linear
 *   Input/Output. Pointer to the MLU memory that stores the \b linear tensor.
 *   The \b linear is the linear coefficient.
 * @param[in]  grad_desc
 *   Input. The descriptor of the \b grad tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in]  grad
 *   Input. Pointer to the MLU memory that stores the \b grad tensor. It is the gradient of \b var.
 *   With the \b grad value, ::cnnlApplyFtrlV2 function can be used to calculate \b accum and \b
 *   var.
 * @param[in] lr
 *   Input. Pointer to the MLU memory that stores the \b lr parameter.
 *   It is the learning rate value of this optimizer, must be positive.
 * @param[in] l1
 *   Input. Pointer to the MLU memory that stores the \b l1 parameter.
 *   It is l1 regularization strength, must be greater than or equal to zero.
 * @param[in] l2
 *   Input. Pointer to the MLU memory that stores the \b l2 parameter.
 *   It is l2 regularization strength, must be greater than or equal to zero.
 * @param[in] l2_shrinkage
 *   Input. Pointer to the MLU memory that stores the \b l2_shrinkage parameter.
 *   It is L2 shrinkage regularization.
 *   If the pointer is nullptr, the function will be in ApplyFtrl mode.
 *   If the pointer is non-nullptr, the function will be in ApplyFtrlV2 mode.
 * @param[in] lr_power
 *   Input. Pointer to the MLU memory that stores the \b lr_power parameter.
 *   It is the learning rate power that controls how the learning rate decreases during training.
 *   Use fixed learning rate if \b lr_power is zero.
 * @param[in] multiply_linear_by_lr
 *   Input. A boolean value that specifies whether the formula to multiply the learning rate.
 *   If the value of this parameter is false, the formula will multiply the learning rate.
 *   If the value of this parameter is true, the formula will not multiply the learning rate.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - See "ApplyFtrlV2 Operator" section in "Cambricon CNNL User Guide" for more details.
 *
 * @par Data Type
 * - This function supports the following data types for
 *   \b var tensor, \b accum tensor, \b linear tensor and \b grad tensor.
 *   <b>Note that the data type of input tensor and output tensor should be same.</b>
 *   - var tensor: half, float
 *   - accum tensor: half, float
 *   - linear tensor: half, float
 *   - grad tensor: half, float
 *
 * @par Scale Limitation
 * - The dimensions of \b var, \b accum, \b linear and \b grad should be same.
 *
 * @note
 * - It is recommended to set \b lr_power a non positive number.
 * - On MLU200 series:
 *   - If lr\_power is integer, \f$(accum + grad*grad)\f$ is recommended to be set in the range of (-inf,-1) and (1,+inf) for high precision.
 *   - If data type is half, the input data \b var, \b accum, \b linear and \b grad must be within [-65504, 65504].
 *   - If data type is half, \f$accum^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If data type is half, \f$(accum + grad*grad)^{-lr\_power}\f$ should be within [-65504, 65504].
 * - On MLU300 series and CE3226:
 *   - If lr\_power is integer, \f$(accum + grad*grad)\f$ is recommended to be set in the range of (-inf,-1] and [1,+inf) for high precision.
 *   - If data type is half, \f$accum^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If data type is half, \f$(accum + grad*grad)^{-lr\_power}\f$ should be within [-65504, 65504].
 *   - If \f$(accum + grad*grad)^{-lr\_power}\f$ and \f$accum^{-lr\_power}\f$ are both positive infinity, then \b var is NaN.
 *   - If \f$(accum + grad*grad)^{-lr\_power}\f$ and \f$accum^{-lr\_power}\f$ are both infinity or NaN, and \b lr_power is 0.0,
 *     then \b linear is NaN.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - The example of this operation is as follows:
     @verbatim
     input three arrays by 4, 4, 4
     --> var:   [0.0, 1.0, 2.0, 3.0]
     --> accum: [0.0, 1.0, 2.0, 3.0]
     --> linear: [0.0, 1.0, 2.0, 3.0]
     --> grad:  [0.0, 1.0, 2.0, 3.0]

     param:
        lr: 0.01, l1: 0.01, l2: 0.01, l2_shrinkgae: 0.01, lr_power: -0.01, multiply_linear_by_lr: false,

     output array by 4, 4
      --> accum: [0,2,6,12]
      --> var: [0, -0.01305111, -0.01772886, -0.01770953]
     @endverbatim
 */
cnnlStatus_t CNNL_WIN_API
cnnlApplyFtrlV2(cnnlHandle_t handle,
                const cnnlTensorDescriptor_t var_desc,
                void *var,
                const cnnlTensorDescriptor_t accum_desc,
                void *accum,
                const cnnlTensorDescriptor_t linear_desc,
                void *linear,
                const cnnlTensorDescriptor_t grad_desc,
                const void *grad,
                const void *lr,
                const void *l1,
                const void *l2,
                const void *l2_shrinkage,
                const void *lr_power,
                const bool multiply_linear_by_lr);

/*****************************************************************************
 * Cambricon CNNL OP: histogram
 *****************************************************************************/
/*!
 * @brief Enumeration variables describing the operation mode that histogram series operations
 * run on.
 *
 * These Enumeration variables are member of ::cnnlHistogramStruct.
 */
typedef enum {
  CNNL_HISTOGRAM_MODE_HISTO_COUNT = 0,
  /*!< The ::cnnlHistc mode. In this mode, the operation computes the histogram of \b input. */
  CNNL_HISTOGRAM_MODE_BIN_COUNT = 1,
  /*!< The Bincount mode. In this mode, the operation counts the frequencies of
   *   non-negative integers in \b input. This mode is not supported currently. */
  CNNL_HISTOGRAM_MODE_HISTO_GRAM = 2,
  /*!< The Histogram mode. In this mode, the operation computes the histogram of
   *   \b input with advanced options. This mode is not supported currently. */
} cnnlHistogramMode_t;

/*! The descriptor of histogram series operations that hold the information
 *  including the histogram mode, bins, min_length, max_length, range_min, range_max, density,
 *  axis, binary_out, ext_range and is_index.
 *
 * You need to call the ::cnnlCreateHistogramDescriptor function to create a descriptor,
 * and call the ::cnnlSetHistogramDescriptor function to set the information of the histogram
 * operation to the descriptor. Also, you need to destroy the Cambricon CNNL context at the end with the
 * ::cnnlDestroyHistogramDescriptor function.
 */
typedef struct cnnlHistogramStruct *cnnlHistogramDescriptor_t;

// Group:Histogram
/*!
 * @brief Creates a descriptor pointed by \b histogam_desc for histogram series operations,
 *        and allocates memory for holding the information about the histogram operation.
 *        The information is defined in ::cnnlHistogramDescriptor_t. For more information
 *        about descriptor, see "Cambricon CNNL User Guide".
 *
 * @param[in] histogram_desc
 *  Input. A host pointer to the histogram descriptor that holds information about the operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_ALLOC_FAILED
 *
 * @par API Dependency
 * - After calling this function, you can call the ::cnnlSetHistogramDescriptor function to initialize
 *   and set the information to the histogram descriptor.
 * - You need to call the ::cnnlDestroyHistogramDescriptor function to destroy the descriptor.
 *
 * @note
 * - Currently, only supports CNNL_HISTOGRAM_MODE_HISTO_COUNT.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None
 */
cnnlStatus_t CNNL_WIN_API cnnlCreateHistogramDescriptor(cnnlHistogramDescriptor_t *histogram_desc);

// Group:Histogram
/*!
 * @brief Initializes the ::cnnlHistc descriptor \b histogram_desc that is previously created
 * with the ::cnnlCreateHistogramDescriptor function, and sets the information
 * about the ::cnnlHistc operation to the histogram descriptor \b histogram_desc.
 * The information includes the mode of the histogram operation, the number of bins to put
 * result in \b bins, the minority of range for input to be counted \b range_min, the maximum
 * of range for input to be counted \b range_max, whether \b input outside \b range_min and
 * \b range_max to be counted \b ext_range, and whether to out the frequency of \b input or the bins
 * index \b is_index.
 *
 * @param[in] histogram_desc
 *   Input. The descriptor of the ::cnnlHistc operation. The value of member ::cnnlHistogramMode_t
 *   of this parameter will be set to CNNL_HISTOGRAM_MODE_HISTO_COUNT. For detailed information,
 *   see ::cnnlHistogramDescriptor_t.
 * @param[in] bins
 *   Input. The number of bins to put input tensor in. Histogram operation is to put \b input
 *   elements in bins, and counts how many elements there are in each bin as \b output.
 *   Total number of bins is given by \b bins. Each bin has same length,
 *   and length equals (\b range_max - \b range_min) / \b bins. The value of this parameter
 *   should be same as the element number of \b output and should be always positive.
 * @param[in] rang_min
 *   Input.  A float number to specify the minimum range for \b input element to be counted.
 *   The value of this parameter should not be NAN/INF or greater than \b range_max.
 *   When \b range_min equals \b range_max, ::cnnlHistc searches for max and min of \input.
 * @param[in] range_max
 *   Input.  A float number to specify the maximum range for \b input element to be counted.
 *   If \b input elements equal range_max, they will be put in last bin.
 *   The value of this parameter should not be NAN/INF or less than \b range_min.
 *   When \b range_min equals \b range_max, ::cnnlHistc searches for max and min of \input.
 * @param[in] ext_range
 *   Input. A bool to switch on extended range function. When switched on, \b input element
 *   less than \b range_min and greater than \b range_max will be put in first bin and last
 *   bin respectively. The value of this parameter is FALSE by default.
 * @param[in] is_index
 *   Input. A bool to switch on bin index function. When switched on, \b output will contain
 *   the index of bin for each \b input element to be put in, and the element number of \b output
 *   will be same as element number of \b input. The value of this parameter is FALSE by default.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @note
 * - Currently, \b ext_range and \b is_index are not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlSetHistogramDescriptorHistoCountMode(
                            cnnlHistogramDescriptor_t histogram_desc,
                            int bins,
                            float range_min,
                            float range_max,
                            bool ext_range,
                            bool is_index);

// Group:Histogram
/*!
 * @brief Destroys a histogram descriptor \b histogram_desc that is previously created with the
 *        ::cnnlCreateHistogramDescriptor function.
 *
 * The histogram descriptor is defined in ::cnnlHistogramDescriptor_t
 * and holds the information about the histogram series operations.
 *
 *
 * @param[in] histogram_desc
 *   Input. The histogram descriptor to be destroyed.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_EXECUTION_FAILED
 *
 * @note
 * - You need to call this function after calling the ::cnnlHistc function.
 *   Otherwise, \p CNNL_STATUS_BAD_PARAM is returned.
 * - This function should be called to destroy the histogram descriptor.
 *   Otherwise, the memory leak may be occurred.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlDestroyHistogramDescriptor(cnnlHistogramDescriptor_t histogram_desc);

// Group:Histogram
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace
 * to reorder \b input tensor with stride feature before entering histogram series operations.
 *
 * The size of extra workspace is based on the given information of the histogram series
 * operations, including the input tensor descriptor \b input_desc,
 * and weight tensor descriptor \b weight_desc.
 * For more information about the workspace, see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the histogram series operations. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] weight_desc
 *   Input. The descriptor of the weight tensor used as a filter in the
 *   Bincount and Histogram operation. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the histogram series operations.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called after the ::cnnlCreateTensorDescriptor and
 *   ::cnnlSetTensorDescriptor functions to create and set the tensor descriptors
 *   \b input_desc and \b weight_desc before calling this function.
 * - The allocated extra workspace should be passed to the ::cnnlHistc function to
 *   store the reordered input with stride feature before entering computing.
 *
 * @note
 * - Currently, \b weight_desc is not supported because Bincount and Histogram are
 *   not supported.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetHistogramWorkspaceSize(cnnlHandle_t handle,
                                                        const cnnlTensorDescriptor_t input_desc,
                                                        const cnnlTensorDescriptor_t weight_desc,
                                                        size_t *workspace_size);

// Group:Histogram
/*!
 * @brief Computes a histogram of input tensor \b input with configuration information
 * stored in \b histogram_desc, and returns the results in the output tensor \b output.
 *
 * When input tensor \b input has stride feature, this function needs extra MLU memory
 * as the workspace to store reordered input tensor before entering computing.
 * You can get the size of the workspace \b workspace_size with the
 * ::cnnlGetHistogramWorkspaceSize function.
 *
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the histogram operation. For detailed information,
 *   see ::cnnlHandle_t.
 * @param[in] histogram_desc
 *   Input. The descriptor of the histogram operation. For detailed information,
 *   see ::cnnlHistogramDescriptor_t.
 * @param[in] input_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] input
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store
 *   reordered input tensor. For more information about workspace,
 *   see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in
 *   the histogram operation. You can get the size of the workspace with
 *   the ::cnnlGetHistogramWorkspaceSize function.
 * @param[out] output_desc
 *   Output. The descriptor of the output tensor. The output
 *   tensor is always \b bins long, one-dimension tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] output
 *   Output. Pointer to the MLU memory that stores the output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_NOT_SUPPORTED
 *
 * @par Formula
 * - Statistic operator has no formula.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b input
 *   and output tensor \b output.
 * - The data type of \b input and \b output should be the same.
 *   - input tensor: uint8, int8, int16, int32, float.
 *   - output tensor: uint8, int8, int16, int32, float.
 *
 * @par Scale Limitation
 * - The input tensor, output tensor, and the histogram descriptor
 *   (including bins, range_min, range_max) must meet the following
 *   requirements:
 *   - \p bins is greater than 0.
 *   - \p bins equals \b output element number.
 *   - \p range_max is no less than range_min.
 *   - \p range_max and \p range_min can not be NAN/INF.
 *   - \p when \b range_max equals \p range_min, input can not contain NAN/INF.
 *   - \p when \b range_max equals \p range_min, input can not all be the minimum
 *        or maximum of its data type.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare
 *   all the parameters passed to this function. See each parameter description
 *   for details.
 *
 * @note
 * - When \b input element number is large enough, \b output may overflow the limit of data type.
 * - When \b bins is too large, precision error may occur.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * * - The example of the histc operation is as follows:
     @verbatim
      input array by 1 * 10  --> input: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

      param:
        bins: 3, range_min: 0, range_max: 0,

      output array by 1 * 3 --> output: [3, 3, 4]
     @endverbatim
 *
 * @par Reference
 * - https://www.pytorch.org/docs/1.9.0/generated/torch.histc
 * - https://www.pytorch.org/docs/1.11/generated/torch.bincount
 * - https://www.pytorch.org/docs/1.11/generated/torch.histogram
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width
 * - http://tensorflow.org/api_docs/python/tf/histogram_fixed_width_bins
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlHistc(cnnlHandle_t handle,
                               const cnnlHistogramDescriptor_t histogram_desc,
                               const cnnlTensorDescriptor_t input_desc,
                               const void *input,
                               void *workspace,
                               size_t workspace_size,
                               const cnnlTensorDescriptor_t output_desc,
                               void *output);

// Group:Im2Col
/*!
 * @brief Extracts sliding local blocks from a batched input tensor.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and
 *   queues in the Im2Col operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor. For detailed information,
 *   see ::cnnlTensorDescriptor_t.
 * @param[in] x
 *   Input. Pointer to the MLU memory that stores the input tensor.
 * @param[in] w_desc
 *   Input. The descriptor of the filter tensor in the Im2Col operation.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the Im2Col operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] input_image_size_desc
 *   Input. The descriptor of the \b input_image_size tensor.
 *   This parameter is only used in PaddlePaddle, and set it NULL in Pytorch framework.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] input_image_size
 *   Input. Pointer to the MLU memory that stores the \b input_image_size tensor, which contains image real size.
 *   This parameter is only used in PaddlePaddle, and set it NULL in Pytorch framework.
 * @param[in] out_stride
 *   Input. An array that stores the output stride. The default value of this parameter should be 1.
 *   It is enabled when \b input_image_size is not NULL.
 *   This parameter is only used in PaddlePaddle, and set it NULL in Pytorch framework.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace to store transposed input tensor.
 *   Reserved for future use, set the \b workspace to NULL now.
 *   For more information about \b workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that needs to be used in the Im2Col operation.
 *   Reserved for future use, set the \b workspace_size to 0 now.
 * @param[in] col_desc
 *   Iuput. The descriptor of the output tensor.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] col
 *   Output. Pointer to the MLU memory that stores the output tensor.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM, ::CNNL_STATUS_ARCH_MISMATCH
 *
 * @par Formula
 * - See "Im2Col Operator" section in "Cambricon CNNL User Guide" for details.
 *
 * @par Data Type
 * - This function supports the following data types for input tensor \b x and output tensor
 *   \b col. Data type of both tensors should be the same.
 *   - input tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     double, complex_half, complex_float.
 *   - output tensor: uint8, int8, uint16, int16, uint32, int32, uint64, int64, bool, half, float,
 *     double, complex_half, complex_float.
 *
 * @par Data Layout
 * - The supported data layout of the input tensor \b x, filter tensor, and
 *   output tensor \b col are as follows:
 *   - input tensor: \p CNNL_LAYOUT_NCHW.
 *   - filter tensor: \p CNNL_LAYOUT_NHWC, \p CNNL_LAYOUT_NCHW and \p CNNL_LAYOUT_HWCN.
 *   - output tensor: \p CNNL_LAYOUT_ARRAY.
 *
 * @par Scale Limitation
 * - None.
 *
 * @par API Dependency
 * - Before calling this function, you need to prepare all the parameters passed to this function.
 *   See each parameter description for details.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - The layout of input tensor \b x only supports CNNL_LAYOUT_NCHW now.
 * - Since the \b col element number is much larger than that of \b x, it is necessary
 *   to ensure that the element number of \b col is less than 2G.
 * - Users do not need to set data type in \b w_desc, since the data type is not used in this function.
 * - When the \b input_image_size_desc is NULL:
 *   - \b input_image_size and \b out_stride should be NULL.
 *   - The shape of \b col should be [N, L, C_col].
 * - When the \b input_image_size_desc is not NULL:
 *   - \b input_image_size and \b out_stride should be not NULL.
 *   - The shape of \b col should be [N * L, C_col]. N is batches, L is the number of the
 *     sliding local blocks, and C_col is the size of each sliding local blocks.
 *   - The shape of \b input_image_size should be [N, 2], and the data type of \b input_image_size should be int32.
 *   - The scale size with out_stride should be less than or equal to that of input tensor \b x.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - https://pytorch.org/docs/1.10/generated/torch.nn.Unfold.html?highlight=im2col.
 * - https://www.paddlepaddle.org.cn/documentation/docs/zh/1.8/api_cn/layers_cn/im2sequence_cn.html.
 *
 */
cnnlStatus_t CNNL_WIN_API cnnlIm2Col(cnnlHandle_t handle,
                                     const cnnlTensorDescriptor_t x_desc,
                                     const void *x,
                                     const cnnlTensorDescriptor_t w_desc,
                                     const cnnlConvolutionDescriptor_t conv_desc,
                                     const cnnlTensorDescriptor_t input_image_size_desc,
                                     const void *input_image_size,
                                     const int out_stride[],
                                     void *workspace,
                                     size_t workspace_size,
                                     const cnnlTensorDescriptor_t col_desc,
                                     void *col);

// Group:Im2Col
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra
 * workspace to optimize the Im2Col operation.
 *
 * @param[in] handle
 *   Input. Handle to a Cambricon CNNL context that is used to manage MLU devices and queues in the Im2Col operation.
 *   For detailed information, see ::cnnlHandle_t.
 * @param[in] x_desc
 *   Input. The descriptor of the input tensor \b x. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] w_desc
 *   Input. The descriptor of the input tensor \b w. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[in] conv_desc
 *   Input. The descriptor of the Im2Col operation. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlConvolutionDescriptor_t.
 * @param[in] col_desc
 *   Input. The descriptor of the output tensor \b col. Reserved for future use, set it NULL now.
 *   For detailed information, see ::cnnlTensorDescriptor_t.
 * @param[out] workspace_size
 *   Output. Pointer to the returned size of the extra workspace in bytes that is used in the Im2Col operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Scale Limitation
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - None.
 */
cnnlStatus_t CNNL_WIN_API cnnlGetIm2ColWorkspaceSize(cnnlHandle_t handle,
                                                     const cnnlTensorDescriptor_t x_desc,
                                                     const cnnlTensorDescriptor_t w_desc,
                                                     const cnnlConvolutionDescriptor_t conv_desc,
                                                     const cnnlTensorDescriptor_t col_desc,
                                                     size_t *workspace_size);

// Group:Col2Im
/*!
 * @brief Returns in \b workspace_size the size of the MLU memory that is used as an extra workspace to
 * optimize the Col2Im operation.
 *
 * The size of the extra workspace is based on the given information of the Col2Im operation,
 * including the input tensor descriptor \b col_input_desc and \b im_output_desc and \b filter_desc. For more information about the workspace,
 * see "Cambricon CNNL User Guide".
 *
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   Col2Im operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] col_input_desc
 *   Input. Descriptor of input data \b col_input, including dimension, data type and data layout.
 * @param[in] im_output_desc
 *   Input. Descriptor of input data \b im_output, including dimension, data type and data layout.
 * @param[in] filter_desc
 *   Input. Descriptor of filter, including dimension and data layout.
 * @param[out] workspace_size
 *   Output. A host pointer to the returned size of the extra workspace in bytes that is used in
 *   the Col2Im operation.
 *
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par API Dependency
 * - This function must be called before the cnnlCol2Im function.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 */

cnnlStatus_t CNNL_WIN_API
cnnlGetCol2ImWorkspaceSize(cnnlHandle_t handle,
                           const cnnlTensorDescriptor_t col_input_desc,
                           const cnnlTensorDescriptor_t filter_desc,
                           const cnnlTensorDescriptor_t im_output_desc,
                           size_t *workspace_size);

// Group:Col2Im
/*!
 * @brief Combines an array of sliding local blocks into a large containing tensor.
 * @param[in] handle
 *   Input. Handle to a CNNL context that is used to manage MLU devices and queues in the
 *   Col2Im operation. For detailed information, see ::cnnlHandle_t.
 * @param[in] col_input_desc
 *   Input. Descriptor of input data \b col_input, including dimension, data type and data layout.
 * @param[in] col_input
 *   Input. Pointer to the MLU memory that stores the col_input tensor.
 * @param[in] filter_desc
 *   Input. Descriptor of filter, including dimension and data layout.
 * @param[in] conv_desc
 *   Input. The descriptor of the convolution operation. For detailed information,
 *   see ::cnnlConvolutionDescriptor_t.
 * @param[in] workspace
 *   Input. Pointer to the MLU memory that is used as an extra workspace for the Col2Im
 *   operation. For more information about workspace, see "Cambricon CNNL User Guide".
 * @param[in] workspace_size
 *   Input. The size of the extra workspace in bytes that is used in
 *   the Col2Im operation.
 * @param[in] im_output_desc
 *   Input. Descriptor of input data \b im_output, including dimension, data type and data layout.
 * @param[out] im_output
 *   Input. Pointer to the MLU memory that stores the \b im_output tensor.
 * @par Return
 * - ::CNNL_STATUS_SUCCESS, ::CNNL_STATUS_BAD_PARAM
 *
 * @par Formula
 * - None.
 *
 * @par Data Type
 * - This function supports the following data types for col_input tensor \b col_input, and im_output tensor
 *   \b im_output .
 *   Data types of col_input tensor and im_output tensor should be the same.
 *   - \b col_input tensor: half, float.
 *   - \b im_output tensor: half, float.
 *
 * @par Data Layout
 * - The supported data layouts of \b col_input and  \b im_output are as follows:
 *   - \b col_input tensor: \p CNNL_LAYOUT_ARRAY.
 *   - \b im_output tensor: \p CNNL_LAYOUT_NCHW or \p CNNL_LAYOUT_NHWC.
 *
 * @par Scale Limitation
 * - The shape of \b col_input should be [N, L, C_col]. L is the local number of blocks.
 *   (This is exactly the same specification as the output shape of Im2Col)
 * - L = Oh * Ow.
 *   Oh is the filter sliding times in height_im.
 *   Oh is the filter sliding times in width_im.
 * - C_col = C_im * kernel_h * kernel_w.
 *   C_col is the \b col_input channels and C_im is the \b im_output channels.
 * - The shape of \b im_output should be [N, C_im, H, W] or [N, H, W, C_im].
 * - The dimension of \b col_input and \b im_output dims[0](N) should be the same.
 * - conv_desc included parammeters conv_desc.pad should meet the following requirements::
 *   - pad[0] = pad[1], pad[2] = pad[3].
 *
 * @par API Dependency
 * - Before calling this function, you need to call cnnlGetCol2ImWorkspaceSize
 *   to get the extra space size needed in cnnlCol2ImForward operation.
 *
 * @par Performance Optimization
 * - None.
 *
 * @note
 * - None.
 *
 * @par Requirements
 * - None.
 *
 * @par Example
 * - None.
 *
 * @par Reference
 * - github.com/pytorch/pytorch/blob/release/1.6/aten/src/ATen/native/cuda/im2col.cu
 */

cnnlStatus_t CNNL_WIN_API
cnnlCol2Im(cnnlHandle_t handle,
           const cnnlTensorDescriptor_t col_input_desc,
           const void *col_input,
           const cnnlTensorDescriptor_t filter_desc,
           const cnnlConvolutionDescriptor_t conv_desc,
           void *workspace,
           const size_t workspace_size,
           const cnnlTensorDescriptor_t im_output_desc,
           void *im_output);

#if defined(__cplusplus)
}
#endif

#endif  // CNNL_H_
